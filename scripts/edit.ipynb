{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from containers import Paper, Institution, Expertise, Author, JournalConference\n",
    "\n",
    "institution_dict = {}\n",
    "expertise_dict = {}\n",
    "whole_author_list = []\n",
    "whole_paper_dict = {}\n",
    "jc_dict = {}\n",
    "\n",
    "WHOLE_PAPER_FILE_PATH = \"./data/processed_paper_dict.json\"\n",
    "if os.path.exists(WHOLE_PAPER_FILE_PATH) :\n",
    "    with open(WHOLE_PAPER_FILE_PATH, \"r\") as f :\n",
    "        whole_paper_dict = json.load(f)\n",
    "    for k, v in whole_paper_dict.items() :\n",
    "        whole_paper_dict[k] = Paper(**v)\n",
    "\n",
    "INSTITUTION_FILE_PATH = \"./data/institution_dict.json\"\n",
    "if os.path.exists(INSTITUTION_FILE_PATH) :\n",
    "    with open(INSTITUTION_FILE_PATH, \"r\") as f :\n",
    "        institution_dict_raw = json.load(f)\n",
    "    for k, v in institution_dict_raw.items() :\n",
    "        institution_dict[k] = Institution(**v)\n",
    "\n",
    "EXPERTISE_FILE_PATH = \"./data/expertise_dict.json\"\n",
    "if os.path.exists(EXPERTISE_FILE_PATH) :\n",
    "    with open(EXPERTISE_FILE_PATH, \"r\") as f :\n",
    "        expertise_dict_raw = json.load(f)\n",
    "    for k, v in expertise_dict_raw.items() :\n",
    "        expertise_dict[k] = Expertise(**v)\n",
    "\n",
    "AUTHOR_FILE_PATH = \"./data/author_list.json\"\n",
    "if os.path.exists(AUTHOR_FILE_PATH) :\n",
    "    with open(AUTHOR_FILE_PATH, \"r\") as f :\n",
    "        author_list_raw = json.load(f)\n",
    "    for author in author_list_raw :\n",
    "        whole_author_list.append(Author(**author))\n",
    "\n",
    "JC_FILE_PATH = \"./data/journal_conference_dict.json\"\n",
    "if os.path.exists(JC_FILE_PATH) :\n",
    "    with open(JC_FILE_PATH, \"r\") as f :\n",
    "        jc_dict_raw = json.load(f)\n",
    "    for k, v in jc_dict_raw.items() :\n",
    "        jc_dict[k] = JournalConference(**v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# set conference_acronym\n",
    "#\n",
    "\n",
    "for k, v in whole_paper_dict.items() :\n",
    "    #if v.conference_acronym is not None :\n",
    "    ISSN = v.crossref_json[\"ISSN\"][0]\n",
    "    if ISSN in jc_dict :\n",
    "        whole_paper_dict[k].conference_acronym = jc_dict[ISSN].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# set authors\n",
    "#\n",
    "count = 0\n",
    "for k, v in whole_paper_dict.items() :\n",
    "\n",
    "    if \"저자\" in v.google_schorlar_metadata :\n",
    "        author_str = v.google_schorlar_metadata[\"저자\"]\n",
    "    elif \"발명자\" in v.google_schorlar_metadata :\n",
    "        author_str = v.google_schorlar_metadata[\"발명자\"]\n",
    "    \n",
    "    whole_paper_dict[k].authors = author_str.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1339)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# set keywords\n",
    "#\n",
    "count = 0\n",
    "for author in whole_author_list :\n",
    "    if author.paper_title_list :\n",
    "        for paper_title in author.paper_title_list :\n",
    "            \n",
    "            expertise = author.expertise_list\n",
    "\n",
    "            if paper_title in whole_paper_dict :\n",
    "                whole_paper_dict[paper_title].keywords = expertise\n",
    "\n",
    "count, len(whole_author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# set abstractsion\n",
    "#\n",
    "count = 0\n",
    "for k, v in whole_paper_dict.items() :\n",
    "    if \"설명\" not in v.google_schorlar_metadata :\n",
    "        continue\n",
    "    abstract = v.google_schorlar_metadata[\"설명\"]\n",
    "    whole_paper_dict[k].abstract = abstract\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paper_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'10.1007/978-3-642-33709-3_2'\n"
     ]
    }
   ],
   "source": [
    "IDX = 11\n",
    "KEY = list(whole_paper_dict.keys())[IDX]\n",
    "\n",
    "paper = whole_paper_dict[KEY]\n",
    "pprint(\n",
    "    paper.DOI\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# delete unneccessary keys\n",
    "#\n",
    "\n",
    "for k, v in whole_paper_dict.items() :\n",
    "    del v.crossref_json\n",
    "    del v.google_schorlar_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper(DOI='10.1007/978-3-642-33709-3_2',\n",
      "      crossref_json=None,\n",
      "      google_schorlar_metadata=None,\n",
      "      title='Statistics of Patch Offsets for Image Completion',\n",
      "      authors=['Kaiming He', 'Jian Sun'],\n",
      "      abstract=' Image completion involves filling missing parts in images. In '\n",
      "               'this paper we address this problem through the statistics of '\n",
      "               'patch offsets. We observe that if we match similar patches in '\n",
      "               'the image and obtain their offsets (relative positions), the '\n",
      "               'statistics of these offsets are sparsely distributed. We '\n",
      "               'further observe that a few dominant offsets provide reliable '\n",
      "               'information for completing the image. With these offsets we '\n",
      "               'fill the missing region by combining a stack of shifted images '\n",
      "               'via optimization. A variety of experiments show that our '\n",
      "               'method yields generally better results and is faster than '\n",
      "               'existing state-of-the-art methods.',\n",
      "      conference=None,\n",
      "      journal=None,\n",
      "      year=None,\n",
      "      reference_list=['10.1145/344779.344972',\n",
      "                      '10.1109/83.935036',\n",
      "                      '10.1109/ICCV.2003.1238360',\n",
      "                      '10.1109/TIP.2003.815261',\n",
      "                      '10.1109/ICCV.1999.790383',\n",
      "                      '10.1109/TIP.2004.833105',\n",
      "                      '10.1145/882262.882267',\n",
      "                      '10.1145/1073204.1073274',\n",
      "                      '10.1109/CVPR.2008.4587842',\n",
      "                      '10.1145/1531326.1531330',\n",
      "                      '10.1109/TIP.2007.906269',\n",
      "                      '10.1109/ICCV.2009.5459159',\n",
      "                      '10.1145/1186562.1015718',\n",
      "                      '10.1145/1201775.882264',\n",
      "                      '10.1109/34.969114',\n",
      "                      '10.1109/CVPR.2007.383047',\n",
      "                      '10.1109/CVPR.2011.5995401',\n",
      "                      '10.1145/2185520.2335433',\n",
      "                      '10.1145/1201775.882269',\n",
      "                      '10.1145/2070781.2024209',\n",
      "                      '10.5244/C.25.121',\n",
      "                      '10.1007/s11263-009-0272-7',\n",
      "                      '10.1145/1141911.1141956',\n",
      "                      '10.1109/ICCV.2009.5459271'],\n",
      "      referenced_list=None,\n",
      "      cite_bibtex=None,\n",
      "      issn_type=None,\n",
      "      url=None,\n",
      "      is_in_favorite=False,\n",
      "      keywords=['Computer Vision', 'Machine Learning'],\n",
      "      conference_acronym='Lecture notes in computer science',\n",
      "      publisher=None,\n",
      "      query_handler=None)\n"
     ]
    }
   ],
   "source": [
    "IDX = 11\n",
    "KEY = list(whole_paper_dict.keys())[IDX]\n",
    "\n",
    "paper = whole_paper_dict[KEY]\n",
    "pprint(\n",
    "    paper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "SAVE_FILE_NAME = \"./data/final_paper_dict.json\"\n",
    "\n",
    "whole_paper_dict_dict = {}\n",
    "for k, v in whole_paper_dict.items() :\n",
    "    whole_paper_dict_dict[k] = v.__dict__\n",
    "\n",
    "with open(SAVE_FILE_NAME, \"w\") as f :\n",
    "    json.dump(whole_paper_dict_dict, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks': Paper(DOI='10.1109/tpami.2016.2577031', crossref_json=None, google_schorlar_metadata=None, title='Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks', authors=['Shaoqing Ren', 'Kaiming He', 'Ross Girshick', 'Jian Sun'], abstract='State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github. com/ShaoqingRen/faster_rcnn.', conference=None, journal=None, year=None, reference_list=['10.5244/C.29.5', '10.1145/2647868.2654889', '10.1145/3065386', '10.1007/s11263-015-0816-y', '10.1162/neco.1989.1.4.541', '10.1109/CVPR.2015.7298594', '10.1109/TPAMI.2016.2577031', '10.1007/s11263-009-0275-4', '10.1007/s11263-013-0620-5', '10.1109/CVPR.2015.7299025', '10.1109/CVPR.2014.81', '10.1109/TPAMI.2009.167', '10.1109/CVPR.2015.7298965', '10.1109/ICCV.2015.169', '10.1109/TPAMI.2015.2465908', '10.1109/TPAMI.2011.231', '10.1109/TPAMI.2012.28', '10.1109/CVPR.2014.49', '10.1109/CVPR.2014.276'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Mask R-CNN': Paper(DOI='10.31202/ecjse.1061270', crossref_json=None, google_schorlar_metadata=None, title='Mask R-CNN', authors=['Kaiming He', 'Georgia Gkioxari', 'Piotr Dollár', 'Ross Girshick'], abstract='We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, eg, allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.', conference=None, journal=None, year=None, reference_list=['10.1109/UBMK.2018.8566635', '10.31202/ecjse.924446', '10.29130/dubited.648387', '10.1109/TPAMI.2015.2437384', '10.1109/TPAMI.2016.2577031', '10.1109/ICCV.2017.322', '10.1109/I2CT51068.2021.9417826', '10.1109/ICCCNT49239.2020.9225384', '10.1109/ICCRD51685.2021.9386366', '10.1109/ICAE50557.2020.9350556', '10.1109/CSICC52343.2021.9420599', '10.1109/PuneCon50868.2020.9362389', '10.1109/SIU.2019.8806447', '10.2339/politeknik.515830', '10.1007/978-3-319-10602-1_48', '10.1109/CVPR.2016.90', '10.1155/2020/9242917', '10.1007/s00464-021-08698-2'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Feature Pyramid Networks for Object Detection': Paper(DOI='10.1109/access.2021.3100369', crossref_json=None, google_schorlar_metadata=None, title='Feature Pyramid Networks for Object Detection', authors=['Tsung-Yi Lin', 'Piotr Dollár', 'Ross Girshick', 'Kaiming He', 'Bharath Hariharan', 'Serge Belongie'], abstract='Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.', conference=None, journal=None, year=None, reference_list=['10.1007/s11263-015-0816-y', '10.1109/ACCESS.2020.2980737', '10.1109/TPAMI.2014.2300479', '10.1109/ACCESS.2020.2993572', '10.1007/978-3-319-10602-1_48', '10.1109/ICCV.2017.324', '10.1109/ICCV.2017.322', '10.1109/CVPR.2019.00091', '10.1109/ACCESS.2019.2941892', '10.1109/CVPR.2014.81', '10.1109/CVPR.2017.106', '10.1109/CVPR.2016.91', '10.1109/CVPR.2016.90', '10.1109/CVPR.2017.690', '10.1109/TPAMI.2016.2577031', '10.1109/ACCESS.2019.2930083', '10.1109/ICCV.2015.169', '10.1007/s11263-019-01204-1', '10.1109/CVPR.2019.00720', '10.1109/ICCV.2019.00667', '10.1109/CVPR.2018.00913'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition': Paper(DOI='10.1109/tpami.2015.2389824', crossref_json=None, google_schorlar_metadata=None, title='Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition', authors=['Kaiming He', 'Xiangyu Zhang', 'Shaoqing Ren', 'Jian Sun'], abstract='Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224   224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2015.2389830', '10.1145/1961189.1961199', '10.1109/TPAMI.2011.235', '10.1109/CVPR.2014.222', '10.1109/CVPR.2014.220', '10.1109/CVPR.2014.212', '10.1109/ICCV.2005.239', '10.1109/CVPR.2006.68', '10.1109/ICCV.2003.1238663', '10.1109/CVPR.2010.5540018', '10.5244/C.25.76', '10.1023/B:VISI.0000029664.99615.94', '10.5244/C.28.72', '10.1109/CVPR.2014.81', '10.1109/CVPRW.2014.131', '10.1162/neco.1989.1.4.541', '10.1109/ICCV.2011.6126456', '10.1007/s11263-009-0275-4', '10.1016/j.cviu.2005.09.012', '10.1109/CVPR.2005.177', '10.1109/TPAMI.2009.167'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Non-local Neural Networks': Paper(DOI='10.1016/j.neunet.2021.05.001', crossref_json=None, google_schorlar_metadata=None, title='Non-local Neural Networks', authors=['Xiaolong Wang', 'Ross Girshick', 'Abhinav Gupta', 'Kaiming He'], abstract='Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.', conference=None, journal=None, year=None, reference_list=['10.1073/pnas.1109168108', '10.1016/j.neunet.2012.09.017', '10.1038/35016072', '10.1109/ICCV.2015.123', '10.1109/CVPR.2016.90', '10.1162/neco.1997.9.8.1735', '10.1098/rsfs.2018.0011', '10.1007/BF00133569'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'robotics'], conference_acronym='Neural networks (Print)', publisher=None, query_handler=None),\n",
       " 'Momentum Contrast for Unsupervised Visual Representation Learning': Paper(DOI='10.1007/s11042-023-15998-3', crossref_json=None, google_schorlar_metadata=None, title='Momentum Contrast for Unsupervised Visual Representation Learning', authors=['Kaiming He', 'Haoqi Fan', 'Yuxin Wu', 'Saining Xie', 'Ross Girshick'], abstract='We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.', conference=None, journal=None, year=None, reference_list=['10.1142/9789812797926_0003', '10.1007/978-3-030-01264-9_9', '10.1109/CVPR46437.2021.01549', '10.1109/ICCV.2015.167', '10.1109/CVPR42600.2020.00975', '10.3390/technologies9010002', '10.1007/978-3-319-46466-4_5', '10.1109/CVPR.2016.278', '10.1007/s11263-015-0816-y', '10.1109/CVPR46437.2021.00304', '10.1109/CVPR.2018.00393', '10.1007/978-3-319-46487-9_40'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Multimedia tools and applications', publisher=None, query_handler=None),\n",
       " 'Single Image Haze Removal using Dark Channel Prior': Paper(DOI='10.18535/ijecs/v5i1.12', crossref_json=None, google_schorlar_metadata=None, title='Single Image Haze Removal using Dark Channel Prior', authors=['Kaiming He', 'Jian Sun', 'Xiaoou Tang'], abstract='In this paper, we propose a simple but effective image prior-dark channel prior to remove haze from a single input image. The dark channel prior is a kind of statistics of outdoor haze-free images. It is based on a key observation-most local patches in outdoor haze-free images contain some pixels whose intensity is very low in at least one color channel. Using this prior with the haze imaging model, we can directly estimate the thickness of the haze and recover a high-quality haze-free image. Results on a variety of hazy images demonstrate the power of the proposed prior. Moreover, a high-quality depth map can also be obtained as a byproduct of haze removal.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Exploring the Limits of Weakly Supervised Pretraining': Paper(DOI='10.1007/978-3-030-01216-8_12', crossref_json=None, google_schorlar_metadata=None, title='Exploring the Limits of Weakly Supervised Pretraining', authors=['Dhruv Mahajan', 'Ross Girshick', 'Vignesh Ramanathan', 'Kaiming He', 'Manohar Paluri', 'Yixuan Li', 'Ashwin Bharambe', 'Laurens van der Maaten'], abstract='State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards\" small\". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4%(97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2014.81', '10.1007/978-3-319-10590-1_53', '10.1109/ICCV.2017.322', '10.1109/CVPR.2015.7298965', '10.1109/CVPR.2017.660', '10.1109/CVPR.2017.143', '10.1109/CVPR.2017.395', '10.1109/CVPR.2017.502', '10.1109/ICCV.2015.304', '10.1007/s11263-015-0816-y', '10.1007/978-3-319-10584-0_22', '10.1109/TPAMI.2017.2723009', '10.1109/CVPR.2017.634', '10.1007/978-3-319-46478-7_5', '10.1109/ICCV.2017.97', '10.1007/978-3-319-10602-1_48', '10.1007/978-3-319-46466-4_15', '10.1109/CVPR.2016.90', '10.1109/CVPR.2017.243', '10.1609/aaai.v31i1.11231', '10.1109/ICCV.2015.123', '10.1109/CVPR.2017.638', '10.1109/CVPR.2009.5206848', '10.1109/CVPR.2018.00907', '10.1145/2814815.2814821', '10.1109/CVPR.2016.320', '10.1109/CVPR.2018.00620', '10.1109/CVPR.2017.106', '10.1109/ICCV.2017.449', '10.1145/2812802', '10.1109/CVPR.2017.540', '10.1145/2783258.2788576', '10.1109/CVPR.2015.7298682', '10.1109/CVPR.2015.7298891', '10.1007/978-3-642-33709-3_48'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Accelerating Very Deep Convolutional Networks for Classification and Detection': Paper(DOI='10.1109/tpami.2015.2502579', crossref_json=None, google_schorlar_metadata=None, title='Accelerating Very Deep Convolutional Networks for Classification and Detection', authors=['Xiangyu Zhang', 'Jianhua Zou', 'Kaiming He', 'Jian Sun'], abstract='This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs    that have substantially impacted the computer vision community. Unlike previous methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly, while previous methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (e.g.,   10) layers are approximated. For the widely used very deep VGG-16 model    , our method achieves a whole-model speedup of 4   with merely a 0.3 percent increase of top-5 error in ImageNet classification. Our 4    accelerated VGG\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2015.123', '10.1137/080724265', '10.1016/j.csda.2007.02.014', '10.1109/TPAMI.2013.240', '10.1109/CVPR.2011.5995432', '10.1093/acprof:oso/9780198510581.001.0001', '10.2333/bhmk.33.179', '10.1109/ICCV.2015.169', '10.1109/CVPR.2013.355', '10.1109/CVPR.2015.7298809', '10.1109/CVPR.2015.7299173', '10.1109/TPAMI.2016.2601099', '10.1109/TPAMI.2016.2577031', '10.1109/CVPR.2015.7298965', '10.1109/CVPR.2015.7299025', '10.1109/CVPR.2015.7298642', '10.5244/C.28.88', '10.1007/s11263-015-0816-y', '10.1162/neco.1989.1.4.541', '10.1109/CVPR.2014.81', '10.1145/2647868.2654889', '10.1109/CVPR.2015.7298754', '10.1109/ICCV.2015.191'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Detectron': Paper(DOI='10.1093/mnras/stad3302', crossref_json=None, google_schorlar_metadata=None, title='Detectron', authors=['Ross Girshick', 'Ilija Radosavovic', 'Georgia Gkioxari', 'Piotr Dollár', 'Kaiming He'], abstract=None, conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Monthly notices of the Royal Astronomical Society (Print)', publisher=None, query_handler=None),\n",
       " 'Object Detection Networks on Convolutional Feature Maps': Paper(DOI='10.1109/tpami.2016.2601099', crossref_json=None, google_schorlar_metadata=None, title='Object Detection Networks on Convolutional Feature Maps', authors=['Shaoqing Ren', 'Kaiming He', 'Ross Girshick', 'Xiangyu Zhang', 'Jian Sun'], abstract='Most object detectors contain two important components: a feature extractor and an object classifier. The feature extractor has rapidly evolved with significant research efforts leading to better deep convolutional architectures. The object classifier, however, has not received much attention and many recent systems (like SPPnet and Fast/Faster R-CNN) use simple multi-layer perceptrons. This paper demonstrates that carefully designing deep networks for object classification is just as important. We experiment with region-wise classifier networks that use shared, region-independent convolutional features. We call them “Networks on Convolutional feature maps” (NoCs). We discover that aside from deep feature maps, a  deep  and  convolutional  per-region classifier is of particular importance for object detection, whereas latest superior image classification models (such as ResNets and GoogLeNets) do not directly\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2015.7298965', '10.1109/ICCV.2015.169', '10.1109/TPAMI.2016.2577031', '10.1109/CVPR.2016.90', '10.1109/CVPR.2001.990517', '10.1016/0893-6080(89)90020-8', '10.1007/s11263-013-0620-5', '10.1109/ICCV.2013.10', '10.1109/CVPR.2014.81', '10.1145/3065386', '10.1109/TPAMI.2009.167', '10.1109/CVPR.2015.7298641', '10.1109/CVPR.2005.177', '10.1109/CVPR.2006.68', '10.1023/B:VISI.0000029664.99615.94', '10.1109/ICCV.2015.135', '10.1007/s11263-009-0275-4'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Statistics of Patch Offsets for Image Completion': Paper(DOI='10.1007/978-3-642-33709-3_2', crossref_json=None, google_schorlar_metadata=None, title='Statistics of Patch Offsets for Image Completion', authors=['Kaiming He', 'Jian Sun'], abstract=' Image completion involves filling missing parts in images. In this paper we address this problem through the statistics of patch offsets. We observe that if we match similar patches in the image and obtain their offsets (relative positions), the statistics of these offsets are sparsely distributed. We further observe that a few dominant offsets provide reliable information for completing the image. With these offsets we fill the missing region by combining a stack of shifted images via optimization. A variety of experiments show that our method yields generally better results and is faster than existing state-of-the-art methods.', conference=None, journal=None, year=None, reference_list=['10.1145/344779.344972', '10.1109/83.935036', '10.1109/ICCV.2003.1238360', '10.1109/TIP.2003.815261', '10.1109/ICCV.1999.790383', '10.1109/TIP.2004.833105', '10.1145/882262.882267', '10.1145/1073204.1073274', '10.1109/CVPR.2008.4587842', '10.1145/1531326.1531330', '10.1109/TIP.2007.906269', '10.1109/ICCV.2009.5459159', '10.1145/1186562.1015718', '10.1145/1201775.882264', '10.1109/34.969114', '10.1109/CVPR.2007.383047', '10.1109/CVPR.2011.5995401', '10.1145/2185520.2335433', '10.1145/1201775.882269', '10.1145/2070781.2024209', '10.5244/C.25.121', '10.1007/s11263-009-0272-7', '10.1145/1141911.1141956', '10.1109/ICCV.2009.5459271'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Exploring Plain Vision Transformer Backbones for Object Detection': Paper(DOI='10.1007/978-3-031-20077-9_17', crossref_json=None, google_schorlar_metadata=None, title='Exploring Plain Vision Transformer Backbones for Object Detection', authors=['Yanghao Li', 'Hanzi Mao', 'Ross Girshick', 'Kaiming He'], abstract='We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP on the COCO dataset using only ImageNet-1K pre-training. We hope our\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2017.593', '10.1109/TPAMI.2019.2956516', '10.1007/978-3-030-58452-8_13', '10.1109/CVPR.2019.00511', '10.1109/CVPR46437.2021.01284', '10.1007/978-3-031-20080-9_41', '10.1109/CVPR46437.2021.00729', '10.1109/CVPR.2009.5206848', '10.1109/ICCV.2019.00667', '10.1109/ICCV48922.2021.00675', '10.1109/CVPR46437.2021.00294', '10.1109/ICCV.2015.169', '10.1109/CVPR.2014.81', '10.1109/CVPR.2019.00550', '10.1109/CVPR52688.2022.01553', '10.1109/ICCV.2017.322', '10.1007/978-3-319-10578-9_23', '10.1109/CVPR.2016.90', '10.1109/ICCV48922.2021.01172', '10.1007/978-3-030-01264-9_45', '10.1162/neco.1989.1.4.541', '10.1109/ICCV.2019.00615', '10.1109/CVPR52688.2022.00476', '10.1109/CVPR.2017.106', '10.1109/ICCV.2017.324', '10.1007/978-3-319-10602-1_48', '10.1007/978-3-319-46448-0_2', '10.1109/CVPR52688.2022.01170', '10.1109/ICCV48922.2021.00986', '10.1109/CVPR.2016.91', '10.1109/CVPR.2015.7298594', '10.1109/ICCV.2019.00972', '10.1109/TPAMI.2022.3206148', '10.1109/ICCV48922.2021.00061', '10.1109/CVPR52688.2022.01179', '10.1007/978-3-031-20077-9_21'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Rectangling Panoramic Images via Warping': Paper(DOI='10.1145/2461912.2462004', crossref_json=None, google_schorlar_metadata=None, title='Rectangling Panoramic Images via Warping', authors=['Kaiming He', 'Huiwen Chang', 'Jian Sun'], abstract='Stitched panoramic images mostly have irregular boundaries. Artists and common users generally prefer rectangular boundaries, which can be obtained through cropping or image completion techniques. In this paper, we present a content-aware warping algorithm that generates rectangular images from stitched panoramic images. Our algorithm consists of two steps. The first local step is mesh-free and preliminarily warps the image into a rectangle. With a grid mesh placed on this rectangle, the second global step optimizes the mesh to preserve shapes and straight lines. In various experiments we demonstrate that the results of our approach are often visually plausible, and the introduced distortion is often unnoticeable.', conference=None, journal=None, year=None, reference_list=['10.1145/1186562.1015718', '10.1145/1275808.1276495', '10.1145/1275808.1276390', '10.1145/1576246.1531330', '10.1109/34.969114', '10.1145/1576246.1531349', '10.1145/1833349.1778864', '10.1109/TIP.2004.833105', '10.1145/1276377.1276382', '10.5555/2964398.2964401', '10.1145/1186822.1073323', '10.1145/1275808.1276466', '10.1145/1186822.1073229', '10.1109/TIP.2007.906269', '10.1111/j.1467-8659.2009.01485.x', '10.1145/2366145.2366150', '10.1145/1399504.1360677', '10.1145/1576246.1531350', '10.1109/34.879794', '10.1145/1201775.882269', '10.1007/978-3-642-33783-3_23', '10.1145/1399504.1360615', '10.1145/1179352.1141920', '10.1145/2185520.2185579', '10.1145/258734.258861', '10.1561/0600000009', '10.1109/TPAMI.2008.300', '10.1145/1457515.1409071', '10.1109/TPAMI.2007.60', '10.1109/ICCV.2005.231', '10.1111/j.1467-8659.2009.01568.x', '10.1145/218380.218449'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " 'Graph Cuts for Supervised Binary Coding': Paper(DOI='10.1007/978-3-319-10584-0_17', crossref_json=None, google_schorlar_metadata=None, title='Graph Cuts for Supervised Binary Coding', authors=['Tiezheng Ge', 'Kaiming He', 'Jian Sun'], abstract=' Learning short binary codes is challenged by the inherent discrete nature of the problem. The graph cuts algorithm is a well-studied discrete label assignment solution in computer vision, but has not yet been applied to solve the binary coding problems. This is partially because it was unclear how to use it to learn the encoding (hashing) functions for out-of-sample generalization. In this paper, we formulate supervised binary coding as a single optimization problem that involves both the encoding functions and the binary label assignment. Then we apply the graph cuts algorithm to address the discrete optimization problem involved, with no continuous relaxation. This method, named as Graph Cuts Coding (GCC), shows competitive results in various datasets.', conference=None, journal=None, year=None, reference_list=['10.1145/1186562.1015718', '10.1109/FOCS.2006.49', '10.7551/mitpress/8579.001.0001', '10.1109/TPAMI.2004.60', '10.1109/ICCV.1999.791245', '10.1007/BF00994018', '10.1109/CVPR.2013.379', '10.1109/TPAMI.2013.240', '10.1109/CVPR.2011.5995432', '10.1007/978-0-387-84858-7', '10.1007/978-3-642-33709-3_2', '10.1109/CVPR.2013.378', '10.1145/276698.276876', '10.1109/TPAMI.2007.1031', '10.1109/ICCV.2009.5459466', '10.1145/882262.882264', '10.1109/CVPR.2013.388', '10.1109/ICCV.2009.5459159', '10.1007/978-3-642-33783-3_63', '10.1109/CVPR.2013.205', '10.1109/CVPR.2008.4587643', '10.1109/CVPR.2008.4587633', '10.1145/1873951.1874249', '10.1109/TPAMI.2011.153', '10.1109/CVPR.2010.5539994'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'GLoMo: unsupervised learning of transferable relational graphs': Paper(DOI='10.1016/j.ins.2022.07.083', crossref_json=None, google_schorlar_metadata=None, title='GLoMo: unsupervised learning of transferable relational graphs', authors=['Zhilin Yang', 'Jake Zhao', 'Bhuwan Dhingra', 'Kaiming He', 'William W Cohen', 'Ruslan R Salakhutdinov', 'Yann LeCun'], abstract='Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (eg, words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden units), or embedding-free units such as image pixels.', conference=None, journal=None, year=None, reference_list=['10.1609/aaai.v33i01.33013296', '10.1109/TPAMI.2010.92', '10.1145/3422622', '10.2991/ijcis.d.191101.001', '10.1016/j.ins.2020.11.048', '10.1016/j.eswa.2021.115798', '10.1109/TPAMI.2020.2991050', '10.1109/TPAMI.2020.2964173', '10.1109/TPAMI.2018.2868685', '10.1109/TPAMI.2018.2868685', '10.1109/TKDE.2013.97', '10.1016/j.ins.2022.01.044', '10.1109/TNN.2010.2091281', '10.1109/TNNLS.2020.2995648', '10.1007/978-3-319-49409-8_35', '10.1016/j.ins.2021.07.073'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Information sciences (Print)', publisher=None, query_handler=None),\n",
       " 'Faster R-CNN: Towards real-time object detection with region proposal networks': Paper(DOI='10.1109/tpami.2016.2577031', crossref_json=None, google_schorlar_metadata=None, title='Faster R-CNN: Towards real-time object detection with region proposal networks', authors=['Shaoqing Ren', 'Kaiming He', 'Ross Girshick', 'Jian Sun'], abstract='State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github. com/ShaoqingRen/faster_rcnn.', conference=None, journal=None, year=None, reference_list=['10.5244/C.29.5', '10.1145/2647868.2654889', '10.1145/3065386', '10.1007/s11263-015-0816-y', '10.1162/neco.1989.1.4.541', '10.1109/CVPR.2015.7298594', '10.1109/TPAMI.2016.2577031', '10.1007/s11263-009-0275-4', '10.1007/s11263-013-0620-5', '10.1109/CVPR.2015.7299025', '10.1109/CVPR.2014.81', '10.1109/TPAMI.2009.167', '10.1109/CVPR.2015.7298965', '10.1109/ICCV.2015.169', '10.1109/TPAMI.2015.2465908', '10.1109/TPAMI.2011.231', '10.1109/TPAMI.2012.28', '10.1109/CVPR.2014.49', '10.1109/CVPR.2014.276'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Microsoft coco: Common objects in context': Paper(DOI='10.1007/978-3-319-10602-1_48', crossref_json=None, google_schorlar_metadata=None, title='Microsoft coco: Common objects in context', authors=['Tsung-Yi Lin', 'Michael Maire', 'Serge Belongie', 'James Hays', 'Pietro Perona', 'Deva Ramanan', 'Piotr Dollár', 'C Lawrence Zitnick'], abstract=' We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2009.5206848', '10.1007/s11263-009-0275-4', '10.1109/CVPR.2010.5539970', '10.1109/TPAMI.2011.155', '10.1109/CVPR.2014.81', '10.1109/CVPR.2009.5206772', '10.1109/CVPR.2012.6247998', '10.1109/ICCV.2009.5459303', '10.1007/978-3-642-33715-4_54', '10.1007/978-3-642-33712-3_25', '10.1016/j.patrec.2008.04.005', '10.1007/s11263-007-0090-8', '10.1145/2461912.2462002', '10.1145/2556288.2557011', '10.1007/978-3-319-10602-1_48', '10.1007/s11263-010-0390-2', '10.1109/TPAMI.2008.128', '10.1109/ICCV.2013.344', '10.7551/mitpress/7287.001.0001', '10.1006/cviu.2001.0921', '10.1109/ICCV.2013.258', '10.1007/s11263-007-0109-1', '10.1109/TPAMI.2010.161', '10.1109/CVPRW.2009.5206594', '10.1007/978-3-540-88682-2_4', '10.1109/CVPR.2009.5204174', '10.1109/CVPR.2011.5995347', '10.1145/1646396.1646421', '10.1109/TPAMI.2009.167', '10.5244/C.26.80', '10.1109/CVPR.2011.5995659', '10.1109/TPAMI.2011.208', '10.1109/CVPR.2007.383271'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Deep Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Rich feature hierarchies for accurate object detection and semantic segmentation': Paper(DOI='10.18127/j00338486-202109-11', crossref_json=None, google_schorlar_metadata=None, title='Rich feature hierarchies for accurate object detection and semantic segmentation', authors=['Ross Girshick', 'Jeff Donahue', 'Trevor Darrell', 'Jitendra Malik'], abstract='Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights:(1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www. cs. berkeley. edu/~ rbg/rcnn.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Machine Learning', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Learning rich features from RGB-D images for object detection and segmentation': Paper(DOI='10.1007/978-3-319-10584-0_23', crossref_json=None, google_schorlar_metadata=None, title='Learning rich features from RGB-D images for object detection and segmentation', authors=['Saurabh Gupta', 'Ross Girshick', 'Pablo Arbeláez', 'Jitendra Malik'], abstract=' In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3%, which is a 56% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2014.49', '10.1109/TPAMI.2010.161', '10.1007/978-3-319-00065-7_27', '10.1109/ICCV.2013.231', '10.1109/TPAMI.2012.231', '10.1109/TPAMI.2009.167', '10.1109/CVPR.2014.81', '10.1109/ICCV.2013.266', '10.1109/CVPR.2013.79', '10.1007/978-1-4471-4640-7_8', '10.1145/2647868.2654889', '10.1109/CVPR.2013.409', '10.1109/ICRA.2011.5980382', '10.1162/neco.1989.1.4.541', '10.1109/CVPR.2013.406', '10.1109/ICCV.2013.179', '10.1109/CVPR.2011.5995316', '10.1109/ICCV.2013.219', '10.1007/978-3-642-33715-4_54', '10.1007/978-3-642-37444-9_41', '10.1109/CVPR.2014.479', '10.1109/CVPR.2013.234'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Robotics', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Faster r-cnn: Towards real-time object detection with region proposal networks': Paper(DOI='10.1109/tpami.2016.2577031', crossref_json=None, google_schorlar_metadata=None, title='Faster r-cnn: Towards real-time object detection with region proposal networks', authors=['Shaoqing Ren', 'Kaiming He', 'Ross Girshick', 'Jian Sun'], abstract='State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github. com/ShaoqingRen/faster_rcnn.', conference=None, journal=None, year=None, reference_list=['10.5244/C.29.5', '10.1145/2647868.2654889', '10.1145/3065386', '10.1007/s11263-015-0816-y', '10.1162/neco.1989.1.4.541', '10.1109/CVPR.2015.7298594', '10.1109/TPAMI.2016.2577031', '10.1007/s11263-009-0275-4', '10.1007/s11263-013-0620-5', '10.1109/CVPR.2015.7299025', '10.1109/CVPR.2014.81', '10.1109/TPAMI.2009.167', '10.1109/CVPR.2015.7298965', '10.1109/ICCV.2015.169', '10.1109/TPAMI.2015.2465908', '10.1109/TPAMI.2011.231', '10.1109/TPAMI.2012.28', '10.1109/CVPR.2014.49', '10.1109/CVPR.2014.276'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Deep Learning', 'Autonomous Driving'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Spatial pyramid pooling in deep convolutional networks for visual recognition': Paper(DOI='10.1109/tpami.2015.2389824', crossref_json=None, google_schorlar_metadata=None, title='Spatial pyramid pooling in deep convolutional networks for visual recognition', authors=['Kaiming He', 'Xiangyu Zhang', 'Shaoqing Ren', 'Jian Sun'], abstract='Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224   224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2015.2389830', '10.1145/1961189.1961199', '10.1109/TPAMI.2011.235', '10.1109/CVPR.2014.222', '10.1109/CVPR.2014.220', '10.1109/CVPR.2014.212', '10.1109/ICCV.2005.239', '10.1109/CVPR.2006.68', '10.1109/ICCV.2003.1238663', '10.1109/CVPR.2010.5540018', '10.5244/C.25.76', '10.1023/B:VISI.0000029664.99615.94', '10.5244/C.28.72', '10.1109/CVPR.2014.81', '10.1109/CVPRW.2014.131', '10.1162/neco.1989.1.4.541', '10.1109/ICCV.2011.6126456', '10.1007/s11263-009-0275-4', '10.1016/j.cviu.2005.09.012', '10.1109/CVPR.2005.177', '10.1109/TPAMI.2009.167'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neural Network Architectures', 'Efficient Deep Learning', 'Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Shufflenet v2: Practical guidelines for efficient cnn architecture design': Paper(DOI='10.1007/978-3-030-01264-9_8', crossref_json=None, google_schorlar_metadata=None, title='Shufflenet v2: Practical guidelines for efficient cnn architecture design', authors=['Ningning Ma', 'Xiangyu Zhang', 'Hai-Tao Zheng', 'Jian Sun'], abstract='Current network architecture design is mostly guided by the indirect metric of computation complexity, ie, FLOPs. However, the direct metric, such as speed, also depends on the other factors such as memory access cost and platform characterics. Taking these factors into account, this work proposes practical guidelines for efficient network de-sign. Accordingly, a new architecture called ShuffleNet V2 is presented. Comprehensive experiments verify that it is the state-of-the-art in both speed and accuracy.', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2017.195', '10.1109/CVPR.2009.5206848', '10.1109/CVPR.2016.90', '10.1007/978-3-319-46493-0_38', '10.1109/ICCV.2017.155', '10.1109/CVPR.2018.00745', '10.1109/CVPR.2018.00291', '10.1109/CVPR.2017.243', '10.1109/CVPR.2017.633', '10.5244/C.28.88', '10.1007/978-3-319-10602-1_48', '10.1109/ICCV.2017.298', '10.1109/CVPR.2017.189', '10.1609/aaai.v33i01.33014780', '10.1007/s11263-015-0816-y', '10.1109/CVPR.2018.00474', '10.1609/aaai.v31i1.11231', '10.1109/CVPR.2015.7298594', '10.1109/CVPR.2016.308', '10.1109/CVPR.2018.00922', '10.1109/ICCV.2017.154', '10.1109/CVPR.2017.634', '10.1109/ICCV.2017.469', '10.1109/CVPR.2018.00716', '10.1109/TPAMI.2015.2502579', '10.1109/CVPR.2015.7298809', '10.1109/CVPR.2018.00907'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neural Network Architectures', 'Efficient Deep Learning', 'Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Region-based convolutional networks for accurate object detection and segmentation': Paper(DOI='10.1109/tpami.2015.2437384', crossref_json=None, google_schorlar_metadata=None, title='Region-based convolutional networks for accurate object detection and segmentation', authors=['Ross Girshick', 'Jeff Donahue', 'Trevor Darrell', 'Jitendra Malik'], abstract='Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2015.7298642', '10.1145/1646396.1646421', '10.1023/A:1011139631724', '10.1016/B978-1-55860-307-3.50012-5', '10.1109/TPAMI.2015.2465908', '10.1109/CVPR.2014.414', '10.1109/CVPR.2006.326', '10.1109/TKDE.2009.191', '10.1109/CVPR.2014.49', '10.1109/CVPR.2014.50', '10.1109/CVPR.2013.417', '10.1109/CVPR.2013.406', '10.1109/ICCV.2011.6126474', '10.1145/2556288.2557011', '10.1109/ICCV.2005.107', '10.1109/TPAMI.2012.231', '10.1109/CVPR.2012.6248077', '10.1109/ICCV.2011.6126343', '10.1023/B:VISI.0000029664.99615.94', '10.1109/TPAMI.2011.231', '10.1007/s11263-013-0620-5', '10.1109/CVPR.2013.237', '10.1109/TPAMI.2012.28', '10.21236/ADA295738', '10.1109/CVPR.2013.423', '10.1109/CVPR.2013.237', '10.1109/ICCV.2013.10', '10.1109/CVPR.2014.81', '10.1109/TPAMI.2013.50', '10.1109/CVPR.2014.276', '10.1109/34.655647', '10.1049/ip-vis:19941301', '10.1109/CVPR.2013.465', '10.1109/TPAMI.2009.167', '10.1007/BF00344251', '10.1007/s11263-009-0275-4', '10.1162/neco.1989.1.4.541', '10.1109/5.726791'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Machine Learning', 'Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Scale-space and edge detection using anisotropic diffusion': Paper(DOI='10.1109/34.56205', crossref_json=None, google_schorlar_metadata=None, title='Scale-space and edge detection using anisotropic diffusion', authors=['Pietro Perona', 'Jitendra Malik'], abstract=\"A new definition of scale-space is suggested, and a class of algorithms used to realize a diffusion process is introduced. The diffusion coefficient is chosen to vary spatially in such a way as to encourage intraregion smoothing rather than interregion smoothing. It is shown that the 'no new maxima should be generated at coarse scales' property of conventional scale space is preserved. As the region boundaries in the approach remain sharp, a high-quality edge detector which successfully exploits global information is obtained. Experimental results are shown on a number of images. Parallel hardware implementations are made feasible because the algorithm involves elementary, local operations replicated over the image.< >\", conference=None, journal=None, year=None, reference_list=['10.1007/978-1-4684-9333-7', '10.1007/BF00336961', '10.1007/BF00128527', '10.1002/cpa.3160420503', '10.1002/cpa.3160060202', '10.1109/TPAMI.1986.4767851', '10.1109/TPAMI.1984.4767596', '10.1109/34.6782', '10.7551/mitpress/7132.001.0001', '10.1109/TPAMI.1986.4767749', '10.1016/S0734-189X(87)80153-6', '10.1109/T-C.1971.223290', '10.1109/ICASSP.1984.1172729'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Applied Mathematics', 'Neuroscience', 'Psychology'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Learning to detect natural image boundaries using local brightness, color, and texture cues': Paper(DOI='10.1109/tpami.2004.1273918', crossref_json=None, google_schorlar_metadata=None, title='Learning to detect natural image boundaries using local brightness, color, and texture cues', authors=['David R Martin', 'Charless C Fowlkes', 'Jitendra Malik'], abstract='The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness, color, and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, we train a classifier using human labeled images as ground truth. The output of this classifier provides the posterior probability of a boundary at each image location and orientation. We present precision-recall curves showing that the resulting detector significantly outperforms existing approaches. Our two main results are 1) that cue combination can be performed adequately with a simple linear model and 2) that a proper, explicit treatment of texture is required to detect boundaries in natural images.', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.1999.790412', '10.1214/aoms/1177692631', '10.1109/ICCV.2001.937632', '10.1109/PROC.1979.11325', '10.1007/BF02288323', '10.1007/BF02278710', '10.1007/3-540-59408-6_49', '10.1007/BF01585996', '10.1109/CVPR.1997.609331', '10.1145/218380.218446', '10.1109/ICCV.2001.937614', '10.1109/ICCV.1995.466910', '10.1023/A:1011174803800', '10.1109/TPAMI.1986.4767851', '10.1109/ICIP.2000.899596', '10.5244/C.2.23', '10.1109/TPAMI.2002.1023800', '10.1023/A:1007614523901', '10.1162/neco.1994.6.2.181', '10.1016/0042-6989(95)00056-6', '10.1007/3-540-56484-5', '10.1109/CVPR.1999.784624', '10.1117/12.143648', '10.1007/BF00204594', '10.1109/ICCV.1999.790384', '10.1109/CVPR.1999.786996', '10.1109/TPAMI.2004.1273918', '10.1109/ICCV.2001.937655', '10.1109/CVPR.1999.786963', '10.1109/ICCV.1990.139492', '10.1098/rspb.1988.0073', '10.1364/JOSAA.7.000923'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'biological image analysis'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Blobworld: Image segmentation using expectation-maximization and its application to image querying': Paper(DOI='10.1109/tpami.2002.1023800', crossref_json=None, google_schorlar_metadata=None, title='Blobworld: Image segmentation using expectation-maximization and its application to image querying', authors=['Chad Carson', 'Serge Belongie', 'Hayit Greenspan', 'Jitendra Malik'], abstract='Retrieving images from large and varied collections using image content as a key is a challenging and important problem. We present a new image representation that provides a transformation from the raw pixel data to a small set of image regions that are coherent in color and texture. This \"Blobworld\" representation is created by clustering pixels in a joint color-texture-position feature space. The segmentation algorithm is fully automatic and has been run on a collection of 10,000 natural images. We describe a system that uses the Blobworld representation to retrieve images from this collection. An important aspect of the system is that the user is allowed to view the internal representation of the submitted image and the query results. Similar systems do not offer the user this view into the workings of the system; consequently, query results from these systems can be inexplicable, despite the availability of knobs for\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/S0031-3203(96)00113-6', '10.1109/CVPR.1996.517092', '10.1007/BF00130487', '10.1037/11496-005', '10.1007/978-3-540-49197-2_7', '10.1145/218380.218454', '10.1016/0031-3203(91)90143-S', '10.1117/12.205289', '10.1109/CVPR.1997.609453', '10.1109/ICIP.1997.647976', '10.1364/JOSAA.7.000923', '10.1109/CVPR.1997.609412', '10.1117/12.234781', '10.1007/s001380050060', '10.1109/34.391417', '10.1007/3-540-48762-X_63', '10.1109/ICCV.1998.710729', '10.1007/978-1-4757-2377-9', '10.1016/0005-1098(78)90005-5', '10.1145/253769.253798', '10.1109/34.85668', '10.1007/BF00123143', '10.1109/34.93808', '10.1109/IVL.1997.629719', '10.1007/3-540-61750-7', '10.1007/BF00058750', '10.1109/ICCV.1995.466859', '10.1214/aos/1176344136', '10.1109/CVPR.1997.609399', '10.1109/ICCV.1998.710790', '10.1109/ICIP.1995.537688', '10.1038/scientificamerican0697-88', '10.1109/2.410146', '10.1117/12.205303', '10.1007/BFb0028370', '10.1109/CVPR.1996.517174', '10.1109/34.464559', '10.1109/2.410150'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Spectral grouping using the nystrom method': Paper(DOI='10.1109/tpami.2004.1262185', crossref_json=None, google_schorlar_metadata=None, title='Spectral grouping using the nystrom method', authors=['Charless Fowlkes', 'Serge Belongie', 'Fan Chung', 'Jitendra Malik'], abstract='Spectral graph theoretic methods have recently shown great promise for the problem of image segmentation. However, due to the computational demands of these approaches, applications to large problems such as spatiotemporal data and high resolution imagery have been slow to appear. The contribution of this paper is a method that substantially reduces the computational requirements of grouping algorithms based on spectral partitioning making it feasible to apply them to very large grouping problems. Our approach is based on a technique for the numerical solution of eigenfunction problems known as the Nystrom method. This method allows one to extrapolate the complete grouping solution using only a small number of samples. In doing so, we leverage the fact that there are far fewer coherent groups in a scene than pixels.', conference=None, journal=None, year=None, reference_list=['10.1007/978-1-4612-1128-0', '10.1007/BF00128525', '10.1023/A:1011174803800', '10.1007/978-1-4615-4413-5_4', '10.1073/pnas.97.14.8186', '10.1109/ICCV.1999.790354', '10.1109/SFCS.1998.743487', '10.1109/CVPR.1996.517092', '10.1109/CVPR.1999.784979', '10.1109/34.868688', '10.1109/CVPR.1997.609375', '10.1007/s004930050052', '10.1109/72.788646', '10.1364/JOSAA.2.000284', '10.1162/089976698300017467', '10.1098/rspb.1991.0045', '10.1109/ICCV.1998.710861', '10.1109/ICPR.2000.903624', '10.1109/CVPR.1997.609331', '10.1109/CVPR.1996.517115'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'biological image analysis'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Large displacement optical flow: descriptor matching in variational motion estimation': Paper(DOI='10.1109/tpami.2010.143', crossref_json=None, google_schorlar_metadata=None, title='Large displacement optical flow: descriptor matching in variational motion estimation', authors=['Thomas Brox', 'Jitendra Malik'], abstract='Optical flow estimation is classically marked by the requirement of dense sampling in time. While coarse-to-fine warping schemes have somehow relaxed this constraint, there is an inherent dependency between the scale of structures and the velocity that can be estimated. This particularly renders the estimation of detailed human motion problematic, as small body parts can move very fast. In this paper, we present a way to approach this problem by integrating rich descriptors into the variational optical flow setting. This way we can estimate a dense optical flow field with almost the same high accuracy as known from variational optical flow, while reaching out to new domains of motion analysis where the requirement of dense sampling in time is no longer satisfied.', conference=None, journal=None, year=None, reference_list=['10.1016/0004-3702(81)90024-2', '10.1007/BF01421489', '10.1109/CVPR.2008.4587756', '10.1007/s11263-009-0273-6', '10.1109/TGRS.2007.906156', '10.1007/978-3-540-74936-3_22', '10.1007/978-3-642-03641-5_16', '10.1007/978-3-642-03061-1_2', '10.1109/CVPR.2007.383205', '10.1109/CVPR.2008.4587751', '10.1007/s11263-005-3960-y', '10.1109/83.668027', '10.1023/B:VISI.0000029664.99615.94', '10.1109/CVPR.2009.5206707', '10.1023/A:1008170101536', '10.7551/mitpress/7132.001.0001', '10.1006/cviu.1996.0006', '10.1109/CVPR.2005.320', '10.1109/ICCV.2007.4408903', '10.1109/CVPR.1991.139705', '10.1109/CVPR.2001.990529', '10.1109/CVPR.2009.5206697', '10.1016/S1077-3142(03)00009-2', '10.1109/ICCV.2005.240', '10.1109/CVPR.2005.177'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence', 'Robotics'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Edge boxes: Locating object proposals from edges': Paper(DOI='10.1007/978-3-319-10602-1_26', crossref_json=None, google_schorlar_metadata=None, title='Edge boxes: Locating object proposals from edges', authors=['C Lawrence Zitnick', 'Piotr Dollár'], abstract=' The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box’s boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1023/B:VISI.0000013087.49260.fb', '10.1109/TPAMI.2009.167', '10.1109/TPAMI.2012.28', '10.1007/s11263-013-0620-5', '10.1109/TPAMI.2011.231', '10.1109/ICCV.2011.6126351', '10.1109/ICCV.2013.315', '10.1109/TPAMI.2013.122', '10.1109/CVPR.2014.310', '10.1109/CVPR.2014.414', '10.1109/ICCV.2013.10', '10.1109/CVPR.2014.81', '10.1109/CVPR.2009.5206848', '10.1007/s11263-009-0275-4', '10.1109/ICCV.2013.231', '10.1145/2185520.2185540', '10.1007/978-3-642-15561-1_33', '10.1109/ICCV.2011.6126261', '10.1109/ICCV.2005.107', '10.5244/C.21.55', '10.5244/C.28.24', '10.1023/B:VISI.0000022288.19776.77', '10.1109/TPAMI.1986.4767851'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'artificial intelligence', 'computer graphics'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Structured forests for fast edge detection': Paper(DOI='10.18535/ijetst/v3i08.06', crossref_json=None, google_schorlar_metadata=None, title='Structured forests for fast edge detection', authors=['Piotr Dollár', 'C Lawrence Zitnick'], abstract='Edge detection is a critical component of many vision systems, including object detectors and image segmentation algorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take advantage of the structure present in local image patches to learn both an accurate and computationally efficient edge detector. We formulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated. The result is an approach that obtains realtime performance that is orders of magnitude faster than many competing state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation dataset and NYU Depth dataset. Finally, we show the potential of our approach as a general purpose edge detector by showing our learned edge models generalize well across datasets.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'artificial intelligence', 'computer graphics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Surface reflection: physical and geometrical perspectives': Paper(DOI='10.1109/34.85654', crossref_json=None, google_schorlar_metadata=None, title='Surface reflection: physical and geometrical perspectives', authors=['Shree K Nayar', 'Katsushi Ikeuchi', 'Takeo Kanade'], abstract='Machine vision can greatly benefit from the development of accurate reflectance models. There are two approaches to the study of reflection: physical and geometrical optics. While geometrical models may be construed as mere approximations to physical models, they possess simpler mathematical forms that often render them more usable than physical models. However, in general, geometrical models are applicable only when the wavelength of incident light is small compared to the dimensions of the surface imperfections. Therefore, it is incorrect to use these models to interpret or predict reflections from smooth surfaces; only physical models are capable of describing the underlying reflection mechanism.In this paper, reflectance models based on physical optics and geometrical optics are studied in detail. More specifically, we consider the Beckmann-Spizzichino (physical optics) model and the Torrance-Sparrow (geometrical optics) model. We have chosen these two particular models as they have been reported to fit experimental data well. Each model is described in detail, and the conditions that determine the validity of the model are clearly stated. By studying reflectance curves predicted by the two models, we propose a reflectance framework comprising three components: the diffuse lobe, the specular lobe, and the specular spike. The effects of surface roughness on the three primary components are analyzed in detail.', conference=None, journal=None, year=None, reference_list=['10.1364/JOSA.57.001105', '10.1109/TAP.1967.1138991', '10.1115/1.3691519', '10.1117/12.956740', '10.1364/AO.18.001770', '10.1109/PROC.1981.11918', '10.1109/TPAMI.1981.4767167', '10.1364/JOSA.40.000055', '10.1115/1.3689061', '10.1364/AO.12.001811', '10.1016/0146-664X(82)90001-6', '10.1109/34.3866', '10.1049/pi-4.1954.0025', '10.1145/800224.806819', '10.1364/AO.12.001816', '10.1109/70.54736', '10.1109/34.85654', '10.1109/70.59367', '10.1109/TPAMI.1984.4767501', '10.1145/360825.360839'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Robotics', 'Computer Vision', 'Computer Graphics'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Single image haze removal using dark channel prior': Paper(DOI='10.18535/ijecs/v5i1.12', crossref_json=None, google_schorlar_metadata=None, title='Single image haze removal using dark channel prior', authors=['Kaiming He', 'Jian Sun', 'Xiaoou Tang'], abstract='In this paper, we propose a simple but effective image prior-dark channel prior to remove haze from a single input image. The dark channel prior is a kind of statistics of outdoor haze-free images. It is based on a key observation-most local patches in outdoor haze-free images contain some pixels whose intensity is very low in at least one color channel. Using this prior with the haze imaging model, we can directly estimate the thickness of the haze and recover a high-quality haze-free image. Results on a variety of hazy images demonstrate the power of the proposed prior. Moreover, a high-quality depth map can also be obtained as a byproduct of haze removal.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Pattern Recognition'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Primitives for the manipulation of general subdivisions and the computation of Voronoi': Paper(DOI='10.1145/282918.282923', crossref_json=None, google_schorlar_metadata=None, title='Primitives for the manipulation of general subdivisions and the computation of Voronoi', authors=['Leonidas Guibas', 'Jorge Stolfi'], abstract='The following problem is discussed: given n points in the plane (the sites) and an arbitrary query point q, find the site that is closest to q. This problem can be solved by constructing the Voronoi diagram of the griven sites and then locating the query point inone of its regions. Two algorithms are given, one that constructs the Voronoi diagram in O(n log n) time, and another that inserts a new sit on O(n) time. Both are based on the use of the Voronoi dual, or Delaunay triangulation, and are simple enough to be of practical value. the simplicity of both algorithms can be attributed to the separation of the geometrical and topological aspects of the problem and to the use of two simple but powerful primitives,  a geometric predicate and an operator for manipulating the topology of the diagram. The topology is represented by a new data structure for generalized diagrams, that is, embeddings of graphs in two-dimensional\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/0020-0190(79)90074-7', '10.1109/MCG.1982.1674396', '10.1016/0304-3975(78)90051-8', '10.1145/359423.359430'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['geometric computing', 'computer vision', 'computer graphics', 'machine learning', 'robotics'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " 'Wireless sensor networks: an information processing approach': Paper(DOI='10.1108/sr.2005.08725bae.001', crossref_json=None, google_schorlar_metadata=None, title='Wireless sensor networks: an information processing approach', authors=['Feng Zhao', 'Leonidas J Guibas'], abstract='Designing, implementing, and operating a wireless sensor network involves a wide range of disciplines and many application-specific constraints. To make sense of and take advantage of these systems, a holistic approach is neededand this is precisely what Wireless Sensor Networks delivers. Inside, two eminent researchers review the diverse technologies and techniques that interact in todays wireless sensor networks. At every step, they are guided by the high-level information-processing tasks that determine how these networks are architected and administered. Zhao and Guibas begin with the canonical problem of localizing and tracking moving objects, then systematically examine the many fundamental sensor network issues that spring from it, including network discovery, service establishment, data routing and aggregation, query processing, programming models, and system organization. The understanding gained as a resulthow different layers support the needs of different applications, and how a wireless sensor network should be built to optimize performance and economyis sure to endure as individual component technologies come and go. Features: Written for practitioners, researchers, and students and relevant to all application areas, including environmental monitoring, industrial sensing and diagnostics, automotive and transportation, security and surveillance, military and battlefield uses, and large-scale infrastructural maintenance. Skillfully integrates the many disciplines at work in wireless sensor network design: signal processing and estimation, communication theory and protocols, distributed algorithms and databases\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['geometric computing', 'computer vision', 'computer graphics', 'machine learning', 'robotics'], conference_acronym='Sensor review', publisher=None, query_handler=None),\n",
       " 'A concise and provably informative multi‐scale signature based on heat diffusion': Paper(DOI='10.1111/j.1467-8659.2009.01515.x', crossref_json=None, google_schorlar_metadata=None, title='A concise and provably informative multi‐scale signature based on heat diffusion', authors=['Jian Sun', 'Maks Ovsjanikov', 'Leonidas Guibas'], abstract=' We propose a novel point signature based on the properties of the heat diffusion process on a shape. Our signature, called the Heat Kernel Signature (or HKS), is obtained by restricting the well‐known heat kernel to the temporal domain. Remarkably we show that under certain mild assumptions, HKS captures all of the information contained in the heat kernel, and characterizes the shape up to isometry. This means that the restriction to the temporal domain, on the one hand, makes HKS much more concise and easily commensurable, while on the other hand, it preserves all of the information about the intrinsic geometry of the shape. In addition, HKS inherits many useful properties from the heat kernel, which means, in particular, that it is stable under perturbations of the shape. Our signature also provides a natural and efficiently computable multi‐scale way to capture information about neighborhoods of a given\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s11263-009-0301-6', '10.1145/1377676.1377725', '10.1007/BF02568142', '10.1023/A:1007981719186', '10.1006/jfan.1997.3147', '10.1111/j.1467-8659.2008.01274.x', '10.1109/TVCG.2007.45', '10.1145/383259.383282', '10.1090/gsm/038', '10.1007/978-3-662-05105-4_2', '10.4153/CJM-1949-021-5', '10.1007/978-3-540-24673-2_8', '10.1145/571647.571648', '10.1111/j.1467-8659.2008.01273.x', '10.1080/10586458.1993.10504266', '10.1016/j.cagd.2008.01.002', '10.1016/j.cad.2005.10.011', '10.1145/1360612.1360696', '10.1137/0714065', '10.1016/j.cagd.2004.07.007'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['geometric computing', 'computer vision', 'computer graphics', 'machine learning', 'robotics'], conference_acronym='Computer graphics forum (Print)', publisher=None, query_handler=None),\n",
       " 'A scalable active framework for region annotation in 3d shape collections': Paper(DOI='10.1145/2980179.2980238', crossref_json=None, google_schorlar_metadata=None, title='A scalable active framework for region annotation in 3d shape collections', authors=['Li Yi', 'Vladimir G Kim', 'Duygu Ceylan', 'I-Chao Shen', 'Mengyan Yan', 'Hao Su', 'Cewu Lu', 'Qixing Huang', 'Alla Sheffer', 'Leonidas Guibas'], abstract='Large repositories of 3D shapes provide valuable input for data-driven analysis and modeling tools. They are especially powerful once annotated with semantic information such as salient regions and functional parts. We propose a novel active learning method capable of enriching massive geometric datasets with accurate semantic region annotations. Given a shape collection and a user-specified region label our goal is to correctly demarcate the corresponding regions with minimal manual work. Our active framework achieves this goal by cycling between manually annotating the regions, automatically propagating these annotations across the rest of the shapes, manually verifying both human and automatic annotations, and learning from the verification results to improve the automatic propagation algorithm. We use a unified utility function that explicitly models the time cost of human input across all steps of our\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/2642918.2647418', '10.1109/ICCV.2011.6126450', '10.1007/s11263-014-0698-4', '10.1145/2010324.1964930', '10.1111/1467-8659.00669', '10.1145/1531326.1531379', '10.1145/2835487', '10.1111/j.1467-8659.2012.03175.x', '10.1145/2024156.2024159', '10.1145/2508363.2508364', '10.1145/2601097.2601111', '10.1145/1833349.1778839', '10.1145/2185520.2185551', '10.1145/2185520.2185550', '10.1145/2601097.2601117', '10.1109/ICCV.2005.20', '10.1007/978-3-319-10602-1_48', '10.1111/j.1467-8659.2012.03217.x', '10.1109/3DV.2014.108', '10.1007/978-3-642-33712-3_7', '10.1111/j.1467-8659.2007.01103.x', '10.1016/j.cagd.2016.02.015', '10.1145/2070781.2024160', '10.1145/2816795.2818094', '10.1109/CVPR.2012.6248050', '10.1145/2366145.2366184', '10.1016/j.cag.2013.11.009', '10.1111/cgf.12434', '10.1145/775047.775151'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics', 'Geometry Processing', 'Machine Learning'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " 'Every picture tells a story: Generating sentences from images': Paper(DOI='10.1007/978-3-642-15561-1_2', crossref_json=None, google_schorlar_metadata=None, title='Every picture tells a story: Generating sentences from images', authors=['Ali Farhadi', 'Mohsen Hejrati', 'Mohammad Amin Sadeghi', 'Peter Young', 'Cyrus Rashtchian', 'Julia Hockenmaier', 'David Forsyth'], abstract=' Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned using data. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.', conference=None, journal=None, year=None, reference_list=['10.1007/3-540-47979-1_7', '10.1145/1101826.1101866', '10.1017/CBO9780511635465.010', '10.6028/NIST.IR.6719', '10.1007/978-3-540-88682-2_3', '10.1109/ICCV.2007.4408872', '10.1109/CVPR.2009.5206718', '10.1109/CVPR.2007.383331', '10.1109/TPAMI.2009.83', '10.1109/CVPR.2010.5540235', '10.1007/978-3-540-88688-4_7', '10.1145/383259.383316', '10.1109/CVPR.2009.5206492', '10.1109/JPROC.2010.2050411', '10.1109/CVPR.2008.4587597', '10.1016/S0079-6123(06)55002-2', '10.1145/1102351.1102464'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Colorful Image Colorization': Paper(DOI='10.3390/s22208010', crossref_json=None, google_schorlar_metadata=None, title='Colorful Image Colorization', authors=['Richard Zhang', 'Phillip Isola', 'Alexei Efros'], abstract=' Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a “colorization Turing test,” asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32\\xa0% of the trials, significantly higher than\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1015706.1015780', '10.1109/TIP.2005.864231', '10.1145/1141911.1142017', '10.1145/566654.566576', '10.1145/1457515.1409105', '10.1007/s11390-012-1290-4', '10.1109/ICCV.2015.55', '10.1007/978-3-319-46493-0_35', '10.1145/2897824.2925974', '10.1007/978-3-319-94544-6_9', '10.1007/978-3-319-71249-9_10', '10.1109/WACV45572.2020.9093389', '10.1145/3072959.3073703', '10.1007/s11263-019-01271-4', '10.1109/cvpr42600.2020.00799', '10.1109/iccv48922.2021.01411', '10.1145/3474085.3481544', '10.1007/978-3-319-24574-4_28', '10.1109/iccv48922.2021.00460', '10.1109/CVPR.2016.90'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Deep Learning', 'Computer Graphics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Generative visual manipulation on the natural image manifold': Paper(DOI='10.1007/978-3-319-46454-1_36', crossref_json=None, google_schorlar_metadata=None, title='Generative visual manipulation on the natural image manifold', authors=['Jun-Yan Zhu', 'Philipp Krähenbühl', 'Eli Shechtman', 'Alexei A Efros'], abstract=' Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to “fall off” the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/38.946629', '10.1145/1015706.1015780', '10.1145/344779.344859', '10.1145/1618452.1618472', '10.1109/CVPR.2010.5540159', '10.1145/2010324.1964956', '10.1038/381607a0', '10.1023/A:1026553619983', '10.1109/ICCV.2011.6126278', '10.1109/ICCV.2015.449', '10.1126/science.1127647', '10.1145/1390156.1390294', '10.1109/CVPR.2015.7298761', '10.1007/978-3-319-46475-6_43', '10.1109/CVPR.2009.5206848', '10.1137/0916069', '10.1007/978-3-540-24673-2_3', '10.1023/B:VISI.0000045324.43199.43', '10.1145/2508363.2508419', '10.1007/978-3-642-15549-9_1', '10.1109/ICCV.2011.6126281', '10.1109/CVPR.2014.32', '10.1145/237170.237196', '10.1145/2502081.2502281', '10.1109/TPAMI.2010.147', '10.1109/CVPR.2013.299'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Computer Graphics'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Describable visual attributes for face verification and image search': Paper(DOI='10.1109/tpami.2011.48', crossref_json=None, google_schorlar_metadata=None, title='Describable visual attributes for face verification and image search', authors=['Neeraj Kumar', 'Alexander Berg', 'Peter N Belhumeur', 'Shree Nayar'], abstract='We introduce the use of describable visual attributes for face verification and image search. Describable visual attributes are labels that can be given to an image to describe its appearance. This paper focuses on images of faces and the attributes used to describe them, although the concepts also apply to other domains. Examples of face attributes include gender, age, jaw shape, nose size, etc. The advantages of an attribute-based representation for vision tasks are manifold: They can be composed to create descriptions at various levels of specificity; they are generalizable, as they can be learned once and then applied to recognize new objects or categories without any further training; and they are efficient, possibly requiring exponentially fewer attributes (and training data) than explicitly naming each category. We show how one can create and label large data sets of real-world images to train classifiers which\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2009.5459250', '10.1109/CVPR.2009.5206594', '10.1145/1126004.1126005', '10.1109/ICCV.2007.4409069', '10.1023/B:VISI.0000029664.99615.94', '10.1109/CVPR.2007.383111', '10.1109/34.1000244', '10.1111/1467-9280.00144', '10.1109/CVPR.2000.855827', '10.1109/CVPR.2006.264', '10.1109/CVPR.2007.382969', '10.1109/TPAMI.2007.1107', '10.1007/BF00123143', '10.1109/34.879790', '10.1109/CVPR.1994.323814', '10.1109/CVPR.2009.5206605', '10.1007/s11263-007-0090-8', '10.1109/CVPR.2005.268', '10.1109/FGR.2006.87', '10.1109/JPROC.2006.884093', '10.1109/CVPR.2009.5206772', '10.1038/384404a0', '10.1109/ICCV.2003.1238663', '10.5244/C.23.77', '10.1007/s11263-007-0093-5', '10.1145/1101826.1101866', '10.1109/ACV.1994.341300', '10.1109/CVPR.2005.177', '10.1109/AFGR.2002.1004124', '10.5244/C.20.92', '10.1109/AFGR.2002.1004130', '10.1109/AFGR.2000.840639', '10.1007/BF00994018', '10.1109/CVPR.1991.139758', '10.1109/CVPR.2001.990517', '10.1109/34.598235', '10.1109/AFGR.2002.1004155', '10.1037/1076-898X.5.4.339', '10.1109/34.598228', '10.1109/CVPR.2004.1315253', '10.1007/s11263-006-8910-9', '10.1109/34.41390', '10.1145/954339.954342', '10.1109/ICCV.2007.4408858', '10.1109/34.927464'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Biometrics', 'Computer Graphics'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Names and faces in the news': Paper(DOI='10.2201/niipi.2010.7.8', crossref_json=None, google_schorlar_metadata=None, title='Names and faces in the news', authors=['Tamara L Berg', 'Alexander C Berg', 'Jaety Edwards', 'Michael Maire', 'Ryan White', 'Yee-Whye Teh', 'Erik Learned-Miller', 'David A Forsyth'], abstract='We show quite good face clustering is possible for a dataset of inaccurately and ambiguously labelled face images. Our dataset is 44,773 face images, obtained by applying a face finder to approximately half a million captioned news images. This dataset is more realistic than usual face recognition datasets, because it contains faces captured \"in the wild\" in a variety of configurations with respect to the camera, taking a variety of expressions, and under illumination of widely varying color. Each face image is associated with a set of names, automatically extracted from the associated caption. Many, but not all such sets contain the correct name. We cluster face images in appropriate discriminant coordinates. We use a clustering procedure to break ambiguities in labelling and identify incorrectly labelled faces. A merging procedure then identifies variants of names that refer to the same individual. The resulting\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1162/jocn.1991.3.1.71', '10.1109/93.752960', '10.1023/B:VISI.0000029664.99615.94'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Statistics', 'Probability Theory'], conference_acronym='Progress in informatics (Print)', publisher=None, query_handler=None),\n",
       " 'Modeling the world from internet photo collections': Paper(DOI='10.1145/2208917.2212756', crossref_json=None, google_schorlar_metadata=None, title='Modeling the world from internet photo collections', authors=['Noah Snavely', 'Steven M Seitz', 'Richard Szeliski'], abstract='  There are billions of photographs on the Internet, comprising the largest and most diverse photo collection ever assembled. How can computer vision researchers exploit this imagery? This paper explores this question from the standpoint of 3D scene modeling and visualization. We present structure-from-motion and image-based rendering algorithms that operate on hundreds of images downloaded as a result of keyword-based image search queries like “Notre Dame” or “Trevi Fountain.” This approach, which we call Photo Tourism, has enabled reconstructions of numerous well-known world sites. This paper presents these algorithms and results as a first step towards 3D modeling of the world’s well-photographed sites, cities, and landscapes from Internet imagery, and discusses key open problems and challenges for the research community. ', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2009.5459148', '10.1109/CVPRW.2010.5543514', '10.1109/34.1000236', '10.1073/pnas.1006155107', '10.1145/1526709.1526812', '10.1145/1526709.1526812', '10.1109/CVPR.2011.5995626', '10.1109/CVPR.2011.5995626', '10.1126/science.1167742', '10.1023/B:VISI.0000029664.99615.94', '10.1126/science.1199644', '10.1007/s11263-007-0107-3', '10.1145/1978942.1979146', '10.1145/2187836.2187938'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics'], conference_acronym='ACM queue', publisher=None, query_handler=None),\n",
       " 'Image alignment and stitching: A tutorial': Paper(DOI='10.1561/0600000009', crossref_json=None, google_schorlar_metadata=None, title='Image alignment and stitching: A tutorial', authors=['Richard Szeliski'], abstract='This tutorial reviews image alignment and image stitching algorithms. Image alignment algorithms can discover the correspondence relationships among images with varying degrees of overlap. They are ideally suited for applications such as video stabilization, summarization, and the creation of panoramic mosaics. Image stitching algorithms take the alignment estimates produced by such registration algorithms and blend the images in a seamless manner, taking care to deal with potential problems such as blurring or ghosting caused by parallax and scene movement as well as varying image exposures. This tutorial reviews the basic motion models underlying alignment and stitching algorithms, describes effective direct (pixel-based) and feature-based alignment algorithms, and describes blending algorithms used to produce seamless mosaics. It ends with a discussion of open research problems in the area.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics'], conference_acronym='Foundations and trends in computer graphics and vision', publisher=None, query_handler=None),\n",
       " 'Building rome in a day': Paper(DOI='10.12968/prtu.2014.1.39.58', crossref_json=None, google_schorlar_metadata=None, title='Building rome in a day', authors=['Sameer Agarwal', 'Yasutaka Furukawa', 'Noah Snavely', 'Ian Simon', 'Brian Curless', 'Steven M Seitz', 'Richard Szeliski'], abstract='We present a system that can reconstruct 3D geometry from large, unorganized collections of photographs such as those found by searching for a given city (e.g., Rome) on Internet photo-sharing sites. Our system is built on a set of new, distributed computer vision algorithms for image matching and 3D reconstruction, designed to maximize parallelism at each stage of the pipeline and to scale gracefully with both the size of the problem and the amount of available computation. Our experimental results demonstrate that it is now possible to reconstruct city-scale image collections with more than a hundred thousand images in less than a day.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics'], conference_acronym='Primary teacher update', publisher=None, query_handler=None),\n",
       " 'Video mosaics for virtual environments': Paper(DOI='10.1109/38.486677', crossref_json=None, google_schorlar_metadata=None, title='Video mosaics for virtual environments', authors=['Richard Szeliski'], abstract='As computer-based video becomes ubiquitous with the expansion of transmission, storage, and manipulation capabilities, it will offer a rich source of imagery for computer graphics applications. This article looks at one way to use video as a new source of high-resolution, photorealistic imagery for these applications. If you walked through an environment, such as a building interior, and filmed a video sequence of what you saw you could subsequently register and composite the video images together into large mosaics of the scene. In this way, you can achieve an essentially unlimited resolution. Furthermore, since you can acquire the images using any optical technology, you can reconstruct any scene regardless of its range or scale. Video mosaics can be used in many different applications, including the creation of virtual reality environments, computer-game settings, and movie special effects. I present\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/MCG.1986.276658', '10.1109/MCG.2002.988743', '10.1109/ICIP.1994.413336', '10.1145/218380.218395', '10.1109/CVPR.1994.323829', '10.1109/ACV.1994.341287', '10.1145/166266.168422', '10.1145/166117.166153', '10.1145/133994.134003', '10.1145/800059.801126'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics'], conference_acronym='IEEE computer graphics and applications', publisher=None, query_handler=None),\n",
       " 'A comparative study of energy minimization methods for markov random fields with smoothness-based priors': Paper(DOI='10.1109/tpami.2007.70844', crossref_json=None, google_schorlar_metadata=None, title='A comparative study of energy minimization methods for markov random fields with smoothness-based priors', authors=['Richard Szeliski', 'Ramin Zabih', 'Daniel Scharstein', 'Olga Veksler', 'Vladimir Kolmogorov', 'Aseem Agarwala', 'Marshall Tappen', 'Carsten Rother'], abstract='Among the most exciting advances in early vision has been the development of efficient energy minimization algorithms for pixel-labeling tasks such as depth or texture computation. It has been known for decades that such problems can be elegantly expressed as Markov random fields, yet the resulting energy minimization problems have been widely viewed as intractable. Algorithms such as graph cuts and loopy belief propagation (LBP) have proven to be very powerful: For example, such methods form the basis for almost all the top-performing stereo methods. However, the trade-offs among different energy minimization algorithms are still not well understood. In this paper, we describe a set of energy minimization benchmarks and use them to compare the solution quality and runtime of several common energy minimization algorithms. We investigate three promising methods-graph cuts, LBP, and tree\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/18.910585', '10.1109/TIT.2005.856938', '10.1023/B:STCO.0000021412.33763.d5', '10.1109/CVPR.2007.383249', '10.1109/ICCV.2003.1238444', '10.1145/1141911.1142005', '10.1109/CVPR.2000.855839', '10.1023/A:1014573219977', '10.1109/TIT.2005.850085', '10.1109/TPAMI.1984.4767596', '10.1023/A:1026501619075', '10.1145/383259.383296', '10.1007/s11263-006-7899-4', '10.1007/BF00054995', '10.1145/236337.236369', '10.1109/34.969114', '10.1109/CVPR.1998.698673', '10.1145/502090.502093', '10.1109/ICCV.2001.937505', '10.1287/opre.13.3.388', '10.1109/TPAMI.2004.60', '10.1109/34.677269', '10.7551/mitpress/7132.001.0001', '10.1109/TPAMI.2006.193', '10.1007/BF00054836', '10.1109/TPAMI.2006.200', '10.1109/TPAMI.2007.1128', '10.1145/1073204.1073268', '10.1109/TPAMI.2003.1233908', '10.1016/S0166-218X(01)00338-9', '10.1109/TPAMI.2007.1031', '10.1145/1015706.1015718', '10.1109/TPAMI.2007.1061', '10.1109/TPAMI.2004.1262177', '10.1145/1201775.882264', '10.1109/CVPR.2007.383095', '10.1109/TPAMI.2004.54', '10.1007/978-4-431-66933-3', '10.1109/ICCV.2005.110', '10.1145/1015706.1015720', '10.1109/CVPR.2007.383203', '10.1109/CVPR.2005.130'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Eulerian video magnification for revealing subtle changes in the world': Paper(DOI='10.1145/2185520.2185561', crossref_json=None, google_schorlar_metadata=None, title='Eulerian video magnification for revealing subtle changes in the world', authors=['Hao-Yu Wu', 'Michael Rubinstein', 'Eugene Shih', 'John Guttag', 'Frédo Durand', 'William Freeman'], abstract='Our goal is to reveal temporal variations in videos that are difficult or impossible to see with the naked eye and display them in an indicative manner. Our method, which we call Eulerian Video Magnification, takes a standard video sequence as input, and applies spatial decomposition, followed by temporal filtering to the frames. The resulting signal is then amplified to reveal hidden information. Using our method, we are able to visualize the flow of blood as it fills the face and also to amplify and reveal small motions. Our technique can run in real time to show phenomena occurring at the temporal frequencies selected by the user.', conference=None, journal=None, year=None, reference_list=['10.1109/TCOM.1983.1095851', '10.1145/127719.122721', '10.1016/j.cag.2010.05.017', '10.1016/0004-3702(81)90024-2', '10.1145/1073204.1073223', '10.1109/CVPR.2006.207', '10.1364/OE.18.010762', '10.1364/OE.16.021434', '10.1145/1141911.1142010'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'computational photography'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " 'Tracking people by learning their appearance': Paper(DOI='10.1109/tpami.2007.250600', crossref_json=None, google_schorlar_metadata=None, title='Tracking people by learning their appearance', authors=['Deva Ramanan', 'David A Forsyth', 'Andrew Zisserman'], abstract='An open vision problem is to automatically track the articulations of people from a video sequence. This problem is difficult because one needs to determine both the number of people in each frame and estimate their configurations. But, finding people and localizing their limbs is hard because people can move fast and unpredictably, can appear in a variety of poses and clothes, and are often surrounded by limb-like clutter. We develop a completely automatic system that works in two stages; it first builds a model of appearance of each person in a video and then it tracks by detecting those models in each frame (\"tracking by model-building and detection\"). We develop two algorithms that build models; one bottom-up approach groups together candidate body parts found throughout a sequence. We also describe a top-down approach that automatically builds people-models by detecting convenient key poses within\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2003.1211409', '10.1109/CVPR.2003.1211410', '10.1109/T-C.1973.223602', '10.1023/A:1011179004708', '10.1109/CVPR.2005.335', '10.1109/CVPR.2003.1211504', '10.1023/B:VISI.0000042934.15159.49', '10.1023/A:1008078328650', '10.1109/CVPR.2000.854758', '10.1023/A:1014899027014', '10.1109/CVPR.2001.990509', '10.1109/CVPR.2004.1315063', '10.1109/ICCV.1999.791203', '10.1016/0262-8856(83)90003-3', '10.1109/TPAMI.1980.6447699', '10.1109/ICCV.2001.937589', '10.1109/CVPR.1996.517056', '10.1109/CVPR.1998.698581', '10.1006/cviu.1998.0716', '10.1109/CVPR.1993.341008', '10.1109/CVPR.2005.353', '10.1109/CVPR.2000.855904', '10.1109/ICCV.2003.1238365', '10.1109/34.1000236', '10.1109/CVPR.2001.990517', '10.1109/ICCV.2003.1238422', '10.1109/CVPR.2003.1211346', '10.1109/TPAMI.2003.1233903', '10.1109/CVPR.2001.991028', '10.1109/ICCV.1995.466882'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics', 'Machine Learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Fast edge detection using structured forests': Paper(DOI='10.18535/ijetst/v3i08.06', crossref_json=None, google_schorlar_metadata=None, title='Fast edge detection using structured forests', authors=['Piotr Dollár', 'C Lawrence Zitnick'], abstract='Edge detection is a critical component of many vision systems, including object detectors and image segmentation algorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take advantage of the structure present in local image patches to learn both an accurate and computationally efficient edge detector. We formulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated. The result is an approach that obtains realtime performance that is orders of magnitude faster than many competing state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation dataset and NYU\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'artificial intelligence', 'computer graphics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Shuffle and learn: unsupervised learning using temporal order verification': Paper(DOI='10.1007/978-3-319-46448-0_32', crossref_json=None, google_schorlar_metadata=None, title='Shuffle and learn: unsupervised learning using temporal order verification', authors=['Ishan Misra', 'C Lawrence Zitnick', 'Martial Hebert'], abstract=' In this paper, we present an approach for learning a visual representation from the raw spatiotemporal signals in videos. Our representation is learned without supervision from semantic labels. We formulate our method as an unsupervised sequential verification task, i.e., we determine whether a sequence of frames from a video is in the correct temporal order. With this simple task and no semantic labels, we learn a powerful visual representation using a Convolutional Neural Network (CNN). The representation contains complementary information to that learned from supervised image datasets like ImageNet. Qualitative results show that our method captures information that is temporally varying, such as human pose. When used as pre-training for action recognition, our method gives significant gains over learning without external data on benchmark datasets like UCF101 and HMDB51. To demonstrate its\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1037/0096-3445.120.3.235', '10.1037/0096-3445.118.3.219', '10.7551/mitpress/4528.001.0001', '10.1207/s15516709cog2502_2', '10.1016/j.visres.2013.10.017', '10.1109/MIS.2001.1463065', '10.1109/ICCV.2015.167', '10.1162/neco.1989.1.4.541', '10.1109/ICCV.2011.6126543', '10.1109/CVPR.2013.471', '10.1109/CVPR.2014.471', '10.1109/CVPR.2009.5206848', '10.1007/978-3-642-33786-4_35', '10.1109/ICCV.2005.77', '10.1007/978-3-642-33709-3_6', '10.1109/CVPR.2013.124', '10.1109/CVPR.2013.115', '10.1109/ICCV.2013.422', '10.1038/381607a0', '10.1145/1390156.1390294', '10.7551/mitpress/7503.003.0105', '10.7551/mitpress/7503.003.0024', '10.1109/ICASSP.2013.6639343', '10.1007/978-3-319-46493-0_20', '10.1109/CVPR.2013.465', '10.1109/ICCV.2015.166', '10.1109/CVPR.2016.418', '10.1145/1553374.1553469', '10.1162/neco.1991.3.2.194', '10.1162/089976602317318938', '10.1109/ICCV.2015.465', '10.1109/TPAMI.2011.157', '10.1162/neco.1997.9.8.1735', '10.1007/978-3-642-15567-3_11', '10.1109/ICCV.2015.511', '10.1109/ICCV.2015.13', '10.1109/CVPR.2016.264', '10.1109/ICCV.2015.320', '10.1016/j.imavis.2009.11.014', '10.3390/s140304189', '10.1109/JRPROC.1949.232969', '10.1007/3-540-45103-X_50', '10.1145/2647868.2654889', '10.5244/C.28.6', '10.1109/ICCV.2015.169', '10.1109/CVPR.2014.81', '10.1007/s11263-015-0816-y', '10.1109/TPAMI.2012.261', '10.1109/CVPR.2014.214', '10.1109/CVPR.2015.7298982', '10.1109/ICCV.2015.120'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Robotics'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'A cooperative algorithm for stereo matching and occlusion detection': Paper(DOI='10.1109/34.865184', crossref_json=None, google_schorlar_metadata=None, title='A cooperative algorithm for stereo matching and occlusion detection', authors=['C Lawrence Zitnick', 'Takeo Kanade'], abstract='Presents a stereo algorithm for obtaining disparity maps with occlusion explicitly detected. To produce smooth and detailed disparity maps, two assumptions that were originally proposed by Marr and Poggio (1976, 1979) are adopted: uniqueness and continuity. That is, the disparity maps have a unique value per pixel and are continuous almost everywhere. These assumptions are enforced within a three-dimensional array of match values in disparity space. Each match value corresponds to a pixel in an image and a disparity relative to another image. An iterative algorithm updates the match values by diffusing support among neighboring values and inhibiting others along similar lines of sight. By applying the uniqueness assumption, occluded regions can be explicitly identified. To demonstrate the effectiveness of the algorithm, we present the processing results from synthetic and real image pairs, including ones\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1023/A:1008015117424', '10.1007/BF00363999', '10.1068/p140449', '10.1109/TPAMI.1985.4767639', '10.1109/34.206955', '10.1016/0146-664X(73)90016-6', '10.1109/CVPR.1996.517099', '10.1109/ICPR.1994.576397', '10.1109/34.310690', '10.1007/BFb0028349', '10.1016/S0734-189X(87)80166-4', '10.1016/0146-664X(73)90024-5', '10.1098/rspb.1979.0029', '10.1126/science.968482', '10.1007/BF01212430', '10.1007/BF01679683', '10.1109/TPAMI.1985.4767615', '10.1109/CVPR.1992.223143', '10.1109/CVPR.1998.698673', '10.1109/CVPR.1996.517097'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'artificial intelligence', 'computer graphics'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Robust anisotropic diffusion': Paper(DOI='10.3724/sp.j.1004.2009.01253', crossref_json=None, google_schorlar_metadata=None, title='Robust anisotropic diffusion', authors=['Michael J Black', 'Guillermo Sapiro', 'David H Marimont', 'David Heeger'], abstract='Relations between anisotropic diffusion and robust statistics are described in this paper. Specifically, we show that anisotropic diffusion can be seen as a robust estimation procedure that estimates a piecewise smooth image from a noisy input image. The \"edge-stopping\" function in the anisotropic diffusion equation is closely related to the error norm and influence function in the robust estimation framework. This connection leads to a new \"edge-stopping\" function based on Tukey\\'s biweight robust estimator that preserves sharper boundaries than previous formulations and improves the automatic stopping of the diffusion. The robust statistical interpretation also provides a means for detecting the boundaries (edges) between the piecewise smooth regions in an image that has been smoothed with anisotropic diffusion. Additionally, we derive a relationship between anisotropic diffusion and regularization with line\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TIP.2006.877360', '10.1109/TIP.2006.871143', '10.1109/TIP.2007.899002', '10.1016/j.sigpro.2005.09.019', '10.1109/TPAMI.2005.190', '10.1109/83.661192', '10.1109/34.56205', '10.1051/jp42002006', '10.1109/TIP.2002.800883', '10.1109/TGRS.2007.894569', '10.1109/TIP.2007.891803', '10.1137/0729012', '10.1109/TPAMI.2004.47'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics', 'Machine Learning', 'Virtual Humans', 'Optical Flow'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields': Paper(DOI='10.1006/cviu.1996.0006', crossref_json=None, google_schorlar_metadata=None, title='The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields', authors=['Michael J Black', 'P. Anandan'], abstract='Most approaches for estimating optical flow assume that, within a finite image region, only a single motion is present. Thissingle motion assumptionis violated in common situations involving transparency, depth discontinuities, independently moving objects, shadows, and specular reflections. To robustly estimate optical flow, the single motion assumption must be relaxed. This paper presents a framework based onrobust estimationthat addresses violations of the brightness constancy and spatial smoothness assumptions caused by multiple motions. We show how therobust estimation frameworkcan be applied to standard formulations of the optical flow problem thus reducing their sensitivity to violations of their underlying assumptions. The approach has been applied to three standard techniques for recovering optical flow: area-based regression, correlation, and regularization with motion discontinuities. This paper\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computing', 'AI', 'Computer Vision', 'Philosophy', 'Quantum Computing'], conference_acronym='Computer vision and image understanding (Print)', publisher=None, query_handler=None),\n",
       " 'A naturalistic open source movie for optical flow evaluation': Paper(DOI='10.1007/978-3-642-33783-3_44', crossref_json=None, google_schorlar_metadata=None, title='A naturalistic open source movie for optical flow evaluation', authors=['Daniel J Butler', 'Jonas Wulff', 'Garrett B Stanley', 'Michael J Black'], abstract=' Ground truth optical flow is difficult to measure in real scenes with natural motion. As a result, optical flow data sets are restricted in terms of size, complexity, and diversity, making optical flow algorithms difficult to train and test on realistic data. We introduce a new optical flow data set derived from the open source 3D animated short film Sintel. This data set has important features not present in the popular Middlebury flow evaluation: long sequences, large motions, specular reflections, motion blur, defocus blur, and atmospheric effects. Because the graphics data that generated the movie is open source, we are able to render scenes under conditions of varying complexity to evaluate where existing flow algorithms fail. We evaluate several recent optical flow algorithms and find that current highly-ranked methods on the Middlebury evaluation have difficulty with this more complex data set suggesting further\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s11263-010-0390-2', '10.1145/2019001.2019066', '10.1007/s11263-006-0016-x', '10.1364/JOSAA.4.002379', '10.1146/annurev.neuro.24.1.1193', '10.1007/BF01420984', '10.1109/CVPR.2008.4587845', '10.1109/CVPR.2012.6248074', '10.1117/1.OE.51.2.021107', '10.1007/978-3-540-88690-7_7', '10.1109/TPAMI.2010.143', '10.1109/CVPR.2010.5539939', '10.5244/C.23.108'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics', 'Machine Learning', 'Virtual Humans', 'Optical Flow'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'HumanEva: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human\\xa0Motion': Paper(DOI='10.1007/s11263-009-0273-6', crossref_json=None, google_schorlar_metadata=None, title='HumanEva: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human\\xa0Motion', authors=['Leonid Sigal', 'Alexandru O Balan', 'Michael J Black'], abstract=' While research on articulated human motion and pose estimation has progressed rapidly in the last few years, there has been no systematic quantitative evaluation of competing methods to establish the current state of the art. We present data obtained using a hardware system that is able to capture synchronized video and ground-truth 3D motion. The resulting HumanEva datasets contain multiple subjects performing a set of predefined actions with a number of repetitions. On the order of 40,000 frames of synchronized motion capture and multi-view video (resulting in over one quarter million image frames in total) were collected at 60\\xa0Hz with an additional 37,000 time instants of pure motion capture data. A standard set of error measures is defined for evaluating both 2D and 3D pose estimation and tracking algorithms. We also describe a baseline algorithm for 3D articulated tracking that uses a relatively\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1015330.1015343', '10.1109/78.978374', '10.1109/ICCV.2007.4408903', '10.1109/CVPR.2007.383340', '10.1109/CVPR.2006.52', '10.1109/VSPETS.2005.1570935', '10.1109/CVPR.2007.383129', '10.1109/CVPR.2008.4587578', '10.1109/CVPR.1998.698581', '10.1109/CVPR.2007.383342', '10.1016/j.jbiomech.2005.02.008', '10.1016/j.jbiomech.2007.05.029', '10.1023/B:VISI.0000043757.18370.9c', '10.1023/A:1008935410038', '10.1016/j.cviu.2006.07.007', '10.1109/ICCV.2007.4409073', '10.1023/B:VISI.0000042934.15159.49', '10.1006/cviu.1998.0716', '10.1109/CVPR.1996.517056', '10.1109/ICCV.2003.1238408', '10.1016/0262-8856(83)90003-3', '10.1023/A:1008078328650', '10.1109/TPAMI.2003.1233903', '10.1109/AFGR.1996.557241', '10.1109/CVPR.1996.517057', '10.1007/s11263-007-0116-2', '10.1109/ICCV.2007.4409030', '10.1007/11744078_29', '10.1109/ICCV.2007.4409044', '10.1007/11744047_11', '10.1023/A:1008122218374', '10.1006/cviu.2000.0897', '10.1109/ICCV.2005.112', '10.1109/CVPR.2007.383302', '10.1109/ICCV.2007.4408976', '10.1109/TPAMI.1980.6447699', '10.1109/ICCV.1999.791203', '10.1109/34.879790', '10.1109/MOTION.2002.1182228', '10.1109/CVPR.2005.335', '10.1109/CVPR.2003.1211504', '10.1007/978-3-540-24673-2_24', '10.1109/CVPR.2000.854946', '10.1109/CVPR.2004.1315125', '10.1109/TPAMI.2005.39', '10.1023/A:1014573219977', '10.1109/ICCV.2003.1238424', '10.1007/3-540-47969-4_52', '10.1109/AFGR.2000.840661', '10.1007/3-540-45053-X_45', '10.1109/CVPR.2004.1315063', '10.1109/CVPR.2006.180', '10.1109/CVPR.2005.132', '10.1145/1015330.1015371', '10.1177/0278364903022006003', '10.1109/AFGR.2002.1004125', '10.1109/CVPR.2007.383301', '10.1006/cviu.2000.0878', '10.1109/CVPR.2006.15', '10.1109/ICCV.2005.193', '10.1145/1360612.1360696', '10.1109/CVPR.2008.4587580', '10.1109/CVPR.2006.32', '10.1006/cviu.1999.0758', '10.1109/ICCV.2007.4408951'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image': Paper(DOI='10.1007/978-3-319-46454-1_34', crossref_json=None, google_schorlar_metadata=None, title='Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image', authors=['Federica Bogo', 'Angjoo Kanazawa', 'Christoph Lassner', 'Peter Gehler', 'Javier Romero', 'Michael J Black'], abstract=' We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fit it to very little\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2015.7298751', '10.1145/1073204.1073207', '10.1109/CVPR.2007.383340', '10.1006/cviu.2000.0888', '10.1007/s11263-008-0204-y', '10.1007/978-3-642-15558-1_22', '10.1201/b14581', '10.1007/978-3-319-10590-1_12', '10.1109/CVPR.2010.5539853', '10.1109/CVPR.2014.215', '10.1109/TPAMI.2013.248', '10.1007/978-3-319-16808-1_21', '10.1109/ICPR.2010.414', '10.5244/C.24.12', '10.1007/978-3-319-10602-1_22', '10.5244/C.28.80', '10.1109/CVPR.2015.7299068', '10.1016/0734-189X(85)90094-5', '10.1007/978-3-319-16808-1_23', '10.1007/978-3-319-10584-0_11', '10.1177/0278364913479413', '10.1109/ICCV.2015.222', '10.1007/978-3-319-16865-4_35', '10.1109/CVPR.2016.533', '10.1109/CVPR.2014.300', '10.1007/978-3-642-33765-9_41', '10.1145/1015706.1015720', '10.1007/s11263-009-0273-6', '10.1109/CVPR.2013.466', '10.1109/CVPR.2012.6247988', '10.1109/CVPR.2015.7298941', '10.1006/cviu.2000.0878', '10.1109/CVPR.2016.113', '10.1109/CVPR.2014.214', '10.1109/CVPR.2014.303', '10.1109/CVPR.2016.511', '10.1109/CVPR.2011.5995741', '10.1109/CVPR.2016.535', '10.1007/978-3-319-10599-4_5', '10.1109/CVPR.2015.7299074', '10.1109/CVPR.2016.537'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Eigentracking: Robust matching and tracking of articulated objects using a view-based representation': Paper(DOI='10.1007/bfb0015548', crossref_json=None, google_schorlar_metadata=None, title='Eigentracking: Robust matching and tracking of articulated objects using a view-based representation', authors=['Michael J Black', 'Allan D Jepson'], abstract=' This paper describes an approach for tracking rigid and articulated objects using a view-based representation. The approach builds on and extends work on eigenspace representations, robust estimation techniques, and parameterized optical flow estimation. First, we note that the least-squares image reconstruction of standard eigenspace techniques has a number of problems and we reformulate the reconstruction problem as one of robust estimation. Second we define a “subspace constancy assumption” that allows us to exploit techniques for parameterized optical flow estimation to simultaneously solve for the view of an object and the affine transformation between the eigenspace and the image. To account for large affine transformations between the eigenspace and the image we define a multi-scale eigenspace representation and a coarse-to-fine matching strategy. Finally, we use these techniques\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/3-540-57956-7_34', '10.1007/BFb0015548', '10.1007/BF01421486', '10.1109/CVPR.1994.323814', '10.1016/0010-0285(89)90009-1'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'On the unification of line processes, outlier rejection, and robust statistics with applications in early vision': Paper(DOI='10.1007/bf00131148', crossref_json=None, google_schorlar_metadata=None, title='On the unification of line processes, outlier rejection, and robust statistics with applications in early vision', authors=['Michael J Black', 'Anand Rangarajan'], abstract=' The modeling of spatial discontinuities for problems such as surface recovery, segmentation, image reconstruction, and optical flow has been intensely studied in computer vision. While “line-process” models of discontinuities have received a great deal of attention, there has been recent interest in the use of robust statistical techniques to account for discontinuities. This paper unifies the two approaches. To achieve this we generalize the notion of a “line process” to that of an analog “outlier process” and show how a problem formulated in terms of outlier processes can be viewed in terms of robust statistics. We also characterize a class of robust statistical problems for which an equivalent outlier-process formulation exists and give a straightforward method for converting a robust estimation problem into an outlier-process formulation. We show how prior assumptions about the spatial structure of outliers can\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1080/00401706.1974.10489171', '10.1109/5.5962', '10.1109/CCV.1988.590039', '10.1109/CVPR.1991.139705', '10.1109/ICCV.1993.378214', '10.1006/cviu.1996.0006', '10.7551/mitpress/7132.001.0001', '10.2307/2346896', '10.1109/TPAMI.1986.4767851', '10.1007/BF00054995', '10.1109/TPAMI.1987.4767871', '10.1016/S0734-189X(87)80144-5', '10.1109/34.134040', '10.1007/BF00115697', '10.1109/TPAMI.1984.4767596', '10.1109/34.120331', '10.1109/34.56204', '10.1007/BF00054996', '10.1109/42.24868', '10.1016/0004-3702(81)90024-2', '10.1002/0471725250', '10.1007/BF00054839', '10.1109/23.273560', '10.1080/01621459.1987.10478393', '10.1007/BF00127126', '10.1109/TPAMI.1987.4767896', '10.1016/0146-664X(80)90049-0', '10.1109/34.56205', '10.1002/0471725382', '10.1109/34.42834', '10.1109/CVPR.1991.139688', '10.1109/WVM.1989.47097', '10.1109/34.107012', '10.1109/TPAMI.1986.4767807'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Hyperspectral and Medical Imaging'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Stochastic tracking of 3D human figures using 2D image motion': Paper(DOI='10.1007/3-540-45053-x_45', crossref_json=None, google_schorlar_metadata=None, title='Stochastic tracking of 3D human figures using 2D image motion', authors=['Hedvig Sidenbladh', 'Michael Black', 'David Fleet'], abstract=' A probabilistic method for tracking 3D articulated human figures in monocular image sequences is presented. Within a Bayesian framework, we define a generative model of image appearance, a robust likelihood function based on image graylevel differences, and a prior probability distribution over pose and joint angles that models how humans move. The posterior probability distribution over model parameters is represented using a discrete set of samples and is propagated over time using particle filtering. The approach extends previous work on parameterized optical flow estimation to exploit a complex 3D articulated motion model. It also extends previous work on human motion tracking by including a perspective camera model, by modeling limb self occlusion, and by recovering 3D motion from a monocular sequence. The explicit posterior probability distribution represents ambiguities due to image\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.1999.791271', '10.1109/ICPR.1996.546039', '10.1109/CVPR.1996.517056', '10.1016/0262-8856(83)90003-3', '10.1007/BFb0015549', '10.1109/CVPR.1996.517057', '10.1109/ICCV.1999.791203', '10.1007/978-1-4757-7107-7', '10.1109/34.598236'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Image Processing', 'Vision', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Fields of experts': Paper(DOI='10.7763/ijmlc.2014.v6.467', crossref_json=None, google_schorlar_metadata=None, title='Fields of experts', authors=['Stefan Roth', 'Michael J Black'], abstract=' We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach provides a practical method for learning high-order Markov random field (MRF) models with potential functions that extend over large pixel neighborhoods. These clique potentials are modeled using the Product-of-Experts framework that uses non-linear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field-of-Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/83.862633', '10.1109/TPAMI.2007.70844', '10.1007/s11263-008-0197-6', '10.1007/978-1-84882-491-1', '10.1162/089976602760128018', '10.1109/TIP.2003.819861', '10.1109/TIP.2003.818640', '10.1109/TIP.2007.901238', '10.1109/TCSVT.2005.844456'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Snakes: Active contour models': Paper(DOI='10.1007/bf00133570', crossref_json=None, google_schorlar_metadata=None, title='Snakes: Active contour models', authors=['Michael Kass', 'Andrew Witkin', 'Demetri Terzopoulos'], abstract=' A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the capture region surrounding a feature. Snakes provide a unified account of a number of visual problems, including detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest.', conference=None, journal=None, year=None, reference_list=['10.1145/355719.355728', '10.1109/TPAMI.1981.4767176', '10.1126/science.7367885', '10.1109/T-C.1973.223602', '10.1038/scientificamerican0476-48', '10.1098/rspb.1984.0030', '10.1098/rspb.1980.0020', '10.1098/rspb.1979.0029', '10.1145/359997.360004', '10.1038/317314a0', '10.2307/1420686', '10.1007/BF00127821', '10.1109/TPAMI.1986.4767807', '10.1016/B978-0-08-051581-6.50073-8', '10.1007/BF00337043', '10.1016/0031-3203(73)90042-3', '10.1109/TC.1977.1674848'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Image Processing', 'Signal Processing', 'Artificial Intelligence'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Deformable models in medical image analysis: a survey': Paper(DOI='10.1016/s1361-8415(96)80007-7', crossref_json=None, google_schorlar_metadata=None, title='Deformable models in medical image analysis: a survey', authors=['Tim McInerney', 'Demetri Terzopoulos'], abstract='This article surveys deformable models, a promising and vigorously researched computer-assisted medical image analysis technique. Among model-based techniques, deformable models offer a unique and powerful approach to image analysis that combines geometry, physics and approximation theory. They have proven to be effective in segmenting, matching and tracking anatomic structures by exploiting (bottom-up) constraints derived from the image data together with (top-down) a priori knowledge about the location, size and shape of these structures. Deformable models are capable of accommodating the significant variability of biological structures over time and across different individuals. Furthermore, they support highly intuitive interaction mechanisms that, when necessary, allow medical scientists and practitioners to bring their expertise to bear on the model-based image interpretation task. This article\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/0262-8856(92)90027-Z', '10.1109/34.57681', '10.1148/radiology.172.2.2748813', '10.1016/0262-8856(95)99717-F', '10.1016/S0734-189X(89)80014-3', '10.1109/34.24792', '10.1016/0262-8856(92)90028-2', '10.1109/42.293928', '10.1109/34.277589', '10.1016/1049-9660(92)90041-Z', '10.1016/1049-9660(91)90028-N', '10.1109/34.244675', '10.1016/0262-8856(94)90060-4', '10.1109/42.192695', '10.1109/42.370403', '10.1109/42.481446', '10.1016/0262-8856(94)90026-4', '10.1016/0262-8856(92)90065-B', '10.1109/T-C.1973.223602', '10.1097/00004728-199303000-00011', '10.1109/34.368194', '10.1007/BF01420985', '10.1016/0262-8856(92)90012-R', '10.1006/ciun.1994.1029', '10.1007/BF00133570', '10.1109/42.414606', '10.1109/42.276150', '10.1109/34.385980', '10.1109/34.216733', '10.1016/0734-189X(89)90107-2', '10.1109/42.370398', '10.1109/34.368173', '10.1016/0895-6111(94)00040-9', '10.1109/34.216727', '10.1145/127719.122742', '10.1109/83.277895', '10.1016/S1361-8415(01)80005-0', '10.1109/34.85661', '10.1109/34.85660', '10.1145/147156.147180', '10.1016/0031-3203(95)00021-Q', '10.1109/34.166621', '10.1145/125137.125155', '10.1016/S1361-8415(01)80003-7', '10.1007/BF00126502', '10.1109/TPAMI.1986.4767807', '10.1007/BF01908877', '10.1109/34.85659', '10.1016/0004-3702(88)90080-X', '10.1145/176579.176583', '10.1016/0031-3203(73)90042-3', '10.1148/radiology.188.1.8511281', '10.1109/42.414605', '10.1007/BF00127169'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Medical Image Analysis', 'Computer-Aided Design', 'Artificial Intelligence'], conference_acronym='Medical image analysis (Print)', publisher=None, query_handler=None),\n",
       " 'Regularization of inverse visual problems involving discontinuities': Paper(DOI='10.1109/tpami.1986.4767807', crossref_json=None, google_schorlar_metadata=None, title='Regularization of inverse visual problems involving discontinuities', authors=['Demetri Terzopoulos'], abstract='Inverse problems, such as the reconstruction problems that arise in early vision, tend to be mathematically ill-posed. Through regularization, they may be reformulated as well-posed variational principles whose solutions are computable. Standard regularization theory employs quadratic stabilizing functionals that impose global smoothness constraints on possible solutions. Discontinuities present serious difficulties to standard regularization, however, since their reconstruction requires a precise spatial control over the smoothing properties of stabilizers. This paper proposes a general class of controlled-continuity stabilizers which provide the necessary control over smoothness. These nonquadratic stabilizing functionals comprise multiple generalized spline kernels combined with (noncontinuous) continuity control functions. In the context of computational vision, they may be thought of as controlled-continuity\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/0734-189X(83)90020-8', '10.1007/BF02162161', '10.1016/B978-0-12-079050-0.50016-3', '10.1002/sapm1966451312', '10.1073/pnas.52.4.947', '10.1007/978-1-4612-5280-1', '10.1007/BF01601941', '10.1016/B978-0-12-079050-0.50015-1', '10.1214/aoms/1177697089', '10.1126/science.220.4598.671', '10.1016/0041-5553(66)90145-5', '10.1051/m2an/197610R300051', '10.1007/BFb0086566', '10.2307/2007474', '10.1109/TPAMI.1984.4767596', '10.1016/B978-0-12-079050-0.50011-4', '10.1016/0734-189X(83)90095-6', '10.2514/3.44330', '10.1073/pnas.81.10.3088', '10.1145/357318.357321', '10.1016/0734-189X(83)90096-8', '10.1016/0167-8655(83)90077-6', '10.1145/360924.360971', '10.1190/1.1440410', '10.1175/1520-0493(1980)108<1122:SNMMFV>2.0.CO;2', '10.1109/TC.1977.1674848', '10.1109/ICASSP.1984.1172729', '10.1109/TPAMI.1986.4767767'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Medical Image Analysis', 'Computer-Aided Design', 'Artificial Intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Multilinear analysis of image ensembles: Tensorfaces': Paper(DOI='10.1007/3-540-47969-4_30', crossref_json=None, google_schorlar_metadata=None, title='Multilinear analysis of image ensembles: Tensorfaces', authors=['M Alex O Vasilescu', 'Demetri Terzopoulos'], abstract=' Natural images are the composite consequence of multiple factors related to scene structure, illumination, and imaging. Multilinear algebra, the algebra of higher-order tensors, offers a potent mathematical framework for analyzing the multifactor structure of image ensembles and for addressing the difficult problem of disentangling the constituent factors or modes. Our multilinear modeling technique employs a tensor extension of the conventional matrix singular value decomposition (SVD), known as the N-mode SVD. As a concrete example, we consider the multilinear analysis of ensembles of facial images that combine several modes, including different facial geometries (people), expressions, head poses, and lighting conditions. Our resulting “TensorFaces” representation has several advantages over conventional eigenfaces. More generally, multilinear analysis shows promise as a unifying framework for\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BFb0015522', '10.1109/5.381842', '10.1007/BF02293984', '10.1137/S0895479800368354', '10.1364/JOSAA.9.001905', '10.1068/p250443', '10.1109/CVPR.1994.323814', '10.1364/JOSAA.4.000519', '10.1162/089976699300016728', '10.1007/BF02289464', '10.1162/jocn.1991.3.1.71'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Medical Image Analysis', 'Computer-Aided Design', 'Artificial Intelligence'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Constraints on deformable models: Recovering 3D shape and nonrigid motion': Paper(DOI='10.1016/0004-3702(88)90080-x', crossref_json=None, google_schorlar_metadata=None, title='Constraints on deformable models: Recovering 3D shape and nonrigid motion', authors=['Demetri Terzopoulos', 'Andrew Witkin', 'Michael Kass'], abstract='Inferring the 3D structures of nonrigidly moving objects from images is a difficult yet basic problem in computational vision. Our approach makes use of dynamic, elastically deformable object models that offer the geometric flexibility to satisfy a diversity of real-world visual constraints. We specialize these models to include intrinsic forces inducing a preference for axisymmetry. Image-based constraints are applied as extrinsic forces that mold the symmetry-seeking model into shapes consistent with image data. We describe an extrinsic force that applies constraints derived from profiles of monocularly viewed objects. We generalize this constraint force to incorporate profile information from multiple views and use it to exploit binocular image data. For time-varying images, the force becomes dynamic and the model is able to infer not only depth, but nonrigid motion as well. We demonstrate the recovery of 3D shape and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF00127821', '10.1007/BF00123162', '10.1007/BF00133570', '10.1016/0734-189X(83)90020-8', '10.1145/964965.808573', '10.1109/TC.1976.1674626', '10.1016/0004-3702(77)90006-6', '10.1016/0004-3702(81)90028-X', '10.1109/T-C.1973.223602', '10.1109/C-M.1981.220561', '10.1016/S0734-189X(83)80032-2', '10.1364/JOSAA.3.000242', '10.1068/p130255', '10.1109/TPAMI.1983.4767367', '10.1016/0146-664X(76)90028-9', '10.1016/0042-6989(79)90205-0', '10.1016/0734-189X(83)90097-X', '10.1364/JOSAA.2.000343', '10.1109/TPAMI.1986.4767806', '10.1109/TPAMI.1986.4767807', '10.1177/027836498400300302'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Image Processing', 'Signal Processing', 'Artificial Intelligence'], conference_acronym='Artificial intelligence (General ed.)', publisher=None, query_handler=None),\n",
       " 'Physically‐based facial modelling, analysis, and animation': Paper(DOI='10.1002/vis.4340010208', crossref_json=None, google_schorlar_metadata=None, title='Physically‐based facial modelling, analysis, and animation', authors=['Demetri Terzopoulos', 'Keith Waters'], abstract=' We develop a new 3D hierarchical model of the human face. The model incorporates a physically‐based approximation to facial tissue and a set of anatomically‐motivated facial muscle actuators. Despite its sophistication, the model is efficient enough to produce facial animation at interactive rates on a high‐end graphics workstation. A second contribution of this paper is a technique for estimating muscle contractions from video sequences of human faces performing expressive articulations. These estimates may be input as dynamic control parameters to the face model in order to produce realistic animation. Using an example, we demonstrate that our technique yields sufficiently accurate muscle contraction estimates for the model to reconstruct expressions from dynamic images of faces.', conference=None, journal=None, year=None, reference_list=['10.1145/965161.806812', '10.1109/MCG.1982.1674492', '10.1145/965161.806812', '10.1145/37402.37405', '10.1007/BF01914864', '10.1007/BF01914861', '10.1088/0031-9155/20/5/001', '10.1288/00005537-198604000-00012', '10.1288/00005537-198604000-00014', '10.1007/BF01908877', '10.1016/S0095-4470(19)30777-6', '10.1145/29933.30874', '10.1007/BF01914863', '10.1145/378456.378522', '10.1007/BF00133570'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Medical Image Analysis', 'Computer-Aided Design', 'Artificial Intelligence'], conference_acronym='The Journal of visualization and computer animation (Print)', publisher=None, query_handler=None),\n",
       " 'Analysis and synthesis of facial image sequences using physical and anatomical models': Paper(DOI='10.1109/34.216726', crossref_json=None, google_schorlar_metadata=None, title='Analysis and synthesis of facial image sequences using physical and anatomical models', authors=['Demetri Terzopoulos', 'Keith Waters'], abstract='An approach to the analysis of dynamic facial images for the purposes of estimating and resynthesizing dynamic facial expressions is presented. The approach exploits a sophisticated generative model of the human face originally developed for realistic facial animation. The face model which may be simulated and rendered at interactive rates on a graphics workstation, incorporates a physics-based synthetic facial tissue and a set of anatomically motivated facial muscle actuators. The estimation of dynamical facial muscle contractions from video sequences of expressive human faces is considered. An estimation technique that uses deformable contour models (snakes) to track the nonrigid motions of facial features in video images is developed. The technique estimates muscle actuator controls with sufficient accuracy to permit the face model to resynthesize transient expressions.< >', conference=None, journal=None, year=None, reference_list=['10.1145/37402.37405', '10.1109/CVPR.1989.37836', '10.1007/BF01914864', '10.1145/97879.97906', '10.1002/vis.4340020405', '10.1109/29.45550', '10.1002/j.1538-7305.1972.tb01927.x', '10.1007/BF01914863', '10.1007/BF00133570', '10.1016/B978-0-12-737140-5.50017-8', '10.1088/0031-9155/20/5/001', '10.1007/BF01914861', '10.1007/BF01908877', '10.1109/ICASSP.1991.150968', '10.1109/ICCV.1990.139628', '10.1109/T-C.1973.223602', '10.1016/0923-5965(89)90006-4', '10.1288/00005537-198604000-00012', '10.1145/29933.30874', '10.1109/MCG.1982.1674492', '10.1109/CVPR.1991.139756', '10.1016/0031-3203(69)90006-5'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Medical Image Analysis', 'Computer-Aided Design', 'Artificial Intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'The computation of visible-surface representations': Paper(DOI='10.1109/34.3908', crossref_json=None, google_schorlar_metadata=None, title='The computation of visible-surface representations', authors=['Demetri Terzopoulos'], abstract=\"A computational theory of visible-surface representations is developed. The visible-surface reconstruction process that computes these quantitative representations unifies formal solutions to the key problems of: (1) integrating multiscale constraints on surface depth and orientation from multiple-visual sources; (2) interpolating dense, piecewise-smooth surfaces from these constraints; (3) detecting surface depth and orientation discontinuities to apply boundary conditions on interpolation; and (4) structuring large-scale, distributed-surface representations to achieve computational efficiency. Visible-surface reconstruction is an inverse problem. A well-posed variational formulation results from the use of a controlled-continuity surface model. Discontinuity detection amounts to the identification of this generic model's distributed parameters from the data. Finite-element shape primitives yield a local discretization of the\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.1987.4767918', '10.1016/0734-189X(84)90033-1', '10.1007/978-3-662-02427-0', '10.1016/0734-189X(85)90163-X', '10.1073/pnas.83.12.4263', '10.1016/0004-3702(81)90022-9', '10.1137/1015003', '10.1016/0734-189X(83)90095-6', '10.1109/TC.1977.1674848', '10.7551/mitpress/3132.001.0001', '10.1007/978-3-642-51590-3_14', '10.1109/TPAMI.1985.4767615', '10.1016/0167-8396(85)90011-1', '10.1109/TPAMI.1984.4767596', '10.1016/0734-189X(83)90020-8', '10.1016/0262-8856(86)90029-6', '10.1109/TPAMI.1986.4767769', '10.1109/TPAMI.1986.4767807', '10.1109/TPAMI.1986.4767767', '10.1016/0734-189X(83)90096-8', '10.1016/0734-189X(85)90001-5', '10.1190/1.1440410', '10.1126/science.7367885', '10.1098/rspb.1985.0020', '10.1016/0004-3702(81)90021-7', '10.1016/0167-8655(83)90077-6', '10.1016/0734-189X(86)90220-3', '10.1190/1.1440688', '10.1109/PROC.1976.10066', '10.1098/rspb.1978.0020', '10.1038/317314a0'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Medical Image Analysis', 'Computer-Aided Design', 'Artificial Intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'T-snakes: Topology adaptive snakes': Paper(DOI='10.1016/s1361-8415(00)00008-6', crossref_json=None, google_schorlar_metadata=None, title='T-snakes: Topology adaptive snakes', authors=['Tim McInerney', 'Demetri Terzopoulos'], abstract='We present a new class of deformable contours (snakes) and apply them to the segmentation of medical images. Our snakes are defined in terms of an affine cell image decomposition (ACID). The ‘snakes in ACID’ framework significantly extends conventional snakes, enabling topological flexibility among other features. The resulting topology adaptive snakes, or ‘T-snakes’, can be used to segment some of the most complex-shaped biological structures from medical images in an efficient and highly automated manner.', conference=None, journal=None, year=None, reference_list=['10.1016/S1361-8415(97)85005-0', '10.1016/0734-189X(84)90035-5', '10.1007/BF01385685', '10.1109/34.244675', '10.1007/BF01898405', '10.1117/12.179272', '10.1007/BF00133570', '10.1016/0734-189X(89)90165-5', '10.1016/S1361-8415(99)80006-1', '10.1109/42.370398', '10.1145/37402.37422', '10.1007/BFb0056309', '10.1109/34.368173', '10.1016/0895-6111(94)00040-9', '10.1007/978-3-540-49197-2_11', '10.1016/S1361-8415(96)80007-7', '10.1109/38.252552', '10.1016/0021-9991(88)90002-2', '10.1117/12.185238', '10.1117/12.49885', '10.1117/12.216429', '10.1016/0004-3702(88)90080-X', '10.1117/12.185173'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Medical Image Analysis', 'Computer-Aided Design', 'Artificial Intelligence'], conference_acronym='Medical image analysis (Print)', publisher=None, query_handler=None),\n",
       " 'A dynamic finite element surface model for segmentation and tracking in multidimensional medical images with application to cardiac 4D image analysis': Paper(DOI='10.1016/0895-6111(94)00040-9', crossref_json=None, google_schorlar_metadata=None, title='A dynamic finite element surface model for segmentation and tracking in multidimensional medical images with application to cardiac 4D image analysis', authors=['Tim McInerney', 'Demetri Terzopoulos'], abstract=\"This paper presents a physics-based approach to anatomical surface segmentation, reconstruction, and tracking in multidimensional medical images. The approach makes use of a dynamic “balloon” model—a spherical thin-plate under tension surface spline which deforms elastically to fit the image data. The fitting process is mediated by internal forces stemming from the elastic properties of the spline and external forces which are produced from the data. The forces interact in accordance with Lagrangian equations of motion that adjust the model's deformational degrees of freedom to fit the data. We employ the finite element method to represent the continuous surface in the form of weighted sums of local polynomial basis functions. We use a quintic triangular finite element whose nodal variables include positions as well as the first and second partial derivatives of the surface. We describe a system, implemented\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1109/42.293928', '10.1145/37402.37422', '10.1109/34.57680', '10.1016/0734-189X(86)90004-6', '10.1145/378456.378484', '10.1016/0004-3702(88)90080-X', '10.1109/34.85659', '10.1109/34.85661', '10.1016/1049-9660(91)90028-N', '10.1109/CVPR.1991.139737', '10.1109/34.134061', '10.1109/34.216727', '10.1016/0734-189X(83)90020-8', '10.1109/TPAMI.1986.4767807', '10.1016/0734-189X(89)90059-5', '10.1109/CVPR.1991.139807', '10.1109/CVPR.1992.223130', '10.1145/127719.122742', '10.1109/CVPR.1989.37825', '10.1148/radiology.172.2.2748813', '10.1109/CVPR.1992.223158', '10.1117/12.59560', '10.1109/34.216732'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Medical Image Analysis', 'Computer-Aided Design', 'Artificial Intelligence'], conference_acronym='Computerized medical imaging and graphics (Print)', publisher=None, query_handler=None),\n",
       " 'Playing for Data: Ground Truth from Computer Games': Paper(DOI='10.1007/978-3-319-46475-6_7', crossref_json=None, google_schorlar_metadata=None, title='Playing for Data: Ground Truth from Computer Games', authors=['Stephan R Richter', 'Vibhav Vineet', 'Stefan Roth', 'Vladlen Koltun'], abstract=' Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1201/b10644', '10.1109/CVPR.2014.487', '10.1109/ICCV.2015.329', '10.1007/s11263-010-0390-2', '10.1007/BF01420984', '10.1016/j.patrec.2008.04.005', '10.1007/978-3-642-33783-3_44', '10.1109/ICCV.2015.312', '10.1109/CVPR.2016.350', '10.1109/ICCV.2015.316', '10.1177/0278364913491297', '10.1007/978-3-642-40602-7_35', '10.1109/CVPR.2016.442', '10.1109/ICRA.2014.6907054', '10.1016/0004-3702(81)90024-2', '10.1109/ICCV.2011.6126508', '10.1109/CVPR.2016.345', '10.1109/CVPR.2008.4587614', '10.1109/CVPR.2015.7299057', '10.1109/CVPR.2015.7298965', '10.1109/CVPR.2010.5540218', '10.1109/CVPR.2016.648', '10.1109/CVPR.2016.438', '10.1109/ICCV.2015.95', '10.1109/ICCV.2015.151', '10.1109/TPAMI.2015.2408347', '10.1109/ICCV.2003.1238308', '10.1016/j.cag.2015.09.001', '10.1109/WACV.2015.38', '10.1145/97880.97901', '10.1145/2702123.2702179', '10.1109/TPAMI.2012.241', '10.1007/s11263-007-0109-1', '10.5244/C.24.106', '10.5244/C.23.62', '10.1109/CVPR.2007.383518', '10.1007/s11263-012-0574-z', '10.1109/ISOCC.2015.7401766', '10.1109/TPAMI.2013.163', '10.1109/CVPR.2016.401', '10.1109/TITS.2014.2310138', '10.5244/C.30.26'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'robotics', 'computer graphics'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Learning Agile and Dynamic Motor Skills for Legged Robots': Paper(DOI='10.1126/scirobotics.aau5872', crossref_json=None, google_schorlar_metadata=None, title='Learning Agile and Dynamic Motor Skills for Legged Robots', authors=['Jemin Hwangbo', 'Joonho Lee', 'Alexey Dosovitskiy', 'Dario Bellicoso', 'Vassilios Tsounis', 'Vladlen Koltun', 'Marco Hutter'], abstract='Legged robots pose one of the greatest challenges in robotics. Dynamic and agile maneuvers of animals cannot be imitated by existing methods that are crafted by humans. A compelling alternative is reinforcement learning, which requires minimal craftsmanship and promotes the natural evolution of a control policy. However, so far, reinforcement learning research for legged robots is mainly limited to simulation, and only few and comparably simple examples have been deployed on real systems. The primary reason is that training with real robots, particularly with dynamically balancing systems, is complicated and expensive. In the present work, we introduce a method for training a neural network policy in simulation and transferring it to a state-of-the-art legged system, thereby leveraging fast, automated, and cost-effective data generation schemes. The approach is applied to the ANYmal robot, a sophisticated\\xa0…', conference=None, journal=None, year=None, reference_list=['10.3182/20080706-5-KR-1001.01833', '10.7210/jrsj.30.372', '10.1109/ICRA.2013.6631038', '10.1109/IROS.2016.7758092', '10.1242/jeb.202.23.3325', '10.1115/1.3139652', '10.1109/ICHR.2006.321385', '10.1023/A:1008844026298', '10.1109/ROBOT.2010.5509805', '10.1109/LRA.2018.2794620', '10.1109/LRA.2017.2665685', '10.1145/2185520.2185539', '10.1109/ICRA.2017.7989016', '10.1177/0278364913506757', '10.1109/LRA.2018.2852785', '10.1177/0278364917710318', '10.1145/3072959.3073602', '10.1109/IROS.2018.8593722', '10.1109/LRA.2018.2799426', '10.1142/9789813149137_0055', '10.1126/science.1133687', '10.1163/016918609X12529286896877', '10.1109/LAB-RS.2008.16', '10.1061/(ASCE)0733-9399(1996)122:10(966)', '10.1109/IROS.2015.7354126', '10.1109/ICRA.2018.8460528', '10.1007/3-540-59496-5_337', '10.1109/MRA.2015.2505910', '10.1109/LRA.2018.2792536', '10.1109/ICRA.2015.7139918', '10.1109/LRA.2017.2723931', '10.1109/ICCSCE.2011.6190579', '10.1177/02783640122067570', '10.1016/S0921-8890(01)00113-0', '10.1109/ICRA.2013.6630645', '10.1109/LRA.2017.2720851', '10.7551/mitpress/9816.003.0008', '10.1145/3099564.3099567', '10.1145/3197517.3201397', '10.1145/1553374.1553380'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'robotics', 'computer graphics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer': Paper(DOI='10.1109/tpami.2020.3019967', crossref_json=None, google_schorlar_metadata=None, title='Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer', authors=['René Ranftl', 'Katrin Lasinger', 'David Hafner', 'Konrad Schindler', 'Vladlen Koltun'], abstract='The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks. Armed with these tools, we experiment with five diverse training datasets, including a new, massive data source: 3D films. To demonstrate the generalization power of our approach we use  zero-shot cross-dataset transfer , i.e. we evaluate on datasets that were not\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2019.00575', '10.1109/CVPR.2019.00465', '10.1109/3DV.2019.00046', '10.1109/CVPR.2018.00040', '10.1109/TIP.2018.2836318', '10.1109/IROS.2012.6385773', '10.1007/978-3-642-33783-3_44', '10.1109/CVPR.2017.272', '10.1109/ICCV.2015.304', '10.1109/CVPR.2017.243', '10.1109/CVPR.2017.634', '10.1007/978-3-030-01216-8_12', '10.1109/CVPR.2012.6248074', '10.1109/CVPR.2016.85', '10.1609/aaai.v33i01.33018001', '10.1177/0301006620908207', '10.1109/CVPR.2015.7298925', '10.1145/1073204.1073232', '10.1126/scirobotics.aaw6661', '10.1109/CVPR.2018.00024', '10.1007/978-3-030-01252-6_30', '10.1109/CVPR.2018.00043', '10.1109/CVPR.2018.00594', '10.1109/CVPR.2017.700', '10.1007/978-3-319-46493-0_51', '10.1007/978-3-642-12392-4_2', '10.1109/3DV.2017.00012', '10.1109/CVPR.2009.5206848', '10.1109/CVPR.2016.90', '10.1109/CVPR.2018.00931', '10.1117/12.823927', '10.1109/CVPR.2017.699', '10.1109/CVPR.2018.00218', '10.1109/CVPR.2011.5995347', '10.1109/TPAMI.2014.2316835', '10.1109/3DV.2016.32', '10.1109/CVPR.2016.594', '10.1109/CVPR.2015.7299152', '10.1109/CVPR.2018.00214', '10.1109/TPAMI.2008.132', '10.3390/s120201437', '10.1109/ICAR.2015.7251485', '10.1007/978-1-4471-4658-2', '10.1007/s11263-016-0917-2', '10.1007/978-3-319-46484-8_45', '10.1109/CVPR.2017.261', '10.1109/CVPR.2016.350', '10.1145/3072959.3073599', '10.1109/CVPR.2017.596', '10.1109/ICCV.2019.00907', '10.1109/CVPR.2015.7298655', '10.1109/CVPR.2019.01210'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'robotics', 'computer graphics'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Tanks and Temples: Benchmarking Large-Scale Scene Reconstruction': Paper(DOI='10.1145/3072959.3073599', crossref_json=None, google_schorlar_metadata=None, title='Tanks and Temples: Benchmarking Large-Scale Scene Reconstruction', authors=['Arno Knapitsch', 'Jaesik Park', 'Qian-Yi Zhou', 'Vladlen Koltun'], abstract='We present a benchmark for image-based 3D reconstruction. The benchmark sequences were acquired outside the lab, in realistic conditions. Ground-truth data was captured using an industrial laser scanner. The benchmark includes both outdoor scenes and indoor environments. High-resolution video sequences are provided as input, supporting the development of novel pipelines that take advantage of video input to increase reconstruction fidelity. We report the performance of many image-based 3D reconstruction pipelines on the new benchmark. The results point to exciting challenges and opportunities for future work.', conference=None, journal=None, year=None, reference_list=['10.1007/s11263-016-0902-9', '10.1145/2001269.2001293', '10.1007/978-3-642-15552-9_3', '10.1145/2451236.2451246', '10.1177/0278364915620033', '10.1109/CVPR.2015.7299195', '10.1007/978-3-319-10605-2_54', '10.1016/j.cag.2015.09.003', '10.1109/ICCV.2009.5459145', '10.1109/CVPR.2010.5539802', '10.1561/0600000052', '10.1109/TPAMI.2009.161', '10.1177/0278364913491297', '10.1109/ICCV.2007.4408933', '10.1109/CVPR.2015.7298949', '10.1109/ICCV.2015.156', '10.1109/CVPR.2011.5995693', '10.1109/CVPR.2014.504', '10.1007/978-3-319-46487-9_29', '10.1007/978-3-540-88682-2_33', '10.1109/CVPR.2009.5206539', '10.1109/ICCV.2007.4409218', '10.1007/978-3-319-56414-2_5', '10.1007/s11263-007-0086-4', '10.1109/CVPR.2016.445', '10.1007/978-3-319-46487-9_31', '10.1109/3DV.2015.40', '10.1109/CVPR.2017.272', '10.1109/CVPR.2006.19', '10.1007/s11263-007-0107-3', '10.1109/CVPR.2008.4587706', '10.1109/IROS.2012.6385773', '10.1145/2733373.2807405', '10.1109/ICCV.2013.15', '10.1007/s00138-011-0346-8', '10.1007/3-540-44480-7_21', '10.1109/34.88573', '10.1016/j.imavis.2011.01.006', '10.1109/TPAMI.2011.172', '10.1145/2999533', '10.1109/CVPR.2012.6247833', '10.1109/CVPR.2011.5995552', '10.1007/s11263-014-0711-y', '10.1145/2461912.2461919'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'robotics', 'computer graphics'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " 'Columbia object image library (coil-20)': Paper(DOI='10.1134/s1054661809030237', crossref_json=None, google_schorlar_metadata=None, title='Columbia object image library (coil-20)', authors=['Sameer A Nene', 'Shree K Nayar', 'Hiroshi Murase'], abstract='Columbia Object Image Library (COIL-20) is a database of gray-scale images of 20 objects. The objects were placed on a motorized turntable against a black background. The turntable was rotated through 360 degrees to vary object pose with respect to a xed camera. Images of the objects were taken at pose intervals of 5 degrees. This corresponds to 72 images per object. The database has two sets of images. The rst set contains 720 unprocessed images of 10 objects. The second contains 1,440 size normalized images of 20 objects. COIL-20 is available online via ftp. i', conference=None, journal=None, year=None, reference_list=['10.1109/JPROC.2004.840303', '10.1109/IPDPS.2001.925089', '10.1109/IPDPS.2002.1016511', '10.1007/3-540-57233-3_110'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image recognition', 'Computer vision', 'pattern recognition'], conference_acronym='Pattern recognition and image analysis', publisher=None, query_handler=None),\n",
       " 'Generalized assorted pixel camera: postcapture control of resolution, dynamic range, and spectrum': Paper(DOI='10.1109/tip.2010.2046811', crossref_json=None, google_schorlar_metadata=None, title='Generalized assorted pixel camera: postcapture control of resolution, dynamic range, and spectrum', authors=['Fumihito Yasuma', 'Tomoo Mitsunaga', 'Daisuke Iso', 'Shree K Nayar'], abstract='We propose the concept of a generalized assorted pixel (GAP) camera, which enables the user to capture a single image of a scene and, after the fact, control the tradeoff between spatial resolution, dynamic range and spectral detail. The GAP camera uses a complex array (or mosaic) of color filters. A major problem with using such an array is that the captured image is severely under-sampled for at least some of the filter types. This leads to reconstructed images with strong aliasing. We make four contributions in this paper: 1) we present a comprehensive optimization method to arrive at the spatial and spectral layout of the color filter array of a GAP camera. 2) We develop a novel algorithm for reconstructing the under-sampled channels of the image while minimizing aliasing artifacts. 3) We demonstrate how the user can capture a single image and then control the tradeoff of spatial resolution to generate a variety\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TIP.2008.2002164', '10.1111/j.1520-6378.1994.tb00053.x', '10.1364/JOSAA.6.000318', '10.1109/83.597274', '10.2352/J.ImagingSci.Technol.2002.46.6.art00003', '10.1109/ICCV.2007.4409090', '10.1109/TPAMI.2005.76', '10.1117/12.642425', '10.1364/OPEX.12.001643', '10.1109/TIP.2004.841200', '10.1109/CVPR.2000.855857', '10.2352/CIC.2002.10.1.art00064', '10.1109/ICIP.2006.313073', '10.1109/MSP.2005.1407714', '10.1109/TIP.2002.801121', '10.1109/ICIP.2003.1247333'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computational Imaging', 'Computational Photography', 'Computer Graphics', 'Robotics'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Detection and removal of rain from videos': Paper(DOI='10.3724/sp.j.1004.2013.01093', crossref_json=None, google_schorlar_metadata=None, title='Detection and removal of rain from videos', authors=['Kshitiz Garg', 'Shree K Nayar'], abstract='The visual effects of rain are complex. Rain consists of spatially distributed drops falling at high velocities. Each drop refracts and reflects the environment, producing sharp intensity changes in an image. A group of such falling drops creates a complex time varying signal in images and videos. In addition, due to the finite exposure time of the camera, intensities due to rain are motion blurred and hence depend on the background intensities. Thus, the visual manifestations of rain are a combination of both the dynamics of rain and the photometry of the environment. In this paper, we present the first comprehensive analysis of the visual effects of rain on an imaging system. We develop a correlation model that captures the dynamics of rain and a physics-based motion blur model that explains the photometry of rain. Based on these models, we develop efficient algorithms for detecting and removing rain from videos. The\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computational Imaging', 'Computational Photography', 'Computer Graphics', 'Robotics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Generative Visual Manipulation on the Natural Image Manifold': Paper(DOI='10.1007/978-3-319-46454-1_36', crossref_json=None, google_schorlar_metadata=None, title='Generative Visual Manipulation on the Natural Image Manifold', authors=['Jun-Yan Zhu', 'Philipp Krähenbühl', 'Eli Shechtman', 'Alexei A Efros'], abstract=' Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to “fall off” the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/38.946629', '10.1145/1015706.1015780', '10.1145/344779.344859', '10.1145/1618452.1618472', '10.1109/CVPR.2010.5540159', '10.1145/2010324.1964956', '10.1038/381607a0', '10.1023/A:1026553619983', '10.1109/ICCV.2011.6126278', '10.1109/ICCV.2015.449', '10.1126/science.1127647', '10.1145/1390156.1390294', '10.1109/CVPR.2015.7298761', '10.1007/978-3-319-46475-6_43', '10.1109/CVPR.2009.5206848', '10.1137/0916069', '10.1007/978-3-540-24673-2_3', '10.1023/B:VISI.0000045324.43199.43', '10.1145/2508363.2508419', '10.1007/978-3-642-15549-9_1', '10.1109/ICCV.2011.6126281', '10.1109/CVPR.2014.32', '10.1145/237170.237196', '10.1145/2502081.2502281', '10.1109/TPAMI.2010.147', '10.1109/CVPR.2013.299'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Computational Photography'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Video-to-Video Synthesis': Paper(DOI='10.2200/s00061ed1v01y200707ivm010', crossref_json=None, google_schorlar_metadata=None, title='Video-to-Video Synthesis', authors=['Ting-Chun Wang', 'Ming-Yu Liu', 'Jun-Yan Zhu', 'Guilin Liu', 'Andrew Tao', 'Jan Kautz', 'Bryan Catanzaro'], abstract='We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image synthesis problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without understanding temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a novel video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generator and discriminator architectures, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our approach to future video prediction, outperforming several state-of-the-art competing systems.', conference=None, journal=None, year=None, reference_list=['10.1109/5.899055', '10.1109/TIP.2005.860353', '10.1109/TMM.2005.850989', '10.1109/TCSVT.2002.800309', '10.1109/TIP.2005.860600', '10.1016/j.image.2005.02.002', '10.1109/TCSVT.2003.814966', '10.1109/5.904503', '10.1109/6046.944477', '10.1109/ICASSP.2002.1006157', '10.1109/79.855913', '10.1109/83.217221', '10.1109/83.334987', '10.1109/79.733495', '10.1007/978-1-4757-2566-7', '10.1002/j.1538-7305.1948.tb01338.x', '10.17487/rfc2474', '10.1109/JPROC.2002.802000', '10.1109/JSAC.2003.815015', '10.1109/6046.909598', '10.1109/JSAC.2003.815231', '10.1109/JPROC.2004.839603', '10.1109/MCOM.2003.1235598', '10.1109/JSAC.2003.816445', '10.1109/MWC.2005.1497855', '10.1002/wcm.469', '10.17487/rfc3550', '10.1145/99517.99553', '10.17487/rfc2736', '10.17487/rfc2038', '10.17487/rfc3016', '10.17487/rfc2032', '10.17487/rfc3984', '10.1109/76.911161', '10.1109/76.867930', '10.1109/TMM.2005.864313', '10.1109/6046.909591', '10.1109/TCSVT.2003.819186', '10.1109/TCSVT.2004.831966', '10.1201/9781482290097', '10.1109/26.848555', '10.1109/5.664283', '10.1109/49.848249', '10.1109/5.790632', '10.1109/49.848250', '10.1109/49.848251', '10.1109/TIP.2002.802507', '10.1109/ICME.2003.1221569', '10.1109/6046.909594', '10.1109/ICME.2003.1220841', '10.1016/0169-7552(95)00051-8', '10.17487/rfc1633', '10.1109/65.238150', '10.1109/83.388074', '10.1109/TIP.2003.819861', '10.1109/TIP.2006.881959', '10.1109/TCSVT.2002.800313', '10.1109/49.848255', '10.1109/ICIP.2005.1530203', '10.1002/0471200611', '10.1109/18.53739', '10.1109/5.790634', '10.1109/JPROC.2004.839621', '10.1109/83.826773', '10.1109/TIP.2003.809006', '10.1109/JSAC.2003.815394', '10.1109/ICC.2004.1312726', '10.1109/TCSVT.2003.815165', '10.1109/78.258085', '10.1109/76.499834', '10.1109/76.744279', '10.1109/76.157160', '10.1109/83.334985', '10.1109/83.743851', '10.1016/j.image.2004.05.002', '10.1016/j.image.2004.05.006', '10.1109/TIP.2003.819433', '10.1109/6046.966110', '10.1109/18.556670', '10.1109/83.908500', '10.1109/TCSVT.2003.814969', '10.1109/76.488825', '10.1109/79.733497', '10.1109/83.902290', '10.1109/ICASSP.2007.366039', '10.1109/90.392383', '10.1109/25.293655', '10.1109/26.2763', '10.1002/j.1538-7305.1960.tb03959.x', '10.1109/25.350282', '10.1109/26.803503', '10.1002/dac.580', '10.1137/0108018', '10.1109/MSP.2004.1311137', '10.1117/12.453072', '10.1109/9780470546345', '10.1109/25.820695', '10.1117/12.532294', '10.1109/49.848250', '10.1109/JSAC.2003.816455', '10.1109/76.499834', '10.1109/TCSVT.2002.800336', '10.1109/JSAC.2003.815228', '10.1109/TCOMM.2004.836436', '10.1109/TWC.2005.847026', '10.1109/JSAC.2007.070511', '10.1109/ICME.2007.4284811'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'Computer graphics'], conference_acronym='Synthesis lectures on image, video, and multimedia processing', publisher=None, query_handler=None),\n",
       " 'Contrastive Learning for Unpaired Image-to-Image Translation': Paper(DOI='10.2139/ssrn.4038043', crossref_json=None, google_schorlar_metadata=None, title='Contrastive Learning for Unpaired Image-to-Image Translation', authors=['Taesung Park', 'Alexei A. Efros', 'Richard Zhang', 'Jun-Yan Zhu'], abstract='   In image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so – maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Deep Learning', 'Computer Graphics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Learning the signatures of the human grasp using a scalable tactile glove': Paper(DOI='10.1088/1674-4926/40/7/070202', crossref_json=None, google_schorlar_metadata=None, title='Learning the signatures of the human grasp using a scalable tactile glove', authors=['Subramanian Sundaram', 'Petr Kellnhofer', 'Yunzhu Li', 'Jun-Yan Zhu', 'Antonio Torralba', 'Wojciech Matusik'], abstract='Humans can feel, weigh and grasp diverse objects, and simultaneously infer their material properties while applying the right amount of force—a challenging set of tasks for a modern robot. Mechanoreceptor networks that provide sensory feedback and enable the dexterity of the human grasp remain difficult to replicate in robots. Whereas computer-vision-based robot grasping strategies– have progressed substantially with the abundance of visual data and emerging machine-learning tools, there are as yet no equivalent sensing platforms and large-scale datasets with which to probe the use of the tactile information that humans rely on when grasping objects. Studying the mechanics of how humans grasp objects will complement vision-based robotic object handling. Importantly, the inability to record and analyse tactile signals currently limits our understanding of the role of tactile information in the human grasp\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Computational Photography'], conference_acronym='Journal of Semiconductors', publisher=None, query_handler=None),\n",
       " 'Real-time User-guided Image Colorization with Learned Deep Priors': Paper(DOI='10.1145/3072959.3073703', crossref_json=None, google_schorlar_metadata=None, title='Real-time User-guided Image Colorization with Learned Deep Priors', authors=['Richard Zhang', 'Jun-Yan Zhu', 'Phillip Isola', 'Xinyang Geng', 'Angela S Lin', 'Tianhe Yu', 'Alexei A Efros'], abstract='We propose a deep learning approach for user-guided image colorization. The system directly maps a grayscale image, along with sparse, local user \"hints\" to an output colorization with a Convolutional Neural Network (CNN). Rather than using hand-defined rules, the network propagates user edits by fusing low-level cues along with high-level semantic information, learned from large-scale data. We train on a million images, with simulated user inputs. To guide the user towards efficient input selection, the system recommends likely colors based on the input image and current user inputs. The colorization is performed in a single feed-forward pass, enabling real-time use. Even with randomly simulated user inputs, we show that the proposed system helps novice users quickly create realistic colorizations, and offers large improvements in colorization quality with just a minute of use. In addition, we demonstrate that the framework can incorporate other user \"hints\" to the desired colorization, showing an application to color histogram transfer. Our code and models are available at https://richzhang.github.io/ideepcolor.', conference=None, journal=None, year=None, reference_list=['10.1145/1531326.1531330', '10.1007/978-3-319-46487-9_38', '10.1109/CVPR.2015.7298970', '10.1145/2766978', '10.1007/978-3-540-88690-7_10', '10.1145/2366145.2366151', '10.1145/1273496.1273517', '10.1109/ICCV.2015.55', '10.1145/2024156.2024190', '10.1109/ICCV.2015.72', '10.1109/CVPR.2016.265', '10.1145/2980179.2982399', '10.1109/CVPR.2014.81', '10.1145/2393347.2393402', '10.1109/CVPR.2015.7298642', '10.1109/CVPR.2009.5206835', '10.1145/383259.383295', '10.1145/1101149.1101223', '10.1214/aoms/1177703732', '10.1145/2897824.2925974', '10.1007/978-3-319-46493-0_35', '10.1109/5.726791', '10.1145/1186562.1015780', '10.1007/s41095-015-0013-5', '10.1145/1457515.1409105', '10.5555/2383847.2383887', '10.1145/1599301.1599333', '10.1145/1179352.1142017', '10.1109/38.946629', '10.1007/978-3-319-24574-4_28', '10.1007/s11263-015-0816-y', '10.1145/2897824.2925968', '10.1145/2897824.2925972', '10.1145/1882261.1866172', '10.1145/566654.566576', '10.1109/ICCV.2015.164', '10.1145/1618452.1618464', '10.1145/2508363.2508404', '10.1145/2790296', '10.1007/978-3-319-46454-1_36'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Computational Photography'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " 'GAN Dissection: Visualizing and Understanding Generative Adversarial Networks': Paper(DOI='10.1145/3329781.3329783', crossref_json=None, google_schorlar_metadata=None, title='GAN Dissection: Visualizing and Understanding Generative Adversarial Networks', authors=['David Bau', 'Jun-Yan Zhu', 'Hendrik Strobelt', 'Bolei Zhou', 'Joshua B Tenenbaum', 'William T Freeman', 'Antonio Torralba'], abstract='Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We provide open source interpretation tools to help peer researchers and practitioners better understand their GAN models.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'NLP', 'Software Engineering', 'HCI'], conference_acronym='ACM queue', publisher=None, query_handler=None),\n",
       " 'Weakly supervised histopathology cancer image segmentation and classification': Paper(DOI='10.1016/j.media.2014.01.010', crossref_json=None, google_schorlar_metadata=None, title='Weakly supervised histopathology cancer image segmentation and classification', authors=['Yan Xu', 'Jun-Yan Zhu', 'I Eric', 'Chao Chang', 'Maode Lai', 'Zhuowen Tu'], abstract='Labeling a histopathology image as having cancerous regions or not is a critical task in cancer diagnosis; it is also clinically important to segment the cancer tissues and cluster them into various classes. Existing supervised approaches for image classification and segmentation require detailed manual annotations for the cancer pixels, which are time-consuming to obtain. In this paper, we propose a new learning method, multiple clustered instance learning (MCIL) (along the line of weakly supervised learning) for histopathology image segmentation. The proposed MCIL method simultaneously performs image-level classification (cancer vs. non-cancer image), medical image segmentation (cancer vs. non-cancer tissue), and patch-level clustering (different classes). We embed the clustering concept into the multiple instance learning (MIL) setting and derive a principled solution to performing the above three tasks in\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-642-02230-2_7', '10.1109/TBME.2009.2033804', '10.1109/TIP.2010.2048612', '10.1109/TBME.2010.2053540', '10.1109/TPAMI.2010.226', '10.1016/S0004-3702(96)00034-3', '10.1007/978-3-540-88688-4_16', '10.1109/TBME.2007.909544', '10.1109/ICPR.2010.669', '10.1109/4233.992163', '10.1109/TPAMI.2009.167', '10.1109/TBME.2007.909544', '10.7551/mitpress/7503.003.0058', '10.1007/978-3-540-88682-2_16', '10.1109/TMI.2009.2012704', '10.1109/CVPR.2009.5206684', '10.1016/j.patcog.2008.10.035', '10.1109/TMI.2011.2141674', '10.1109/TIP.2002.800889', '10.1007/978-3-540-73273-0_52', '10.1007/978-3-642-15705-9_59', '10.1023/B:VISI.0000029664.99615.94', '10.1109/CVPR.2011.5995359', '10.2217/iim.09.9', '10.1016/j.media.2010.04.007', '10.1109/TPAMI.2002.1017623', '10.1109/TMI.2011.2106796', '10.1023/A:1026553619983', '10.1145/1390156.1390258', '10.1016/j.patcog.2008.08.027', '10.1109/CVPR.2008.4587503', '10.1109/TMI.2006.879967', '10.1109/TMI.2007.898536', '10.1016/j.patcog.2008.10.029', '10.1007/s11263-009-0271-8', '10.1109/ICCV.2009.5459183', '10.1109/CVPR.2010.5540060', '10.1109/CVPR.2008.4587632', '10.1023/B:VISI.0000013087.49260.fb', '10.1109/TMI.2006.875426', '10.1007/978-3-642-33454-2_77', '10.1007/978-3-540-85988-8_99', '10.1007/s10489-007-0111-x', '10.7551/mitpress/7503.003.0206'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Deep Learning', 'Neural Computation'], conference_acronym='Medical image analysis (Print)', publisher=None, query_handler=None),\n",
       " 'Image completion with structure propagation': Paper(DOI='10.3724/sp.j.1004.2009.01041', crossref_json=None, google_schorlar_metadata=None, title='Image completion with structure propagation', authors=['Jian Sun', 'Lu Yuan', 'Jiaya Jia', 'Heung-Yeung Shum'], abstract='In this paper, we introduce a novel approach to image completion, which we call structure propagation. In our system, the user manually specifies important missing structure information by extending a few curves or line segments from the known to the unknown regions. Our approach synthesizes image patches along these user-specified curves in the unknown region using patches selected around the curves in the known region. Structure propagation is formulated as a global optimization problem by enforcing structure and consistency constraints. If only a single curve is specified, structure propagation is solved using Dynamic Programming. When multiple intersecting curves are specified, we adopt the Belief Propagation algorithm to find the optimal patches. After completing structure propagation, we fill in the remaining unknown regions using patch-based texture synthesis. We show that our approach works\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TIP.2003.815261', '10.1016/S1004-4132(07)60102-9', '10.1109/TIP.2004.833105'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Object recognition with gradient-based learning': Paper(DOI='10.3724/sp.j.1146.2012.01241', crossref_json=None, google_schorlar_metadata=None, title='Object recognition with gradient-based learning', authors=['David A Forsyth', 'Joseph L Mundy', 'Vito di Gesú', 'Roberto Cipolla', 'Yann LeCun', 'Patrick Haffner', 'Léon Bottou', 'Yoshua Bengio'], abstract=' Finding an appropriate set of features is an essential problem in the design of shape recognition systems. This paper attempts to show that for recognizing simple objects with high shape variability such as handwritten characters, it is possible, and even advantageous, to feed the system directly with minimally processed images and to rely on learning to extract the right set of features. Convolutional Neural Networks are shown to be particularly well suited to this task. We also show that these networks can be used to recognize multiple objects without requiring explicit segmentation of the objects from their surrounding. The second part of the paper presents the Graph Transformer Network model which extends the applicability of gradient-based learning to systems that use graphs to represents features, objects, and their combinations.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics', 'Machine Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Eigenfaces vs. fisherfaces: Recognition using class specific linear projection': Paper(DOI='10.1109/34.598228', crossref_json=None, google_schorlar_metadata=None, title='Eigenfaces vs. fisherfaces: Recognition using class specific linear projection', authors=['Peter N.  Belhumeur', 'Joao P Hespanha', 'David J.  Kriegman'], abstract=\"We develop a face recognition algorithm which is insensitive to large variation in lighting direction and facial expression. Taking a pattern classification approach, we consider each pixel in an image as a coordinate in a high-dimensional space. We take advantage of the observation that the images of a particular face, under varying illumination but fixed pose, lie in a 3D linear subspace of the high dimensional image space-if the face is a Lambertian surface without shadowing. However, since faces are not truly Lambertian surfaces and do indeed produce self-shadowing, images will deviate from this linear subspace. Rather than explicitly modeling this deviation, we linearly project the image into a subspace in a manner which discounts those regions of the face with large deviation. Our projection method is based on Fisher's linear discriminant and produces well separated classes in a low-dimensional subspace\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1016/0031-3203(92)90007-6', '10.1109/5.381842', '10.21236/ADA259443', '10.1109/CAMP.1993.622458', '10.1109/34.254061', '10.1109/ICCV.1995.466858', '10.1109/ICCV.1995.466917', '10.1109/ICCV.1995.466878', '10.1109/ICCV.1995.466885', '10.1109/ICCV.1995.466919', '10.1016/0262-8856(94)90039-6', '10.1162/jocn.1991.3.1.71', '10.1109/CVPR.1994.323893', '10.1109/CVPR.1991.139758', '10.1111/j.1469-1809.1936.tb02137.x', '10.1364/JOSAA.4.000519', '10.1109/CVPR.1994.323941', '10.1007/BF01421486', '10.1109/CVPR.1994.323814', '10.1109/CVPR.1996.517125', '10.1109/ICCV.1995.466879', '10.1109/ROBOT.1996.506890', '10.1016/0004-3702(81)90022-9', '10.1109/34.598228', '10.1109/CVPR.1996.517085'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Biometrics', 'Computer Graphics'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'From few to many: Illumination cone models for face recognition under variable lighting and pose': Paper(DOI='10.1109/34.927464', crossref_json=None, google_schorlar_metadata=None, title='From few to many: Illumination cone models for face recognition under variable lighting and pose', authors=['Athinodoros S.  Georghiades', 'Peter N.  Belhumeur', 'David J.  Kriegman'], abstract='We present a generative appearance-based method for recognizing human faces under variation in lighting and viewpoint. Our method exploits the fact that the set of images of an object in fixed pose, but under all possible illumination conditions, is a convex cone in the space of images. Using a small number of training images of each face taken with different lighting directions, the shape and albedo of the face can be reconstructed. In turn, this reconstruction serves as a generative model that can be used to render (or synthesize) images of the face under novel poses and illumination conditions. The pose space is then sampled and, for each pose, the corresponding illumination cone is approximated by a low-dimensional linear subspace whose basis vectors are estimated using the generative model. Our recognition algorithm assigns to a test image the identity of the closest approximated illumination cone. Test\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/0031-3203(96)00036-2', '10.1109/ICCV.1995.466878', '10.1109/72.750575', '10.1109/AFGR.2000.840650', '10.1109/ICCV.1995.466858', '10.1109/34.598227', '10.1007/BF01421486', '10.1007/3-540-61750-7_28', '10.1068/p260075', '10.1016/S0262-8856(97)00003-6', '10.1145/280814.280874', '10.1016/0004-3702(81)90022-9', '10.1016/0031-3203(95)00129-8', '10.1109/AFGR.2000.840642', '10.1109/CVPR.1997.609321', '10.1364/JOSAA.4.000519', '10.1007/978-3-0348-5737-6', '10.1109/TSMC.1976.5409181', '10.1109/AFGR.2000.840648', '10.1109/ICCV.1995.466919', '10.1109/CVPR.1997.609314', '10.1109/34.598231', '10.1142/S0218001495000353', '10.21236/ADA259443', '10.1023/A:1007975506780', '10.1109/CVPR.1994.323941', '10.1201/9781439863930', '10.1016/0031-3203(81)90008-X', '10.1109/CVPR.1998.698587', '10.1109/PROC.1971.8254', '10.1007/BF00055801', '10.1109/34.598230', '10.1109/ICCV.1995.466885', '10.1023/A:1008058932445', '10.1109/5.381842', '10.1109/CVPR.2000.855827', '10.1109/34.598235', '10.1109/ICCV.1995.466898', '10.1016/0031-3203(78)90001-8', '10.1364/JOSAA.11.003079', '10.1109/34.254061', '10.1109/34.406651', '10.1109/34.598228', '10.1162/jocn.1991.3.1.71', '10.1109/CVPR.1994.323893', '10.1007/BF00129684', '10.1109/34.824823', '10.1109/CVPR.1994.323814', '10.1109/CVPR.1997.609461', '10.1023/A:1008005721484', '10.1109/ICCV.1999.791209', '10.1109/PBMCV.1995.514675', '10.1109/CVPR.1996.517076', '10.1109/AFGR.2000.840639', '10.1016/0031-3203(92)90007-6', '10.1109/CVPR.1998.698585', '10.1109/34.655647', '10.1109/CVPR.1999.784968', '10.1109/MVIEW.1999.781082', '10.1109/AFGR.2000.840647', '10.1016/S0262-8856(97)00070-X', '10.1007/BFb0016020', '10.1109/34.3909', '10.1109/CVPR.1997.609311'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Biometrics', 'Computer Graphics'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Pixelwise view selection for unstructured multi-view stereo': Paper(DOI='10.1007/978-3-319-46487-9_31', crossref_json=None, google_schorlar_metadata=None, title='Pixelwise view selection for unstructured multi-view stereo', authors=['Johannes L Schönberger', 'Enliang Zheng', 'Jan-Michael Frahm', 'Marc Pollefeys'], abstract=' This work presents a Multi-View Stereo system for robust and efficient dense modeling from unstructured image collections. Our core contributions are the joint estimation of depth and normal information, pixelwise view selection using photometric and geometric priors, and a multi-view geometric consistency term for the simultaneous refinement and image-based depth and normal fusion. Experiments on benchmarks and large-scale Internet photo collections demonstrate state-of-the-art performance in terms of accuracy, completeness, and efficiency.', conference=None, journal=None, year=None, reference_list=['10.1007/3-540-47969-4_28', '10.1145/1141911.1141964', '10.1109/ICCV.2009.5459148', '10.1007/978-3-642-15561-1_27', '10.1109/CVPR.2015.7298949', '10.1109/ICCV.2015.240', '10.1109/CVPR.2015.7299148', '10.1109/CVPR.2016.445', '10.1109/CVPR.2007.383246', '10.1109/CVPR.2010.5539802', '10.1007/978-3-642-33712-3_29', '10.1109/3DV.2013.12', '10.1109/CVPR.2014.511', '10.1109/CVPR.2014.196', '10.1109/ICCV.2015.106', '10.1145/2398356.2398381', '10.1145/166117.166153', '10.1109/IROS.2013.6696924', '10.1109/CVPR.2008.4587706', '10.1007/BFb0028349', '10.1109/34.310690', '10.1023/A:1014573219977', '10.1109/CVPR.2011.5995372', '10.1007/978-3-540-88682-2_58', '10.1109/CVPR.2009.5206867', '10.1109/ICCV.2009.5459145', '10.1109/CVPR.2011.5995693', '10.1109/CVPR.2013.20', '10.1109/ICCV.2009.5459384', '10.1007/978-3-319-10590-1_10', '10.1109/ICCVW.2013.46', '10.1109/ICCV.2015.157', '10.1109/CVPR.2016.592', '10.1109/TPAMI.2008.99', '10.1109/34.865184', '10.1109/CVPR.2008.4587671', '10.1109/CVPR.2007.383245', '10.1109/ICCV.1999.791261', '10.5244/C.25.14', '10.1109/ICCV.2007.4408933', '10.1007/978-3-642-15986-2_1', '10.5244/C.26.34', '10.1109/ICCV.2015.107', '10.1109/TPAMI.2008.221', '10.1109/ICCV.2007.4408984', '10.1007/978-3-319-10602-1_54', '10.1145/2487228.2487237', '10.1109/3DIMPVT.2012.60', '10.20870/IJVR.2010.9.1.2761', '10.1109/TPAMI.2010.116'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'structure from motion', 'photo collection reconstruction', 'stereo', 'robust estimation'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Semantic3d. net: A new large-scale point cloud classification benchmark': Paper(DOI='10.3390/s23083869', crossref_json=None, google_schorlar_metadata=None, title='Semantic3d. net: A new large-scale point cloud classification benchmark', authors=['Timo Hackel', 'Nikolay Savinov', 'Lubor Ladicky', 'Jan D Wegner', 'Konrad Schindler', 'Marc Pollefeys'], abstract='This paper presents a new 3D point cloud classification benchmark data set with over four billion manually labelled points, meant as input for data-hungry (deep) learning methods. We also discuss first submissions to the benchmark that use deep convolutional neural networks (CNNs) as a work horse, which already show remarkable performance improvements over state-of-the-art. CNNs have become the de-facto standard for many tasks in computer vision and machine learning like semantic segmentation or object detection in images, but have no yet led to a true breakthrough for 3D point cloud labelling tasks due to lack of training data. With the massive data set presented in this paper, we aim at closing this data gap to help unleash the full potential of deep learning methods for 3D labelling tasks. Our semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains 8 semantic classes and covers a wide range of urban outdoor scenes: churches, streets, railroad tracks, squares, villages, soccer fields and castles. We describe our labelling interface and show that our data set provides more dense and complete point clouds with much higher overall number of labelled points compared to those already available to the research community. We further provide baseline method descriptions and comparison between methods submitted to our online system. We hope semantic3D.net will pave the way for deep learning methods in 3D point cloud labelling to learn richer, more general 3D representations, and first submissions after only a few months indicate that this might indeed be the case.', conference=None, journal=None, year=None, reference_list=['10.1109/TITS.2022.3155925', '10.1109/CSCWD54268.2022.9776241', '10.1109/TPAMI.2020.3005434', '10.1007/s11263-022-01601-z', '10.1109/DSAA54385.2022.10032415', '10.2139/ssrn.4396774', '10.1109/ICCV48922.2021.01595', '10.1016/j.isprsjprs.2021.01.007', '10.1016/j.eswa.2022.118815', '10.1109/ICASSP43922.2022.9746657', '10.1109/CVPR.2009.5206590', '10.3390/math9121328', '10.3390/rs10081192', '10.1145/3205326.3205355', '10.1109/ACCESS.2019.2909742', '10.1016/j.patrec.2020.06.005', '10.1080/01431161.2020.1734252'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics', 'Robotics', 'Machine Learning', 'Augmented Reality'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Building rome on a cloudless day': Paper(DOI='10.1007/978-3-642-15561-1_27', crossref_json=None, google_schorlar_metadata=None, title='Building rome on a cloudless day', authors=['Jan-Michael Frahm', 'Pierre Fite-Georgel', 'David Gallup', 'Tim Johnson', 'Rahul Raguram', 'Changchang Wu', 'Yi-Hung Jen', 'Enrique Dunn', 'Brian Clipp', 'Svetlana Lazebnik', 'Marc Pollefeys'], abstract=' This paper introduces an approach for dense 3D reconstruction from unregistered Internet-scale photo collections with about 3 million images within the span of a day on a single PC (“cloudless”). Our method advances image clustering, stereo, stereo fusion and structure from motion to achieve high computational performance. We leverage geometric and appearance constraints to obtain a highly parallel implementation on modern graphics processors and multi-core architectures. This leads to two orders of magnitude higher performance on an order of magnitude larger dataset than competing state-of-the-art approaches.', conference=None, journal=None, year=None, reference_list=['10.1145/1141911.1141964', '10.1109/ICCV.2009.5459148', '10.1007/978-3-540-88682-2_33', '10.1109/ICCV.2007.4408933', '10.1109/CVPR.2010.5539802', '10.1007/s11263-007-0086-4', '10.1007/978-3-642-15986-2_1', '10.1023/A:1011139631724', '10.1145/1327452.1327494', '10.1109/CVPR.2008.4587633', '10.1002/9780470316801', '10.1007/978-3-540-88688-4_37', '10.1109/CVPR.2008.4587678', '10.1007/3-540-47969-4_28', '10.1007/s11263-007-0107-3', '10.1109/ICCV.2007.4408891', '10.1109/ICCV.2007.4408863', '10.1109/CVPR.2010.5540184', '10.1109/ICVGIP.2008.103', '10.1109/ICCV.2007.4409085', '10.1023/B:VISI.0000029664.99615.94', '10.1017/CBO9780511811685', '10.1007/11861898_66', '10.1109/TPAMI.2004.17'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'structure from motion', 'photo collection reconstruction', 'stereo', 'robust estimation'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'A comparative analysis of RANSAC techniques leading to adaptive real-time random sample consensus': Paper(DOI='10.1007/978-3-540-88688-4_37', crossref_json=None, google_schorlar_metadata=None, title='A comparative analysis of RANSAC techniques leading to adaptive real-time random sample consensus', authors=['Rahul Raguram', 'Jan-Michael Frahm', 'Marc Pollefeys'], abstract=' The Random Sample Consensus (RANSAC) algorithm is a popular tool for robust estimation problems in computer vision, primarily due to its ability to tolerate a tremendous fraction of outliers. There have been a number of recent efforts that aim to increase the efficiency of the standard RANSAC algorithm. Relatively fewer efforts, however, have been directed towards formulating RANSAC in a manner that is suitable for real-time implementation. The contributions of this work are two-fold: First, we provide a comparative analysis of the state-of-the-art RANSAC algorithms and categorize the various approaches. Second, we develop a powerful new framework for real-time robust estimation. The technique we develop is capable of efficiently adapting to the constraints presented by a fixed time budget, while at the same time providing accurate estimation over a wide range of inlier ratios. The method shows\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/358669.358692', '10.1016/j.imavis.2004.02.009', '10.5244/C.19.78', '10.1109/ICCV.2005.198', '10.1007/978-3-540-45243-0_31', '10.1007/3-540-47969-4_6', '10.1109/ICCV.2003.1238341', '10.1006/cviu.1999.0832'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'structure from motion', 'photo collection reconstruction', 'stereo', 'robust estimation'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate': Paper(DOI='10.1007/11744085_8', crossref_json=None, google_schorlar_metadata=None, title='A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate', authors=['Jingyu Yan', 'Marc Pollefeys'], abstract=' We cast the problem of motion segmentation of feature trajectories as linear manifold finding problems and propose a general framework for motion segmentation under affine projections which utilizes two properties of trajectory data: geometric constraint and locality. The geometric constraint states that the trajectories of the same motion lie in a low dimensional linear manifold and different motions result in different linear manifolds; locality, by which we mean in a transformed space a data and its neighbors tend to lie in the same linear manifold, provides a cue for efficient estimation of these manifolds. Our algorithm estimates a number of linear manifolds, whose dimensions are unknown beforehand, and segment the trajectories accordingly. It first transforms and normalizes the trajectories; secondly, for each trajectory it estimates a local linear manifold through local sampling; then it derives the affinity\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/358669.358692', '10.1023/A:1008000628999', '10.1007/978-3-540-24673-2_46', '10.1068/p130255', '10.1109/ICCV.1999.791279', '10.1142/S0219467802000585', '10.1016/S0005-1098(02)00224-8', '10.1109/ICCV.1999.790354', '10.1007/BF00129684'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics', 'Robotics', 'Machine Learning', 'Augmented Reality'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'USAC: A universal framework for random sample consensus': Paper(DOI='10.1109/tpami.2012.257', crossref_json=None, google_schorlar_metadata=None, title='USAC: A universal framework for random sample consensus', authors=['Rahul Raguram', 'Ondrej Chum', 'Marc Pollefeys', 'Jiri Matas', 'Jan-Michael Frahm'], abstract='A computational problem that arises frequently in computer vision is that of estimating the parameters of a model from data that have been contaminated by noise and outliers. More generally, any practical system that seeks to estimate quantities from noisy data measurements must have at its core some means of dealing with data contamination. The random sample consensus (RANSAC) algorithm is one of the most popular tools for robust estimation. Recent years have seen an explosion of activity in this area, leading to the development of a number of techniques that improve upon the efficiency and robustness of the basic RANSAC algorithm. In this paper, we present a comprehensive overview of recent research in RANSAC-based robust estimation by analyzing and comparing various approaches that have been explored over the years. We provide a common context for this analysis by introducing a new\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2008.4587474', '10.1109/ICCV.2001.937672', '10.1007/978-3-540-45243-0_31', '10.1109/ICPR.2004.1334020', '10.1023/A:1007941100561', '10.1109/CVPR.2005.221', '10.5244/C.16.44', '10.1016/0004-3702(95)00022-4', '10.1109/ICCV.2009.5459459', '10.1109/ICCV.2011.6126382', '10.1109/34.589215', '10.5244/C.14.38', '10.1002/0471725250', '10.1109/70.976019', '10.1016/S0734-189X(88)80033-1', '10.1093/biomet/69.1.242', '10.2307/2288718', '10.1007/s11263-007-0107-3', '10.1145/358669.358692', '10.1109/ROBOT.2006.1641163', '10.1006/cviu.1997.0559', '10.1109/TPAMI.2007.70787', '10.1109/IROS.2010.5653696', '10.1109/34.464558', '10.1109/CVPR.1996.517089', '10.1109/ICCV.2003.1238441', '10.15607/RSS.2006.II.018', '10.1109/34.659940', '10.1109/TPAMI.2004.109', '10.5244/C.19.78', '10.1109/ICCV.2009.5459148', '10.1109/ICCVW.2009.5457551', '10.1109/CVPR.2004.1315094', '10.1109/IROS.2006.282572', '10.1007/s11263-007-0086-4', '10.1109/ICCV.2005.198', '10.1109/CVPR.2005.354', '10.1109/ICCV.2003.1238341', '10.1109/ICCV.2009.5459456', '10.1109/CVPR.2006.235', '10.1109/TPAMI.2004.17'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'structure from motion', 'photo collection reconstruction', 'stereo', 'robust estimation'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'GPU-based video feature tracking and matching': Paper(DOI='10.3745/jips.02.0040', crossref_json=None, google_schorlar_metadata=None, title='GPU-based video feature tracking and matching', authors=['Sudipta N Sinha', 'Jan-Michael Frahm', 'Marc Pollefeys', 'Yakup Genc'], abstract='This paper describes novel implementations of the KLT feature tracking and SIFT feature extraction algorithms that run on the graphics processing unit (GPU) and is suitable for video analysis in real-time vision systems. While significant acceleration over standard CPU implementations is obtained by exploiting parallelism provided by modern programmable graphics hardware, the CPU is freed up to run other computations in parallel. Our GPU-based KLT implementation tracks about a thousand features in real-time at 30 Hz on 1024× 768 resolution video which is a 20 times improvement over the CPU. It works on both ATI and NVIDIA graphics cards. The GPU-based SIFT implementation works on NVIDIA cards and extracts about 800 features from 640× 480 video at 10Hz which is approximately 10 times faster than an optimized CPU implementation.', conference=None, journal=None, year=None, reference_list=['10.1109/TITS.2011.2113340', '10.4028/www.scientific.net/AMM.536-537.176', '10.1007/s00138-011-0355-7', '10.1109/LGRS.2013.2274939', '10.1109/TPAMI.2009.109', '10.1016/j.cviu.2008.12.008', '10.1007/s11042-013-1453-5', '10.1007/s11042-010-0676-y', '10.1016/j.patcog.2014.04.008'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'structure from motion', 'photo collection reconstruction', 'stereo', 'robust estimation'], conference_acronym='Journal of information processing systems (Online)', publisher=None, query_handler=None),\n",
       " 'PIXHAWK: A micro aerial vehicle design for autonomous flight using onboard computer vision': Paper(DOI='10.1007/s10514-012-9281-4', crossref_json=None, google_schorlar_metadata=None, title='PIXHAWK: A micro aerial vehicle design for autonomous flight using onboard computer vision', authors=['Lorenz Meier', 'Petri Tanskanen', 'Lionel Heng', 'Gim Hee Lee', 'Friedrich Fraundorfer', 'Marc Pollefeys'], abstract=' We describe a novel quadrotor Micro Air Vehicle (MAV) system that is designed to use computer vision algorithms within the flight control loop. The main contribution is a MAV system that is able to run both the vision-based flight control and stereo-vision-based obstacle detection parallelly on an embedded computer onboard the MAV. The system design features the integration of a powerful onboard computer and the synchronization of IMU-Vision measurements by hardware timestamping which allows tight integration of IMU measurements into the computer vision pipeline. We evaluate the accuracy of marker-based visual pose estimation for flight control and demonstrate marker-based autonomous flight including obstacle detection using stereo vision. We also show the benefits of our IMU-Vision synchronization for egomotion estimation in additional experiments where we use the synchronized\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICRA.2011.5980343', '10.1109/ROBOT.2010.5509990', '10.1109/ICRA.2011.5980136', '10.1109/ROBOT.2010.5509920', '10.1109/IROS.2006.282188', '10.1109/IROS.2007.4399042', '10.1109/ICRA.2011.5980315', '10.1109/SIES.2009.5196224', '10.1007/s10846-010-9494-8', '10.1109/CIRA.2007.382886', '10.1109/ICRA.2011.5980095', '10.1002/rob.20284', '10.1109/IROS.2010.5649358', '10.1109/ROBOT.2005.1570727', '10.1177/0278364907079276', '10.1109/ICRA.2011.5980229', '10.1109/IROS.2003.1249235', '10.1002/rob.20155', '10.1177/0278364908090949', '10.1109/ICRA.2011.5980357', '10.1109/AERO.2001.931701', '10.1109/ICRA.2011.5979579', '10.1007/s10846-010-9473-0', '10.1109/ICRA.2011.5979997'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Robotics', 'Machine Learning', 'Photogrammetry', 'Remote Sensing'], conference_acronym='Autonomous robots', publisher=None, query_handler=None),\n",
       " 'Real-time texture synthesis by patch-based sampling': Paper(DOI='10.1145/501786.501787', crossref_json=None, google_schorlar_metadata=None, title='Real-time texture synthesis by patch-based sampling', authors=['Lin Liang', 'Ce Liu', 'Ying-Qing Xu', 'Baining Guo', 'Heung-Yeung Shum'], abstract='We present an algorithm for synthesizing textures from an input sample. This patch-based sampling algorithm is fast and it makes high-quality texture synthesis a real-time process. For generating textures of the same size and comparable quality, patch-based sampling is orders of magnitude faster than existing algorithms. The patch-based sampling algorithm works well for a wide variety of textures ranging from regular to stochastic. By sampling patches according to a nonparametric estimation of the local conditional MRF density function, we avoid mismatching features across patch boundaries. We also experimented with documented cases for which pixel-based nonparametric sampling algorithms cease to be effective but our algorithm continues to work well.', conference=None, journal=None, year=None, reference_list=['10.1145/293347.293348', '10.1109/2945.928165', '10.1145/258734.258882', '10.1145/383259.383296', '10.1145/358523.358553', '10.1145/355744.355745', '10.1145/218380.218446', '10.1145/383259.383295', '10.1016/0167-8655(94)90018-3', '10.1109/34.615448', '10.1145/344779.344987', '10.1145/383259.383297', '10.1145/383259.383298', '10.1145/344779.345009', '10.1145/237170.237267', '10.1023/A:1008199424771', '10.1109/34.862195', '10.1162/neco.1997.9.8.1627', '10.1023/A:1007925832420', '10.1016/0146-664X(80)90016-7'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'computer graphics', 'machine learning'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " 'Systems and experiment paper: Construction of panoramic image mosaics with global and local alignment': Paper(DOI='10.1007/978-1-4757-3482-9_13', crossref_json=None, google_schorlar_metadata=None, title='Systems and experiment paper: Construction of panoramic image mosaics with global and local alignment', authors=['Heung-Yeung Shum', 'Richard Szeliski'], abstract=' This paper presents a complete system for constructing panoramic image mosaics from sequences of images. Our mosaic representation associates a transformation matrix with each input image, rather than explicitly projecting all of the images onto a common surface (e.g., a cylinder). In particular, to construct a full view panorama, we introduce a rotational mosaic representation that associates a rotation matrix (and optionally a focal length) with each input image. A patch-based alignment algorithm is developed to quickly align two images given motion models. Techniques for estimating and refining camera focal lengths are also presented. In order to reduce accumulated registration errors, we apply global alignment (block adjustment) to the whole sequence of images, which results in an optimally registered image mosaic. To compensate for small amounts of motion parallax introduced by\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Full-frame video stabilization with motion inpainting': Paper(DOI='10.1109/tpami.2006.141', crossref_json=None, google_schorlar_metadata=None, title='Full-frame video stabilization with motion inpainting', authors=['Yasuyuki Matsushita', 'Eyal Ofek', 'Weina Ge', 'Xiaoou Tang', 'Heung-Yeung Shum'], abstract='Video stabilization is an important video enhancement technology which aims at removing annoying shaky motion from videos. We propose a practical and robust approach of video stabilization that produces full-frame stabilized videos with good visual quality. While most previous methods end up with producing smaller size stabilized videos, our completion method can produce full-frame videos by naturally filling in missing image parts by locally aligning image data of neighboring frames. To achieve this, motion inpainting is proposed to enforce spatial and temporal consistency of the completion in both static and dynamic image areas. In addition, image quality in the stabilized video is enhanced with a new practical deblurring algorithm. Instead of estimating point spread functions, our method transfers and interpolates sharper image pixels of neighboring frames to increase the sharpness of the frame. The\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ACV.1994.341287', '10.1109/ICCV.1999.790383', '10.1109/VISUAL.2004.88', '10.1109/CVPR.1998.698643', '10.1145/1015706.1015720', '10.1109/CVPR.1998.698709', '10.1109/CVPR.1997.609359', '10.1109/ICCV.2003.1238630', '10.1109/ICIAP.1999.797671', '10.1109/WACV.2000.895398', '10.1117/1.1305319', '10.1109/ICCV.2001.937505', '10.1145/1201775.882264', '10.1109/TPAMI.2005.129', '10.1145/344779.344972', '10.1109/CVPR.2003.1211538', '10.1023/A:1008195814169', '10.1561/0600000009', '10.1109/CVPR.2005.366', '10.1109/CVPR.2004.1315022', '10.1016/1049-9652(91)90045-L', '10.1109/CVPR.2004.1315090', '10.1007/BF00158167', '10.1007/s00138-002-0085-y', '10.1109/CVPR.2001.991019', '10.1109/ACV.1994.341288', '10.1007/BF00127822', '10.1002/sca.4950210404', '10.1080/10867651.2004.10487596', '10.1109/ICCV.1998.710815', '10.1016/1049-9652(91)90025-F'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Artificial Intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'The design and implementation of xiaoice, an empathetic social chatbot': Paper(DOI='10.1162/coli_a_00368', crossref_json=None, google_schorlar_metadata=None, title='The design and implementation of xiaoice, an empathetic social chatbot', authors=['Li Zhou', 'Jianfeng Gao', 'Di Li', 'Heung-Yeung Shum'], abstract=' This article describes the development of Microsoft XiaoIce, the                     most popular social chatbot in the world. XiaoIce is uniquely designed as an                     artifical intelligence companion with an emotional connection to satisfy the                     human need for communication, affection, and social belonging. We take into                     account both intelligent quotient and emotional quotient in system design, cast                     human–machine social chat as decision-making over Markov Decision                     Processes, and optimize XiaoIce for long-term user engagement, measured in                     expected Conversation-turns Per Session (CPS). We detail the system architecture                     and key components, including dialogue manager, core chat, skills, and an                     empathetic computing module. We show how XiaoIce dynamically recognizes human                     feelings and states, understands user\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-319-46454-1_24', '10.1109/CVPR.2018.00636', '10.1007/11825890_3', '10.3115/v1/W14-4012', '10.1016/0004-3702(71)90002-6', '10.18653/v1/W18-0802', '10.18653/v1/N18-5020', '10.1109/CVPR.2015.7298754', '10.1007/978-3-030-01204-5_9', '10.3115/v1/P15-2073', '10.1561/1500000074', '10.1162/089120105775299177', '10.3115/v1/D14-1002', '10.1145/2505515.2505665', '10.18653/v1/N16-1014', '10.18653/v1/P16-1094', '10.18653/v1/D16-1127', '10.18653/v1/D16-1230', '10.18653/v1/P17-1103', '10.1017/CBO9780511809071', '10.1037/h0054346', '10.1145/2858036.2858116', '10.18653/v1/D17-1237', '10.7551/mitpress/1140.001.0001', '10.1109/CVPR.2017.131', '10.18653/v1/W17-1101', '10.3115/v1/P15-1152', '10.3115/1556328.1556341', '10.1145/2661829.2661935', '10.3115/v1/N15-1020', '10.1016/S0004-3702(99)00052-1', '10.1109/CVPR.2015.7299087', '10.1109/CVPR.2015.7298935', '10.1007/978-1-4020-6710-5_13', '10.1145/365153.365168', '10.1007/s10791-009-9112-1', '10.1145/2835776.2835786'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics'], conference_acronym='Computational linguistics - Association for Computational Linguistics (Print)', publisher=None, query_handler=None),\n",
       " 'From Eliza to XiaoIce: challenges and opportunities with social chatbots': Paper(DOI='10.1631/fitee.1700826', crossref_json=None, google_schorlar_metadata=None, title='From Eliza to XiaoIce: challenges and opportunities with social chatbots', authors=['Heung-Yeung Shum', 'Xiao-dong He', 'Di Li'], abstract=' Conversational systems have come a long way since their inception in the 1960s. After decades of research and development, we have seen progress from Eliza and Parry in the 1960s and 1970s, to task-completion systems as in the Defense Advanced Research Projects Agency (DARPA) communicator program in the 2000s, to intelligent personal assistants such as Siri, in the 2010s, to today’s social chatbots like XiaoIce. Social chatbots’ appeal lies not only in their ability to respond to users’ diverse requests, but also in being able to establish an emotional connection with users. The latter is done by satisfying users’ need for communication, affection, as well as social belonging. To further the advancement and adoption of social chatbots, their design must focus on user engagement and take both intellectual quotient (IQ) and emotional quotient (EQ) into account. Users should want to engage with a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.csl.2017.12.003', '10.3115/1075812.1075823', '10.1016/0167-6393(95)00008-C', '10.1109/JPROC.2012.2236631', '10.1109/MSP.2017.2741510', '10.3115/116580.116613', '10.1109/MSP.2012.2205597', '10.1162/neco.1997.9.8.1735', '10.1037/h0054346', '10.1109/TASLP.2014.2383614', '10.1109/TASL.2010.2076804', '10.1111/j.1744-6570.2007.00071_2.x', '10.3115/116580.116612', '10.1109/MSP.2016.2617341', '10.1145/175208.175217', '10.1002/9781119992691', '10.1002/9781119992691', '10.1093/mind/LIX.236.433', '10.1007/978-1-4020-6710-5_13', '10.1145/365153.365168', '10.1016/j.csl.2006.06.008'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics'], conference_acronym='Frontiers of Informaion Technology & Electronic Engineering (Print)', publisher=None, query_handler=None),\n",
       " 'Fundamental limits of reconstruction-based superresolution algorithms under local translation': Paper(DOI='10.1109/tpami.2004.1261081', crossref_json=None, google_schorlar_metadata=None, title='Fundamental limits of reconstruction-based superresolution algorithms under local translation', authors=['Zhouchen Lin', 'Heung-Yeung Shum'], abstract='Superresolution is a technique that can produce images of a higher resolution than that of the originally captured ones. Nevertheless, improvement in resolution using such a technique is very limited in practice. This makes it significant to study the problem: \"Do fundamental limits exist for Superresolution?\" In this paper, we focus on a major class of superresolution algorithms, called the reconstruction-based algorithms, which compute high-resolution images by simulating the image formation process. Assuming local translation among low-resolution images, this paper is the first attempt to determine the explicit limits of reconstruction-based algorithms, under both real and synthetic conditions. Based on the perturbation theory of linear systems, we obtain the superresolution limits from the conditioning analysis of the coefficient matrix. Moreover, we determine the number of low-resolution images that are sufficient to\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.1999.790414', '10.1109/83.650116', '10.1090/psapm/048/1314843', '10.1016/1049-9652(91)90045-L', '10.1109/ICIP.1994.413336', '10.1109/TASSP.1987.1165145', '10.1109/83.605404', '10.1007/3-540-09832-1', '10.1109/ICASSP.1993.319799', '10.1109/83.650118', '10.1109/CVPR.2000.854852', '10.1364/JOSAA.11.001727', '10.1109/ICCV.2001.937506', '10.1109/83.503915'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'computer vision', 'image processing', 'numerical optimization'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Joint bilateral upsampling': Paper(DOI='10.3724/sp.j.1016.2009.00241', crossref_json=None, google_schorlar_metadata=None, title='Joint bilateral upsampling', authors=['Johannes Kopf', 'Michael F Cohen', 'Dani Lischinski', 'Matt Uyttendaele'], abstract='Image analysis and enhancement tasks such as tone mapping, colorization, stereo depth, and photomontage, often require computing a solution (e.g., for exposure, chromaticity, disparity, labels) over the pixel grid. Computational and memory costs often require that a smaller solution be run over a downsampled image. Although general purpose upsampling methods can be used to interpolate the low resolution solution to the full resolution, these methods generally assume a smoothness prior for the interpolation. We demonstrate that in cases, such as those above, the available high resolution input image may be leveraged as a prior in the context of a joint bilateral upsampling procedure to produce a better high resolution solution. We show results for each of the applications above and compare them to traditional upsampling methods.', conference=None, journal=None, year=None, reference_list=['10.1145/1073204.1073272', '10.1145/1015706.1015778', '10.1109/TPAMI.2007.60', '10.1145/1015706.1015777', '10.1145/1073204.1073263', '10.1145/1141911.1141918', '10.1145/882262.882368', '10.1145/1179352.1141921', '10.1109/34.1000236'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['AI for Climate', 'computer vision', 'computational photography', 'deep learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'The hemi-cube: A radiosity solution for complex environments': Paper(DOI='10.1145/325165.325171', crossref_json=None, google_schorlar_metadata=None, title='The hemi-cube: A radiosity solution for complex environments', authors=['Michael F Cohen', 'Donald P Greenberg'], abstract='This paper presents a comprehensive method to calculate object to object diffuse reflections within complex environments containing hidden surfaces and shadows. In essence, each object in the environment is treated as a secondary light source. The method provides an accurate representation of the \"diffuse\" and \"ambient\" terms found in typical image synthesis algorithms. The phenomena of \"color bleeding\" from one surface to another, shading within shadow envelopes, and penumbras along shadow boundaries are accurately reproduced. Additional advantages result because computations are indepedent of viewer position. This allows the efficient rendering of multiple views of the same scene for dynamic sequences. Light sources can be modulated and object reflectivities can be changed, with minimal extra computation. The procedures extend the radiosity method beyond the bounds previously imposed.', conference=None, journal=None, year=None, reference_list=['10.1145/800031.808589', '10.1145/800031.808590', '10.1145/800031.808601', '10.1145/357332.357335', '10.1145/358876.358882'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Image Based Rendering'], conference_acronym='Computer graphics', publisher=None, query_handler=None),\n",
       " 'Verbs and adverbs: Multidimensional motion interpolation': Paper(DOI='10.1109/38.708559', crossref_json=None, google_schorlar_metadata=None, title='Verbs and adverbs: Multidimensional motion interpolation', authors=['Charles Rose', 'Michael F Cohen', 'Bobby Bodenheimer'], abstract='The article describes a system for real-time interpolated animation that addresses some of these problems. Through creating parameterized motions-which the authors call verbs parameterized by adverbs-a single authored verb produces a continuous range of subtle variations of a given motion at real-time rates. As a result, simulated figures alter their actions based on their momentary mood or in response to changes in their goals or environmental stimuli. For example, they demonstrate a walk verb that can show emotions such as happiness and sadness, and demonstrate subtle variations due to walking up or down hill while turning to the left and right. They also describe verb graphs, which act as the glue to assemble verbs and their adverbs into a runtime data structure. Verb graphs provide the means for seamless transition from verb to verb for the simulated figures within an interactive runtime system. Finally\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/218380.218419', '10.1145/218380.218405', '10.1145/218380.218421', '10.1007/978-3-7091-6874-5_1', '10.1145/218380.218422', '10.1109/2945.468392', '10.1145/325165.325244', '10.1007/BF01893414', '10.1109/38.365004', '10.1006/cgip.1994.1015', '10.1145/237170.237229', '10.1007/978-3-7091-7486-9_7', '10.1109/VRAIS.1997.583065', '10.1145/237170.237258', '10.1145/258734.258822'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Image Based Rendering'], conference_acronym='IEEE computer graphics and applications', publisher=None, query_handler=None),\n",
       " 'Entrepreneurial self-efficacy: A systematic review of the literature on its theoretical foundations, measurement, antecedents, and outcomes, and an agenda for future research': Paper(DOI='10.1016/j.jvb.2018.05.012', crossref_json=None, google_schorlar_metadata=None, title='Entrepreneurial self-efficacy: A systematic review of the literature on its theoretical foundations, measurement, antecedents, and outcomes, and an agenda for future research', authors=['Alexander Newman', 'Martin Obschonka', 'Susan Schwarz', 'Michael Cohen', 'Ingrid Nielsen'], abstract=\"With increased emphasis being placed on entrepreneurial thinking and acting in today's careers, we have witnessed growing research on entrepreneurial self-efficacy (ESE) over the last two decades. The present study provides a systematic review of the literature on the theoretical foundations, measurement, antecedents, and outcomes of ESE, and work which treats ESE as a moderator. Based on the review, an agenda for future research is developed and implications for entrepreneurship education and training highlighted. In doing so, the need to consider alternative theoretical perspectives to improve understanding of how ESE influences outcomes at different levels of analysis is highlighted. In addition, the review identifies a need to a) examine the factors which drive short-term fluctuations and long-term changes in ESE, b) examine the developmental precursors of ese in childhood, adolescence and early\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1108/ET-02-2014-0008', '10.1007/s11187-013-9531-7', '10.1016/0749-5978(91)90020-T', '10.1016/S0883-9026(99)00054-3', '10.1108/CDI-10-2015-0135', '10.1177/0894845315597475', '10.1177/0266242616658943', '10.1007/s10775-014-9269-z', '10.1108/02683940710733115', '10.1207/s15427617rhd0103_1', '10.1111/j.1745-6916.2006.00011.x', '10.1177/0149206311410606', '10.1037/0021-9010.88.1.87', '10.1016/j.ijme.2014.05.007', '10.1177/10717919070130041001', '10.1111/j.1559-1816.2010.00713.x', '10.1016/j.jbusvent.2015.08.002', '10.1287/orsc.1090.0445', '10.1037/0021-9010.89.4.587', '10.1016/j.obhdp.2012.06.009', '10.1111/jsbm.12242', '10.1177/0149206312466149', '10.1108/EBR-12-2014-0090', '10.1016/j.jbvi.2017.12.001', '10.1002/sej.1197', '10.1073/pnas.1321202111', '10.1142/S1084946711001768', '10.1016/j.jbusres.2006.12.016', '10.1002/sej.73', '10.1016/S0883-9026(97)00029-3', '10.1037/0021-9010.87.3.549', '10.1108/17561391111144573', '10.1080/13691066.2013.863063', '10.1111/jsbm.12228', '10.1142/S1084946715500041', '10.1108/IJGE-01-2013-0001', '10.1177/0266242614543336', '10.1177/0266242614534281', '10.1177/0266242612445462', '10.1108/IJGE-02-2013-0013', '10.1007/s11365-008-0103-2', '10.1016/j.jbusvent.2012.07.005', '10.1007/s11187-012-9419-y', '10.1108/13552551011054516', '10.1016/j.jvb.2013.06.006', '10.1016/j.jbvi.2014.09.002', '10.1108/13552551011020063', '10.1007/s10843-015-0144-x', '10.1016/j.sbspro.2011.11.250', '10.1177/0266242615575760', '10.1016/j.jbusvent.2010.01.001', '10.1111/j.1540-6520.2005.00100.x', '10.1561/0300000028', '10.1016/j.paid.2016.02.023', '10.5465/amle.2012.0107', '10.1016/j.jbusvent.2017.02.003', '10.5465/amr.1992.4279530', '10.1177/0047287513513170', '10.1177/1096348012461545', '10.1016/j.tourman.2011.02.013', '10.3727/108354211X13202764960744', '10.1108/IJEBR-09-2015-0202', '10.5465/amr.1984.4277628', '10.1016/j.jvb.2004.05.006', '10.1007/s11187-011-9355-2', '10.1177/0266242614549779', '10.1037/0033-295X.94.3.319', '10.1037/0003-066X.62.6.575', '10.1002/sej.42', '10.1016/j.jbusvent.2007.04.002', '10.1037/0003-066X.44.3.513', '10.1016/j.jvb.2015.02.007', '10.1080/08985626.2012.742326', '10.1556/204.2016.38.4.5', '10.1007/s10961-014-9333-3', '10.1016/j.jbusvent.2016.03.002', '10.1177/0971355712469155', '10.1016/j.jbusvent.2012.10.006', '10.1177/1069072716679921', '10.1037/0021-9010.86.1.80', '10.1016/j.ijme.2012.10.001', '10.1108/IJEBR-07-2014-0123', '10.1177/104225879502000103', '10.1111/jsbm.12032', '10.1108/14626000810871709', '10.1016/j.joep.2006.11.002', '10.1016/j.jbusvent.2005.06.008', '10.1108/JSBED-11-2016-0189', '10.1016/S0883-9026(98)00033-0', '10.1108/ET-04-2016-0076', '10.1111/j.1559-1816.2012.00994.x', '10.1177/0266242615612882', '10.1080/13596748.2010.503997', '10.1108/13552551211268148', '10.1016/j.tourman.2015.09.017', '10.1037/a0033446', '10.1006/jvbe.1994.1027', '10.1057/palgrave.jibs.8400173', '10.1002/job.305', '10.1177/0266242614544198', '10.1111/jsbm.12324', '10.1111/j.1540-6520.2009.00304.x', '10.1111/jsbm.12240', '10.1080/0965254X.2015.1035038', '10.1177/0894845310384481', '10.1142/S108494670800082X', '10.1177/0149206311433855', '10.1007/s11365-009-0123-6', '10.1111/cdep.12185', '10.1016/j.jvb.2018.01.003', '10.1007/s11187-016-9798-6', '10.1016/j.jvb.2010.02.008', '10.1007/s11187-016-9821-y', '10.1111/j.1559-1816.2009.00482.x', '10.1111/jsbm.12133', '10.1177/0149206311410060', '10.1007/s12186-013-9101-9', '10.1111/jsbm.12116', '10.1108/13552551211253937', '10.1177/097135571102000204', '10.1016/j.technovation.2010.02.002', '10.1080/13594320701595438', '10.1111/jsbm.12090', '10.1007/s11365-010-0156-x', '10.1111/jsbm.12025', '10.1177/0894486510391783', '10.1016/j.jvb.2009.04.004', '10.1142/S1084946714500058', '10.1108/IJEBR-11-2015-0251', '10.1111/etap.12087', '10.1027//1015-5759.18.3.242', '10.1108/13552550510580834', '10.1142/S108494670700068X', '10.1108/ET-05-2012-0059', '10.1007/s11187-009-9215-5', '10.5465/amr.2011.0078', '10.5465/amj.2011.0776', '10.1016/j.ijme.2014.09.005', '10.1177/0149206309337489', '10.1111/jsbm.12273', '10.1111/jsbm.12181', '10.2307/30040610', '10.1016/j.jbusvent.2011.04.003', '10.1111/peps.12035', '10.1108/JSTP-04-2014-0064', '10.1177/1069072716664302', '10.1177/0894845314568190', '10.1016/j.obhdp.2014.09.002', '10.1108/14626001311326743', '10.1142/S1084946715500181', '10.1007/s11187-016-9727-8', '10.1016/j.jvb.2014.09.002', '10.1017/S1833367200001681', '10.1007/s11365-014-0351-2', '10.1016/j.jbusvent.2011.08.001', '10.1142/S1084946706000386', '10.1016/j.jvb.2014.11.005', '10.1002/sej.1222', '10.1037/0021-9010.93.1.35', '10.1037/0021-9010.87.3.506', '10.1111/jsbm.12026', '10.1509/jppm.14.181', '10.1080/08985626.2013.862975', '10.1142/S1084946709001247', '10.3233/DEV-2012-12111', '10.1016/j.jbusvent.2010.04.001', '10.1108/IJEBR-08-2016-0248', '10.1037/0021-9010.90.6.1265'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Image Based Rendering'], conference_acronym='Journal of vocational behavior (Print)', publisher=None, query_handler=None),\n",
       " 'Initial public health response and interim clinical guidance for the 2019 novel coronavirus outbreak—United States, December 31, 2019–February 4, 2020': Paper(DOI='10.1111/ajt.15805', crossref_json=None, google_schorlar_metadata=None, title='Initial public health response and interim clinical guidance for the 2019 novel coronavirus outbreak—United States, December 31, 2019–February 4, 2020', authors=['Anita Patel'], abstract='In December 2019, an outbreak of acute respiratory illness caused by a novel coronavirus (2019-nCoV) was detected in mainland China. Cases have been reported in 26 additional locations, including the United States.', conference=None, journal=None, year=None, reference_list=['10.3201/eid1901.120124', '10.1016/S0140-6736(20)30211-7'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Image Based Rendering'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Rotation invariant spherical harmonic representation of 3 d shape descriptors': Paper(DOI='10.1680/jgele.17.00011', crossref_json=None, google_schorlar_metadata=None, title='Rotation invariant spherical harmonic representation of 3 d shape descriptors', authors=['Michael Kazhdan', 'Thomas Funkhouser', 'Szymon Rusinkiewicz'], abstract='One of the challenges in 3D shape matching arises from the fact that in many applications, models should be considered to be the same if they differ by a rotation. Consequently, when comparing two models, a similarity metric implicitly provides the measure of similarity at the optimal alignment. Explicitly solving for the optimal alignment is usually impractical. So, two general methods have been proposed for addressing this issue:(1) Every model is represented using rotation invariant descriptors.(2) Every model is described by a rotation dependent descriptor that is aligned into a canonical coordinate system defined by the model. In this paper, we describe the limitations of canonical alignment and discuss an alternate method, based on spherical harmonics, for obtaining rotation invariant representations. We describe the properties of this tool and show how it can be applied to a number of existing, orientation\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1111/j.1365-3091.1980.tb01179.x', '10.1006/cviu.1995.1013', '10.1061/(ASCE)1090-0241(2006)132:5(591)', '10.1038/srep09147', '10.1016/S0008-8846(02)00836-0', '10.1016/j.powtec.2006.03.026', '10.1016/j.powtec.2010.10.012', '10.1007/s10035-006-0021-3', '10.1007/s10035-012-0356-x', '10.1016/j.cma.2014.06.022', '10.1007/s10035-007-0038-2', '10.1016/j.imavis.2006.01.011', '10.1016/j.powtec.2015.12.029', '10.1680/geot.4.P.157', '10.1680/geolett.14.00082', '10.1007/s10035-013-0409-9'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Shape Analysis', 'Geometric Modeling', 'Geometry Processing'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop': Paper(DOI='10.3390/app10144913', crossref_json=None, google_schorlar_metadata=None, title='Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop', authors=['Fisher Yu', 'Ari Seff', 'Yinda Zhang', 'Shuran Song', 'Thomas Funkhouser', 'Jianxiong Xiao'], abstract='While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset.', conference=None, journal=None, year=None, reference_list=['10.1177/1745691610393980', '10.1109/ICCVW.2013.77', '10.1007/s11263-020-01316-z', '10.1109/ACCESS.2019.2956775', '10.1007/s11263-015-0816-y', '10.1177/0278364913491297', '10.1109/TPAMI.2018.2856256', '10.1109/ICCV.2017.167', '10.1162/neco.1995.7.1.108', '10.1109/ICCCN.2017.8038465', '10.1109/ICCV.2015.123', '10.1109/CVPR.2016.90', '10.1109/TPAMI.2016.2577031', '10.1007/978-3-319-10602-1_48', '10.1109/CVPR.2016.308'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Surface reconstruction from unorganized points': Paper(DOI='10.3901/jme.2009.10.151', crossref_json=None, google_schorlar_metadata=None, title='Surface reconstruction from unorganized points', authors=['Hugues Hoppe', 'Tony DeRose', 'Tom Duchamp', 'John McDonald', 'Werner Stuetzle'], abstract='We describe and demonstrate an algorithm that takes as input an unorganized set of points {xl, . . . . xn} ⊂ R3 on or near an unknown manifold M, and produces as output a simplicial surface that approximates M. Neither the topology, the presence of boundaries, nor the geometry of M are assumed to be known in advance - all are inferred automatically from the data. This problem naturally arises in a variety of practical situations such as range scanning an object from multiple view points, recovery of biological shapes from two-dimensional slices, and interactive surface sketching.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Geometry Processing', 'Geometric Modeling', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Poisson surface reconstruction': Paper(DOI='10.54294/hyamzv', crossref_json=None, google_schorlar_metadata=None, title='Poisson surface reconstruction', authors=['Michael Kazhdan', 'Matthew Bolitho', 'Hugues Hoppe'], abstract='We show that surface reconstruction from oriented points can be cast as a spatial Poisson problem. This Poisson formulation considers all the points at once, without resorting to heuristic spatial partitioning or blending, and is therefore highly resilient to data noise. Unlike radial basis function schemes, our Poisson approach allows a hierarchy of locally supported basis functions, and therefore the solution reduces to a well conditioned sparse linear system. We describe a spatially adaptive multiscale algorithm whose time and space complexities are proportional to the size of the reconstructed model. Experimenting with publicly available scan data, we demonstrate reconstruction of surfaces with greater detail than previously achievable.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Geometry Processing', 'Geometric Modeling', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Screened poisson surface reconstruction': Paper(DOI='10.1145/2487228.2487237', crossref_json=None, google_schorlar_metadata=None, title='Screened poisson surface reconstruction', authors=['Michael Kazhdan', 'Hugues Hoppe'], abstract='Poisson surface reconstruction creates watertight surfaces from oriented point sets. In this work we extend the technique to explicitly incorporate the points as interpolation constraints. The extension can be interpreted as a generalization of the underlying mathematical framework to a screened Poisson equation. In contrast to other image and geometry processing techniques, the screening term is defined over a sparse set of points rather than over the full domain. We show that these sparse constraints can nonetheless be integrated efficiently. Because the modified linear system retains the same finite-element discretization, the sparsity structure is unchanged, and the system can still be solved using a multigrid approach. Moreover we present several algorithmic improvements that together reduce the time complexity of the solver to linear in the number of points, thereby enabling faster, higher-quality surface\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/376957.376986', '10.1145/218380.218424', '10.1007/978-3-540-88688-4_9', '10.1016/j.gmod.2005.01.004', '10.1007/978-3-642-10331-5_63', '10.1111/j.1467-8659.2011.02058.x', '10.1145/383259.383266', '10.1007/978-3-540-33259-6_6', '10.1145/2010324.1964952', '10.1111/1467-8659.00236', '10.1145/237170.237269', '10.1111/j.1467-8659.2011.01848.x', '10.1145/133994.134011', '10.1145/1057432.1057434', '10.1111/j.1467-8659.2009.01530.x', '10.1145/344779.344849', '10.1111/j.1467-8659.2009.01511.x', '10.1145/1073204.1073226', '10.1145/344779.344940'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Geometry Processing', 'Geometric Modeling', 'Computer Vision'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " 'High-quality streamable free-viewpoint video': Paper(DOI='10.1145/2766945', crossref_json=None, google_schorlar_metadata=None, title='High-quality streamable free-viewpoint video', authors=['Alvaro Collet', 'Ming Chuang', 'Pat Sweeney', 'Don Gillett', 'Dennis Evseev', 'David Calabrese', 'Hugues Hoppe', 'Adam Kirk', 'Steve Sullivan'], abstract='We present the first end-to-end solution to create high-quality free-viewpoint video encoded as a compact data stream. Our system records performances using a dense set of RGB and IR video cameras, generates dynamic textured surfaces, and compresses these to a streamable 3D video format. Four technical advances contribute to high fidelity and robustness: multimodal multi-view stereo fusing RGB, IR, and silhouette information; adaptive meshing guided by automatic detection of perceptually salient areas; mesh tracking to create temporally coherent subsequences; and encoding of tracked textured meshes as an MPEG video stream. Quantitative experiments demonstrate geometric accuracy, texture fidelity, and encoding efficiency. We release several datasets with calibrated inputs and processed results to foster future research.', conference=None, journal=None, year=None, reference_list=['10.1145/2185520.2185549', '10.1145/1198555.1198596', '10.1007/s11263-012-0553-4', '10.1007/978-3-540-88682-2_58', '10.1145/882262.882309', '10.1111/cgf.12296', '10.1145/1360612.1360697', '10.1109/3DPVT.2006.148', '10.1109/TPAMI.2009.161', '10.1111/j.1467-8659.2009.01617.x', '10.1109/CVPR.2009.5206755', '10.1145/258734.258849', '10.1109/CVPR.2006.199', '10.1109/TIT.1966.1053907', '10.1016/j.cviu.2004.03.016', '10.1109/TPAMI.2012.46', '10.1109/CVPR.2014.440', '10.1109/93.580394', '10.1145/2487228.2487237', '10.1007/978-3-642-33765-9_53', '10.1145/1073204.1073244', '10.1145/1618452.1618521', '10.1145/353981.353995', '10.1109/TVCG.2009.88', '10.1145/344779.344951', '10.1109/93.580392', '10.1109/ICCV.2005.159', '10.1007/s00371-010-0429-y', '10.1109/MCG.2007.68', '10.1145/1276377.1276478', '10.1145/1360612.1360696', '10.1145/1618452.1618520', '10.1145/1516522.1516526', '10.1109/TIP.2003.819861', '10.1145/990002.990007', '10.1109/ICCV.2011.6126358', '10.1007/978-3-642-12651-2_2', '10.1145/2601097.2601134', '10.1145/1015706.1015766', '10.1145/2601097.2601165'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Geometry Processing', 'Geometric Modeling', 'Computer Vision'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " 'Efficient region tracking with parametric models of geometry and illumination': Paper(DOI='10.1109/34.722606', crossref_json=None, google_schorlar_metadata=None, title='Efficient region tracking with parametric models of geometry and illumination', authors=['Gregory D Hager', 'Peter N Belhumeur'], abstract='As an object moves through the field of view of a camera, the images of the object may change dramatically. This is not simply due to the translation of the object across the image plane; complications arise due to the fact that the object undergoes changes in pose relative to the viewing camera, in illumination relative to light sources, and may even become partially or fully occluded. We develop an efficient general framework for object tracking, which addresses each of these complications. We first develop a computationally efficient method for handling the geometric distortions produced by changes in pose. We then combine geometry and illumination into an algorithm that tracks large image regions using no more computation than would be required to track with no accommodation for illumination changes. Finally, we augment these methods with techniques from robust statistics and treat occluded regions on the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.1996.517055', '10.1007/BF01212361', '10.1109/ICCV.1995.466914', '10.1109/70.538972', '10.1109/70.210792', '10.1109/70.238279', '10.1080/00949658108810482', '10.1007/s100440050017', '10.5244/C.10.31', '10.1006/cviu.1997.0586', '10.1007/BF01421486', '10.1109/34.99234', '10.1109/ICCV.1995.466915', '10.1007/BFb0028333', '10.1109/CVPR.1997.609382', '10.1007/BF00158167', '10.1007/BF01212369', '10.1007/BF00126395', '10.1007/BF00127170', '10.1007/BF01469225', '10.1017/CBO9780511526657', '10.1007/BF00129684', '10.1109/ICCV.1995.466895', '10.1109/CVPR.1994.323941', '10.1109/ACV.1994.341287', '10.21236/ADA259443', '10.1016/0004-3702(81)90022-9', '10.1007/BFb0015547', '10.1002/0471725250', '10.1109/ICCV.1995.466872', '10.1109/CVPR.1996.517085'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision', 'Medicine', 'HCI', 'Computer Science'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Leafsnap: A computer vision system for automatic plant species identification': Paper(DOI='10.1007/978-3-642-33709-3_36', crossref_json=None, google_schorlar_metadata=None, title='Leafsnap: A computer vision system for automatic plant species identification', authors=['Neeraj Kumar', 'Peter N Belhumeur', 'Arijit Biswas', 'David W Jacobs', 'W John Kress', 'Ida C Lopez', 'João VB Soares'], abstract=' We describe the first mobile app for identifying plant species using automatic visual recognition. The system – called Leafsnap – identifies tree species from photographs of their leaves. Key to this system are computer vision components for discarding non-leaf images, segmenting the leaf from an untextured background, extracting features representing the curvature of the leaf’s contour over multiple scales, and identifying the species from a dataset of the 184 trees in the Northeastern United States. Our system obtains state-of-the-art performance on the real-world images from the new Leafsnap Dataset – the largest of its kind. Throughout the paper, we document many of the practical steps needed to produce a computer vision system such as ours, which currently has nearly a million users.', conference=None, journal=None, year=None, reference_list=['10.1023/A:1011139631724', '10.1007/978-3-642-15561-1_32', '10.1109/ICVGIP.2008.47', '10.1007/978-3-540-88693-8_9', '10.2307/25065637', '10.1109/TPAMI.2007.41', '10.1007/BF00994018', '10.1145/1961189.1961199', '10.1145/1646396.1646421', '10.1080/01621459.1975.10480319', '10.1016/0263-7855(86)80086-8', '10.1109/TPAMI.2006.208', '10.1016/j.cagd.2008.01.002'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Facetracer: A search engine for large collections of images with faces': Paper(DOI='10.1007/978-3-540-88693-8_25', crossref_json=None, google_schorlar_metadata=None, title='Facetracer: A search engine for large collections of images with faces', authors=['Neeraj Kumar', 'Peter Belhumeur', 'Shree Nayar'], abstract=' We have created the first image search engine based entirely on faces. Using simple text queries such as “smiling men with blond hair and mustaches,” users can search through over 3.1 million faces which have been automatically labeled on the basis of several facial attributes. Faces in our database have been extracted and aligned from images downloaded from the internet using a commercial face detector, and the number of images and attributes continues to grow daily. Our classification approach uses a novel combination of Support Vector Machines and Adaboost which exploits the strong structure of faces to select and train on the optimal set of features for each attribute. We show state-of-the-art classification results compared to previous works, and demonstrate the power of our architecture through a functional, large-scale face search engine. Our framework is fully automatic, easy to scale, and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF00994018', '10.1109/34.1000244', '10.1109/34.879790', '10.1007/s11263-006-8910-9', '10.1109/CVPRW.2003.10057', '10.1145/1101826.1101866', '10.1007/BF00123143', '10.1109/34.927464', '10.1109/CVPR.1994.323814', '10.1007/978-3-642-72201-1_32', '10.1214/aos/1024691352'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Biometrics', 'Computer Graphics'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'A Bayesian approach to binocular steropsis': Paper(DOI='10.1007/bf00055146', crossref_json=None, google_schorlar_metadata=None, title='A Bayesian approach to binocular steropsis', authors=['Peter N Belhumeur'], abstract=' We develop a computational model for binocular stereopsis, attempting to explain the process by which the information detailing the 3-D geometry of object surfaces is encoded in a pair of stereo images. We design our model within a Bayesian framework, making explicit all of our assumptions about the nature of image coding and the structure of the world. We start by deriving our model for image formation, introducing a definition of half-occluded regions and deriving simple equations relating these regions to the disparity function. We show that the disparity function alone contains enough information to determine the half-occluded regions. We use these relations to derive a model for image formation in which the half-occluded regions are explicitly represented and computed. Next, we present our prior model in a series of three stages, or “worlds,” where each world considers an additional complication to\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.1992.223143', '10.7551/mitpress/7132.001.0001', '10.1068/p200145', '10.5244/C.6.35', '10.1109/TPAMI.1983.4767341', '10.1109/34.134040', '10.1007/3-540-55426-2_48', '10.1109/TPAMI.1984.4767596', '10.1007/978-1-4612-5905-3_6', '10.7551/mitpress/3132.001.0001', '10.1117/12.957520', '10.1007/BFb0028349', '10.1007/3-540-55426-2_45', '10.1109/ICCV.1993.378210', '10.1016/0042-6989(67)90091-0', '10.1126/science.968482', '10.1098/rspb.1979.0029', '10.1080/01621459.1987.10478393', '10.1016/0042-6989(90)90161-D', '10.1038/317314a0', '10.1068/p140449', '10.1007/978-1-4613-1637-4', '10.1137/1.9781611970128', '10.1109/CVPR.1993.340969', '10.1007/BF00204595', '10.1007/978-3-642-58148-9'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Biometrics', 'Computer Graphics'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Res2Net: A New Multi-scale Backbone Architecture': Paper(DOI='10.1109/tpami.2019.2938758', crossref_json=None, google_schorlar_metadata=None, title='Res2Net: A New Multi-scale Backbone Architecture', authors=['Shang-Hua Gao', 'Ming-Ming Cheng', 'Kai Zhao', 'Xin-Yu Zhang', 'Ming-Hsuan Yang', 'Philip Torr'], abstract='Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1023/B:VISI.0000029664.99615.94', '10.1109/CVPR.2015.7298965', '10.1109/CVPR.2017.106', '10.1109/CVPR.2014.43', '10.1109/TPAMI.2018.2878849', '10.1109/TPAMI.2010.70', '10.1109/CVPR.2019.00404', '10.1109/CVPR.2018.00255', '10.1109/ICCV.2017.31', '10.5244/C.30.87', '10.1109/CVPR.2017.512', '10.1109/CVPR.2017.660', '10.1109/CVPR.2019.00405', '10.1109/ICCV.2019.00894', '10.24963/ijcai.2018/166', '10.1109/CVPR.2015.7298731', '10.1007/s41095-019-0149-9', '10.1109/34.993558', '10.1109/ICCV.2011.6126343', '10.1109/TPAMI.2015.2389824', '10.1109/ICCV.2017.322', '10.1109/TPAMI.2018.2815688', '10.1109/CVPR.2016.90', '10.1109/CVPR.2017.243', '10.1109/CVPR.2018.00745', '10.1109/CVPR.2015.7298594', '10.1109/CVPR.2013.407', '10.1109/CVPR.2013.153', '10.1109/ICCV.2015.164', '10.1109/CVPR.2017.634', '10.1109/CVPR.2017.687', '10.1007/s11263-016-0977-3', '10.1109/CVPR.2016.308', '10.1007/s41095-018-0120-1', '10.1109/TPAMI.2014.2345401', '10.1109/CVPR.2017.195', '10.1007/s11263-014-0733-5', '10.1007/s11263-009-0275-4', '10.1109/CVPR.2014.81', '10.1109/ICCV.2017.116', '10.1109/TIP.2015.2487833', '10.1109/TPAMI.2017.2699184', '10.1109/ICCV.2019.00353', '10.1109/ICCV.2017.74', '10.1109/CVPR.2019.00584', '10.1109/CVPR.2019.00340', '10.1109/ICCV.2017.522', '10.1007/s11263-015-0816-y'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics', 'Image Processing', 'Visual Attention', 'Saliency'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'PoolNet+: Exploring the Potential of Pooling for Salient Object Detection': Paper(DOI='10.1109/tpami.2021.3140168', crossref_json=None, google_schorlar_metadata=None, title='PoolNet+: Exploring the Potential of Pooling for Salient Object Detection', authors=['Jiang-Jiang Liu', 'Qibin Hou', 'Zhi-Ang Liu', 'Ming-Ming Cheng'], abstract='We explore the potential of pooling techniques on the task of salient object detection by expanding its role in convolutional neural networks. In general, two pooling-based modules are proposed. A global guidance module (GGM) is first built based on the bottom-up pathway of the U-shape architecture, which aims to guide the location information of the potential salient objects into layers at different feature levels. A feature aggregation module (FAM) is further designed to seamlessly fuse the coarse-level semantic information with the fine-level features in the top-down pathway. We can progressively refine the high-level semantic features with these two modules and obtain detail enriched saliency maps. Experimental results show that our proposed approach can locate the salient objects more accurately with sharpened details and substantially improve the performance compared with the existing state-of-the-art\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2018.00474', '10.1109/CVPR.2016.90', '10.1109/CVPR.2017.404', '10.1109/CVPR.2014.43', '10.1109/TPAMI.2021.3051099', '10.1109/CVPR.2013.407', '10.1007/s41095-019-0149-9', '10.1109/CVPR.2013.153', '10.1109/CVPR.2015.7298938', '10.1007/s11263-015-0822-0', '10.1109/CVPR.2015.7298731', '10.1109/TIP.2015.2487833', '10.1109/CVPR.2015.7298965', '10.1007/978-3-319-46493-0_28', '10.1109/CVPR.2016.618', '10.1109/TPAMI.2004.1273918', '10.1109/CVPR.2006.298', '10.1109/TPAMI.2010.161', '10.1109/TPAMI.2014.2377715', '10.1109/CVPR.2012.6247743', '10.1109/ICCV.2013.370', '10.1109/ICCV.2015.164', '10.1109/CVPR.2017.191', '10.1109/TIP.2018.2874279', '10.1109/TPAMI.2018.2878849', '10.1109/CVPR.2019.00404', '10.1109/CVPR.2017.698', '10.1109/CVPR42600.2020.01011', '10.1109/ICCV.2017.31', '10.1109/ICCV.2017.32', '10.1109/CVPR.2018.00187', '10.1109/CVPR.2013.271', '10.1109/TPAMI.2014.2345401', '10.1109/CVPR.2019.00834', '10.1109/TPAMI.2019.2905607', '10.1109/TPAMI.1986.4767851', '10.1109/CVPR42600.2020.00406', '10.1109/ICCV.2019.00345', '10.1109/WACV.2019.00171', '10.1109/TPAMI.2017.2703082', '10.1109/5.726791', '10.1109/TPAMI.2018.2815688', '10.1109/CVPR.2017.34', '10.1109/ICCV.2017.433', '10.1109/CVPR.2018.00330', '10.1109/CVPR.2017.106', '10.1109/CVPR.2017.660', '10.1007/978-3-030-01267-0_22', '10.1109/CVPR.2018.00081', '10.1109/ICCV.2017.436', '10.1109/CVPR.2018.00326', '10.1109/CVPR.2019.00154', '10.1109/CVPR.2016.80', '10.1109/ICCV.2019.00389', '10.1007/978-3-319-46493-0_50', '10.1007/s11263-021-01490-8', '10.1109/TPAMI.2017.2700300', '10.1145/1833349.1778820', '10.1109/TPAMI.2018.2840724', '10.1109/TPAMI.2017.2662005', '10.1109/TIP.2012.2199502', '10.1109/TPAMI.2021.3107956', '10.1109/TPAMI.2016.2636150', '10.1109/CVPR.2014.119', '10.1109/ICRA.2016.7487379', '10.1109/ICCV.2019.00736', '10.1109/CVPR.2016.28', '10.1109/CVPR.2016.32', '10.1007/978-3-030-58536-5_21', '10.1109/CVPR.2019.00172', '10.1007/978-3-030-01240-3_15', '10.1109/CVPR.2019.00612', '10.1109/CVPR.2019.00766', '10.1109/ICCV.2019.00733', '10.1109/CVPR.2016.58', '10.1109/CVPR.2019.00320', '10.1109/CVPR.2019.00403'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics', 'Image Processing', 'Visual Attention', 'Saliency'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Salient object detection: A survey': Paper(DOI='10.1007/s11042-017-5329-y', crossref_json=None, google_schorlar_metadata=None, title='Salient object detection: A survey', authors=['Ali Borji', 'Ming-Ming Cheng', 'Qibin Hou', 'Huaizu Jiang', 'Jia Li'], abstract=' Detecting and segmenting salient objects from natural scenes, often referred to as salient object detection, has attracted great interest in computer vision. While many models have been proposed and several applications have emerged, a deep understanding of achievements and issues remains lacking. We aim to provide a comprehensive review of recent progress in salient object detection and situate this field among other closely related areas such as generic scene segmentation, object proposal generation, and saliency for fixation prediction. Covering 228 publications, we survey i) roots, key concepts, and tasks, ii) core techniques and main modeling trends, and iii) datasets and evaluation metrics for salient object detection. We also discuss open problems such as evaluation metrics and dataset bias in model performance, and suggest future research directions.', conference=None, journal=None, year=None, reference_list=['10.1109/ICDAR.1997.620597', '10.1007/978-3-540-79547-6_7', '10.1109/CVPRW.2009.5206596', '10.1109/CVPR.2010.5540226', '10.1007/s11042-015-2750-y', '10.1145/1276377.1276390', '10.1109/TPAMI.2009.53', '10.1109/TAES.1986.310772', '10.1109/CVPR.2012.6247706', '10.1109/TIP.2014.2383320', '10.1109/CVPR.2012.6247711', '10.1109/TPAMI.2012.89', '10.1016/j.visres.2013.07.016', '10.1109/TSMC.2013.2279715', '10.1109/TPAMI.2004.60', '10.1007/s11263-006-7934-5', '10.1109/TPAMI.1986.4767851', '10.1109/ICCV.2011.6126333', '10.1007/s00530-003-0105-4', '10.1109/ICCV.2013.193', '10.1109/TPAMI.2014.2345401', '10.1023/A:1022627411411', '10.1016/0167-8655(83)90080-6', '10.1016/j.jvcir.2013.11.009', '10.1109/LSP.2013.2290547', '10.1016/j.jvcir.2014.09.003', '10.1016/j.neucom.2015.10.030', '10.1016/j.jvcir.2015.08.002', '10.1109/TPAMI.1981.4767131', '10.1109/ICPR.2000.905377', '10.1109/ICPR.2014.404', '10.1016/j.image.2013.07.005', '10.1109/ICME.2014.6890142', '10.1109/TIP.2015.2485782', '10.1109/ISCAS.2014.6865190', '10.1109/ICIP.2014.7025666', '10.1109/TPAMI.2011.272', '10.1111/j.1467-8659.2012.03005.x', '10.1109/TIP.2009.2030969', '10.1109/CVPR.2008.4587715', '10.1016/j.neucom.2014.04.054', '10.1007/s11263-015-0822-0', '10.1109/CVPR.2007.383267', '10.1109/LGRS.2012.2210188', '10.1109/TIP.2004.834657', '10.1109/34.730558', '10.1016/S0031-3203(97)00004-6', '10.1016/j.image.2012.11.008', '10.1109/ICCV.2013.221', '10.1016/j.neucom.2015.03.122', '10.1109/TCYB.2014.2356200', '10.1109/CVPR.2013.266', '10.1109/CVPR.2013.271', '10.1049/el.2014.4316', '10.1016/j.image.2015.07.002', '10.1109/ICCV.2009.5459462', '10.1006/cgip.1993.1015', '10.1109/LSP.2014.2366192', '10.1016/0734-189X(85)90125-2', '10.1109/ICRA.2013.6630857', '10.1007/978-3-642-15552-9-46', '10.1109/ICIP.2013.6738707', '10.1109/CVPR.2014.118', '10.1109/TSMC.1979.4310138', '10.1016/0031-3203(86)90030-0', '10.1109/ICCV.2011.6126499', '10.1007/978-94-009-3833-5-5', '10.1007/978-94-009-3833-5-5', '10.1049/iet-cvi.2013.0285', '10.1186/1687-5281-2013-40', '10.1007/978-3-319-34111-8-15', '10.1007/978-3-540-30542-2-18', '10.1109/TNNLS.2015.2513393', '10.1016/j.bica.2013.05.009', '10.1016/0031-3203(96)00009-X', '10.1006/gmip.1997.0455', '10.1007/978-3-540-89646-3-77', '10.1109/TIP.2011.2156803', '10.1016/S0167-8655(98)00057-9', '10.1109/ICCV.2013.413', '10.1109/ICASSP.2014.6854110', '10.1109/CAC.2015.7382582', '10.1109/ICIP.2015.7351189', '10.1109/TIP.2015.2440755', '10.1016/j.patcog.2012.04.017', '10.1016/j.neucom.2015.02.050', '10.1016/j.neucom.2016.04.036', '10.1016/j.visres.2008.09.007', '10.1109/ICME.2006.262821', '10.1109/ICIP.2009.5414466', '10.1016/j.neucom.2014.06.041', '10.1109/CVPR.2007.383047', '10.1109/TPAMI.2010.70', '10.1007/s11042-012-1077-1', '10.1109/TIP.2014.2307434', '10.1109/CVPR.2014.494', '10.1109/ICIP.2015.7351569', '10.1109/APSIPA.2016.7820744', '10.1109/CVPR.2014.357', '10.1109/TIP.2016.2524198', '10.1109/TCSVT.2011.2147230', '10.1016/j.image.2011.10.004', '10.1145/957013.957094', '10.1109/TMM.2005.854410', '10.1016/j.jvcir.2015.08.003', '10.1109/ICTEA.2012.6462867', '10.1049/iet-ipr.2013.0434', '10.1049/el.2014.3334', '10.1109/ICCV.2009.5459467', '10.1109/CVPR.2013.151', '10.1016/j.patrec.2009.08.003', '10.5244/C.24.110', '10.1109/MMSP.2013.6659320', '10.1016/0167-8655(90)90006-N', '10.1016/0031-3203(79)90006-2', '10.1016/j.patcog.2015.09.026', '10.1016/j.visres.2004.07.042', '10.1109/TSMC.1979.4310076', '10.1016/0167-8655(83)90053-3', '10.1117/12.976229', '10.1109/TPAMI.2016.2562626', '10.1109/CVPR.2012.6247743', '10.1109/CVPR.2007.383337', '10.1016/0165-1684(80)90020-1', '10.1016/0146-664X(81)90038-1', '10.1016/j.neucom.2013.09.021', '10.1016/j.neucom.2015.04.055', '10.1007/s00371-015-1176-x', '10.1007/s41095-015-0028-y', '10.1109/CVPR.2015.7298606', '10.1007/978-3-642-15555-0_27', '10.1016/S0167-8655(99)00120-8', '10.1007/3-540-36181-2-46', '10.1016/S0042-6989(02)00040-8', '10.1109/ICMLC.2014.7009083', '10.1109/TCSVT.2013.2280096', '10.1007/978-3-540-76414-4_13', '10.1109/TSMC.1983.6313118', '10.1016/j.patcog.2009.04.021', '10.1145/1015706.1015720', '10.1109/NCVPRIPG.2013.6776270', '10.1016/S0031-3203(96)00065-9', '10.1016/S0031-3203(99)00055-2', '10.1109/CVPR.2013.131', '10.1109/ISCE.2014.6884549', '10.1016/0734-189X(90)90161-N', '10.1117/1.1631315', '10.1016/S0167-8655(99)00142-7', '10.1016/j.jvcir.2006.08.002', '10.1109/CVPR.2012.6247758', '10.1109/TPAMI.2015.2465960', '10.1109/TRO.2009.2022424', '10.1007/s11760-013-0457-y', '10.1109/CIMSIVP.2014.7013277', '10.1016/j.patcog.2013.11.012', '10.1109/CVPR.2013.416', '10.1109/CVPR.2010.5539984', '10.1016/j.ijleo.2015.03.004', '10.1007/s11042-015-2622-5', '10.1007/s00371-014-1059-6', '10.1109/LSP.2014.2323407', '10.1016/j.patcog.2014.12.005', '10.1016/0010-0285(80)90005-5', '10.1109/34.476511', '10.1109/ICCV.2009.5459240', '10.1016/j.neunet.2006.10.001', '10.1109/ICMLC.2016.7860915', '10.1007/s10489-015-0739-x', '10.1016/j.image.2014.01.004', '10.1109/CCDC.2015.7162345', '10.1147/rd.274.0400', '10.1109/TPAMI.1982.4767203', '10.1007/s11042-016-3310-9', '10.1109/TIP.2012.2216276', '10.1109/ICSESS.2013.6615442', '10.1186/1687-5281-2014-31', '10.1109/ICIP.2015.7351379', '10.1109/CVPR.2013.407', '10.1109/ICIP.2014.7025233', '10.1016/j.patcog.2014.12.017', '10.1109/PROC.1980.11753', '10.1016/j.patrec.2014.05.006', '10.1145/1873951.1874105', '10.1016/0031-3203(95)00169-7', '10.1016/j.bica.2014.06.005', '10.1109/LSP.2014.2377216', '10.1167/8.7.32.Introduction', '10.1109/ICIP.2013.6738036', '10.1109/ICIP.2014.7025234', '10.1016/j.ijleo.2014.07.132', '10.1016/j.ijleo.2013.09.007', '10.1007/s00371-015-1065-3', '10.1109/CVPR.2016.618', '10.1016/j.patcog.2016.10.025', '10.1016/j.neucom.2017.02.064', '10.1007/s00530-014-0373-1', '10.1109/CVPR.2015.7298731', '10.1109/ICASSP.2014.6854629', '10.1109/ICIST.2015.7288950', '10.1109/TIP.2015.2438546', '10.1109/ICASSP.2015.7178213', '10.1109/TIP.2014.2361024', '10.5244/C.27.78', '10.1007/s00530-014-0449-y', '10.1109/TIP.2015.2456497'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Neuroscience'], conference_acronym='Multimedia tools and applications', publisher=None, query_handler=None),\n",
       " 'STC: A Simple to Complex Framework for Weakly-supervised Semantic Segmentation': Paper(DOI='10.1109/tpami.2016.2636150', crossref_json=None, google_schorlar_metadata=None, title='STC: A Simple to Complex Framework for Weakly-supervised Semantic Segmentation', authors=['Yunchao Wei', 'Xiaodan Liang', 'Yunpeng Chen', 'Xiaohui Shen', 'Ming-Ming Cheng', 'Yao Zhao', 'Shuicheng Yan'], abstract='Recently, significant improvement has been made on semantic object segmentation due to the development of deep convolutional neural networks (DCNNs). Training such a DCNN usually relies on a large number of images with pixel-level segmentation masks, and annotating these images is very costly in terms of both finance and human effort. In this paper, we propose a simple to complex (STC) framework in which only image-level annotations are utilized to learn DCNNs for semantic segmentation. Specifically, we first train an initial segmentation network called Initial-DCNN with the saliency maps of simple images (i.e., those with a single category of major object(s) and clean background). These saliency maps can be automatically obtained by existing bottom-up salient object detection techniques, where no supervision information is needed. Then, a better network called Enhanced-DCNN is learned with\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2009.5206536', '10.1007/s11263-007-0109-1', '10.1109/ICCV.2015.120', '10.1109/CVPR.2012.6247757', '10.1007/978-3-642-33712-3_7', '10.1109/TMM.2011.2174780', '10.1109/CVPR.2014.408', '10.1109/ICCV.2015.179', '10.1145/1618452.1618470', '10.1109/CVPR.2015.7298965', '10.1109/CVPR.2015.7299025', '10.1109/CVPR.2015.7298780', '10.1109/CVPR.2015.7299002', '10.1109/ICCV.2015.209', '10.1109/TIP.2015.2487833', '10.1007/978-3-319-46493-0_42', '10.1109/TPAMI.2015.2491929', '10.1007/978-3-319-46484-8_25', '10.1109/CVPR.2015.7298594', '10.1109/ICCV.2015.169', '10.1109/CVPR.2014.81', '10.1109/ICCV.2015.191', '10.1109/ICCV.2013.271', '10.1007/978-3-319-10584-0_20', '10.1109/CVPR.2014.49', '10.1109/TPAMI.2014.2345401', '10.1109/CVPR.2014.414', '10.1109/CVPR.2013.271', '10.1109/TPAMI.2015.2465960', '10.1109/ICCV.2011.6126343', '10.1007/s11263-014-0733-5', '10.1007/s00371-013-0867-4', '10.1109/34.730558', '10.1007/978-3-319-46493-0_14', '10.1145/2647868.2654889', '10.1109/CVPR.2009.5206848'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics', 'Image Processing', 'Visual Attention', 'Saliency'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Synthesizing obama: learning lip sync from audio': Paper(DOI='10.1145/3072959.3073640', crossref_json=None, google_schorlar_metadata=None, title='Synthesizing obama: learning lip sync from audio', authors=['Supasorn Suwajanakorn', 'Steven M Seitz', 'Ira Kemelmacher-Shlizerman'], abstract='Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. Given the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.', conference=None, journal=None, year=None, reference_list=['10.1145/2503385.2503473', '10.1109/CVPR.2013.434', '10.1145/2185520.2185563', '10.1145/311535.311537', '10.1145/258734.258880', '10.1145/245.247', '10.1145/2897824.2925873', '10.1145/1095878.1095881', '10.1109/34.927467', '10.1145/2070781.2024164', '10.1145/566570.566594', '10.1109/ICASSP.2015.7178899', '10.1109/TMM.2005.843341', '10.1109/CVPR.2014.537', '10.1109/ASRU.2013.6707742', '10.1016/j.neunet.2005.06.042', '10.1162/neco.1997.9.8.1735', '10.2197/ipsjjip.22.401', '10.1109/CVPR.2012.6247876', '10.1145/2783258.2783356', '10.5555/1577069.1755843', '10.1007/978-3-319-46475-6_23', '10.1109/CVPR.2008.4587845', '10.1016/j.specom.2013.02.005', '10.1016/j.specom.2014.11.001', '10.1145/1141911.1141919', '10.21437/ICSLP.2000-469', '10.1145/2601097.2601137', '10.1109/SII.2015.7404961', '10.1007/978-3-319-10593-2_52', '10.1109/ICCV.2015.450', '10.21437/Interspeech.2016-483', '10.1080/10867651.2004.10487596', '10.1145/2816795.2818056', '10.1109/CVPR.2016.262', '10.1145/1186822.1073209', '10.1109/ICASSP.2012.6288925', '10.21437/Interspeech.2010-194', '10.1016/j.patcog.2006.12.001', '10.1109/TMM.2006.888009', '10.1109/CVPR.2013.75', '10.21437/Interspeech.2013-629'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " 'Computer vision: a modern approach': Paper(DOI='10.1007/978-3-030-42128-1', crossref_json=None, google_schorlar_metadata=None, title='Computer vision: a modern approach', authors=['David A Forsyth', 'Jean Ponce'], abstract='From the Publisher: The accessible presentation of this book gives both a general view of the entire computer vision enterprise and also offers sufficient detail to be able to build useful applications. Users learn techniques that have proven to be useful by first-hand experience and a wide range of mathematical methods. A  CD-ROM with every copy of the text  contains source code for programming practice, color images, and illustrative movies. Comprehensive and up-to-date, this book includes essential topics that either reflect practical significance or are of theoretical importance. Topics are discussed in substantial and increasing depth.  Application surveys  describe numerous important application areas such as image based rendering and digital libraries. Many important algorithms broken down and illustrated in pseudo code. Appropriate for use by engineers as a comprehensive reference to the computer vision\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics', 'Machine Learning'], conference_acronym='Advances in computer vision and pattern recognition (Print)', publisher=None, query_handler=None),\n",
       " 'Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary': Paper(DOI='10.1007/3-540-47979-1_7', crossref_json=None, google_schorlar_metadata=None, title='Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary', authors=['Pinar Duygulu', 'Kobus Barnard', 'Joao FG de Freitas', 'David A Forsyth'], abstract=' We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well — for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1023/A:1009995816485', '10.7551/mitpress/2708.001.0001'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'interdisciplinary computational intelligence (ICI)'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Matching words and pictures': Paper(DOI='10.3724/sp.j.1041.2011.00347', crossref_json=None, google_schorlar_metadata=None, title='Matching words and pictures', authors=['Kobus Barnard', 'Pinar Duygulu', 'David Forsyth', 'Nando De Freitas', 'David M Blei', 'Michael I Jordan'], abstract='We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann’s hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real c 2003 Kobus Barnard, Pinar Duygulu, David Forsyth, Nando de Freitas, David Blei and Michael Jordan.', conference=None, journal=None, year=None, reference_list=['10.1017/S1366728907002891', '10.3758/BF03197716', '10.1017/S1366728998000364', '10.1006/jmla.1997.2538', '10.1037/0033-2909.123.1.71', '10.1016/0010-0277(92)90040-O', '10.1016/j.cogbrainres.2004.05.006', '10.1016/0010-0277(85)90029-0', '10.1037/0033-295X.82.6.407', '10.1002/hbm.20643', '10.1037/0033-295X.111.3.721'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'interdisciplinary computational intelligence (ICI)'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'A novel algorithm for color constancy': Paper(DOI='10.1007/bf00056770', crossref_json=None, google_schorlar_metadata=None, title='A novel algorithm for color constancy', authors=['David A Forsyth'], abstract=' Color constancy is the skill by which it is possible to tell the color of an object even under a colored light. I interpret the color of an object as its color under a fixed canonical light, rather than as a surface reflectance function. This leads to an analysis that shows two distinct sets of circumstances under which color constancy is possible. In this framework, color constancy requires estimating the illuminant under which the image was taken. The estimate is then used to choose one of a set of linear maps, which is applied to the image to yield a color descriptor at each point. This set of maps is computed in advance. The illuminant can be estimated using image measurements alone, because, given a number of weak assumptions detailed in the text, the color of the illuminant is constrained by the colors observed in the image. This constraint arises from the fact that surfaces can reflect no more light than is\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/0016-0032(80)90058-7', '10.1364/JOSAA.3.001651', '10.1016/0167-8655(87)90034-1', '10.1016/0022-5193(78)90175-3', '10.1007/BF00276901', '10.1177/027836498900800101', '10.1364/JOSAA.3.001662', '10.1007/978-1-4899-6431-1', '10.1109/CVPR.1989.37889', '10.1016/0262-8856(90)90055-A', '10.1364/JOSAA.3.001700', '10.3758/BF03202893', '10.1016/0146-664X(74)90022-7', '10.1364/JOSA.50.000254', '10.1364/JOSAA.4.002101', '10.1364/JOSAA.4.001314', '10.1073/pnas.45.1.115', '10.1073/pnas.45.4.636', '10.1073/pnas.80.16.5163', '10.1016/0042-6989(86)90067-2', '10.1364/JOSA.61.000001', '10.1364/JOSAA.3.001694', '10.1364/JOSAA.3.001673', '10.1364/JOSAA.3.000029', '10.1016/0042-6989(76)90020-1', '10.1364/JOSA.67.000779', '10.1109/TPAMI.1987.4767868', '10.1007/BF00275077', '10.1016/0010-4485(82)90173-7', '10.1364/JOSAA.2.001014', '10.1364/JOSAA.3.001708', '10.1016/0042-6989(80)90138-8'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics', 'Machine Learning'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Level set methods and dynamic implicit surfaces': Paper(DOI='10.1007/0-387-22746-6_9', crossref_json=None, google_schorlar_metadata=None, title='Level set methods and dynamic implicit surfaces', authors=['Stanley Osher', 'Ronald P Fedkiw'], abstract='This book is intended primarily as a textbook for second-year undergraduate students in mathematics, mathematical physics, and engineering. The book is designed as a first introduction to the use of mathematical techniques, within continuum theories. It is presumed that the readers have some knowledge of several variable calculus and partial derivatives. The author presents many physical problems to motivate the discussion of the conservation and balance laws derived in the text; however, the emphasis of the book is on the solution to the resulting ordinary and partial differential equations. The physical and practical aspects are used to aid in the formulation of the models and in interpreting the mathematical predictions. To this extent many simple examples that allow closed form solutions are included in the text, which help provide insight into the mathematical solutions presented. Each chapter also includes\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Computational Physics', 'Partial Differential Equations'], conference_acronym='Applied mathematical sciences', publisher=None, query_handler=None),\n",
       " 'A non-oscillatory Eulerian approach to interfaces in multimaterial flows (the ghost fluid method)': Paper(DOI='10.1006/jcph.1999.6236', crossref_json=None, google_schorlar_metadata=None, title='A non-oscillatory Eulerian approach to interfaces in multimaterial flows (the ghost fluid method)', authors=['Ronald P Fedkiw', 'Tariq Aslam', 'Barry Merriman', 'Stanley Osher'], abstract='While Eulerian schemes work well for most gas flows, they have been shown to admit nonphysical oscillations near some material interfaces. In contrast, Lagrangian schemes work well at multimaterial interfaces, but suffer from their own difficulties in problems with large deformations and vorticity characteristic of most gas flows. We believe that the most robust schemes will combine the best properties of Eulerian and Lagrangian schemes. In this paper, we propose a new numerical method for treating interfaces in Eulerian schemes that maintains a Heaviside profile of the density with no numerical smearing along the lines of earlier work and most Lagrangian schemes. We use a level set function to track the motion of a multimaterial interface in an Eulerian framework. In addition, the use of ghost cells (actually ghost nodes in our finite difference framework) and a new isobaric fix technique allows us to keep the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1006/jcph.1998.6090', '10.1016/0045-7825(92)90042-I', '10.1006/jcph.1997.5768', '10.1016/S0168-9274(06)80001-2', '10.1006/jcph.1998.6129', '10.1006/jcph.1998.5913', '10.1006/jcph.1996.5622', '10.1007/978-94-011-5169-6_9', '10.1006/jcph.1999.6379', '10.1103/RevModPhys.40.652', '10.1017/S0022112087002003', '10.1006/jcph.1996.5625', '10.1006/jcph.1996.0130', '10.1137/S106482759528003X', '10.1006/jcph.1994.1080', '10.1007/978-3-0348-8629-1', '10.1006/jcph.1996.0142', '10.1006/jcph.1998.5937', '10.1017/S0022112058000653', '10.1016/0021-9991(92)90229-R', '10.1016/0021-9991(88)90002-2', '10.1137/0728049', '10.1002/cpa.3160130207', '10.1007/BF01065581', '10.1016/0021-9991(89)90222-2', '10.1006/jcph.1994.1155', '10.1017/S0022112096007069', '10.1080/713665233'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Computational Physics', 'Partial Differential Equations'], conference_acronym='Journal of computational physics (Print)', publisher=None, query_handler=None),\n",
       " 'Level set methods: an overview and some recent results': Paper(DOI='10.1006/jcph.2000.6636', crossref_json=None, google_schorlar_metadata=None, title='Level set methods: an overview and some recent results', authors=['Stanley Osher', 'Ronald P Fedkiw'], abstract='The level set method was devised by S. Osher and J. A. Sethian (1988, J. Comput. Phys.79, 12–49) as a simple and versatile method for computing and analyzing the motion of an interface Γ in two or three dimensions. Γ bounds a (possibly multiply connected) region Ω. The goal is to compute and analyze the subsequent motion of Γ under a velocity field v. This velocity can depend on position, time, the geometry of the interface, and the external physics. The interface is captured for later time as the zero level set of a smooth (at least Lipschitz continuous) function ϕ (x, t); i.e., Γ(t)={x|ϕ(x, t)=0}. ϕ is positive inside Ω, negative outside Ω, and is zero on Γ(t). Topological merging and breaking are well defined and easily performed. In this review article we discuss recent variants and extensions, including the motion of curves in three dimensions, the dynamic surface extension method, fast methods for steady state\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1006/jcph.1998.6090', '10.1006/jcph.1995.1098', '10.1006/jcph.1995.1221', '10.1007/BF00375127', '10.4310/jdg/1214458529', '10.1016/0362-546X(84)90020-8', '10.1137/0522022', '10.1080/03605309808821392', '10.1137/S0036142997323521', '10.1016/0021-9991(92)90240-Y', '10.1016/S0893-9659(99)00026-9', '10.1007/BF01385685', '10.1023/A:1007979827043', '10.1007/3-540-48236-9_13', '10.1006/jcph.1996.0072', '10.4310/jdg/1214446564', '10.1006/jcph.1997.5721', '10.1137/0907073', '10.1090/S0273-0979-1992-00266-5', '10.1002/cpa.3160450903', '10.4310/jdg/1214446559', '10.1006/jcph.1999.6236', '10.1006/jcph.1999.6320', '10.1137/S0036139995290794', '10.1006/jcph.1996.0155', '10.1006/jcph.1998.6115', '10.1117/12.240959', '10.1017/S0962492900002567', '10.1006/jcph.1997.5689', '10.1137/S106482759732455X', '10.1137/S106482759528003X', '10.1006/jcph.1994.1080', '10.1103/PhysRevE.62.2471', '10.1016/0167-2789(93)90120-P', '10.1006/jcph.2000.6444', '10.1006/jcph.1994.1105', '10.1201/9780203755518-4', '10.1016/0021-9991(92)90229-R', '10.1002/cpa.3160420503', '10.1007/3-540-48236-9', '10.1137/0912065', '10.1007/3-540-08004-X_336', '10.4310/AJM.1997.v1.n3.a6', '10.1137/0524066', '10.1016/0021-9991(88)90002-2', '10.1137/0728049', '10.1006/jcph.1999.6345', '10.1090/conm/238/03552', '10.1017/S0308210500023106', '10.1137/0729053', '10.1016/0167-2789(92)90242-F', '10.1137/S0036144598347059', '10.1016/0021-9991(92)90140-T', '10.1063/1.369429', '10.1063/1.369430', '10.1016/0362-546X(94)90108-2', '10.1006/jcph.1999.6389', '10.1016/S0045-7930(97)00053-4', '10.1006/jcph.1994.1155', '10.1109/9.412624', '10.1016/0021-9991(92)90307-K', '10.1006/jcph.1996.0167', '10.1006/jcph.1997.5810'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Computational Physics', 'Partial Differential Equations'], conference_acronym='Journal of computational physics (Print)', publisher=None, query_handler=None),\n",
       " 'A hybrid particle level set method for improved interface capturing': Paper(DOI='10.1006/jcph.2002.7166', crossref_json=None, google_schorlar_metadata=None, title='A hybrid particle level set method for improved interface capturing', authors=['Douglas Enright', 'Ronald Fedkiw', 'Joel Ferziger', 'Ian Mitchell'], abstract='In this paper, we propose a new numerical method for improving the mass conservation properties of the level set method when the interface is passively advected in a flow field. Our method uses Lagrangian marker particles to rebuild the level set in regions which are underresolved. This is often the case for flows undergoing stretching and tearing. The overall method maintains a smooth geometrical description of the interface and the implementation simplicity characteristic of the level set method. Our method compares favorably with volume of fluid methods in the conservation of mass and purely Lagrangian schemes for interface resolution. The method is presented in three spatial dimensions.', conference=None, journal=None, year=None, reference_list=['10.1006/jcph.1995.1098', '10.2172/4456296', '10.1016/0021-9991(89)90151-4', '10.1006/jcph.2000.6624', '10.1006/jcph.2001.6810', '10.1006/jcph.1995.1025', '10.1002/(SICI)1097-0363(19971015)25:7<749::AID-FLD584>3.0.CO;2-O', '10.1063/1.1761178', '10.1063/1.1761178', '10.1016/0021-9991(81)90145-5', '10.1137/S106482759732455X', '10.1023/A:1011178417620', '10.1006/jcph.1994.1123', '10.1006/jcph.1994.1188', '10.1137/0733033', '10.1006/jcph.2000.6636', '10.1016/0021-9991(88)90002-2', '10.1006/jcph.1999.6345', '10.1006/jcph.1996.5590', '10.1115/1.2817323', '10.1006/jcph.1998.5906', '10.1007/BF01210742', '10.1007/978-1-4612-4656-5_18', '10.1073/pnas.93.4.1591', '10.1137/S0036144598347059', '10.1006/jcph.2000.6657', '10.1016/0021-9991(88)90177-5', '10.1175/1520-0493(1982)110<1968:TMDCAS>2.0.CO;2', '10.1006/jcph.2000.6537', '10.1006/jcph.1998.6106', '10.1137/S1064827596298245', '10.1016/S0045-7930(97)00053-4', '10.1006/jcph.1994.1155', '10.1006/jcph.2000.6635', '10.1006/jcph.2001.6726', '10.1016/0021-9991(92)90307-K', '10.2514/6.1999-3320', '10.1016/0021-9991(79)90051-2', '10.1006/jcph.2000.6658'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Computational Physics', 'Partial Differential Equations'], conference_acronym='Journal of computational physics (Print)', publisher=None, query_handler=None),\n",
       " 'A second-order-accurate symmetric discretization of the Poisson equation on irregular domains': Paper(DOI='10.1006/jcph.2001.6977', crossref_json=None, google_schorlar_metadata=None, title='A second-order-accurate symmetric discretization of the Poisson equation on irregular domains', authors=['Frederic Gibou', 'Ronald P Fedkiw', 'Li-Tien Cheng', 'Myungjoo Kang'], abstract='In this paper, we consider the variable coefficient Poisson equation with Dirichlet boundary conditions on an irregular domain and show that one can obtain second-order accuracy with a rather simple discretization. Moreover, since our discretization matrix is symmetric, it can be inverted rather quickly as opposed to the more complicated nonsymmetric discretization matrices found in other second-order-accurate discretizations of this problem. Multidimensional computational results are presented to demonstrate the second-order accuracy of this numerical method. In addition, we use our approach to formulate a second-order-accurate symmetric implicit time discretization of the heat equation on irregular domains. Then we briefly consider Stefan problems.', conference=None, journal=None, year=None, reference_list=['10.1006/jcph.1998.6090', '10.1016/S0021-9991(83)71112-5', '10.1137/0729022', '10.1006/jcph.1997.5721', '10.1006/jcph.1999.6236', '10.1006/jcph.1999.6320', '10.1006/jcph.1998.6129', '10.1006/jcph.1998.5965', '10.1006/jcph.2000.6595', '10.1006/jcph.1996.0011', '10.1023/A:1011178417620', '10.1103/PhysRevE.62.2471', '10.1137/0731054', '10.1137/S0036142995291329', '10.1006/jcph.2000.6444', '10.1006/jcph.2001.6812', '10.1016/0021-9991(88)90002-2', '10.1016/0021-9991(77)90100-0', '10.1006/jcph.2000.6634', '10.1006/jcph.1996.0095', '10.1137/S0036144598347059', '10.1016/0021-9991(92)90140-T', '10.1016/0021-9991(88)90177-5', '10.1006/jcph.1994.1155', '10.1006/jcph.1999.6294'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Computational Physics', 'Partial Differential Equations'], conference_acronym='Journal of computational physics (Print)', publisher=None, query_handler=None),\n",
       " 'Invertible finite elements for robust simulation of large deformation': Paper(DOI='10.1145/1028523.1028541', crossref_json=None, google_schorlar_metadata=None, title='Invertible finite elements for robust simulation of large deformation', authors=['Geoffrey Irving', 'Joseph Teran', 'Ronald Fedkiw'], abstract='We present an algorithm for the finite element simulation of elastoplastic solids which is capable of robustly and efficiently handling arbitrarily large deformation. In fact, our model remains valid even when large parts of the mesh are inverted. The algorithm is straightforward to implement and can be used with any material constitutive model, and for both volumetric solids and thin shells such as cloth. We also provide a mechanism for controlling plastic deformation, which allows a deformable object to be guided towards a desired final shape without sacrificing realistic behavior. Finally, we present an improved method for rigid body collision handling in the context of mixed explicit/implicit time-stepping.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Computational Physics', 'Partial Differential Equations'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'A survey on visual surveillance of object motion and behaviors': Paper(DOI='10.1109/tsmcc.2004.829274', crossref_json=None, google_schorlar_metadata=None, title='A survey on visual surveillance of object motion and behaviors', authors=['Weiming Hu', 'Tieniu Tan', 'Liang Wang', 'Steve Maybank'], abstract='Visual surveillance in dynamic scenes, especially for humans and vehicles, is currently one of the most active research topics in computer vision. It has a wide spectrum of promising applications, including access control in special areas, human identification at a distance, crowd flux statistics and congestion analysis, detection of anomalous behaviors, and interactive surveillance using multiple cameras, etc. In general, the processing framework of visual surveillance in dynamic scenes includes the following stages: modeling of environments, detection of motion, classification of moving objects, tracking, understanding and description of behaviors, human identification, and fusion of data from multiple cameras. We review recent developments and general strategies of all these stages. Finally, we analyze possible research directions, e.g., occlusion handling, a combination of twoand three-dimensional tracking, a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.982901', '10.1006/cviu.2000.0894', '10.1109/34.917571', '10.1109/CBMS.1997.596423', '10.1109/76.611169', '10.1109/ICSMC.1995.538259', '10.1109/34.868678', '10.1016/S0031-3203(02)00100-0', '10.1109/CVPR.1999.786987', '10.1006/cviu.2001.0907', '10.1109/MNRAO.1994.346251', '10.1109/34.771328', '10.1109/34.841758', '10.1023/A:1020346032608', '10.1109/CVPR.1997.609450', '10.1016/S0031-3203(99)00100-4', '10.1109/AFGR.2002.1004148', '10.1109/AFGR.2002.1004176', '10.1016/S0262-8856(02)00098-7', '10.1006/ciun.1994.1006', '10.1109/CVPR.1994.323868', '10.1109/CVPR.2001.990508', '10.1109/AFGR.2002.1004151', '10.1006/cviu.1999.0758', '10.1023/A:1008151520284', '10.1109/5.959335', '10.1109/AFGR.1998.671000', '10.1109/TPAMI.2000.868676', '10.1109/CVPR.1999.784654', '10.1109/ICPR.2002.1048115', '10.5244/C.12.46', '10.1006/cviu.1997.0620', '10.1016/j.imavis.2005.06.003', '10.1109/AFGR.1996.557241', '10.5244/C.14.36', '10.1109/AFGR.2000.840634', '10.1109/34.655647', '10.1109/CVPR.1997.609382', '10.1109/6046.766736', '10.1109/ICCV.2001.937545', '10.1109/CVPR.2001.990509', '10.1109/ICCV.1999.790292', '10.1006/cviu.2000.0892', '10.1109/MNRAO.1994.346253', '10.1109/VS.2000.856852', '10.1109/AFGR.2002.1004182', '10.1109/ICPR.1998.711911', '10.1016/S0167-8655(98)00044-0', '10.1109/6046.784465', '10.1109/ACV.1998.732851', '10.1006/cviu.2000.0870', '10.1109/ICPR.1998.711214', '10.1109/CVPR.1999.784637', '10.1007/BF01420984', '10.1109/ICPR.1994.576243', '10.1109/ICPR.1996.547291', '10.1109/34.868681', '10.1109/ICCV.1999.791228', '10.1109/CVPR.1998.698583', '10.1109/34.868683', '10.1016/S0262-8856(99)00023-2', '10.1109/34.598236', '10.1109/ICMI.2002.1167025', '10.1109/AFGR.1998.670921', '10.1016/0031-3203(92)90007-6', '10.1109/TPAMI.2002.1046148', '10.1109/AFGR.1996.557263', '10.1109/5.381842', '10.1109/34.809119', '10.1109/ICPR.1996.546796', '10.1109/CVPR.1999.784638'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'image processing', 'applications of probability'], conference_acronym='IEEE transactions on systems, man and cybernetics. Part C, Applications and reviews', publisher=None, query_handler=None),\n",
       " 'Silhouette analysis-based gait recognition for human identification': Paper(DOI='10.1016/j.patcog.2010.10.011', crossref_json=None, google_schorlar_metadata=None, title='Silhouette analysis-based gait recognition for human identification', authors=['Liang Wang', 'Tieniu Tan', 'Huazhong Ning', 'Weiming Hu'], abstract='Human identification at a distance has recently gained growing interest from computer vision researchers. Gait recognition aims essentially to address this problem by identifying people based on the way they walk. In this paper, a simple but efficient gait recognition algorithm using spatial-temporal silhouette analysis is proposed. For each image sequence, a background subtraction algorithm and a simple correspondence procedure are first used to segment and track the moving silhouettes of a walking figure. Then, eigenspace transformation based on principal component analysis (PCA) is applied to time-varying distance signals derived from a sequence of silhouette images to reduce the dimensionality of the input feature space. Supervised pattern classification techniques are finally performed in the lower-dimensional eigenspace for recognition. This method implicitly captures the structural and transitional\\xa0…', conference=None, journal=None, year=None, reference_list=['10.2106/00004623-196446020-00009', '10.3758/BF03208295', '10.3758/BF03337021', '10.1016/j.patcog.2003.09.012', '10.1109/AFGR.2004.1301502', '10.1109/CVPR.2001.990506', '10.1109/TCSVT.2003.821972', '10.1109/TIP.2004.832865', '10.1109/TPAMI.2005.39', '10.1109/TPAMI.2006.38', '10.1109/TPAMI.2007.1096', '10.1109/TNN.2007.901277', '10.1109/TIP.2007.906769', '10.1109/TSMCB.2007.911536', '10.1109/TSMCC.2007.913886', '10.1109/TIP.2009.2020535', '10.1016/j.patcog.2007.06.019', '10.1016/j.patcog.2006.11.014', '10.1145/212094.212141', '10.1145/1177352.1177355', '10.1049/ip-vis:19990187', '10.1016/0167-8655(95)00109-3', '10.1016/S0167-8655(03)00094-1', '10.1109/TPAMI.2003.1251144', '10.5244/C.23.113', '10.1109/34.910878', '10.1016/j.sigpro.2010.01.024', '10.1016/0004-3702(81)90024-2', '10.1109/34.598228', '10.1162/jocn.1991.3.1.71', '10.1109/34.879790', '10.1016/j.patcog.2010.03.011'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'Recent developments in human motion analysis': Paper(DOI='10.1016/s0031-3203(02)00100-0', crossref_json=None, google_schorlar_metadata=None, title='Recent developments in human motion analysis', authors=['Liang Wang', 'Weiming Hu', 'Tieniu Tan'], abstract='Visual analysis of human motion is currently one of the most active research topics in computer vision. This strong interest is driven by a wide spectrum of promising applications in many areas such as virtual reality, smart surveillance, perceptual interface, etc. Human motion analysis concerns the detection, tracking and recognition of people, and more generally, the understanding of human behaviors, from image sequences involving humans. This paper provides a comprehensive survey of research on computer-vision-based human motion analysis. The emphasis is on three major issues involved in a general human motion analysis system, namely human detection, tracking and activity understanding. Various methods for each issue are discussed in order to examine the state of the art. Finally, some research challenges and future directions are discussed.', conference=None, journal=None, year=None, reference_list=['10.1109/34.868683', '10.1016/S0262-8856(98)00099-7', '10.1006/cviu.1998.0716', '10.1109/TPAMI.2000.868676', '10.1023/A:1008151520284', '10.1109/AFGR.1998.671000', '10.1109/ACV.1996.572043', '10.1109/AFGR.1998.670921', '10.1109/ICASSP.1998.679696', '10.1049/ic:20000470', '10.1049/ip-vis:19990187', '10.1177/002029409903200902', '10.1109/ICECS.1999.813392', '10.1109/CVPR.1999.786981', '10.1109/CVPR.1996.517058', '10.1109/AFGR.1996.557293', '10.1049/ic:19990575', '10.1109/CBMS.1997.596423', '10.1109/ICIP.1997.631988', '10.1109/AFGR.1996.557250', '10.1109/MNRAO.1994.346261', '10.1016/0262-8856(95)93154-K', '10.1109/NAMW.1997.609859', '10.1006/cviu.1998.0744', '10.1109/34.824823', '10.1006/cviu.2000.0897', '10.1109/ACV.1992.240332', '10.1007/BF01213527', '10.1109/34.598236', '10.1109/CVPR.1999.784637', '10.1006/cviu.2000.0870', '10.1109/PACRIM.1999.799484', '10.1007/3-540-45053-X_48', '10.1109/ACV.1998.732851', '10.1117/12.950785', '10.1109/34.161348', '10.5244/C.3.36', '10.1109/ICCV.1990.139525', '10.1007/BF01420984', '10.1109/CVPR.1997.609440', '10.5244/C.9.41', '10.1109/CVPR.1997.609382', '10.5244/C.14.42', '10.1109/ICPR.1996.547291', '10.1109/34.868681', '10.1109/CVPR.1997.609319', '10.1109/ICPR.1998.711083', '10.1109/ICCV.2001.937617', '10.1109/ICIP.1997.631988', '10.1109/34.917571', '10.1023/A:1011179004708', '10.1023/A:1008078328650', '10.1007/3-540-45053-X_45', '10.1109/ICCV.1999.791203', '10.1109/ICCV.1995.466861', '10.1007/BFb0028333', '10.5244/C.12.46', '10.1109/CVPR.1997.609292', '10.1016/S0031-3203(99)00100-4', '10.5244/C.14.36', '10.1006/jvci.1994.1001', '10.1109/ICPR.1994.576929', '10.1109/34.385981', '10.1109/ICPR.1996.546985', '10.1109/CVPR.1994.323868', '10.1109/AFGR.1996.557241', '10.1006/ciun.1994.1006', '10.1006/cviu.1999.0758', '10.1109/ICCV.1995.466882', '10.1109/CVPR.1996.517057', '10.1109/MNRAO.1994.346250', '10.1016/S0031-3203(00)00014-5', '10.1109/MNRAO.1994.346236', '10.1007/BFb0015549', '10.1109/34.841758', '10.1109/34.865191', '10.1109/34.771328', '10.1109/34.857008', '10.1016/0262-8856(96)01092-X', '10.1016/S0031-3203(00)00019-4', '10.1109/ICPR.1996.547022', '10.1109/ICIP.1995.529584', '10.1109/ICPR.1996.546795', '10.1109/34.868682', '10.1023/A:1008103604354', '10.1109/ICIP.1994.413857', '10.1109/ACV.1998.732852', '10.1109/ICPR.1996.546796', '10.1109/CVPR.1996.517056', '10.1109/ICPR.1998.711214', '10.5244/C.14.57', '10.1007/BF01450849', '10.1006/cviu.2000.0888', '10.1109/ICCV.2001.937587', '10.1006/cviu.1997.0620', '10.1109/AFGR.2000.840662', '10.1109/ICCV.1999.791221', '10.1016/0097-8493(92)90021-M', '10.1007/3-540-49384-0_3', '10.1109/CVPR.1997.609290', '10.1109/CVPR.1998.698581', '10.1109/ICPR.1998.711082', '10.1109/ICCV.1999.790292', '10.1007/3-540-44690-7_3', '10.1109/TASSP.1980.1163491', '10.1109/ICCV.1995.466914', '10.1109/MNRAO.1994.346259', '10.1109/ICASSP.1988.196495', '10.1109/5.18626', '10.1109/ISCV.1995.477012', '10.1109/CVPR.1992.223161', '10.1109/CVPR.1997.609450', '10.1109/ICCV.1998.710744', '10.1109/MNRAO.1994.346256', '10.1109/ICCV.1998.710709', '10.1006/cviu.2000.0894', '10.1142/S0129065799000319', '10.1109/NAMW.1997.609846', '10.1109/ACV.1996.571995', '10.1109/CVPR.1997.609439', '10.1109/ICCV.1995.466880', '10.1109/ICCV.1998.710817', '10.1109/ICPR.2000.903020', '10.1007/3-540-60343-3_42', '10.1162/089976699300016890', '10.1109/VS.2000.856858', '10.1109/AFGR.2000.840657', '10.1023/A:1008071332753', '10.1109/AFGR.2000.840661', '10.1109/ICCV.2001.937545', '10.1006/cviu.2001.0907'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " \"Changes in hippocampal connectivity in the early stages of Alzheimer's disease: evidence from resting state fMRI\": Paper(DOI='10.1016/j.neuroimage.2005.12.033', crossref_json=None, google_schorlar_metadata=None, title=\"Changes in hippocampal connectivity in the early stages of Alzheimer's disease: evidence from resting state fMRI\", authors=['Liang Wang', 'Yufeng Zang', 'Yong He', 'Meng Liang', 'Xinqing Zhang', 'Lixia Tian', 'Tao Wu', 'Tianzi Jiang', 'Kuncheng Li'], abstract=\"A selective distribution of Alzheimer's disease (AD) pathological lesions in specific cortical layers isolates the hippocampus from the rest of the brain. However, functional connectivity between the hippocampus and other brain regions remains unclear in AD. Here, we employ a resting state functional MRI (fMRI) to examine changes in hippocampal connectivity comparing 13 patients with mild AD versus 13 healthy age-matched controls. Hippocampal connectivity was investigated by examination of the correlation between low frequency fMRI signal fluctuations in the hippocampus and those in all other brain regions. We found that functional connectivity between the right hippocampus and a set of regions was disrupted in AD; these regions are: medial prefrontal cortex (MPFC), ventral anterior cingulate cortex (vACC), right inferotemporal cortex, right cuneus extending into precuneus, left cuneus, right superior and\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1176/ajp.152.11.1576', '10.1017/S1355617702870072', '10.1002/mrm.1910340409', '10.1002/(SICI)1099-1492(199706/08)10:4/5<165::AID-NBM454>3.0.CO;2-7', '10.1097/00004647-199703000-00007', '10.1056/NEJM200008173430701', '10.1038/35090048', '10.1162/0898929042568578', '10.1006/cbmr.1996.0014', '10.1006/nimg.1998.0424', '10.1016/0197-4580(93)90015-4', '10.1023/A:1023832305702', '10.1073/pnas.0504136102', '10.1038/jcbfm.1993.4', '10.1136/jnnp.68.1.93', '10.1093/brain/124.4.739', '10.1523/JNEUROSCI.23-03-00986.2003', '10.1073/pnas.0135058100', '10.1073/pnas.0308627101', '10.1038/35094500', '10.1002/hbm.10022', '10.1001/archneur.1962.04210030065009', '10.1016/S1053-8119(18)31587-8', '10.1097/00001756-199511270-00005', '10.1126/science.6474172', '10.1002/ana.410200406', '10.1002/hbm.460030306', '10.1002/hbm.20012', '10.1093/cercor/8.5.451', '10.1038/35090055', '10.1162/0898929042568587', '10.1148/radiol.2251011301', '10.1006/nimg.1997.0315', '10.1006/nimg.2000.0654', '10.1212/WNL.34.7.939', '10.1002/ana.410420114', '10.1146/annurev.ne.16.030193.001333', '10.1212/WNL.43.11.2412-a', '10.1001/archneur.58.3.397', '10.1038/nrn1433', '10.1017/S1355617704105080', '10.1073/pnas.98.2.676', '10.1016/S1053-8119(03)00386-0', '10.1016/j.biopsych.2004.12.031', '10.1162/jocn.1997.9.5.648', '10.1016/S1053-8119(03)00391-4', '10.1146/annurev.ne.19.030196.000545', '10.1002/ana.410300410', '10.1212/WNL.42.9.1743'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Fluid mechanics', 'Computational fluid dynamics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Mobile-edge computing: Partial computation offloading using dynamic voltage scaling': Paper(DOI='10.1109/tcomm.2016.2599530', crossref_json=None, google_schorlar_metadata=None, title='Mobile-edge computing: Partial computation offloading using dynamic voltage scaling', authors=['Yanting Wang', 'Min Sheng', 'Xijun Wang', 'Liang Wang', 'Jiandong Li'], abstract='The incorporation of dynamic voltage scaling technology into computation offloading offers more flexibilities for mobile edge computing. In this paper, we investigate partial computation offloading by jointly optimizing the computational speed of smart mobile device (SMD), transmit power of SMD, and offloading ratio with two system design objectives: energy consumption of SMD minimization (ECM) and latency of application execution minimization (LM). Considering the case that the SMD is served by a single cloud server, we formulate both the ECM problem and the LM problem as nonconvex problems. To tackle the ECM problem, we recast it as a convex one with the variable substitution technique and obtain its optimal solution. To address the nonconvex and nonsmooth LM problem, we propose a locally optimal algorithm with the univariate search technique. Furthermore, we extend the scenario to a multiple\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/VTCFall.2012.6399281', '10.1109/MWC.2011.5876496', '10.1109/ICOIN.2013.6496404', '10.1109/TCE.2007.4429222', '10.1017/CBO9780511804441', '10.1109/TVT.2014.2310394', '10.1109/WCNCW.2014.6934853', '10.1109/VTCSpring.2015.7146129', '10.1109/PIMRC.2014.7136401', '10.1109/MC.2010.98', '10.1109/SPAWC.2013.6612005', '10.1109/TVT.2014.2372852', '10.1109/TWC.2012.041912.110912', '10.1109/TC.2014.2366735', '10.1145/2479942.2479946', '10.1109/TWC.2013.072513.121842', '10.1109/EuCNC.2014.6882634', '10.1145/1814433.1814441', '10.1109/ICC.2015.7249203', '10.1016/j.future.2012.05.023', '10.1109/INFCOM.2012.6195845', '10.1109/TVT.2011.2157370', '10.1145/1966445.1966473', '10.1109/MWC.2014.7000967', '10.1109/MNET.2013.6616112', '10.1109/TSIPN.2015.2448520', '10.1007/s11036-012-0368-0', '10.1109/ICCW.2015.7247586', '10.1109/TPDS.2014.2316834', '10.1109/TWC.2014.2331051', '10.1109/TCC.2016.2560808', '10.1109/PIMRC.2014.7136330'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Fluid mechanics', 'Computational fluid dynamics'], conference_acronym='IEEE transactions on communications (Print)', publisher=None, query_handler=None),\n",
       " 'Predicting the next location: A recurrent model with spatial and temporal contexts': Paper(DOI='10.1609/aaai.v30i1.9971', crossref_json=None, google_schorlar_metadata=None, title='Predicting the next location: A recurrent model with spatial and temporal contexts', authors=['Qiang Liu', 'Shu Wu', 'Liang Wang', 'Tieniu Tan'], abstract='Spatial and temporal contextual information plays a key role for analyzing user behaviors, and is helpful for predicting where he or she will go next. With the growing ability of collecting information, more and more temporal and spatial contextual information is collected in systems, and the location prediction problem becomes crucial and feasible. Some works have been proposed to address this problem, but they all have their limitations. Factorizing Personalized Markov Chain (FPMC) is constructed based on a strong independence assumption among different factors, which limits its performance. Tensor Factorization (TF) faces the cold start problem in predicting future actions. Recurrent Neural Networks (RNN) model shows promising performance comparing with PFMC and TF, but all these methods have problem in modeling continuous time interval and geographical distance. In this paper, we extend RNN and propose a novel method called Spatial Temporal Recurrent Neural Networks (ST-RNN). ST-RNN can model local temporal and spatial contexts in each layer with time-specific transition matrices for different time intervals and distance-specific transition matrices for different geographical distances. Experimental results show that the proposed ST-RNN model yields significant improvements over the competitive compared methods on two typical datasets, ie, Global Terrorism Database (GTD) and Gowalla dataset.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Fluid mechanics', 'Computational fluid dynamics'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Thin-film Sb2Se3 photovoltaics with oriented one-dimensional ribbons and benign grain boundaries': Paper(DOI='10.1038/nphoton.2015.78', crossref_json=None, google_schorlar_metadata=None, title='Thin-film Sb2Se3 photovoltaics with oriented one-dimensional ribbons and benign grain boundaries', authors=['Ying Zhou', 'Liang Wang', 'Shiyou Chen', 'Sikai Qin', 'Xinsheng Liu', 'Jie Chen', 'Ding-Jiang Xue', 'Miao Luo', 'Yuanzhi Cao', 'Yibing Cheng', 'Edward H Sargent', 'Jiang Tang'], abstract='Solar cells based on inorganic absorbers, such as Si, GaAs, CdTe and Cu(In,Ga)Se2, permit a high device efficiency and stability. The crystals’ three-dimensional structure means that dangling bonds inevitably exist at the grain boundaries (GBs), which significantly degrades the device performance via recombination losses. Thus, the growth of single-crystalline materials or the passivation of defects at the GBs is required to address this problem, which introduces an added processing complexity and cost. Here we report that antimony selenide (Sb2Se3)—a simple, non-toxic and low-cost material with an optimal solar bandgap of ∼1.1\\u2005eV—exhibits intrinsically benign GBs because of its one-dimensional crystal structure. Using a simple and fast (∼1\\u2005μm min–1) rapid thermal evaporation process, we oriented crystal growth perpendicular to the substrate, and produced Sb2Se3 thin-film solar cells with a certified\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1002/aenm.201301465', '10.1038/nmat4014', '10.1002/adma.201203146', '10.1002/pip.1160', '10.1038/nmat4065', '10.1038/ncomms3306', '10.1021/ja805845q', '10.1038/nmat3789', '10.1016/j.solmat.2014.07.002', '10.1002/adfm.201304238', '10.1103/PhysRevB.57.9642', '10.1021/nn5052585', '10.1002/adma.201306281', '10.1038/nmat3118', '10.1002/pip.823', '10.1002/anie.201308331', '10.1002/adfm.201101103', '10.1063/1.4874878', '10.1021/am502427s', '10.1038/nature13435', '10.1038/ncomms2664', '10.1063/1.1906331', '10.1063/1.1737796', '10.1002/adma.201103470', '10.1103/PhysRevLett.112.156103', '10.1002/aenm.201400496', '10.1039/c3ee43169j', '10.1063/1.4894170', '10.1103/PhysRevB.54.11169', '10.1103/PhysRevLett.92.246401'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Fluid mechanics', 'Computational fluid dynamics'], conference_acronym='Nature photonics (Print)', publisher=None, query_handler=None),\n",
       " 'Gram-scale synthesis of single-crystalline graphene quantum dots with superior optical properties': Paper(DOI='10.1038/ncomms6357', crossref_json=None, google_schorlar_metadata=None, title='Gram-scale synthesis of single-crystalline graphene quantum dots with superior optical properties', authors=['Liang Wang', 'Yanli Wang', 'Tao Xu', 'Haobo Liao', 'Chenjie Yao', 'Yuan Liu', 'Zhen Li', 'Zhiwen Chen', 'Dengyu Pan', 'Litao Sun', 'Minghong Wu'], abstract='Graphene quantum dots (GQDs) have various alluring properties and potential applications, but their large-scale applications are limited by current synthetic methods that commonly produce GQDs in small amounts. Moreover, GQDs usually exhibit polycrystalline or highly defective structures and thus poor optical properties. Here we report the gram-scale synthesis of single-crystalline GQDs by a facile molecular fusion route under mild and green hydrothermal conditions. The synthesis involves the nitration of pyrene followed by hydrothermal treatment in alkaline aqueous solutions, where alkaline species play a crucial role in tuning their size, functionalization and optical properties. The single-crystalline GQDs are bestowed with excellent optical properties such as bright excitonic fluorescence, strong excitonic absorption bands extending to the visible region, large molar extinction coefficients and long-term\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1126/science.1154663', '10.1002/adma.200902825', '10.1021/ja1009376', '10.1021/nl101060h', '10.1021/jz100862f', '10.1021/ja2036749', '10.1038/nnano.2011.30', '10.1002/adma.201003819', '10.1021/ja204953k', '10.1039/c2ee22982j', '10.1002/adma.201201930', '10.1021/ja206030c', '10.1021/nl2038979', '10.1021/ja309270h', '10.1021/nl400368v', '10.1039/c2jm16005f', '10.1021/nn4053342', '10.1038/ncomms3943', '10.1021/ja4132246', '10.1039/c3nr03623e', '10.1039/c3ta00059a', '10.1002/adma.201305299', '10.1002/adfm.201203771', '10.1021/nn500368m', '10.1021/nn4023137', '10.1039/c1cc11122a', '10.1016/j.biomaterials.2012.06.060', '10.1039/c2jm16835a', '10.1002/adfm.201201499', '10.1016/j.bios.2013.09.022', '10.1039/C3NR06353D', '10.1039/C3CC47701K', '10.1002/ppsc.201300170', '10.1021/nl404281h', '10.1021/am404638e', '10.1038/am.2013.38', '10.1002/smll.201302286', '10.1021/ja062677d', '10.1021/ja073527l', '10.1002/anie.201303927', '10.1002/anie.200906623', '10.1007/s12274-008-8021-8', '10.1002/anie.201200474', '10.1002/anie.201106102', '10.1021/cr068010r', '10.1021/ja710234t', '10.1002/anie.200704909', '10.1002/anie.200907289', '10.1038/nature09211', '10.3123/jemsge.28.160', '10.1002/mrc.1270150215', '10.1126/science.6996095', '10.1126/science.281.5385.2013'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Fluid mechanics', 'Computational fluid dynamics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Uncovering intrinsic modular organization of spontaneous brain activity in humans': Paper(DOI='10.1371/journal.pone.0005226', crossref_json=None, google_schorlar_metadata=None, title='Uncovering intrinsic modular organization of spontaneous brain activity in humans', authors=['Yong He', 'Jinhui Wang', 'Liang Wang', 'Zhang J Chen', 'Chaogan Yan', 'Hong Yang', 'Hehan Tang', 'Chaozhe Zhu', 'Qiyong Gong', 'Yufeng Zang', 'Alan C Evans'], abstract='The characterization of topological architecture of complex brain networks is one of the most challenging issues in neuroscience. Slow (<0.1 Hz), spontaneous fluctuations of the blood oxygen level dependent (BOLD) signal in functional magnetic resonance imaging are thought to be potentially important for the reflection of spontaneous neuronal activity. Many studies have shown that these fluctuations are highly coherent within anatomically or functionally linked areas of the brain. However, the underlying topological mechanisms responsible for these coherent intrinsic or spontaneous fluctuations are still poorly understood. Here, we apply modern network analysis techniques to investigate how spontaneous neuronal activities in the human brain derived from the resting-state BOLD signals are topologically organized at both the temporal and spatial scales. We first show that the spontaneous brain functional networks have an intrinsically cohesive modular structure in which the connections between regions are much denser within modules than between them. These identified modules are found to be closely associated with several well known functionally interconnected subsystems such as the somatosensory/motor, auditory, attention, visual, subcortical, and the “default” system. Specifically, we demonstrate that the module-specific topological features can not be captured by means of computing the corresponding global network parameters, suggesting a unique organization within each module. Finally, we identify several pivotal network connectors and paths (predominantly associated with the association and limbic/paralimbic cortex regions\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1038/nrn2201', '10.1126/science. 1134405', '10.1146/annurev.neuro.29.051605.112819', '10.1016/j.neuroimage.2003.12.030', '10.1016/j.neuroimage.2006.11.042', '10.1016/j.neurobiolaging.2005.08.011', '10.1038/jcbfm.1993.4', '10.1002/mrm.1910340409', '10.1002/hbm.20012', '10.1006/nimg.1997.0315', '10.1002/hbm.10022', '10.1073/pnas.0504136102', '10.1073/pnas.0135058100', '10.1073/pnas.0604187103', '10.1093/cercor/bhi016', '10.1016/S1053-8119(03)00097-1', '10.1098/rstb.2005.1634', '10.1073/pnas.0601417103', '10.1016/j.neuroimage.2005.08.035', '10.1038/35065725', '10.1016/j.physrep.2005.10.009', '10.1523/JNEUROSCI.3874-05.2006', '10.1093/brain/awn018', '10.1371/journal.pcbi.0030017', '10.1093/cercor/bhl149', '10.1523/JNEUROSCI.0141-08.2008', '10.1371/journal.pone.0000597', '10.1016/j.neuroimage.2007.10.060', '10.1073/pnas.0407994102', '10.1038/nature03288', '10.1038/nature02555', '10.1038/35011540', '10.1093/cercor/9.3.277', '10.1103/PhysRevLett.97.238103', '10.1371/journal.pbio.0060159', '10.1016/j.mri.2008.01.048', '10.1016/j.neuroimage.2008.09.062', '10.1006/nimg.2001.0978', '10.1103/PhysRevE.70.025101', '10.1093/brain/121.6.1013', '10.1038/nrn755', '10.1073/pnas.98.2.676', '10.1002/hbm.20113', '10.1093/cercor/bhm207', '10.1093/cercor/bhj127', '10.1038/30918', '10.1103/PhysRevLett.87.198701', '10.1038/nphys489', '10.2307/3033543', '10.1038/35019019', '10.1111/j.1460-9568.2007.05574.x', '10.1007/s00422-004-0479-1', '10.1016/j.neuroimage.2006.11.054', '10.1016/S1369-5274(03)00033-X', '10.1080/15326900701399939', '10.1093/cercor/10.2.127', '10.1016/j.neunet.2003.06.002', '10.1371/journal.pone.0001049', '10.1038/ng1530', '10.1073/pnas.0510258103', '10.1371/journal.pcbi.0040023', '10.1038/nature05758', '10.1073/pnas.0701519104', '10.1371/journal.pcbi.0010042', '10.1196/annals.1440.011', '10.1126/science.286.5439.509', '10.1098/rstb.2005.1645', '10.1016/j.neuroimage.2008.09.036', '10.1038/nn1075', '10.1111/j.1460-9568.2008.06117.x', '10.1016/j.neuroimage.2008.01.066', '10.1126/science.1086025', '10.1016/j.neuroimage.2006.02.048', '10.1103/PhysRevE.69.026113', '10.1016/j.neuroimage.2005.06.058'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Fluid mechanics', 'Computational fluid dynamics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Multimodal human–computer interaction: A survey': Paper(DOI='10.1080/10447318.2011.582022', crossref_json=None, google_schorlar_metadata=None, title='Multimodal human–computer interaction: A survey', authors=['Alejandro Jaimes', 'Nicu Sebe'], abstract='In this paper, we review the major approaches to multimodal human–computer interaction, giving an overview of the field from a computer vision perspective. In particular, we focus on body, gesture, gaze, and affective interaction (facial expression recognition and emotion in audio). We discuss user and task modeling, and multimodal fusion, highlighting challenges, open issues, and emerging applications for multimodal human–computer interaction (MMHCI) research.', conference=None, journal=None, year=None, reference_list=['10.1016/j.biopsycho.2007.10.014', '10.1145/383952.384007', '10.1109/TNSRE.2002.806829', '10.1007/BF00313105', '10.1088/1741-2560/8/2/025022', '10.1007/BF01797193', '10.3814/2010/967027', '10.1145/1851322.1851326', '10.7551/mitpress/2697.001.0001', '10.1016/j.ijhcs.2009.03.005', '10.1007/11848035_70', '10.1088/0031-9155/54/7/016', '10.1007/978-3-642-00437-7_1', '10.1007/978-3-642-02812-0_83', '10.1017/CBO9780511546396.004', '10.1016/0013-4694(88)90149-6', '10.1016/j.clinph.2006.10.019', '10.1007/s10827-006-0017-3', '10.1002/hbm.20538', '10.1007/978-3-642-12654-3_25', '10.1126/science.1174521', '10.1111/j.1469-8986.2008.00783.x', '10.1162/pres.19.1.1', '10.1201/9781420028669.ch8', '10.1006/imms.1993.1011', '10.1023/A:1021470822922', '10.1007/s00221-003-1690-3', '10.1100/tsw.2009.83', '10.1007/978-1-84996-272-8_6', '10.1016/j.cviu.2006.10.019', '10.1016/j.neuroimage.2009.07.056', '10.1016/S0953-5438(01)00053-4', '10.1007/s11042-006-0094-3', '10.1016/j.clinph.2008.06.019', '10.1155/ASP.2005.3156', '10.1145/1518701.1518813', '10.1016/j.tins.2006.07.004', '10.1155/2007/79642', '10.1088/1741-2560/8/2/025011', '10.1080/10447318.2011.535753', '10.1109/TBME.2008.918566', '10.1145/1690388.1690452', '10.1080/01449290500331156', '10.1155/2007/94561', '10.1007/978-3-642-02812-0_50', '10.1007/s12193-010-0046-0', '10.1109/TNSRE.2005.863842', '10.1109/3468.477859', '10.1080/10447311003781326', '10.1007/BF02462988', '10.1016/j.ipm.2005.03.023', '10.1016/j.clinph.2006.01.017', '10.1145/319382.319398', '10.1207/S15327051HCI1504_1', '10.1109/MSP.2008.4408447', '10.1016/j.neuroimage.2005.12.003', '10.1016/S1388-2457(99)00141-8', '10.1007/978-1-84996-272-8_10', '10.1109/TBME.2009.2016670', '10.1364/JOSA.67.001475', '10.1088/1741-2560/6/4/046011', '10.1016/S1388-2457(00)00457-0', '10.1155/2007/79826', '10.1080/13554790902724904', '10.1016/j.clinph.2005.06.027', '10.1109/TNSRE.2004.841878', '10.1109/5.664275', '10.1016/j.clinph.2006.04.025', '10.1145/1622176.1622207', '10.1097/WNR.0b013e3280c1e315', '10.1145/330534.330535', '10.4018/jgcms.2010100103', '10.1088/1741-2560/8/2/025018', '10.1109/QOMEX.2009.5246984', '10.1109/TNSRE.2003.814442', '10.1016/S1388-2457(02)00057-3', '10.1080/10447318.2011.535752', '10.1007/978-3-642-02812-0_86', '10.1007/978-1-84800-306-4_14'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'multimedia'], conference_acronym='International journal of human-computer interaction', publisher=None, query_handler=None),\n",
       " 'Facial expression recognition from video sequences: temporal and static modeling': Paper(DOI='10.1016/s1077-3142(03)00081-x', crossref_json=None, google_schorlar_metadata=None, title='Facial expression recognition from video sequences: temporal and static modeling', authors=['Ira Cohen', 'Nicu Sebe', 'Ashutosh Garg', 'Lawrence S Chen', 'Thomas S Huang'], abstract='The most expressive way humans display emotions is through facial expressions. In this work we report on several advances we have made in building a system for classification of facial expressions from continuous video input. We introduce and test different Bayesian network classifiers for classifying expressions from video, focusing on changes in distribution assumptions, and feature dependency structures. In particular we use Naive–Bayes classifiers and change the distribution from Gaussian to Cauchy, and use Gaussian Tree-Augmented Naive Bayes (TAN) classifiers to learn the dependencies among different facial motion features. We also introduce a facial expression recognition from live video input using temporal cues. We exploit the existing methods and propose a new architecture of hidden Markov models (HMMs) for automatically segmenting and recognizing human facial expression from video\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.1995.466915', '10.1037/0003-066X.45.1.16', '10.1109/TIT.1968.1054142', '10.1109/34.799905', '10.1037/0033-2909.115.2.268', '10.1109/34.598232', '10.1023/A:1009778005914', '10.1023/A:1007465528199', '10.1007/3-540-44795-4_16', '10.1037/0033-2909.115.2.288', '10.1109/AFGR.2000.840611', '10.1109/ICCV.1995.466919', '10.1002/j.1538-7305.1983.tb03114.x', '10.1109/IVL.1999.781120', '10.1023/A:1007692713085', '10.1016/S0031-3203(99)00113-2', '10.1109/CVPR.1997.609309', '10.1109/ICIP.1997.638829', '10.1109/34.895976', '10.1109/5.18626', '10.1109/72.536309', '10.2190/DUGG-P24E-52WK-6CDG', '10.1109/34.879793', '10.1109/34.506414'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'multimedia'], conference_acronym='Computer vision and image understanding (Print)', publisher=None, query_handler=None),\n",
       " 'A survey on learning to hash': Paper(DOI='10.1007/s10115-022-01734-0', crossref_json=None, google_schorlar_metadata=None, title='A survey on learning to hash', authors=['Jingdong Wang', 'Ting Zhang', 'Nicu Sebe', 'Heng Tao Shen'], abstract='Nearest neighbor search is a problem of finding the data points from the database such that the distances from them to the query point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this paper, we present a comprehensive survey of the learning to hash algorithms, categorize them according to the manners of preserving the similarities into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, as well as quantization, and discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different though quantization, as we show, can be derived from preserving the pairwise similarities. In addition, we present the evaluation protocols, and the general performance analysis, and point out that the quantization algorithms perform superiorly in terms of search\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s10462-020-09825-6', '10.1109/TPAMI.2019.2914897', '10.1109/CVPR.2018.00140', '10.1109/CVPR.2018.00134', '10.1109/CVPR.2017.104', '10.1609/aaai.v30i1.10455', '10.1109/ICCV.2017.598', '10.1007/s11042-012-1192-z', '10.1145/1646396.1646452', '10.1007/978-3-319-46454-1_14', '10.1109/CVPR.2019.01062', '10.1145/1460096.1460104', '10.1109/ICCV.2017.96', '10.1609/aaai.v32i1.11814', '10.1016/j.neucom.2019.01.020', '10.1145/3065386', '10.1109/CVPR.2015.7298947', '10.1038/nature14539', '10.1109/ACCESS.2019.2927524', '10.1109/TGRS.2020.2981997', '10.1109/CVPR.2016.133', '10.1109/CVPRW.2015.7301269', '10.1007/978-3-319-10602-1_48', '10.1109/TPAMI.2018.2882816', '10.1109/CVPR.2016.227', '10.1109/ACPR.2015.7486599', '10.1109/TIP.2017.2678163', '10.1109/CVPR.2006.42', '10.1007/978-3-540-78773-0_34', '10.1109/ICPR48806.2021.9413103', '10.1145/3077136.3080842', '10.1109/CVPR.2017.587', '10.1007/s11263-015-0816-y', '10.1007/s11263-007-0090-8', '10.1145/3123266.3123345', '10.1007/BF01759061', '10.1109/CVPR.2016.308', '10.1109/JPROC.2015.2487976', '10.1109/TPAMI.2017.2699960', '10.1016/j.diin.2014.03.004', '10.1609/aaai.v28i1.8952', '10.1109/CVPR.2010.5539970', '10.1109/TIP.2019.2938307', '10.1016/j.patcog.2021.107976', '10.1109/TIP.2015.2467315', '10.1145/3404835.3462896', '10.1145/3442204', '10.1109/ICASSP40776.2020.9054178', '10.1609/aaai.v30i1.10235', '10.1109/CVPR.2016.641'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Artificial Intelligence', 'Big Data'], conference_acronym='Knowledge and information systems (Print)', publisher=None, query_handler=None),\n",
       " 'First order motion model for image animation': Paper(DOI='10.3390/app13074137', crossref_json=None, google_schorlar_metadata=None, title='First order motion model for image animation', authors=['Aliaksandr Siarohin', 'Stéphane Lathuilière', 'Sergey Tulyakov', 'Elisa Ricci', 'Nicu Sebe'], abstract='Image animation consists of generating a video sequence so that an object in a source image is animated according to the motion of a driving video. Our framework addresses this problem without using any annotation or prior information about the specific object to animate. Once trained on a set of videos depicting objects of the same category (eg faces, human bodies), our method can be applied to any object of this class. To achieve this, we decouple appearance and motion information using a self-supervised formulation. To support complex motions, we use a representation consisting of a set of learned keypoints along with their local affine transformations. A generator network models occlusions arising during target motions and combines the appearance extracted from the source image and the motion derived from the driving video. Our framework scores best on diverse benchmarks and on a variety of object categories.', conference=None, journal=None, year=None, reference_list=['10.1109/FMEC49853.2020.9144916', '10.1145/311535.311556', '10.1109/CVPR.2016.262', '10.1109/IEEEGCC.2011.5752541', '10.1109/JEEIT.2019.8717509', '10.1109/ICIT52682.2021.9491125', '10.1145/3422622', '10.1109/CVPR42600.2020.00537', '10.1109/CVPR42600.2020.00711', '10.1145/3240508.3240704', '10.1109/CVPR.2018.00870', '10.1007/978-3-030-01261-8_41', '10.1109/CVPR.2019.00248', '10.1007/978-3-030-01234-2_1', '10.1016/0893-6080(89)90020-8', '10.1109/CVPR.2016.90', '10.21437/Interspeech.2017-950', '10.1109/CVPR.2017.143', '10.1109/ICCV.2017.116', '10.1162/neco.1997.9.8.1735'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Deep Learning', 'Robotics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Amigos: A dataset for affect, personality and mood research on individuals and groups': Paper(DOI='10.1109/taffc.2018.2884461', crossref_json=None, google_schorlar_metadata=None, title='Amigos: A dataset for affect, personality and mood research on individuals and groups', authors=['Juan Abdon Miranda-Correa', 'Mojtaba Khomami Abadi', 'Nicu Sebe', 'Ioannis Patras'], abstract=\"We present AMIGOS- A dataset for Multimodal research of affect, personality traits and mood on Individuals and GrOupS. Different to other databases, we elicited affect using both short and long videos in two social contexts, one with individual viewers and one with groups of viewers. The database allows the multimodal study of the affective responses, by means of neuro-physiological signals of individuals in relation to their personality and mood, and with respect to the social context and videos' duration. The data is collected in two experimental settings. In the first one, 40 participants watched 16 short emotional videos. In the second one, the participants watched 4 long videos, some of them alone and the rest in groups. The participants' signals, namely, Electroencephalogram (EEG), Electrocardiogram (ECG) and Galvanic Skin Response (GSR), were recorded using wearable sensors. Participants' frontal HD\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1145/2414536.2414602', '10.1037//0033-295X.97.3.377', '10.1007/978-1-4757-5093-5', '10.1038/79871', '10.3389/neuro.09.061.2009', '10.1109/CVPRW.2016.185', '10.1016/0191-8869(92)90025-K', '10.5244/C.27.108', '10.1016/0191-8869(91)90208-S', '10.1109/TAFFC.2016.2625250', '10.1016/S0167-8760(00)00176-8', '10.1016/S0092-6566(03)00046-1', '10.1017/CBO9780511801389', '10.1007/978-1-4757-1904-8', '10.1109/ICCV.2001.937619', '10.1109/TSMCA.2011.2116000', '10.1109/T-AFFC.2011.15', '10.1017/CBO9780511812736', '10.1037/0022-3514.54.6.1063', '10.1037/0033-2909.98.2.219', '10.1109/T-AFFC.2013.4', '10.1109/CVPRW.2013.130', '10.1109/ICSMC.2012.6378301', '10.1145/2964284.2967276', '10.1109/ISDEA.2010.311', '10.1109/TPAMI.2008.26', '10.1109/LSP.2009.2035731', '10.1016/j.csda.2003.11.020', '10.1007/BF02310555', '10.3758/BF03202594', '10.1109/ICSCSE.2016.0051', '10.1037/a0039210', '10.1007/s10579-007-9060-6', '10.1109/FG.2015.7163100', '10.1145/2818346.2820736', '10.1511/2001.4.344', '10.1037/h0077714', '10.1109/ROMAN.2015.7333686', '10.1109/ACII.2009.5349482', '10.1109/TAFFC.2015.2392932', '10.1109/T-AFFC.2011.25', '10.1109/CVPR.2016.374', '10.1109/TAFFC.2015.2397456', '10.1109/T-AFFC.2011.20', '10.1007/978-3-642-15314-3_9', '10.1109/FG.2015.7163151', '10.1016/j.imavis.2012.10.002', '10.1007/s11042-010-0632-x', '10.1111/j.1467-6494.1992.tb00970.x', '10.1037/0022-3514.63.6.962', '10.1016/0013-4694(87)90206-9', '10.2466/pms.2000.91.2.515', '10.1109/CW.2012.15', '10.1109/CW.2013.52'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence', 'Face and gesture recognition', 'Multimedia Analysis'], conference_acronym='IEEE transactions on affective computing', publisher=None, query_handler=None),\n",
       " 'Detecting anomalous events in videos by learning deep representations of appearance and motion': Paper(DOI='10.1016/j.cviu.2016.10.010', crossref_json=None, google_schorlar_metadata=None, title='Detecting anomalous events in videos by learning deep representations of appearance and motion', authors=['Dan Xu', 'Yan Yan', 'Elisa Ricci', 'Nicu Sebe'], abstract='Anomalous event detection is of utmost importance in intelligent video surveillance. Currently, most approaches for the automatic analysis of complex video scenes typically rely on hand-crafted appearance and motion features. However, adopting user defined representations is clearly suboptimal, as it is desirable to learn descriptors specific to the scene of interest. To cope with this need, in this paper we propose Appearance and Motion DeepNet (AMDN), a novel approach based on deep neural networks to automatically learn feature representations. To exploit the complementary information of both appearance and motion patterns, we introduce a novel double fusion framework, combining the benefits of traditional early fusion and late fusion strategies. Specifically, stacked denoising autoencoders are proposed to separately learn both appearance and motion features as well as a joint representation (early\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2007.70825', '10.1145/1961189.1961199', '10.1162/neco.2006.18.7.1527', '10.1007/s11263-011-0510-7', '10.1109/TPAMI.2006.176', '10.1016/0262-8856(96)01101-8', '10.1109/TSMCB.2005.846652', '10.1109/34.946990', '10.1109/TITS.2008.922970', '10.1016/j.patrec.2006.02.004', '10.1109/TCSVT.2008.2005599', '10.1109/TSMCC.2011.2178594', '10.1109/TPAMI.2012.131', '10.1162/089976601750264965', '10.1109/34.868677'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Deep Learning', 'Robotics'], conference_acronym='Computer vision and image understanding (Print)', publisher=None, query_handler=None),\n",
       " 'ASCERTAIN: Emotion and personality recognition using commercial sensors': Paper(DOI='10.1109/taffc.2016.2625250', crossref_json=None, google_schorlar_metadata=None, title='ASCERTAIN: Emotion and personality recognition using commercial sensors', authors=['Ramanathan Subramanian', 'Julia Wache', 'Mojtaba Khomami Abadi', 'Radu L Vieriu', 'Stefan Winkler', 'Nicu Sebe'], abstract=\"We present ASCERTAIN-a multimodal databaASe for impliCit pERsonaliTy and Affect recognitIoN using commercial physiological sensors. To our knowledge, ASCERTAIN is the first database to connect personality traits and emotional states via physiological responses. ASCERTAIN contains big-five personality scales and emotional self-ratings of 58 users along with their Electroencephalogram (EEG), Electrocardiogram (ECG), Galvanic Skin Response (GSR) and facial activity data, recorded using off-the-shelf sensors while viewing affective movie clips. We first examine relationships between users' affective ratings and personality scales in the context of prior observations, and then study linear and non-linear physiological correlates of emotion and personality. Our analysis suggests that the emotion-personality relationship is better captured by non-linear rather than linear statistics. We finally attempt binary\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1016/S0272-7358(97)00057-3', '10.1109/TAFFC.2014.2326402', '10.1145/2522848.2522862', '10.1109/T-AFFC.2012.17', '10.1109/TPAMI.2015.2496269', '10.1109/TSMCB.2008.2006638', '10.1145/2393347.2393397', '10.1145/2370216.2370266', '10.1613/jair.2349', '10.1145/1878039.1878048', '10.1167/14.3.31', '10.1016/j.paid.2009.01.049', '10.1145/1180995.1181029', '10.1109/ICPR.2006.489', '10.1016/j.neunet.2005.03.006', '10.1109/TPAMI.2008.26', '10.1109/TAFFC.2015.2432791', '10.1371/journal.pone.0138198', '10.1155/S1110865704406192', '10.1016/S0191-8869(02)00244-1', '10.1016/S0031-3203(98)00091-0', '10.1080/02699930601000672', '10.1145/1502650.1502702', '10.1016/j.imavis.2012.10.002', '10.1037/0022-3514.38.4.668', '10.1109/T-AFFC.2011.15', '10.1109/T-AFFC.2011.25', '10.1016/j.paid.2010.04.020', '10.1109/FG.2015.7163100', '10.1016/S0167-6393(02)00079-1', '10.1109/TSA.2004.838534', '10.1109/CVPRW.2010.5543262', '10.1109/ICME.2005.1521424', '10.1016/0191-8869(92)90025-K', '10.1093/scan/nsr059', '10.1109/TAFFC.2014.2330816', '10.1016/S0191-8869(03)00159-4', '10.1080/02699939508408966', '10.1145/2818346.2820736', '10.1037/0033-2909.106.2.265', '10.1109/TAFFC.2015.2392932', '10.1080/08838150701304852', '10.1111/j.2517-6161.1995.tb02031.x', '10.1007/s11042-010-0632-x', '10.1007/s10484-013-9234-5', '10.1016/0191-8869(85)90139-4', '10.1016/j.paid.2004.11.002', '10.1111/j.1467-6494.2006.00405.x'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['video processing', 'computer vision', 'perception', 'machine learning'], conference_acronym='IEEE transactions on affective computing', publisher=None, query_handler=None),\n",
       " 'Combining head pose and eye location information for gaze estimation': Paper(DOI='10.1109/tip.2011.2162740', crossref_json=None, google_schorlar_metadata=None, title='Combining head pose and eye location information for gaze estimation', authors=['Roberto Valenti', 'Nicu Sebe', 'Theo Gevers'], abstract='Head pose and eye location for gaze estimation have been separately studied in numerous works in the literature. Previous research shows that satisfactory accuracy in head pose and eye location estimation can be achieved in constrained settings. However, in the presence of nonfrontal faces, eye locators are not adequate to accurately locate the center of the eyes. On the other hand, head pose estimation techniques are able to deal with these conditions; hence, they may be suited to enhance the accuracy of eye localization. Therefore, in this paper, a hybrid scheme is proposed to combine head pose and eye location information to obtain enhanced gaze estimation. To this end, the transformation matrix obtained from the head pose is used to normalize the eye regions, and in turn, the transformation matrix generated by the found eye location is used to correct the pose estimation procedure. The scheme is\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/506443.506634', '10.1109/TPAMI.2007.70773', '10.1109/AFGR.2000.840622', '10.1109/TPAMI.2008.106', '10.1109/CVPR.2003.1211435', '10.1007/s001380100063', '10.1007/BF01418978', '10.1145/1111449.1111464', '10.1006/rtim.2002.0279', '10.1145/1386352.1386401', '10.1016/0262-8856(92)90076-F', '10.3758/BF03194970', '10.1016/S0031-3203(01)00140-6', '10.1016/j.patcog.2003.09.006', '10.1109/34.927467', '10.1007/s11263-007-0125-1', '10.5244/C.18.30', '10.1117/12.529999', '10.1109/TITS.2009.2026675', '10.1109/TPAMI.2005.179', '10.1145/1117309.1117340', '10.1109/ICPR.2004.1334689', '10.1109/ICPR.2004.1333754', '10.5244/C.20.20', '10.1109/CVPR.2001.990639', '10.1002/ima.10048', '10.1109/34.845375', '10.1109/CVPR.2009.5206640', '10.1145/1452392.1452425', '10.1023/B:VISI.0000013087.49260.fb', '10.1109/CVPR.2008.4587529'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'image understanding', 'deep learning', 'object recognition', 'color'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Structural resemblance to emotional expressions predicts evaluation of emotionally neutral faces.': Paper(DOI='10.1037/a0014681', crossref_json=None, google_schorlar_metadata=None, title='Structural resemblance to emotional expressions predicts evaluation of emotionally neutral faces.', authors=['Christopher P Said', 'Nicu Sebe', 'Alexander Todorov'], abstract='[Correction Notice: An erratum for this article was reported in Vol 9 (4) of Emotion (see record 2009-11528-009). In this article a symbol was incorrectly omitted from Figure 1, part C. To see the complete article with the corrected figure, please go to http://dx. doi. org/10.1037/a0014681.] People make trait inferences based on facial appearance despite little evidence that these inferences accurately reflect personality. The authors tested the hypothesis that these inferences are driven in part by structural resemblance to emotional expressions. The authors first had participants judge emotionally neutral faces on a set of trait dimensions. The authors then submitted the face images to a Bayesian network classifier trained to detect emotional expressions. By using a classifier, the authors can show that neutral faces perceived to possess various personality traits contain objective resemblance to emotional expression. In\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'multimedia'], conference_acronym='Emotion (Washington, D.C. : Online)', publisher=None, query_handler=None),\n",
       " 'DECAF: MEG-based multimodal database for decoding affective physiological responses': Paper(DOI='10.1109/taffc.2015.2392932', crossref_json=None, google_schorlar_metadata=None, title='DECAF: MEG-based multimodal database for decoding affective physiological responses', authors=['Mojtaba Khomami Abadi', 'Ramanathan Subramanian', 'Seyed Mostafa Kia', 'Paolo Avesani', 'Ioannis Patras', 'Nicu Sebe'], abstract=\"In this work, we present DECAF-a multimodal data set for decoding user physiological responses to affective multimedia content. Different from data sets such as DEAP [15] and MAHNOB-HCI [31], DECAF contains (1) brain signals acquired using the Magnetoencephalogram (MEG) sensor, which requires little physical contact with the user's scalp and consequently facilitates naturalistic affective response, and (2) explicit and implicit emotional responses of 30 participants to 40 one-minute music video segments used in [15] and 36 movie clips, thereby enabling comparisons between the EEG versus MEG modalities as well as movie versus music stimuli for affect recognition. In addition to MEG data, DECAF comprises synchronously recorded near-infra-red (NIR) facial videos, horizontal Electrooculogram (hEOG), Electrocardiogram (ECG), and trapezius-Electromyogram (tEMG) peripheral physiological responses\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1145/1921081.1921100', '10.1109/TCSVT.2006.873781', '10.1109/T-AFFC.2011.25', '10.1016/j.brat.2003.08.006', '10.1109/TIP.2014.2365699', '10.1109/TMM.2004.840618', '10.1016/j.tics.2009.10.011', '10.1007/s11042-010-0632-x', '10.1109/PRNI.2013.42', '10.1109/TPAMI.2008.26', '10.1109/T-AFFC.2011.15', '10.1016/j.imavis.2012.10.002', '10.1006/nimg.2002.1107', '10.1016/j.jesp.2013.03.013', '10.1016/j.imavis.2005.12.021', '10.3758/s13428-011-0106-8', '10.1111/j.2517-6161.1995.tb02031.x', '10.4018/jse.2012010101', '10.1080/026999396380321', '10.1109/ICME.2006.262954', '10.1007/0-387-30038-4', '10.1080/02699939508408966', '10.1109/ACII.2013.102', '10.1016/S0167-8655(00)00119-7', '10.1155/S1110865704406192', '10.1109/TMM.2009.2021722', '10.1016/S0006-3495(99)77236-X', '10.1155/2011/156869'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence', 'Face and gesture recognition', 'Multimedia Analysis'], conference_acronym='IEEE transactions on affective computing', publisher=None, query_handler=None),\n",
       " 'Semisupervised learning of classifiers: Theory, algorithms, and their application to human-computer interaction': Paper(DOI='10.1109/tpami.2004.127', crossref_json=None, google_schorlar_metadata=None, title='Semisupervised learning of classifiers: Theory, algorithms, and their application to human-computer interaction', authors=['Ira Cohen', 'Fabio Gagliardi Cozman', 'Nicu Sebe', 'Marcelo Cesar Cirelo', 'Thomas S Huang'], abstract='Automatic classification is one of the basic tasks required in any pattern recognition and human computer interaction application. In this paper, we discuss training probabilistic classifiers with labeled and unlabeled data. We provide a new analysis that shows under what conditions unlabeled data can be used in learning to improve classification performance. We also show that, if the conditions are violated, using unlabeled data can be detrimental to classification performance. We discuss the implications of this analysis to a specific type of probabilistic classifiers, Bayesian networks, and propose a new structure learning algorithm that can utilize unlabeled data to improve classification. Finally, we show how the resulting algorithms are successfully employed in two applications related to human-computer interaction and pattern recognition: facial expression recognition and face detection.', conference=None, journal=None, year=None, reference_list=['10.1007/3-540-46769-6_18', '10.1109/JPROC.2003.817119', '10.1145/279943.279962', '10.2307/1912526', '10.1109/36.312897', '10.1016/B978-0-12-714250-0.50016-4', '10.1023/A:1007692713085', '10.1016/0031-3203(90)90121-Z', '10.1093/biomet/65.3.658', '10.1080/01621459.1978.10480106', '10.2307/2529141', '10.1109/T-C.1970.222832', '10.1145/225298.225348', '10.1016/S0004-3702(02)00191-1', '10.1063/1.1699114', '10.1016/0031-3203(90)90073-T', '10.1007/978-1-4612-0711-5', '10.1016/0031-3203(80)90026-6', '10.1080/01621459.1984.10477109', '10.1023/A:1009778005914', '10.1007/s10994-005-0469-0', '10.1023/A:1007465528199', '10.1109/CVPR.1998.698685', '10.1109/AFGR.2000.840611', '10.1109/ICME.2002.1035527', '10.1109/34.982883', '10.2307/1403615', '10.1109/34.895976', '10.1037//0033-2909.115.2.268', '10.1287/moor.13.2.311'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'multimedia'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Learning a Mahalanobis distance metric for data clustering and classification': Paper(DOI='10.1016/j.patcog.2008.05.018', crossref_json=None, google_schorlar_metadata=None, title='Learning a Mahalanobis distance metric for data clustering and classification', authors=['Shiming Xiang', 'Feiping Nie', 'Changshui Zhang'], abstract='Distance metric is a key issue in many machine learning algorithms. This paper considers a general problem of learning from pairwise constraints in the form of must-links and cannot-links. As one kind of side information, a must-link indicates the pair of the two data points must be in a same class, while a cannot-link indicates that the two data points must be in two different classes. Given must-link and cannot-link information, our goal is to learn a Mahalanobis distance metric. Under this metric, we hope the distances of point pairs in must-links are as small as possible and those of point pairs in cannot-links are as large as possible. This task is formulated as a constrained optimization problem, in which the global optimum can be obtained effectively and efficiently. Finally, some applications in data clustering, interactive natural image segmentation and face pose estimation are given in this paper. Experimental results\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.506411', '10.1023/B:VISI.0000004832.02269.45', '10.1109/TCSVT.2002.808087', '10.1201/9781584888796.ch15', '10.1016/j.patcog.2005.12.004', '10.1117/12.710076', '10.1109/34.598228', '10.1016/S0031-3203(99)00139-9', '10.1016/S0031-3203(00)00162-X', '10.1016/S0167-8655(02)00207-6', '10.1007/11744047_18'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Signal Processing', 'Computer Vision', 'Artificial Intelligence'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'The constrained laplacian rank algorithm for graph-based clustering': Paper(DOI='10.1609/aaai.v30i1.10302', crossref_json=None, google_schorlar_metadata=None, title='The constrained laplacian rank algorithm for graph-based clustering', authors=['Feiping Nie', 'Xiaoqian Wang', 'Michael Jordan', 'Heng Huang'], abstract='Graph-based clustering methods perform clustering on a fixed input data graph. If this initial construction is of low quality then the resulting clustering may also be of low quality. Moreover, existing graph-based clustering methods require post-processing on the data graph to extract the clustering indicators. We address both of these drawbacks by allowing the data graph itself to be adjusted as part of the clustering procedure. In particular, our Constrained Laplacian Rank (CLR) method learns a graph with exactly k connected components (where k is the number of clusters). We develop two versions of this method, based upon the L1-norm and the L2-norm, which yield two new graph-based clustering objectives. We derive optimization algorithms to solve these objectives. Experimental results on synthetic datasets and real-world benchmark datasets exhibit the effectiveness of this new graph-based clustering method.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Computer Vision', 'Data Mining', 'Artificial Intelligence'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Multi-view k-means clustering on big data': Paper(DOI='10.26599/bdma.2018.9020003', crossref_json=None, google_schorlar_metadata=None, title='Multi-view k-means clustering on big data', authors=['Xiao Cai', 'Feiping Nie', 'Heng Huang'], abstract='In past decade, more and more data are collected from multiple sources or represented by multiple views, where different views describe distinct perspectives of the data. Although each view could be individually used for finding patterns by clustering, the clustering performance could be more accurate by exploring the rich information among multiple views. Several multi-view clustering methods have been proposed to unsupervised integrate different views of data. However, they are graph based approaches, eg based on spectral clustering, such that they cannot handle the large-scale data. How to combine these heterogeneous features for unsupervised large-scale data clustering has become a challenging problem. In this paper, we propose a new robust large-scale multi-view clustering method to integrate heterogeneous representations of largescale data. We evaluate the proposed new methods by six benchmark data sets and compared the performance with several commonly used clustering approaches as well as the baseline multi-view clustering methods. In all experimental results, our proposed methods consistently achieve superiors clustering performances.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Computer Vision', 'Data Mining', 'Artificial Intelligence'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Joint embedding learning and sparse regression: A framework for unsupervised feature selection': Paper(DOI='10.1109/tcyb.2013.2272642', crossref_json=None, google_schorlar_metadata=None, title='Joint embedding learning and sparse regression: A framework for unsupervised feature selection', authors=['Chenping Hou', 'Feiping Nie', 'Xuelong Li', 'Dongyun Yi', 'Yi Wu'], abstract='Feature selection has aroused considerable research interests during the last few decades. Traditional learning-based feature selection methods separate embedding learning and feature ranking. In this paper, we propose a novel unsupervised feature selection framework, termed as the joint embedding learning and sparse regression (JELSR), in which the embedding learning and sparse regression are jointly performed. Specifically, the proposed JELSR joins embedding learning with sparse regression to perform feature selection. To show the effectiveness of the proposed framework, we also provide a method using the weight via local linear approximation and adding the ℓ 2 , 1 -norm regularization, and design an effective algorithm to solve the corresponding optimization problem. Furthermore, we also conduct some insightful discussion on the proposed feature selection approach, including the convergence\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TKDE.2007.190672', '10.1109/TKDE.2008.204', '10.1145/1401890.1401903', '10.1137/1.9781611972801.54', '10.1137/S1064827502419154', '10.1126/science.290.5500.2323', '10.1162/089976603321780317', '10.1109/TSMCB.2010.2057422', '10.1016/j.patcog.2008.12.009', '10.1109/ICDM.2002.1183893', '10.1109/TSMCB.2011.2151256', '10.1109/TPAMI.2006.111', '10.1109/TNN.2007.894042', '10.1145/1273496.1273641', '10.1016/S0004-3702(97)00043-X', '10.1016/S0004-3702(97)00063-5', '10.1109/TKDE.2005.66', '10.1109/34.990133', '10.1109/TSMCB.2010.2085433', '10.1109/TFUZZ.2008.924209', '10.1023/A:1025667309714', '10.1109/TPAMI.2007.250598', '10.3233/HIS-2010-0118', '10.1109/TIP.2010.2044958', '10.1109/TSMCB.2007.911536', '10.1109/ICCV.2007.4408855', '10.2307/2347842', '10.1109/TSMCB.2011.2161607', '10.1109/TSMCB.2008.927276'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Computer Vision', 'Data Mining', 'Artificial Intelligence'], conference_acronym='IEEE transactions on cybernetics (Print)', publisher=None, query_handler=None),\n",
       " 'Flexible manifold embedding: A framework for semi-supervised and unsupervised dimension reduction': Paper(DOI='10.1109/tip.2010.2044958', crossref_json=None, google_schorlar_metadata=None, title='Flexible manifold embedding: A framework for semi-supervised and unsupervised dimension reduction', authors=['Feiping Nie', 'Dong Xu', 'Ivor Wai-Hung Tsang', 'Changshui Zhang'], abstract='We propose a unified manifold learning framework for semi-supervised and unsupervised dimension reduction by employing a simple but effective linear regression function to map the new data points. For semi-supervised dimension reduction, we aim to find the optimal prediction labels F for all the training samples X, the linear regression function h(X) and the regression residue F 0  = F - h(X) simultaneously. Our new objective function integrates two terms related to label fitness and manifold smoothness as well as a flexible penalty term defined on the residue F 0 . Our Semi-Supervised learning framework, referred to as flexible manifold embedding (FME), can effectively utilize label information from labeled data as well as a manifold structure from both labeled and unlabeled data. By modeling the mismatch between h(X) and F, we show that FME relaxes the hard linear constraint F = h(X) in manifold\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1150402.1150510', '10.1145/1631272.1631298', '10.1145/1143844.1143978', '10.1109/TPAMI.2007.250598', '10.1109/TKDE.2008.212', '10.1137/1.9781611972788.12', '10.1109/TPAMI.2005.55', '10.1109/TSMCB.2007.911536', '10.1007/s11263-006-0029-5', '10.1109/ICDM.2008.101', '10.1016/j.patcog.2009.04.001', '10.1126/science.290.5500.2323', '10.1109/TPAMI.2003.1251154', '10.1109/TIP.2009.2018015', '10.1145/1273496.1273627', '10.1109/ICCV.2007.4408856', '10.1109/TPAMI.2009.51', '10.1145/279943.279962', '10.1109/ICCV.2007.4408855', '10.1109/34.598228', '10.1145/1451983.1451994', '10.1109/34.927464', '10.1145/1102351.1102455', '10.1109/ICDM.2008.113', '10.1126/science.290.5500.2319', '10.1109/CVPR.1991.139758'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Signal Processing', 'Computer Vision', 'Artificial Intelligence'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Large-scale multi-view spectral clustering via bipartite graph': Paper(DOI='10.1609/aaai.v29i1.9598', crossref_json=None, google_schorlar_metadata=None, title='Large-scale multi-view spectral clustering via bipartite graph', authors=['Yeqing Li', 'Feiping Nie', 'Heng Huang', 'Junzhou Huang'], abstract='In this paper, we address the problem of large-scale multi-view spectral clustering. In many real-world applications, data can be represented in various heterogeneous features or views. Different views often provide different aspects of information that are complementary to each other. Several previous methods of clustering have demonstrated that better accuracy can be achieved using integrated information of all the views than just using each view individually. One important class of such methods is multi-view spectral clustering, which is based on graph Laplacian. However, existing methods are not applicable to large-scale problem for their high computational complexity. To this end, we propose a novel large-scale multi-view spectral clustering approach based on the bipartite graph. Our method uses local manifold fusion to integrate heterogeneous features. To improve efficiency, we approximate the similarity graphs using bipartite graphs. Furthermore, we show that our method can be easily extended to handle the out-of-sample problem. Extensive experimental results on five benchmark datasets demonstrate the effectiveness and efficiency of the proposed method, where our method runs up to nearly 3000 times faster than the state-of-the-art methods.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Medical Image Analysis', 'Graph Neural Networks'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Multi-view clustering and semi-supervised classification with adaptive neighbours': Paper(DOI='10.1609/aaai.v31i1.10909', crossref_json=None, google_schorlar_metadata=None, title='Multi-view clustering and semi-supervised classification with adaptive neighbours', authors=['Feiping Nie', 'Guohao Cai', 'Xuelong Li'], abstract='Due to the efficiency of learning relationships and complex structures hidden in data, graph-oriented methods have been widely investigated and achieve promising performance in multi-view learning. Generally, these learning algorithms construct informative graph for each view or fuse different views to one graph, on which the following procedure are based. However, in many real world dataset, original data always contain noise and outlying entries that result in unreliable and inaccurate graphs, which cannot be ameliorated in the previous methods. In this paper, we propose a novel multi-view learning model which performs clustering/semi-supervised classification and local structure learning simultaneously. The obtained optimal graph can be partitioned into specific clusters directly. Moreover, our model can allocate ideal weight for each view automatically without additional weight and penalty parameters. An efficient algorithm is proposed to optimize this model. Extensive experimental results on different real-world datasets show that the proposed model outperforms other state-of-the-art multi-view algorithms.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Computer Vision', 'Data Mining', 'Artificial Intelligence'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Multi-class active learning by uncertainty sampling with diversity maximization': Paper(DOI='10.1007/s11263-014-0781-x', crossref_json=None, google_schorlar_metadata=None, title='Multi-class active learning by uncertainty sampling with diversity maximization', authors=['Yi Yang', 'Zhigang Ma', 'Feiping Nie', 'Xiaojun Chang', 'Alexander G Hauptmann'], abstract=' As a way to relieve the tedious work of manual annotation, active learning plays important roles in many applications of visual concept recognition. In typical active learning scenarios, the number of labelled data in the seed set is usually small. However, most existing active learning algorithms only exploit the labelled data, which often suffers from over-fitting due to the small number of labelled examples. Besides, while much progress has been made in binary class active learning, little research attention has been focused on multi-class active learning. In this paper, we propose a semi-supervised batch mode multi-class active learning algorithm for visual concept recognition. Our algorithm exploits the whole active pool to evaluate the uncertainty of the data. Considering that uncertain data are always similar to each other, we propose to make the selected data as diverse as possible, for which we explicitly\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/2339530.2339647', '10.1613/jair.295', '10.5948/UPO9781614440222', '10.1007/s11263-014-0718-4', '10.1109/TNNLS.2014.2314123', '10.1145/1277741.1277764', '10.1109/CVPR.2008.4587350', '10.1109/CVPR.2009.5206651', '10.1007/s11263-013-0657-5', '10.1109/CVPR.2009.5206627', '10.1007/s11263-009-0268-3', '10.1109/CVPR.2011.5995638', '10.1109/CVPR.2006.68', '10.1145/1873951.1874135', '10.1109/TPAMI.2006.156', '10.1023/B:MACH.0000011805.60520.fe', '10.1109/CVPR.2009.5206744', '10.1007/s11263-014-0717-5', '10.1109/TPAMI.2014.2306419', '10.1109/ICPR.2004.1334462', '10.1007/978-3-319-10602-1_23', '10.1145/1007352.1007372', '10.1126/science.290.5500.2319', '10.1145/500141.500159', '10.1109/ICCV.2003.1238391', '10.1109/TMM.2012.2237023', '10.1109/ICCV.2013.456', '10.1109/TPAMI.2011.170', '10.1145/1143844.1143980'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Deep Learning', 'Multimedia Computing', 'Video Analysis'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'A multimedia retrieval framework based on semi-supervised ranking and relevance feedback': Paper(DOI='10.1109/tpami.2011.170', crossref_json=None, google_schorlar_metadata=None, title='A multimedia retrieval framework based on semi-supervised ranking and relevance feedback', authors=['Yi Yang', 'Feiping Nie', 'Dong Xu', 'Jiebo Luo', 'Yueting Zhuang', 'Yunhe Pan'], abstract='We present a new framework for multimedia content analysis and retrieval which consists of two independent algorithms. First, we propose a new semi-supervised algorithm called ranking with Local Regression and Global Alignment (LRGA) to learn a robust Laplacian matrix for data ranking. In LRGA, for each data point, a local linear regression model is used to predict the ranking scores of its neighboring points. A unified objective function is then proposed to globally align the local models from all the data points so that an optimal ranking score can be assigned to each data point. Second, we propose a semi-supervised long-term Relevance Feedback (RF) algorithm to refine the multimedia data representation. The proposed long-term RF algorithm utilizes both the multimedia data distribution in multimedia feature space and the history RF information provided by users. A trace ratio optimization problem is then\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2007.382983', '10.1109/TKDE.2007.190672', '10.1109/TMM.2007.911822', '10.1109/CVPR.2000.855822', '10.1023/B:VISI.0000004830.93820.78', '10.1145/1007352.1007372', '10.1109/CVPR.2000.855825', '10.1109/ICCV.1998.710701', '10.1126/science.290.5500.2323', '10.1145/1073204.1073247', '10.1109/TIP.2006.888350', '10.1007/978-3-540-45080-1_66', '10.1145/1126004.1126005', '10.1145/1027527.1027549', '10.1145/1646396.1646452', '10.1080/15427951.2004.10129091', '10.1007/978-3-642-02172-5_2', '10.1109/TMM.2003.819583', '10.1109/ICIP.2001.958595', '10.1145/1180639.1180710', '10.1109/34.598228', '10.1109/ICCV.2007.4408839', '10.1109/TPAMI.2009.85', '10.1145/1631272.1631282', '10.1162/neco.1992.4.6.888', '10.1145/1631272.1631298', '10.1145/1027527.1027531', '10.1109/TIP.2010.2049235', '10.1145/1126004.1126006', '10.1109/TNN.2009.2015760', '10.1109/ICDE.2008.4497429', '10.1109/TMM.2008.917359', '10.1109/CVPR.1997.609412', '10.1109/ICCV.2007.4408856', '10.1145/1631272.1631316'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Multimedia', 'Transfer Learning', 'Compression'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Trace ratio criterion for feature selection.': Paper(DOI='10.1109/access.2018.2888924', crossref_json=None, google_schorlar_metadata=None, title='Trace ratio criterion for feature selection.', authors=['Feiping Nie', 'Shiming Xiang', 'Yangqing Jia', 'Changshui Zhang', 'Shuicheng Yan'], abstract='Fisher score and Laplacian score are two popular feature selection algorithms, both of which belong to the general graph-based feature selection framework. In this framework, a feature subset is selected based on the corresponding score (subset-level score), which is calculated in a trace ratio form. Since the number of all possible feature subsets is very huge, it is often prohibitively expensive in computational cost to search in a brute force manner for the feature subset with the maximum subset-level score. Instead of calculating the scores of all the feature subsets, traditional methods calculate the score for each feature, and then select the leading features based on the rank of these feature-level scores. However, selecting the feature subset based on the feature-level score cannot guarantee the optimum of the subset-level score. In this paper, we directly optimize the subset-level score, and propose a novel algorithm to efficiently find the global optimal feature subset such that the subset-level score is maximized. Extensive experiments demonstrate the effectiveness of our proposed algorithm in comparison with the traditional methods for feature selection.', conference=None, journal=None, year=None, reference_list=['10.21437/Interspeech.2010-739', '10.1007/3-540-57868-4_57', '10.1109/TPAMI.2007.1093', '10.1016/j.patcog.2014.08.004', '10.1016/j.isatra.2015.12.011', '10.1016/j.patcog.2013.02.012', '10.1145/1015330.1015352', '10.1109/34.9121', '10.1016/j.patcog.2008.05.018', '10.1109/ICDEW.2006.145', '10.1016/j.patcog.2009.12.013', '10.1109/TPAMI.2009.190', '10.1016/j.neucom.2016.11.047', '10.1109/TAI.1995.479783', '10.1016/B978-1-55860-247-2.50037-1', '10.1016/S0031-3203(99)00139-9', '10.1137/090776603', '10.1109/CVPR.2007.382983', '10.1063/1.1755673', '10.1109/TPAMI.2007.250598'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Computer Vision', 'Data Mining', 'Artificial Intelligence'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Discriminative least squares regression for multiclass classification and feature selection': Paper(DOI='10.1109/tnnls.2012.2212721', crossref_json=None, google_schorlar_metadata=None, title='Discriminative least squares regression for multiclass classification and feature selection', authors=['Shiming Xiang', 'Feiping Nie', 'Gaofeng Meng', 'Chunhong Pan', 'Changshui Zhang'], abstract='This paper presents a framework of discriminative least squares regression (LSR) for multiclass classification and feature selection. The core idea is to enlarge the distance between different classes under the conceptual framework of LSR. First, a technique called ε-dragging is introduced to force the regression targets of different classes moving along opposite directions such that the distances between classes can be enlarged. Then, the ε-draggings are integrated into the LSR model for multiclass classification. Our learning framework, referred to as discriminative LSR, has a compact model form, where there is no need to train two-class machines that are independent of each other. With its compact form, this model can be naturally extended for feature selection. This goal is achieved in terms of  L 2,1  norm of matrix, generating a sparse learning model for feature selection. The model for multiclass classification\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-1-4757-3264-1', '10.1023/A:1012487302797', '10.1002/0471722146', '10.1016/B978-1-55860-247-2.50037-1', '10.1109/TNNLS.2012.2200262', '10.1109/TPAMI.2010.34', '10.1137/0905052', '10.1109/TNN.2011.2128342', '10.1145/347090.347105', '10.1145/1273496.1273620', '10.3233/IDA-2010-0421', '10.1017/CBO9780511809071', '10.1016/j.ccr.2006.03.003', '10.1200/JCO.2004.12.133', '10.1038/89044', '10.1142/9781860947995_0011', '10.1073/pnas.191502998', '10.1137/1.9781611972771.75', '10.1109/TNN.2010.2047114', '10.1109/TNNLS.2012.2190420', '10.1016/j.patrec.2010.12.014', '10.1109/TNN.2010.2044189', '10.1109/TPAMI.2005.159', '10.1109/TNN.2008.2005601', '10.1109/TNNLS.2011.2178447', '10.1109/TNN.2006.877531', '10.1109/TNN.2007.909535', '10.1017/CBO9780511801389', '10.1016/S0167-8655(03)00054-0', '10.1109/TNN.2010.2042729', '10.1201/9781584888796', '10.1162/153244303322753616', '10.1023/A:1009953814988', '10.1109/ACV.1994.341300', '10.1109/TSMC.1977.4309803', '10.1109/TSMCB.2011.2123886', '10.1007/s10208-004-0162-x', '10.1109/TIT.2006.871582'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Signal Processing', 'Computer Vision', 'Artificial Intelligence'], conference_acronym='IEEE transactions on neural networks and learning systems (Print)', publisher=None, query_handler=None),\n",
       " 'Image clustering using local discriminant models and global integration': Paper(DOI='10.1109/tip.2010.2049235', crossref_json=None, google_schorlar_metadata=None, title='Image clustering using local discriminant models and global integration', authors=['Yi Yang', 'Dong Xu', 'Feiping Nie', 'Shuicheng Yan', 'Yueting Zhuang'], abstract='In this paper, we propose a new image clustering algorithm, referred to as clustering using local discriminant models and global integration (LDMGI). To deal with the data points sampled from a nonlinear manifold, for each data point, we construct a local clique comprising this data point and its neighboring data points. Inspired by the Fisher criterion, we use a local discriminant model for each local clique to evaluate the clustering performance of samples within the local clique. To obtain the clustering result, we further propose a unified objective function to globally integrate the local models of all the local cliques. With the unified objective function, spectral relaxation and spectral rotation are used to obtain the binary cluster indicator matrix for all the samples. We show that LDMGI shares a similar objective function with the spectral clustering (SC) algorithms, e.g., normalized cut (NCut). In contrast to NCut in which the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2003.1238361', '10.1109/TKDE.2007.190672', '10.1109/TIP.2007.908076', '10.1145/1631272.1631298', '10.1145/1386352.1386369', '10.1109/TIP.2007.906769', '10.1109/TCSVT.2007.903317', '10.1109/TIP.2005.860593', '10.1109/ICCV.2003.1238368', '10.1109/TCSVT.2009.2035852', '10.1145/1459359.1459421', '10.1145/1367497.1367539', '10.1109/5.726791', '10.1145/1101149.1101261', '10.1109/ICCV.2007.4408863', '10.1109/TIP.2005.849770', '10.1109/34.868688', '10.1016/S0031-3203(04)00162-1', '10.1145/1348246.1348248', '10.1162/153244303321897735', '10.1145/1014052.1014118', '10.1109/34.993558', '10.1162/089976603321780317', '10.1109/34.927464', '10.1109/34.817413', '10.1109/ACV.1994.341300', '10.1109/TPAMI.2005.39'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Multimedia', 'Transfer Learning', 'Compression'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Multiview consensus graph clustering': Paper(DOI='10.1109/tip.2018.2877335', crossref_json=None, google_schorlar_metadata=None, title='Multiview consensus graph clustering', authors=['Kun Zhan', 'Feiping Nie', 'Jing Wang', 'Yi Yang'], abstract='A graph is usually formed to reveal the relationship between data points and graph structure is encoded by the affinity matrix. Most graph-based multiview clustering methods use predefined affinity matrices and the clustering performance highly depends on the quality of graph. We learn a consensus graph with minimizing disagreement between different views and constraining the rank of the Laplacian matrix. Since diverse views admit the same underlying cluster structure across multiple views, we use a new disagreement cost function for regularizing graphs from different views toward a common consensus. Simultaneously, we impose a rank constraint on the Laplacian matrix to learn the consensus graph with exactly     connected components where     is the number of clusters, which is different from using fixed affinity matrices in most existing graph-based methods. With the learned consensus graph, we can\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2017.461', '10.1109/ICCV.2015.482', '10.1109/TIP.2017.2665976', '10.1109/CVPR.2016.578', '10.1109/TPAMI.2008.277', '10.1109/ICCV.2015.185', '10.24963/ijcai.2017/419', '10.1145/1014052.1014118', '10.1137/1.9781611972757.70', '10.1109/34.868688', '10.1145/279943.279962', '10.1007/978-3-540-72927-3_8', '10.1109/TMM.2014.2298841', '10.1145/1553374.1553391', '10.1038/44565', '10.1109/MSP.2010.939739', '10.1109/TPAMI.2007.250598', '10.1073/pnas.35.11.652', '10.1017/CBO9780511804441', '10.1137/0201010', '10.1007/s10107-015-0946-6', '10.1145/1390156.1390191', '10.1109/TCYB.2017.2751646', '10.1007/11564089_7', '10.1145/1273496.1273599', '10.1109/CVPR.2014.523', '10.1109/TIP.2015.2463223', '10.1109/CVPR.2015.7298657', '10.1109/TIP.2016.2627806', '10.1162/neco_a_01055', '10.1109/TCYB.2017.2747400', '10.1007/s11222-007-9033-z', '10.1145/2623330.2623726', '10.1109/TKDE.2012.95', '10.24963/ijcai.2017/254', '10.1109/TCYB.2014.2376938', '10.1109/TKDE.2015.2445757', '10.1109/TIP.2018.2867747'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Computer Vision', 'Data Mining', 'Artificial Intelligence'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Low-rank matrix recovery via efficient schatten p-norm minimization': Paper(DOI='10.1609/aaai.v26i1.8210', crossref_json=None, google_schorlar_metadata=None, title='Low-rank matrix recovery via efficient schatten p-norm minimization', authors=['Feiping Nie', 'Heng Huang', 'Chris Ding'], abstract='As an emerging machine learning and information retrieval technique, the matrix completion has been successfully applied to solve many scientific applications, such as collaborative prediction in information retrieval, video completion in computer vision,\\\\emph {etc}. The matrix completion is to recover a low-rank matrix with a fraction of its entries arbitrarily corrupted. Instead of solving the popularly used trace norm or nuclear norm based objective, we directly minimize the original formulations of trace norm and rank norm. We propose a novel Schatten -Norm optimization framework that unifies different norm formulations. An efficient algorithm is derived to solve the new objective and followed by the rigorous theoretical proof on the convergence. The previous main solution strategy for this problem requires computing singular value decompositions-a task that requires increasingly cost as matrix sizes and rank increase. Our algorithm has closed form solution in each iteration, hence it converges fast. As a consequence, our algorithm has the capacity of solving large-scale matrix completion problems. Empirical studies on the recommendation system data sets demonstrate the promising performance of our new optimization framework and efficient algorithm.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Computer Vision', 'Data Mining', 'Artificial Intelligence'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Spectral embedded clustering: A framework for in-sample and out-of-sample spectral clustering': Paper(DOI='10.1109/tnn.2011.2162000', crossref_json=None, google_schorlar_metadata=None, title='Spectral embedded clustering: A framework for in-sample and out-of-sample spectral clustering', authors=['Feiping Nie', 'Zinan Zeng', 'Ivor W Tsang', 'Dong Xu', 'Changshui Zhang'], abstract='Spectral clustering (SC) methods have been successfully applied to many real-world applications. The success of these SC methods is largely based on the manifold assumption, namely, that two nearby data points in the high-density region of a low-dimensional data manifold have the same cluster label. However, such an assumption might not always hold on high-dimensional data. When the data do not exhibit a clear low-dimensional manifold structure (e.g., high-dimensional and sparse data), the clustering performance of SC will be degraded and become even worse than  K  -means clustering. In this paper, motivated by the observation that the true cluster assignment matrix for high-dimensional data can be always embedded in a linear space spanned by the data, we propose the spectral embedded clustering (SEC) framework, in which a linearity regularization is explicitly added into the objective function of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TNN.2009.2015760', '10.1023/A:1013637720281', '10.1109/TPAMI.2007.250598', '10.1109/TIP.2010.2049235', '10.1145/1273496.1273633', '10.1109/TPAMI.2008.292', '10.1109/TNN.2003.820841', '10.1002/0471721182', '10.1109/CVPR.2007.383103', '10.1162/089976603321780317', '10.1109/TPAMI.2005.9', '10.1137/0105003', '10.1109/CVPR.2008.4587756', '10.1109/CVPR.2010.5539870', '10.1109/5.726791', '10.1109/ICCV.2003.1238361', '10.1109/TPAMI.2004.1262185', '10.1016/j.patcog.2007.05.018', '10.1109/TMM.2008.917359', '10.1109/TNN.2008.2010620', '10.1109/TNN.2010.2064177', '10.1109/TNN.2010.2040835', '10.1109/TNN.2009.2030190', '10.1109/TNN.2010.2040200', '10.1109/TNN.2008.2005409', '10.1109/TNN.2005.860840', '10.1109/TNN.2002.1000150', '10.1109/CVPR.2000.855850', '10.1109/34.868688', '10.1109/TPAMI.2003.1251154', '10.1109/34.927464', '10.1109/ICDM.2002.1183897', '10.1145/1008992.1009031'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Signal Processing', 'Computer Vision', 'Artificial Intelligence'], conference_acronym='IEEE transactions on neural networks', publisher=None, query_handler=None),\n",
       " 'Image melding: Combining inconsistent images using patch-based synthesis': Paper(DOI='10.1145/2185520.2185578', crossref_json=None, google_schorlar_metadata=None, title='Image melding: Combining inconsistent images using patch-based synthesis', authors=['Soheil Darabi', 'Eli Shechtman', 'Connelly Barnes', 'Dan B Goldman', 'Pradeep Sen'], abstract='Current methods for combining two different images produce visible artifacts when the sources have very different textures and structures. We present a new method for synthesizing a transition region between two source images, such that inconsistent color, texture, and structural properties all change gradually from one source to the other. We call this process image melding. Our method builds upon a patch-based optimization foundation with three key generalizations: First, we enrich the patch search space with additional geometric and photometric transformations. Second, we integrate image gradients into the patch representation and replace the usual color averaging with a screened Poisson equation solver. And third, we propose a new energy based on mixed L2/L0 norms for colors and gradients that produces a gradual transition between sources without sacrificing texture sharpness. Together, all three\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1186562.1015718', '10.1007/s11263-010-0418-7', '10.1145/1531326.1531330', '10.1007/978-3-642-15558-1_3', '10.1007/978-3-540-88688-4_9', '10.1145/1731047.1731048', '10.1109/TIP.2010.2049240', '10.1145/245.247', '10.1109/ICCV.1999.790383', '10.1145/1275808.1276392', '10.1145/1964921.1964965', '10.1145/1276377.1276382', '10.1145/882262.882264', '10.1145/1186822.1073263', '10.1109/CVPR.2011.5995314', '10.5244/C.25.121', '10.1145/882262.882269', '10.1109/ICCV.2009.5459159', '10.1145/1179352.1141965', '10.1111/j.1467-8659.2010.01739.x', '10.1109/CVPR.2010.5540159', '10.1109/CVPR.2008.4587842', '10.1145/1833349.1778862', '10.1145/258734.258861', '10.1109/TPAMI.2005.185', '10.1109/TIT.2007.909108', '10.1145/344779.345009', '10.1109/TPAMI.2007.60', '10.5244/C.23.116'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Human-Computer Interaction'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " 'Robust patch-based HDR reconstruction of dynamic scenes': Paper(DOI='10.1145/2366145.2366222', crossref_json=None, google_schorlar_metadata=None, title='Robust patch-based HDR reconstruction of dynamic scenes', authors=['Pradeep Sen', 'Nima Khademi Kalantari', 'Maziar Yaesoubi', 'Soheil Darabi', 'Dan B Goldman', 'Eli Shechtman'], abstract='High dynamic range (HDR) imaging from a set of sequential exposures is an easy way to capture high-quality images of static scenes, but suffers from artifacts for scenes with significant motion. In this paper, we propose a new approach to HDR reconstruction that draws information from all the exposures but is more robust to camera/scene motion than previous techniques. Our algorithm is based on a novel patch-based energy-minimization formulation that integrates alignment and reconstruction in a joint optimization through an equation we call the HDR image synthesis equation. This allows us to produce an HDR result that is aligned to one of the exposures yet contains information from all of them. We present results that show considerable improvement over previous approaches.', conference=None, journal=None, year=None, reference_list=['10.1007/s11263-010-0390-2', '10.1201/b11373', '10.1145/1531326.1531330', '10.5555/876865.877232', '10.1145/146370.146374', '10.1109/TPAMI.2010.143', '10.5555/1028916.1035417', '10.1145/258734.258884', '10.1109/CVPR.2006.268', '10.1145/2010324.1964965', '10.1109/MCG.2008.23', '10.1145/882262.882270', '10.1109/CVPR.2005.128', '10.1109/CVMP.2010.8', '10.1007/s00371-011-0653-0', '10.1145/2010324.1964936', '10.1080/10867651.2003.10487583', '10.1109/TPAMI.2007.60', '10.1117/12.876630', '10.1111/j.1467-8659.2011.01870.x', '10.1016/S0262-8856(03)00137-9'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computer Graphics', 'Machine Learning'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " 'Stochastic pooling for regularization of deep convolutional neural networks': Paper(DOI='10.5373/jardcs/v12sp4/20201654', crossref_json=None, google_schorlar_metadata=None, title='Stochastic pooling for regularization of deep convolutional neural networks', authors=['Matthew D Zeiler', 'Rob Fergus'], abstract='We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Artificial Intelligence'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Stochastic pooling for regularization of deep convolutional neural networks. arXiv': Paper(DOI='10.5373/jardcs/v12sp4/20201654', crossref_json=None, google_schorlar_metadata=None, title='Stochastic pooling for regularization of deep convolutional neural networks. arXiv', authors=['Mathew D Zeiler', 'Rob Fergus'], abstract=None, conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Artificial Intelligence'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'European Conf. on Computer Vision': Paper(DOI='10.1016/0262-8856(92)90008-q', crossref_json=None, google_schorlar_metadata=None, title='European Conf. on Computer Vision', authors=['MD Zeiler', 'R Fergus'], abstract=None, conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Artificial Intelligence'], conference_acronym='Image and vision computing', publisher=None, query_handler=None),\n",
       " 'Visualizing and understanding convolutional networks. In processing of European Conference on Computer Vision, Zurich, Switzerland, 5-12 September, 2014': Paper(DOI='10.1007/978-3-319-10590-1_53', crossref_json=None, google_schorlar_metadata=None, title='Visualizing and understanding convolutional networks. In processing of European Conference on Computer Vision, Zurich, Switzerland, 5-12 September, 2014', authors=['MD Zeiler', 'R Fergus'], abstract=None, conference=None, journal=None, year=None, reference_list=['10.7551/mitpress/7503.003.0024', '10.1162/neco.2006.18.8.1868', '10.1109/CVPR.2013.91', '10.1109/CVPR.2012.6248110', '10.1109/CVPR.2009.5206848', '10.1109/CVPR.2014.81', '10.1162/neco.2006.18.7.1527', '10.1109/ICCV.2009.5459469', '10.1109/CVPR.2009.5206757', '10.1162/neco.1989.1.4.541', '10.1109/CVPR.2014.222', '10.1109/CVPR.2011.5995347', '10.1145/1390156.1390294', '10.1109/ICCV.2011.6126474'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Artificial Intelligence'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'RGBD salient object detection: A benchmark and algorithms': Paper(DOI='10.1007/978-3-319-10578-9_7', crossref_json=None, google_schorlar_metadata=None, title='RGBD salient object detection: A benchmark and algorithms', authors=['Houwen Peng', 'Bing Li', 'Weihua Xiong', 'Weiming Hu', 'Rongrong Ji'], abstract=' Although depth information plays an important role in the human vision system, it is not yet well-explored in existing visual saliency computational models. In this work, we first introduce a large scale RGBD image dataset to address the problem of data deficiency in current research of RGBD salient object detection. To make sure that most existing RGB saliency models can still be adequate in RGBD scenarios, we continue to provide a simple fusion framework that combines existing RGB-produced saliency with new depth-induced saliency, the former one is estimated from existing RGB models while the latter one is based on the proposed multi-contextual contrast model. Moreover, a specialized multi-stage RGBD model is also proposed which takes account of both depth and appearance cues derived from low-level feature contrast, mid-level region grouping and high-level priors enhancement. Extensive\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2009.5206596', '10.1109/TPAMI.2012.120', '10.1109/CVPR.2010.5540226', '10.1109/CVPR.2012.6247711', '10.1007/978-3-642-33709-3_30', '10.1109/TPAMI.2011.231', '10.1109/CVPR.2011.5995344', '10.1109/ICCV.2013.193', '10.5244/C.27.112', '10.5244/C.27.98', '10.1109/CVPR.2010.5539929', '10.7551/mitpress/7503.003.0073', '10.1007/978-3-642-32060-6_26', '10.1109/34.730558', '10.1109/ICCV.2013.221', '10.1109/ICCV.2013.209', '10.5244/C.25.110', '10.1109/CVPR.2013.271', '10.1109/ICCV.2013.248', '10.1109/ICCV.2009.5459462', '10.1007/978-3-642-33709-3_8', '10.1145/1015706.1015780', '10.1109/ICCV.2013.413', '10.1109/ICCV.2013.370', '10.1109/CVPR.2007.383047', '10.1109/ICCV.2013.315', '10.1109/ICCV.2009.5459467', '10.1109/CVPR.2013.151', '10.1109/CVPR.2012.6247743', '10.1002/j.1538-7305.1957.tb01515.x', '10.1109/CVPR.2013.131', '10.1002/9780470316849', '10.1109/CVPR.2012.6248093', '10.1007/978-3-642-33712-3_3', '10.1038/nrn1411', '10.1109/CVPR.2013.153', '10.1109/CVPR.2013.407'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Transfer learning based visual tracking with gaussian processes regression': Paper(DOI='10.1007/978-3-319-10578-9_13', crossref_json=None, google_schorlar_metadata=None, title='Transfer learning based visual tracking with gaussian processes regression', authors=['Jin Gao', 'Haibin Ling', 'Weiming Hu', 'Junliang Xing'], abstract=' Modeling the target appearance is critical in many modern visual tracking algorithms. Many tracking-by-detection algorithms formulate the probability of target appearance as exponentially related to the confidence of a classifier output. By contrast, in this paper we directly analyze this probability using Gaussian Processes Regression (GPR), and introduce a latent variable to assist the tracking decision. Our observation model for regression is learnt in a semi-supervised fashion by using both labeled samples from previous frames and the unlabeled samples that are tracking candidates extracted from the current frame. We further divide the labeled samples into two categories: auxiliary samples collected from the very early frames and target samples from most recent frames. The auxiliary samples are dynamically re-weighted by the regression, and the final tracking result is determined by fusing decisions\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2009.5206737', '10.1109/CVPR.2011.5995733', '10.1016/j.cviu.2012.03.005', '10.5244/C.20.6', '10.1007/978-3-540-88682-2_19', '10.1109/ICCV.2011.6126251', '10.1109/CVPR.2013.314', '10.1007/978-3-642-33765-9_50', '10.7551/mitpress/4170.001.0001', '10.1109/TPAMI.2012.42', '10.1023/A:1008078328650', '10.1109/TPAMI.2011.239', '10.1109/CVPR.2010.5539821', '10.1109/CVPR.2013.305', '10.1109/CVPR.2013.313', '10.1109/CVPR.2007.383199', '10.1109/CVPR.2011.5995730', '10.1109/TPAMI.2011.66', '10.1109/CVPR.2011.5995421', '10.1109/CVPR.2012.6247895', '10.1109/ICCV.2013.346', '10.1007/s11263-007-0075-7', '10.1109/ICCV.2013.36', '10.1109/CVPR.2013.308', '10.1109/CVPR.2013.307', '10.1109/CVPR.2013.312', '10.1109/ICCV.2013.88', '10.1109/CVPR.2013.306', '10.1109/CVPR.2010.5539857', '10.1145/1177352.1177355', '10.1007/978-3-642-33712-3_62', '10.1109/CVPR.2013.240', '10.1007/978-3-642-33783-3_34', '10.1007/s11263-012-0582-z'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'augmented reality', 'medical image analysis', 'human-computer interaction', 'machine learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Ocean: Object-aware anchor-free tracking': Paper(DOI='10.1007/978-3-030-58589-1_46', crossref_json=None, google_schorlar_metadata=None, title='Ocean: Object-aware anchor-free tracking', authors=['Zhipeng Zhang', 'Houwen Peng', 'Jianlong Fu', 'Bing Li', 'Weiming Hu'], abstract=' Anchor-based Siamese trackers have achieved remarkable advancements in accuracy, yet the further improvement is restricted by the lagged tracking robustness. We find the underlying reason is that the regression network in anchor-based methods is only trained on the positive anchor boxes (i.e., ). This mechanism makes it difficult to refine the anchors whose overlap with the target objects are small. In this paper, we propose a novel object-aware anchor-free network to address this issue. First, instead of refining the reference anchor boxes, we directly predict the position and scale of target objects in an anchor-free fashion. Since each pixel in groundtruth boxes is well trained, the tracker is capable of rectifying inexact predictions of target objects during inference. Second, we introduce a feature alignment module to learn an object-aware feature from predicted bounding boxes. The object-aware\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-319-48881-3_56', '10.1109/ICCV.2019.00628', '10.1109/TPAMI.2017.2699184', '10.1109/CVPR.2019.00479', '10.1109/CVPR.2017.733', '10.1007/s10479-005-5724-z', '10.1109/ICCV.2019.00667', '10.1109/CVPR.2019.00552', '10.1109/CVPR.2019.00478', '10.1109/ICCV.2017.196', '10.1109/ICCV.2017.322', '10.1109/CVPR.2016.90', '10.1007/978-3-030-01225-0_6', '10.1109/ICCV.2017.129', '10.1007/978-3-030-01264-9_45', '10.1162/neco.1989.1.4.541', '10.1109/CVPR.2019.00441', '10.1109/CVPR.2018.00935', '10.1109/CVPR.2018.00515', '10.1109/ICCV.2019.00626', '10.1145/2508037.2508039', '10.1007/978-3-319-10602-1_48', '10.1109/CVPR.2016.465', '10.1007/978-3-030-01219-9_35', '10.1109/CVPR.2017.789', '10.1109/CVPR.2016.91', '10.1007/s11263-015-0816-y', '10.1109/CVPR.2018.00937', '10.1109/CVPR.2016.158', '10.1109/ICCV.2019.00972', '10.1109/CVPR.2017.531', '10.1109/CVPR42600.2020.00661', '10.1109/CVPR.2019.00376', '10.1109/CVPR.2019.00140', '10.1109/CVPR.2019.00142', '10.1109/TPAMI.2014.2388226', '10.1007/978-3-030-01240-3_10', '10.1145/2964284.2967274', '10.1109/CVPR.2018.00747', '10.1109/TPAMI.2014.2315808', '10.1109/CVPR.2019.00472'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia Analysis', 'Computer Vision', 'Multi-modal Perception'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Supervised tensor learning': Paper(DOI='10.3389/fphy.2022.858388', crossref_json=None, google_schorlar_metadata=None, title='Supervised tensor learning', authors=['Dacheng Tao', 'Xuelong Li', 'Weiming Hu', 'Stephen Maybank', 'Xindong Wu'], abstract='This paper aims to take general tensors as inputs for supervised learning. A supervised tensor learning (STL) framework is established for convex optimization based learning techniques such as support vector machines (SVM) and minimax probability machines (MPM). Within the STL framework, many conventional learning machines can be generalized to take n/sup th/-order tensors as inputs. We also study the applications of tensors to learning machine design and feature extraction by linear discriminant analysis (LDA). Our method for tensor based feature extraction is named the tenor rank-one discriminant analysis (TR1DA). These generalized algorithms have several advantages: 1) reduce the curse of dimension problem in machine learning and data mining; 2) avoid the failure to converge; and 3) achieve better separation between the different categories of samples. As an example, we generalize MPM to its\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1103/RevModPhys.91.045002', '10.1038/nphys4035', '10.1038/s41598-017-09098-0', '10.1103/physrevx.7.031038', '10.1103/physrevb.97.134109', '10.1103/physrevlett.120.176401', '10.1103/PhysRevB.100.045129', '10.1038/s41567-018-0048-5', '10.1038/s42256-019-0028-1', '10.1103/physrevb.94.165134', '10.1126/science.aag2302', '10.1103/PhysRevB.100.125124', '10.1016/j.aop.2014.06.013', '10.1103/PhysRevLett.122.065301', '10.1103/PhysRevLett.127.170601', '10.1103/physrevx.7.021021', '10.1103/physrevx.8.011006', '10.1103/physrevb.97.035116', '10.48550/arXiv.1605.05775', '10.48550/arXiv.1705.02302', '10.1103/PhysRevX.8.031012', '10.1561/2200000067', '10.1088/1367-2630/ab31ef', '10.48550/arXiv.1711.03357', '10.1088/2058-9565/aaba1a', '10.3389/fams.2021.716044', '10.1088/2058-9565/aaea94', '10.1038/s41534-018-0116-9', '10.48550/arXiv.1806.05964', '10.1007/bf02099178', '10.1103/physrevlett.75.3537', '10.1103/physreva.74.022320', '10.1103/physrevb.80.235127', '10.1103/physrevlett.101.110501', '10.1103/physrevlett.100.240603', '10.1103/physrevlett.102.180406', '10.1162/neco.1989.1.4.541', '10.1145/3065386', '10.48550/arXiv.1409.1556', '10.1103/physrevlett.109.207202', '10.22331/q-2021-09-21-546', '10.1103/PhysRevB.100.214430', '10.1109/jproc.2012.2188013', '10.1214/009053607000000677', '10.1103/physrevb.79.144108', '10.1145/1015330.1015332', '10.1063/1.1699114', '10.1103/physrevb.89.235113', '10.3390/e21121236'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'image processing', 'applications of probability'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Automatic gait recognition based on statistical shape analysis': Paper(DOI='10.1109/tip.2003.815251', crossref_json=None, google_schorlar_metadata=None, title='Automatic gait recognition based on statistical shape analysis', authors=['Liang Wang', 'Tieniu Tan', 'Weiming Hu', 'Huazhong Ning'], abstract='Gait recognition has recently gained significant attention from computer vision researchers. This interest is strongly motivated by the need for automated person identification systems at a distance in visual surveillance and monitoring applications. The paper proposes a simple and efficient automatic gait recognition algorithm using statistical shape analysis. For each image sequence, an improved background subtraction procedure is used to extract moving silhouettes of a walking figure from the background. Temporal changes of the detected silhouettes are then represented as an associated sequence of complex vector configurations in a common coordinate frame, and are further analyzed using the Procrustes shape analysis method to obtain mean shape as gait signature. Supervised pattern classification techniques, based on the full Procrustes distance measure, are adopted for recognition. This method does\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/6046.766736', '10.1109/CVPR.2001.990924', '10.1109/CBMS.1997.596423', '10.1162/jocn.1991.3.1.71', '10.1109/MNRAO.1994.346253', '10.5244/C.12.46', '10.1109/HUMO.2000.897363', '10.3758/BF03337021', '10.1109/34.879790', '10.3758/BF03212378', '10.1016/0167-8655(95)00109-3', '10.1007/3-540-45344-X_44', '10.1109/ICPR.2002.1044626', '10.1006/cviu.1998.0716', '10.1109/AFGR.2002.1004181', '10.1109/AFGR.2002.1004182', '10.1109/ICPR.2002.1047863', '10.1016/S0031-3203(02)00100-0', '10.1109/AFGR.2002.1004165', '10.1007/3-540-45344-X_42', '10.1109/CVPR.2001.991036', '10.1109/CVPR.2001.990508', '10.1109/CVPR.2001.990506', '10.1007/BF01213527', '10.1109/ICPR.1996.547291', '10.1002/(SICI)1099-0720(199912)13:6<513::AID-ACP616>3.0.CO;2-8', '10.1109/CVPR.1994.323868', '10.1049/ip-vis:19990187', '10.1007/3-540-45344-X_40', '10.1109/ICPR.2002.1044575', '10.1109/IAI.2000.839618', '10.1007/3-540-45344-X_45', '10.1109/AFGR.2002.1004145', '10.3758/BF03198740', '10.1109/AFGR.2002.1004148', '10.1109/ICIP.2002.1038998', '10.5244/C.12.64'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Visual interpretability for deep learning: a survey': Paper(DOI='10.1631/fitee.1700808', crossref_json=None, google_schorlar_metadata=None, title='Visual interpretability for deep learning: a survey', authors=['Quan-shi Zhang', 'Song-Chun Zhu'], abstract=' This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles’ heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for\\xa0…', conference=None, journal=None, year=None, reference_list=['10.18653/v1/P16-1228', '10.1109/5.726791', '10.1145/2939672.2939778', '10.1007/s11263-010-0346-6'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Learning', 'Cognition', 'Autonomy', 'Artificial Intelligence'], conference_acronym='Frontiers of Informaion Technology & Electronic Engineering (Print)', publisher=None, query_handler=None),\n",
       " 'On advances in statistical modeling of natural images': Paper(DOI='10.1016/j.patcog.2004.07.011', crossref_json=None, google_schorlar_metadata=None, title='On advances in statistical modeling of natural images', authors=['A Srivastava', 'AB Lee', 'EP Simoncelli', 'S-C Zhu'], abstract=' Statistical analysis of images reveals two interesting properties: (i) invariance of image statistics to scaling of images, and (ii) non-Gaussian behavior of image statistics, i.e. high kurtosis, heavy tails, and sharp central cusps. In this paper we review some recent results in statistical modeling of natural images that attempt to explain these patterns. Two categories of results are considered: (i) studies of probability models of images or image decompositions (such as Fourier or wavelet decompositions), and (ii) discoveries of underlying image manifolds while restricting to natural images. Applications of these models in areas such as texture analysis, image classification, compression, and denoising are also considered.', conference=None, journal=None, year=None, reference_list=['10.1006/jvci.1999.0413', '10.1117/12.403791', '10.1109/93.621578', '10.1109/ICIP.2003.1247318', '10.1145/641007.641080', '10.1109/93.998050', '10.1109/ICME.2004.1394514', '10.1006/cviu.1999.0771', '10.1007/3-540-47979-1_7', '10.1109/ICCV.2001.937654', '10.1109/CAIVD.1998.646032', '10.1109/6046.909601', '10.1109/TPAMI.2004.1262334', '10.1109/IVL.1997.629719', '10.1145/290747.290774', '10.1142/S0129065797000161', '10.1109/34.955109', '10.1109/83.892448', '10.1109/CVPR.1997.609453', '10.1007/3-540-45113-7_21', '10.1109/TCSVT.2002.808079', '10.1145/957013.957051', '10.1117/12.469523', '10.1109/ICCV.1999.790424', '10.1007/3-540-45113-7_20', '10.1109/ICIP.1998.727321', '10.1016/S0031-3203(03)00170-5', '10.1109/CVPR.2003.1211359', '10.1016/j.imavis.2003.09.012', '10.1198/016214501753168398', '10.1109/CVPR.2000.855823', '10.1214/aoms/1177729694', '10.1145/500141.500159', '10.1109/34.1000236', '10.1117/12.403809', '10.1109/TPAMI.2003.1201820'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'computational statistics', 'shape analysis', 'functional data analysis', 'statistics on manifolds'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'Minimax entropy principle and its application to texture modeling': Paper(DOI='10.1162/neco.1997.9.8.1627', crossref_json=None, google_schorlar_metadata=None, title='Minimax entropy principle and its application to texture modeling', authors=['Song Chun Zhu', 'Ying Nian Wu', 'David Mumford'], abstract='This article proposes a general theory and methodology, called the minimax entropy principle, for building statistical models for images (or signals) in a variety of applications. This principle consists of two parts. The first is the maximum entropy principle for feature binding (or fusion): for a given set of observed feature statistics, a distribution can be built to bind these feature statistics together by maximizing the entropy over all distributions that reproduce them. The second part is the minimum entropy principle for feature selection: among all plausible sets of feature statistics, we choose the set whose maximum entropy distribution has the minimum entropy. Computational and inferential issues in both parts are addressed; in particular, a feature pursuit procedure is proposed for approximately selecting the optimal set of features. The minimax entropy principle is then corrected by considering the sample variation in the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1162/neco.1989.1.3.412', '10.1109/18.119732', '10.1109/TPAMI.1983.4767341', '10.1364/JOSAA.2.001160', '10.1162/neco.1995.7.5.889', '10.1162/neco.1994.6.4.559', '10.1109/TPAMI.1984.4767596', '10.1103/PhysRev.106.620', '10.1162/neco.1994.6.2.181', '10.1109/TIT.1962.1057698', '10.1073/pnas.88.11.4966', '10.1214/aoms/1177729694', '10.2307/2001373', '10.1007/BF00198477', '10.1002/cpa.3160420503', '10.1109/18.119725', '10.1364/JOSAA.4.002401', '10.1145/127719.122750'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Generative modeling', 'Unsupervised learning', 'Representation learning', 'Computer vision', 'Bioinformatics'], conference_acronym='Neural computation', publisher=None, query_handler=None),\n",
       " 'Learning human-object interactions by graph parsing neural networks': Paper(DOI='10.1007/978-3-030-01240-3_25', crossref_json=None, google_schorlar_metadata=None, title='Learning human-object interactions by graph parsing neural networks', authors=['Siyuan Qi', 'Wenguan Wang', 'Baoxiong Jia', 'Jianbing Shen', 'Song-Chun Zhu'], abstract='This paper addresses the task of detecting and recognizing human-object interactions (HOI) in images and videos. We introduce the Graph Parsing Neural Network (GPNN), a framework that incorporates structural knowledge while being differentiable end-to-end. For a given scene, GPNN infers a parse graph that includes i) the HOI graph structure represented by an adjacency matrix, and ii) the node labels. Within a message passing inference framework, GPNN iteratively computes the adjacency matrices and node labels. We extensively evaluate our model on three HOI detection benchmarks on images and videos: HICO-DET, V-COCO, and CAD-120 datasets. Our approach significantly outperforms state-of-art methods, verifying that GPNN is scalable to large datasets and applies to spatial-temporal settings.', conference=None, journal=None, year=None, reference_list=['10.1109/WACV.2018.00048', '10.1109/ICCV.2015.122', '10.3115/v1/W14-4012', '10.1109/ICCV.2017.89', '10.1007/978-3-642-33765-9_12', '10.1207/s15516709cog1402_1', '10.1609/aaai.v32i1.12270', '10.1109/ICCV.2015.169', '10.1109/CVPR.2018.00872', '10.1109/CVPR.2007.383331', '10.1109/TPAMI.2009.83', '10.1162/neco.1997.9.8.1735', '10.1109/ICCV.2013.390', '10.1109/CVPR.2016.573', '10.1109/TPAMI.2015.2430335', '10.1177/0278364913478446', '10.1109/ICCV.2017.448', '10.1109/CVPR.2017.234', '10.1007/978-3-319-46448-0_8', '10.1007/978-3-319-10602-1_48', '10.1007/978-3-319-46448-0_25', '10.1109/CVPR.2017.10', '10.1109/CVPR.2017.576', '10.1109/TPAMI.2017.2731842', '10.1109/ICCV.2017.132', '10.1109/WACV.2018.00181', '10.1109/CVPR.2017.11', '10.1109/CVPR.2017.344', '10.1109/CVPR.2018.00449', '10.1007/978-3-319-46484-8_18', '10.1609/aaai.v30i1.10460', '10.1109/CVPR.2017.330', '10.1109/CVPR.2010.5540234', '10.1109/CVPR.2010.5540235', '10.1109/ICCV.2011.6126386', '10.1109/ICCV.2017.200', '10.1109/ICCV.2015.179'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neural-Symbolic AI', 'Embodied AI', 'Autonomous Cars', 'Computer Vision', 'Artificial Intelligence'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Prior learning and Gibbs reaction-diffusion': Paper(DOI='10.1109/34.632983', crossref_json=None, google_schorlar_metadata=None, title='Prior learning and Gibbs reaction-diffusion', authors=['Song Chun Zhu', 'David Mumford'], abstract='This article addresses two important themes in early visual computation: it presents a novel theory for learning the universal statistics of natural images, and, it proposes a general framework of designing reaction-diffusion equations for image processing. We studied the statistics of natural images including the scale invariant properties, then generic prior models were learned to duplicate the observed statistics, based on minimax entropy theory. The resulting Gibbs distributions have potentials of the form U(I; /spl Lambda/, S)=/spl Sigma//sub /spl alpha/=1//sup k//spl Sigma//sub x,y//spl lambda//sup (/spl alpha/)/((F/sup (/spl alpha/)/*I)(x,y)) with S={F/sup (1)/, F/sup (2)/,...,F/sup (K)/} being a set of filters and /spl Lambda/={/spl lambda//sup (1)/(),/spl lambda//sup (2)/(),...,/spl lambda//sup (K)/()} the potential functions. The learned Gibbs distributions confirm and improve the form of existing prior models such as line-process\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1103/PhysRevLett.73.814', '10.1109/83.502393', '10.1088/0954-898X/7/2/014', '10.1109/34.56205', '10.1038/317314a0', '10.1109/5.58326', '10.1002/cpa.3160420503', '10.1016/0022-5193(81)90334-9', '10.1023/A:1007995731951', '10.1109/34.149593', '10.1364/JOSAA.4.002379', '10.1109/TPAMI.1984.4767596', '10.1109/83.661192', '10.1007/BF00131148', '10.7551/mitpress/7132.001.0001', '10.1364/JOSAA.2.001160', '10.1007/BF00127126', '10.1214/aoms/1177729694', '10.2307/2289127', '10.1109/34.537343', '10.1007/978-3-642-97522-6', '10.1109/34.120331', '10.1162/neco.1997.9.8.1627', '10.1145/127719.122750', '10.1109/34.134040', '10.1364/JOSAA.4.002401', '10.1137/0324060', '10.1103/RevModPhys.47.773', '10.1098/rstb.1952.0012', '10.1145/122718.122749', '10.1007/BF01451741', '10.1016/0734-189X(83)90020-8', '10.1007/BF00115697', '10.1109/34.16712', '10.1098/rspb.1980.0051', '10.1109/CVPR.1997.609366'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Learning', 'Cognition', 'Autonomy', 'Artificial Intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Statistical edge detection: Learning and evaluating edge cues': Paper(DOI='10.1109/tpami.2003.1159946', crossref_json=None, google_schorlar_metadata=None, title='Statistical edge detection: Learning and evaluating edge cues', authors=['Scott Konishi', 'Alan L.  Yuille', 'James M.  Coughlan', 'Song Chun Zhu'], abstract='We formulate edge detection as statistical inference. This statistical edge detection is data driven, unlike standard methods for edge detection which are model based. For any set of edge detection filters (implementing local edge cues), we use presegmented images to learn the probability distributions of filter responses conditioned on whether they are evaluated on or off an edge. Edge detection is formulated as a discrimination task specified by a likelihood ratio test on the filter responses. This approach emphasizes the necessity of modeling the image background (the off-edges). We represent the conditional probability distributions nonparametrically and illustrate them on two different data sets of 100 (Sowerby) and 50 (South Florida) images. Multiple edges cues, including chrominance and multiple-scale, are combined by using their joint distributions. Hence, this cue combination is optimal in the statistical sense\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.1999.790391', '10.1109/34.825754', '10.1117/12.395579', '10.1109/34.632983', '10.1162/089976602317250898', '10.1007/BF00133570', '10.1016/0042-6989(92)90039-L', '10.1364/JOSAA.4.002379', '10.1002/0471200611', '10.1109/34.476006', '10.1109/TPAMI.1984.4767596', '10.1109/TPAMI.1986.4767851', '10.1006/cviu.2001.0931', '10.1109/CVPR.1998.698687', '10.1162/neco.1992.4.2.196', '10.1162/089976600300015231', '10.1103/PhysRevLett.73.814', '10.1016/S0042-6989(00)00099-7', '10.1109/CVPR.1999.786963', '10.1109/CVPR.1999.786964', '10.1109/ICCV.1990.139492', '10.1017/CBO9780511812651', '10.1007/3-540-56484-5', '10.1109/34.659932', '10.1162/neco.1997.9.8.1627', '10.1109/CVPR.1999.786996'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Learning', 'Cognition', 'Autonomy', 'Artificial Intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'I2t: Image parsing to text description': Paper(DOI='10.1109/jproc.2010.2050411', crossref_json=None, google_schorlar_metadata=None, title='I2t: Image parsing to text description', authors=['Benjamin Z Yao', 'Xiong Yang', 'Liang Lin', 'Mun Wai Lee', 'Song-Chun Zhu'], abstract='In this paper, we present an image parsing to text description (I2T) framework that generates text descriptions of image and video content based on image understanding. The proposed I2T framework follows three steps: 1) input images (or video frames) are decomposed into their constituent visual patterns by an image parsing engine, in a spirit similar to parsing sentences in natural language; 2) the image parsing results are converted into semantic representation in the form of Web ontology language (OWL), which enables seamless integration with general knowledge bases; and 3) a text generation engine converts the results from previous steps into semantically meaningful, human readable, and query-able text reports. The centerpiece of the I2T framework is an and-or graph (AoG) visual knowledge representation, which provides a graphical representation serving as prior knowledge for representing diverse\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1015706.1015720', '10.1109/TPAMI.2005.161', '10.1103/PhysRevLett.58.86', '10.1023/B:VISI.0000042934.15159.49', '10.1006/cviu.1995.1004', '10.1109/ICCV.1990.139511', '10.1109/CVPR.2007.383190', '10.1007/s11263-006-6995-9', '10.1145/1400181.1400202', '10.1109/34.1000239', '10.1109/CVPR.2007.383272', '10.1007/s11263-008-0137-5', '10.1561/0600000018', '10.1109/ICCV.2007.4408960', '10.1038/381520a0', '10.1038/290091a0', '10.1109/CVPR.2007.382980', '10.1007/s11263-010-0346-6', '10.1162/153244303322533214', '10.1109/CVPR.2001.990517', '10.1037//0033-295X.94.2.115', '10.1007/s11263-006-0006-z', '10.1038/nn870', '10.1007/s11263-006-8707-x', '10.1109/ICCV.1999.790410', '10.1109/34.993558', '10.1109/34.391417', '10.1109/CVPR.2005.177', '10.1109/TPAMI.2008.129', '10.1109/TPAMI.2007.70847', '10.1007/s11263-009-0306-1', '10.1109/ICCV.2007.4408980', '10.1007/978-3-540-74198-5_17', '10.1016/j.cviu.2005.09.004', '10.1109/ICCV.2005.137', '10.1023/B:VISI.0000042934.15159.49', '10.1109/CVPR.2000.854754', '10.1145/1348246.1348248', '10.1007/s00138-006-0017-3', '10.1016/0167-2789(90)90087-6', '10.1109/34.868688', '10.1007/s11263-005-6642-x', '10.1109/ICCV.1999.791245', '10.1109/TPAMI.2006.178', '10.1016/j.cviu.2005.09.004', '10.1145/62029.62030', '10.1023/A:1007925832420', '10.1023/A:1011126920638', '10.1006/ijhc.1995.1081', '10.1145/219717.219748', '10.1109/34.895972', '10.1038/scientificamerican0501-34', '10.1145/1126004.1126005', '10.1109/CVPR.2004.301', '10.1006/jvci.1999.0413', '10.1023/B:MTAP.0000046380.27575.a5', '10.1109/TPAMI.2006.79', '10.1007/s11263-007-0068-6', '10.1109/ICCV.2001.937655', '10.1007/978-3-540-74198-5_14', '10.1007/s11263-007-0090-8'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Learning', 'Cognition', 'Autonomy', 'Artificial Intelligence'], conference_acronym='Proceedings of the IEEE', publisher=None, query_handler=None),\n",
       " 'Textons': Paper(DOI='10.1007/978-3-031-01823-7_2', crossref_json=None, google_schorlar_metadata=None, title='Textons', authors=['Song-Chun Zhu', 'Ying Nian Wu'], abstract='Textons refer to fundamental micro-structures in natural images and are considered the atoms of pre-attentive human visual perception. Unfortunately, the term “texton” remains a vague concept in the literature for lacking a good mathematical model. In this chapter, we present various generative image models for textons.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Generative modeling', 'Unsupervised learning', 'Representation learning', 'Computer vision', 'Bioinformatics'], conference_acronym='Synthesis lectures on computer vision (Print)', publisher=None, query_handler=None),\n",
       " 'A compositional and dynamic model for face aging': Paper(DOI='10.1109/tpami.2009.39', crossref_json=None, google_schorlar_metadata=None, title='A compositional and dynamic model for face aging', authors=['Jinli Suo', 'Song-Chun Zhu', 'Shiguang Shan', 'Xilin Chen'], abstract='In this paper, we present a compositional and dynamic model for face aging. The compositional model represents faces in each age group by a hierarchical And-or graph, in which And nodes decompose a face into parts to describe details (e.g., hair, wrinkles, etc.) crucial for age perception and Or nodes represent large diversity of faces by alternative selections. Then a face instance is a transverse of the And-or graph-parse graph. Face aging is modeled as a Markov process on the parse graph representation. We learn the parameters of the dynamic model from a large annotated face data set and the stochasticity of face aging is modeled in the dynamics explicitly. Based on this model, we propose a face aging simulation and prediction algorithm. Inversely, an automatic age estimation algorithm is also developed under this representation. We study two criteria to evaluate the aging results using human perception\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1002/vis.4340060403', '10.1109/ICPR.2006.230', '10.1109/ICCV.2007.4409050', '10.1109/TPAMI.2008.50', '10.1109/AFGR.2008.4813314', '10.1007/978-3-540-74198-5_14', '10.1109/AFGR.2008.4813337', '10.1109/TIP.2006.881993', '10.1109/CVPR.2006.187', '10.1145/882262.882269', '10.1109/AFGR.2008.4813408', '10.1109/CVPR.2007.383055', '10.1007/978-3-540-77046-6_71', '10.1016/j.patrec.2006.02.007', '10.1109/FGR.2006.78', '10.1016/j.forsciint.2007.03.015', '10.1109/PCCGA.2002.1167852', '10.1006/cviu.1997.0549', '10.1155/2008/239480', '10.1109/34.993553', '10.1109/AFGR.2008.4813349', '10.1109/TSMCB.2003.817091', '10.1109/MCG.2004.1297008', '10.1109/ICCV.2007.4409069', '10.1109/TCSVT.2006.877398', '10.1109/TMM.2008.921847', '10.1109/34.927467', '10.1049/ic:20050076', '10.1109/TPAMI.2007.70733', '10.1109/TIP.2008.924280', '10.1109/TPAMI.2006.131', '10.1109/CVPR.2006.81', '10.1098/rspb.1995.0021', '10.1109/ICIP.2008.4712088', '10.1109/TMI.2003.814784', '10.1109/IV.2006.8'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Learning', 'Cognition', 'Autonomy', 'Artificial Intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Learning pose grammar to encode human body configuration for 3d pose estimation': Paper(DOI='10.1609/aaai.v32i1.12270', crossref_json=None, google_schorlar_metadata=None, title='Learning pose grammar to encode human body configuration for 3d pose estimation', authors=['Hao-Shu Fang', 'Yuanlu Xu', 'Wenguan Wang', 'Xiaobai Liu', 'Song-Chun Zhu'], abstract='In this paper, we propose a pose grammar to tackle the problem of 3D human pose estimation. Our model directly takes 2D pose as input and learns a generalized 2D-3D mapping function. The proposed model consists of a base network which efficiently captures pose-aligned features and a hierarchy of Bi-directional RNNs (BRNN) on the top to explicitly incorporate a set of knowledge regarding human body configuration (ie, kinematics, symmetry, motor coordination). The proposed model thus enforces high-level constraints over human poses. In learning, we develop a pose sample simulator to augment training samples in virtual camera views, which further improves our model generalizability. We validate our method on public 3D human pose benchmarks and propose a new evaluation protocol working on cross-view setting to verify the generalization capability of different methods. We empirically observe that most state-of-the-art methods encounter difficulty under such setting while our method can well handle such challenges.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neural-Symbolic AI', 'Embodied AI', 'Autonomous Cars', 'Computer Vision', 'Artificial Intelligence'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'EPnP: An Accurate O(n) Solution to the PnP Problem': Paper(DOI='10.1007/s11263-008-0152-6', crossref_json=None, google_schorlar_metadata=None, title='EPnP: An Accurate O(n) Solution to the PnP Problem', authors=['Vincent Lepetit', 'Francesc Moreno-Noguer', 'Pascal Fua'], abstract=' We propose a non-iterative solution to the PnP problem—the estimation of the pose of a calibrated camera from n 3D-to-2D point correspondences—whose computational complexity grows linearly with n. This is in contrast to state-of-the-art methods that are O(n 5) or even O(n 8), without being more accurate. Our method is applicable for all n≥4 and handles properly both planar and non-planar configurations. Our central idea is to express the n 3D points as a weighted sum of four virtual control points. The problem then reduces to estimating the coordinates of these control points in the camera referential, which can be done in O(n) time by expressing these coordinates as weighted sum of the eigenvectors of a 12×12 matrix and solving a small constant number of quadratic equations to pick the right weights. Furthermore, if maximal precision is required, the output of the closed\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2003.1195992', '10.1109/TPAMI.1987.4767965', '10.1145/192161.192220', '10.1007/BF01450852', '10.1109/34.41365', '10.1109/34.908965', '10.1145/358669.358692', '10.1109/TPAMI.2003.1217599', '10.1109/CVPR.1991.139759', '10.1016/0734-189X(89)90052-2', '10.1023/A:1007940112931', '10.1364/JOSAA.5.001127', '10.1007/3-540-48405-1_2', '10.1006/cviu.1994.1066', '10.1109/TPAMI.2006.188', '10.1109/34.134043', '10.1109/34.862199', '10.1109/ICCV.2007.4409116', '10.1006/cviu.1996.0037', '10.1109/34.784291', '10.1109/TPAMI.2006.252', '10.1145/15886.15903', '10.1109/ISMAR.2004.53', '10.1016/j.isprsjprs.2006.03.005', '10.1109/ICCV.1999.791231', '10.1109/34.88573'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Science', 'Deep Learning', 'Computer Vision', 'Augmented Reality'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes': Paper(DOI='10.1007/978-3-642-37331-2_42', crossref_json=None, google_schorlar_metadata=None, title='Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes', authors=['Stefan Hinterstoisser', 'Vincent Lepetit', 'Slobodan Ilic', 'Stefan Holzer', 'Gary Bradski', 'Kurt Konolige', 'Nassir Navab'], abstract=' We propose a framework for automatic modeling, detection, and tracking of 3D objects with a Kinect. The detection part is mainly based on the recent template-based LINEMOD approach [1] for object detection. We show how to build the templates automatically from 3D models, and how to estimate the 6 degrees-of-freedom pose accurately and in real-time. The pose estimation and the color information allow us to check the detection hypotheses and improves the correct detection rate by 13% with respect to the original LINEMOD. These many improvements make our framework suitable for object manipulation in Robotics applications. Moreover we propose a new dataset made of 15 registered, 1100+ frame video sequences of 15 various objects for the evaluation of future competing methods.', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2011.6126326', '10.1109/ISMAR.2011.6162880', '10.5244/C.23.112', '10.1109/ICCVW.2009.5457479', '10.1109/ICCV.2011.6126513', '10.5244/C.24.106', '10.1109/CVPR.2010.5539836', '10.1007/s11263-009-0270-9', '10.1109/ICCV.2011.6126342', '10.1109/ICCV.1999.791202', '10.1109/34.232073', '10.1007/3-540-45404-7_20', '10.1109/CVPR.2010.5539908', '10.1142/S0218654305000797', '10.1007/BF01427149', '10.1109/34.765655', '10.1109/CVPR.2010.5540108', '10.1109/TPAMI.2006.213', '10.1109/ROBOT.2009.5152473', '10.1007/978-3-642-15558-1_26', '10.1007/978-3-642-15555-0_48', '10.1109/ICRA.2011.5980377', '10.1109/CVPR.2007.382995', '10.1109/CVPR.2010.5540231', '10.5244/C.22.10', '10.5244/C.15.43', '10.1109/TPAMI.2011.206'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Science', 'Deep Learning', 'Computer Vision', 'Augmented Reality'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Lift: Learned invariant feature transform': Paper(DOI='10.1007/978-3-319-46466-4_28', crossref_json=None, google_schorlar_metadata=None, title='Lift: Learned invariant feature transform', authors=['Kwang Moo Yi', 'Eduard Trulls', 'Vincent Lepetit', 'Pascal Fua'], abstract=' We introduce a novel Deep Network architecture that implements the full feature point handling pipeline, that is, detection, orientation estimation, and feature description. While previous works have successfully tackled each one of these problems individually, we show how to learn to do all three in a unified manner while preserving end-to-end differentiability. We then demonstrate that our Deep pipeline outperforms state-of-the-art methods on a number of benchmark datasets, without the need of retraining.', conference=None, journal=None, year=None, reference_list=['10.1023/B:VISI.0000029664.99615.94', '10.1109/CVPR.2008.4587673', '10.1109/ICCV.2011.6126544', '10.1109/CVPR.2015.7299165', '10.1109/CVPR.2015.7299064', '10.1109/CVPR.2016.19', '10.1109/ICCV.2015.22', '10.1007/s10791-009-9110-3', '10.5244/C.2.23', '10.1007/11744023_34', '10.5244/C.16.36', '10.1007/3-540-47969-4_9', '10.1109/ICCV.2009.5459458', '10.1109/ICCV.2011.6126263', '10.1007/s11263-013-0622-3', '10.1007/978-3-540-76390-1_24', '10.1109/ICPR.2006.1153', '10.1109/TPAMI.2011.103', '10.1109/CVPR.2007.382971', '10.3233/IDA-130594', '10.1109/TPAMI.2014.2301163', '10.1109/CVPR.2015.7298767', '10.1007/978-3-319-10578-9_5', '10.1109/3DV.2013.25', '10.1109/ICCV.2015.19', '10.1109/CVPR.2008.4587706', '10.1007/s11263-011-0473-8', '10.1007/978-3-642-33783-3_16', '10.1109/TPAMI.2009.77', '10.1109/ICPR.2010.845', '10.1023/B:VISI.0000027790.02288.f2', '10.1109/CVPR.2011.5995385', '10.1007/978-3-642-15552-9_13', '10.1109/ICCV.2011.6126542', '10.1109/CVPR.2012.6247715', '10.1109/TPAMI.2009.167'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Science', 'Deep Learning', 'Computer Vision', 'Augmented Reality'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Gradient response maps for real-time detection of textureless objects': Paper(DOI='10.1109/tpami.2011.206', crossref_json=None, google_schorlar_metadata=None, title='Gradient response maps for real-time detection of textureless objects', authors=['Stefan Hinterstoisser', 'Cedric Cagniart', 'Slobodan Ilic', 'Peter Sturm', 'Nassir Navab', 'Pascal Fua', 'Vincent Lepetit'], abstract='We present a method for real-time 3D object instance detection that does not require a time-consuming training stage, and can handle untextured objects. At its core, our approach is a novel image representation for template matching designed to be robust to small image transformations. This robustness is based on spread image gradient orientations and allows us to test only a small subset of all possible pixel locations when parsing the image, and to represent a 3D object with a limited set of templates. In addition, we demonstrate that if a dense depth sensor is available we can extend our approach for an even better performance also taking 3D surface normal orientations into account. We show how to take advantage of the architecture of modern computers to build an efficient but very discriminant representation of the input images that can be used to consider thousands of templates in real time. We\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s11263-006-9038-7', '10.1109/CVPR.2009.5206638', '10.1109/CVPR.2010.5540111', '10.1109/ICCV.2007.4409092', '10.1109/CVPR.2010.5539836', '10.1109/CVPR.2009.5206633', '10.5244/C.24.106', '10.1109/34.232073', '10.1023/B:VISI.0000029664.99615.94', '10.1109/CVPR.2010.5539908', '10.1007/s11263-009-0270-9', '10.1007/s11263-006-8707-x', '10.1109/ICCV.2007.4409066', '10.1109/CVPR.2005.177', '10.1109/ICCV.2011.6126326', '10.1109/TPAMI.2009.77', '10.1109/34.9107', '10.1023/A:1007975324482', '10.1109/TPAMI.2009.23', '10.1109/CVPR.2009.5206777', '10.1109/TPAMI.1986.4767851', '10.1109/ICCV.1999.791202', '10.1109/83.552100', '10.1109/MSP.2009.934110', '10.1007/s11263-010-0379-x', '10.1109/TPAMI.2004.111', '10.1109/CVPR.2010.5540231'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Sustainability Science', 'Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Building generalizable agents with a realistic and rich 3d environment': Paper(DOI='10.3997/2214-4609.20142591', crossref_json=None, google_schorlar_metadata=None, title='Building generalizable agents with a realistic and rich 3d environment', authors=['Yi Wu', 'Yuxin Wu', 'Georgia Gkioxari', 'Yuandong Tian'], abstract='Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et.al.). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Toubin et. al.) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Chained predictions using convolutional neural networks': Paper(DOI='10.1007/978-3-319-46493-0_44', crossref_json=None, google_schorlar_metadata=None, title='Chained predictions using convolutional neural networks', authors=['Georgia Gkioxari', 'Alexander Toshev', 'Navdeep Jaitly'], abstract=' In this work, we present an adaptation of the sequence-to-sequence model for structured vision tasks. In this model, the output variables for a given input are predicted sequentially using neural networks. The prediction for each output variable depends not only on the input but also on the previously predicted output variables. The model is applied to spatial localization tasks and uses convolutional neural networks (CNNs) for processing input images and a multi-scale deconvolutional architecture for making spatial predictions at each step. We explore the impact of weight sharing with a recurrent connection matrix between consecutive predictions, and compare it to a formulation where these weights are not tied. Untied weights are particularly suited for problems with a fixed sized structure, where different classes of output are predicted at different steps. We show that chain models achieve top performing\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s10994-009-5106-x', '10.1109/CVPR.2015.7298935', '10.1109/CVPR.2015.7298761', '10.1109/CVPR.2015.7298965', '10.1109/CVPR.2015.7298594', '10.1023/B:VISI.0000042934.15159.49', '10.7551/mitpress/7503.003.0146', '10.1109/CVPR.2009.5206754', '10.5244/C.23.3', '10.1109/CVPR.2011.5995741', '10.1109/CVPR.2011.5995318', '10.1007/978-3-642-33715-4_19', '10.1109/CVPR.2013.83', '10.1109/CVPR.2013.471', '10.1109/CVPR.2013.82', '10.1109/CVPR.2010.5540232', '10.1109/CVPR.2014.214', '10.1007/978-3-319-10605-2_3', '10.1109/CVPR.2016.512', '10.1109/CVPR.2016.511', '10.1109/CVPR.2014.471', '10.1109/ICCV.2013.280', '10.1109/CVPR.2011.5995741', '10.1109/CVPR.2016.308', '10.1007/978-3-642-33712-3_25', '10.1007/978-3-319-46484-8_29', '10.1109/CVPR.2015.7298664', '10.1109/CVPR.2016.604', '10.1109/CVPR.2016.533', '10.1007/978-3-319-46475-6_16'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'embodied AI', 'Robotics'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition': Paper(DOI='10.1109/tpami.2016.2646371', crossref_json=None, google_schorlar_metadata=None, title='An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition', authors=['Baoguang Shi', 'Xiang Bai', 'Cong Yao'], abstract='Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s13735-012-0004-6', '10.5244/C.26.127', '10.1109/ICDAR.2013.221', '10.1007/s10032-004-0134-3', '10.1109/ICDAR.2013.87', '10.1007/s11704-015-4488-0', '10.1109/TPAMI.2014.2339814', '10.1007/s11263-014-0793-6', '10.1109/CVPR.2014.516', '10.1109/TIP.2016.2555080', '10.1109/CVPR.2015.7298914', '10.1109/TIP.2014.2353813', '10.1109/CVPRW.2015.7301268', '10.1109/5.726791', '10.1007/s11263-015-0823-z', '10.1109/ICCV.2013.102', '10.1007/978-3-319-16865-4_3', '10.1109/TPAMI.2008.137', '10.1145/3065386', '10.1109/TPAMI.2014.2366765', '10.1109/CVPR.2014.81', '10.1109/72.279181', '10.1162/neco.1997.9.8.1735', '10.1145/1143844.1143891', '10.1109/ICASSP.2013.6638947', '10.1145/362003.362025'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Vision-Language Models', 'OCR', 'Document Understanding', 'Scene Text Detection and Recognition'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'AID: A benchmark data set for performance evaluation of aerial scene classification': Paper(DOI='10.1109/tgrs.2017.2685945', crossref_json=None, google_schorlar_metadata=None, title='AID: A benchmark data set for performance evaluation of aerial scene classification', authors=['Gui-Song Xia', 'Jingwen Hu', 'Fan Hu', 'Baoguang Shi', 'Xiang Bai', 'Yanfei Zhong', 'Liangpei Zhang', 'Xiaoqiang Lu'], abstract='Aerial scene classification, which aims to automatically label an aerial image with a specific semantic category, is a fundamental problem for understanding high-resolution remote sensing imagery. In recent years, it has become an active task in the remote sensing area, and numerous algorithms have been proposed for this task, including many machine learning and data-driven approaches. However, the existing data sets for aerial scene classification, such as UC-Merced data set and WHU-RS19, contain relatively small sizes, and the results on them are already saturated. This largely limits the development of scene classification algorithms. This paper describes the Aerial Image data set (AID): a large-scale data set for aerial scene classification. The goal of AID is to advance the state of the arts in scene classification of remote sensing images. For creating AID, we collect and annotate more than 10000 aerial\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TGRS.2015.2435801', '10.1109/TGRS.2015.2488681', '10.1109/JSTARS.2015.2444405', '10.1109/IGARSS.2015.7326956', '10.3390/rs71114988', '10.1109/TGRS.2014.2357078', '10.1109/TGRS.2015.2400449', '10.1109/LGRS.2015.2483680', '10.1109/CVPRW.2015.7301382', '10.1117/1.JRS.9.095064', '10.1109/TGRS.2015.2411331', '10.1109/LGRS.2014.2357392', '10.1109/TGRS.2013.2241444', '10.1109/CBMI.2014.6849835', '10.1117/1.JRS.8.083690', '10.1109/JSTARS.2014.2339842', '10.1080/01431161.2014.890762', '10.1109/TGRS.2014.2351395', '10.1016/j.rse.2010.12.017', '10.1016/j.rse.2011.11.020', '10.1109/JURSE.2011.5764800', '10.1145/1869790.1869829', '10.1109/LGRS.2009.2023536', '10.1109/ICIP.2008.4712139', '10.1109/TGRS.2014.2360100', '10.1109/TGRS.2014.2306692', '10.1007/s11760-015-0804-2', '10.3390/rs71114680', '10.1016/j.isprsjprs.2014.10.002', '10.1109/TGRS.2014.2374218', '10.1016/j.isprsjprs.2016.03.014', '10.1109/ICCV.2011.6126403', '10.1016/j.isprsjprs.2009.06.004', '10.1080/01431161.2011.608740', '10.2307/3545542', '10.1080/01431160600702632', '10.1109/WARSD.2003.1295182', '10.1109/LGRS.2015.2475299', '10.1109/JSTSP.2011.2139193', '10.1109/TGRS.2008.2010404', '10.1137/080730627', '10.1109/TIP.2008.925367', '10.1109/TPAMI.2002.1017623', '10.1109/CVPR.2006.68', '10.1109/CVPR.2012.6248021', '10.1109/LGRS.2010.2055033', '10.1109/IGARSS.2013.6721125', '10.1007/978-3-642-20267-4_6', '10.1023/A:1011139631724', '10.1023/B:VISI.0000029664.99615.94', '10.1007/s11263-013-0640-1', '10.1007/BF00130487', '10.1109/34.531803', '10.1007/s11263-009-0312-3', '10.1109/ICPR.2014.288', '10.1109/TGRS.2015.2393857', '10.3390/rs5116026', '10.1162/neco.2006.18.7.1527', '10.1109/TPAMI.2011.235', '10.1109/ICCV.2003.1238663', '10.1145/2647868.2654889', '10.1007/s11263-015-0816-y', '10.1109/CVPR.2007.383266', '10.1109/CVPR.2010.5540018', '10.3390/rs5052275', '10.1109/JSTARS.2012.2228254', '10.1007/978-3-642-39402-7_33', '10.1080/01431161.2013.845925', '10.1109/LGRS.2012.2225596', '10.1109/TGRS.2012.2205158', '10.1007/11744085_40', '10.1080/2150704X.2013.858843', '10.1109/LGRS.2012.2216499', '10.1007/s11760-014-0704-x', '10.1109/TSMC.1973.4309314', '10.1109/TGRS.2016.2523563', '10.1109/TNNLS.2014.2359471', '10.1109/TGRS.2016.2601622'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'OCR'], conference_acronym='IEEE transactions on geoscience and remote sensing', publisher=None, query_handler=None),\n",
       " 'Textboxes: A fast text detector with a single deep neural network': Paper(DOI='10.1609/aaai.v31i1.11196', crossref_json=None, google_schorlar_metadata=None, title='Textboxes: A fast text detector with a single deep neural network', authors=['Minghui Liao', 'Baoguang Shi', 'Xiang Bai', 'Xinggang Wang', 'Wenyu Liu'], abstract='This paper presents an end-to-end trainable fast scene text detector, named TextBoxes, which detects scene text with both high accuracy and efficiency in a single network forward pass, involving no post-process except for a standard non-maximum suppression. TextBoxes outperforms competing methods in terms of text localization accuracy and is much faster, taking only 0.09 s per image in a fast implementation. Furthermore, combined with a text recognizer, TextBoxes significantly outperforms state-of-the-art approaches on word spotting and end-to-end text recognition tasks.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Computer Vision', 'Deep Learning', 'Object Detection', 'Object Segmentation'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Textboxes++: A single-shot oriented scene text detector': Paper(DOI='10.1109/tip.2018.2825107', crossref_json=None, google_schorlar_metadata=None, title='Textboxes++: A single-shot oriented scene text detector', authors=['Minghui Liao', 'Baoguang Shi', 'Xiang Bai'], abstract='Scene text detection is an important step of scene text recognition system and also a challenging problem. Different from general object detections, the main challenges of scene text detection lie on arbitrary orientations, small sizes, and significantly variant aspect ratios of text in natural images. In this paper, we present an end-to-end trainable fast scene text detector, named TextBoxes++, which detects arbitrary-oriented scene text with both high accuracy and efficiency in a single network forward pass. No post-processing other than efficient non-maximum suppression is involved. We have evaluated the proposed TextBoxes++ on four public data sets. In all experiments, TextBoxes++ outperforms competing methods in terms of text localization accuracy and runtime. More specifically, TextBoxes++ achieves an f-measure of 0.817 at 11.6 frames/s for 1024 × 1024 ICDAR 2015 incidental text images and an f-measure\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2017.371', '10.1007/978-3-319-46484-8_4', '10.1109/TIP.2012.2199327', '10.1109/CVPR.2016.451', '10.1109/TIP.2014.2353813', '10.1016/j.imavis.2004.02.006', '10.1109/TIP.2015.2400217', '10.1109/TIP.2016.2547588', '10.1109/WACV.2016.7477663', '10.1016/j.neucom.2017.03.078', '10.1109/TIP.2017.2656474', '10.1109/CVPR.2015.7298871', '10.1109/CVPR.2016.254', '10.1007/978-3-319-24574-4_28', '10.1109/CVPR.2015.7298965', '10.1109/WACV.2016.7477575', '10.1109/TIP.2014.2317980', '10.1109/TIP.2014.2302896', '10.1109/ICDAR.2013.100', '10.1007/978-3-319-10593-2_33', '10.1016/j.imavis.2010.04.002', '10.1117/12.2083709', '10.1016/j.patcog.2017.04.027', '10.1109/ICDAR.2013.221', '10.1109/ICCV.2015.528', '10.1007/s10032-015-0237-z', '10.1109/ICCV.2015.143', '10.1109/TIP.2013.2249082', '10.1007/978-3-642-15549-9_43', '10.1007/978-3-319-46448-0_2', '10.1109/CVPR.2017.283', '10.1109/TPAMI.2016.2646371', '10.1109/CVPR.2014.81', '10.1109/ICCV.2015.169', '10.1109/CVPR.2016.91', '10.1007/s11263-013-0620-5', '10.1109/5254.708428', '10.1007/978-3-319-46604-0_8', '10.1109/TIP.2010.2070803', '10.1109/TPAMI.2014.2366765', '10.1007/s11263-015-0823-z', '10.1109/CVPR.2012.6248097', '10.1109/ICDAR.2015.7333942', '10.1109/ICDAR.2013.279', '10.1145/1143844.1143891', '10.1109/CVPR.2015.7298594', '10.1109/CVPR.2017.368', '10.1016/j.neunet.2005.06.042'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'OCR'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Aster: An attentional scene text recognizer with flexible rectification': Paper(DOI='10.1109/tpami.2018.2848939', crossref_json=None, google_schorlar_metadata=None, title='Aster: An attentional scene text recognizer with flexible rectification', authors=['Baoguang Shi', 'Mingkun Yang', 'Xinggang Wang', 'Pengyuan Lyu', 'Cong Yao', 'Xiang Bai'], abstract='A challenging aspect of scene text recognition is to handle text with distortions or irregular layout. In particular, perspective text and curved text are common in natural scenes and are difficult to recognize. In this work, we introduce ASTER, an end-to-end neural network model that comprises a rectification network and a recognition network. The rectification network adaptively transforms an input image into a new one, rectifying the text in it. It is powered by a flexible Thin-Plate Spline transformation which handles a variety of text irregularities and is trained without human annotations. The recognition network is an attentional sequence-to-sequence model that predicts a character sequence directly from the rectified image. The whole model is trained end to end, requiring only images and their groundtruth text. Through extensive experiments, we verify the effectiveness of the rectification and demonstrate the state-of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s11704-015-4488-0', '10.1109/CVPR.2017.283', '10.1109/CVPR.2016.451', '10.1109/ICDAR.2013.221', '10.1109/ICDAR.2015.7333942', '10.1109/ICCV.2017.560', '10.1109/CVPR.2016.245', '10.1109/CVPR.2014.516', '10.1109/5.726791', '10.1007/978-3-642-15549-9_43', '10.1109/TPAMI.2013.126', '10.24963/ijcai.2017/458', '10.1007/s11263-015-0823-z', '10.1109/TIP.2014.2353813', '10.1109/CVPR.2014.515', '10.1109/TPAMI.2014.2366765', '10.1109/TPAMI.2014.2339814', '10.1109/CVPR.2016.90', '10.1162/neco.1997.9.8.1735', '10.1109/CVPR.2017.243', '10.1007/s11263-015-0823-z', '10.1109/TPAMI.2016.2577031', '10.1016/j.eswa.2014.07.008', '10.1016/j.patcog.2016.10.016', '10.1109/CVPR.2016.452', '10.1109/TPAMI.2016.2646371', '10.1109/TPAMI.2016.2572683', '10.1007/s11263-014-0793-6', '10.1109/34.506792', '10.1109/ICCV.2017.543', '10.1016/j.imavis.2005.01.003', '10.3115/v1/D14-1179', '10.1109/CVPR.2010.5540041', '10.1109/CVPR.2015.7298914', '10.1145/1143844.1143891', '10.1109/TPAMI.2008.137', '10.1109/CVPR.2016.254', '10.1609/aaai.v32i1.12242', '10.1109/34.993558', '10.1109/34.24792', '10.1109/ICCV.2013.102', '10.1109/ICCV.2013.76', '10.1109/ICCV.2017.242', '10.1016/j.cviu.2016.01.002', '10.1109/TPAMI.2015.2496234', '10.1109/CVPR.2012.6248097', '10.1007/s10032-004-0134-3', '10.1109/CVPR.2012.6247990', '10.1016/j.imavis.2004.02.006'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Computer Vision', 'Deep Learning', 'Object Detection', 'Object Segmentation'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Auto-context and its application to high-level vision tasks and 3D brain image segmentation': Paper(DOI='10.1109/tpami.2009.186', crossref_json=None, google_schorlar_metadata=None, title='Auto-context and its application to high-level vision tasks and 3D brain image segmentation', authors=['Zhuowen Tu', 'Xiang Bai'], abstract='The notion of using context information for solving high-level vision and medical image segmentation problems has been increasingly realized in the field. However, how to learn an effective and efficient context model, together with an image appearance model, remains mostly unknown. The current literature using Markov Random Fields (MRFs) and Conditional Random Fields (CRFs) often involves specific algorithm design in which the modeling and computing stages are studied in isolation. In this paper, we propose a learning algorithm, auto-context. Given a set of training images and their corresponding label maps, we first learn a classifier on local image patches. The discriminative probability (or classification confidence) maps created by the learned classifier are then used as context information, in addition to the original image patches, to train a new classifier. The algorithm then iterates until convergence\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1214/aos/1016218223', '10.1109/TPAMI.1984.4767596', '10.1007/s11263-006-7538-0', '10.1023/B:VISI.0000013087.49260.fb', '10.1109/ICCV.2005.107', '10.1097/00004728-199307000-00004', '10.1109/CVPR.2008.4587587', '10.1109/TPAMI.2008.146', '10.1007/s11263-008-0137-5', '10.1109/TMI.2004.830802', '10.1109/TIT.2005.850085', '10.1016/S0896-6273(02)00569-X', '10.1006/jcss.1997.1504', '10.1016/S0006-3223(00)01120-3', '10.1016/j.tics.2007.09.009', '10.1109/ICCV.2003.1238478', '10.1109/ICCV.2005.9', '10.1007/11744023_1', '10.1109/CVPR.2003.1211359', '10.1109/TMI.2004.830803', '10.1214/aos/1024691352', '10.1109/CVPR.2008.4587503', '10.1023/A:1026313132218', '10.1016/j.neuroimage.2005.11.044', '10.1109/ICCV.2007.4408986', '10.1109/CVPR.2004.314', '10.1145/1143844.1143865', '10.1109/34.993558', '10.1109/CVPR.2007.383098', '10.1109/CVPR.2005.177', '10.1007/s11263-005-6642-x', '10.1613/jair.105', '10.1109/TMI.2007.908121', '10.1002/hbm.10062'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'OCR'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Mask TextSpotter: An end-to-end trainable neural network for spotting text with arbitrary shapes': Paper(DOI='10.1109/tpami.2019.2937086', crossref_json=None, google_schorlar_metadata=None, title='Mask TextSpotter: An end-to-end trainable neural network for spotting text with arbitrary shapes', authors=['Pengyuan Lyu', 'Minghui Liao', 'Cong Yao', 'Wenhao Wu', 'Xiang Bai'], abstract='Recently, models based on deep neural networks have dominated the fields of scene text detection and recognition. In this paper, we investigate the problem of scene text spotting, which aims at simultaneous text detection and recognition in natural images. An end-to-end trainable neural network model for scene text spotting is proposed. The proposed model, named as Mask TextSpotter, is inspired by the newly published work Mask R-CNN. Different from previous methods that also accomplish text spotting with end-to-end trainable deep neural networks, Mask TextSpotter takes advantage of simple and smooth end-to-end learning procedure, in which precise text detection and recognition are acquired via semantic segmentation. Moreover, it is superior to previous methods in handling text instances of irregular shapes, for example, curved text. Experiments on ICDAR2013, ICDAR2015 and Total-Text demonstrate that the proposed method achieves state-of-the-art results in both scene text detection and end-to-end text recognition tasks.', conference=None, journal=None, year=None, reference_list=['10.1109/ICDAR.2017.143', '10.1109/TIP.2014.2353813', '10.1109/CVPR.2014.515', '10.1007/978-3-030-01270-0_22', '10.1109/ICCV.2017.560', '10.24963/ijcai.2017/458', '10.1109/CVPR.2016.245', '10.1109/CVPR.2016.451', '10.1007/s11263-015-0823-z', '10.1007/978-3-319-10593-2_33', '10.1109/ICDAR.2013.221', '10.1109/ICDAR.2015.7333942', '10.1007/978-3-319-10593-2_34', '10.1109/CVPR.2016.91', '10.1016/j.eswa.2014.07.008', '10.1109/TPAMI.2016.2577031', '10.1007/s11263-014-0793-6', '10.1162/neco.1997.9.8.1735', '10.1109/CVPR.2017.371', '10.1109/ICCV.2017.87', '10.1109/TPAMI.2016.2646371', '10.1109/TPAMI.2018.2848939', '10.1109/ICCV.2017.529', '10.1016/j.patcog.2016.10.016', '10.1007/978-3-319-46484-8_4', '10.1109/TPAMI.2014.2339814', '10.1145/1143844.1143891', '10.1109/ICCV.2017.322', '10.1109/CVPR.2016.254', '10.1109/ICCV.2017.331', '10.1109/CVPR.2016.90', '10.1109/CVPR.2018.00527', '10.1007/978-3-030-01264-9_5', '10.1109/CVPR.2018.00788', '10.1109/TPAMI.2015.2496234', '10.1109/CVPR.2012.6248097', '10.1109/ICDAR.2017.237', '10.1109/CVPR.2012.6247990', '10.5244/C.26.127', '10.1007/978-3-319-46466-4_32', '10.1109/CVPR.2017.472', '10.1109/ICPR.2018.8546066', '10.1109/CVPR.2015.7298878', '10.1109/ICCV.2015.169', '10.1109/CVPR.2017.283', '10.1109/CVPR.2014.81', '10.1016/j.patcog.2017.04.027', '10.1109/ICDAR.2017.234', '10.1109/CVPR.2015.7298914', '10.1109/CVPR.2017.660', '10.1109/ACCESS.2018.2878899', '10.1109/CVPR.2018.00163', '10.1109/ICCV.2013.102', '10.1145/1553374.1553380', '10.1109/ICCV.2017.543', '10.1109/ICCV.2017.242', '10.1109/ICDAR.2003.1227749', '10.1109/CVPR.2018.00584', '10.1109/CVPR.2018.00595', '10.1007/978-3-030-01216-8_2', '10.1109/CVPR.2015.7298965', '10.1109/TIP.2018.2825107', '10.1109/CVPR.2017.106', '10.1109/CVPR.2018.00619'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'OCR'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Skeleton pruning by contour partitioning with discrete curve evolution': Paper(DOI='10.1109/tpami.2007.59', crossref_json=None, google_schorlar_metadata=None, title='Skeleton pruning by contour partitioning with discrete curve evolution', authors=['Xiang Bai', 'Longin Jan Latecki', 'Wen-Yu Liu'], abstract='In this paper, we introduce a new skeleton pruning method based on contour partitioning. Any contour partition can be used, but the partitions obtained by discrete curve evolution (DCE) yield excellent results. The theoretical properties and the experiments presented demonstrate that obtained skeletons are in accord with human visual perception and stable, even in the presence of significant noise and shape variations, and have the same topology as the original skeletons. In particular, we have proven that the proposed approach never produces spurious branches, which are common when using the known skeleton pruning methods. Moreover, the proposed pruning method does not displace the skeleton points. Consequently, all skeleton points are centers of maximal disks. Again, many existing methods displace skeleton points in order to produces pruned skeletons', conference=None, journal=None, year=None, reference_list=['10.1006/cviu.1997.0598', '10.1109/ICCV.1998.710721', '10.1109/CVPR.2003.1211439', '10.1016/S0031-3203(02)00348-5', '10.1016/S0031-3203(02)00098-5', '10.1006/cviu.1998.0680', '10.1007/BFb0015444', '10.1016/1049-9660(92)90030-7', '10.1109/ICCV.1995.466903', '10.1117/1.482748', '10.1109/CVPR.2000.855849', '10.1023/A:1016376116653', '10.1109/TPAMI.2002.1114849', '10.1109/34.368189', '10.1109/34.969119', '10.1109/ICCV.1998.710722', '10.1016/0022-5193(73)90175-6', '10.1016/j.cviu.2004.03.006', '10.1016/j.patcog.2003.07.004', '10.1016/S0262-8856(97)00074-7', '10.1016/0031-3203(94)00105-U', '10.2140/pjm.1997.181.57', '10.1016/0262-8856(93)90055-L', '10.1109/34.107013', '10.1109/CVPR.2000.855792', '10.1006/cviu.1998.0738', '10.1109/CVPR.1994.323787', '10.1109/34.149591', '10.1109/34.544075', '10.1109/TPAMI.1987.4767938', '10.1016/S0031-3203(01)00039-5', '10.1109/TPAMI.2004.1273924', '10.1109/CVPR.2000.855850', '10.1016/S0734-189X(86)80047-0', '10.1109/TPAMI.1985.4767685', '10.1006/cviu.1995.1062', '10.1109/34.879802'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Robot Perception', 'Machine Learning', 'Data Mining', 'Artificial Intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Real-time scene text detection with differentiable binarization': Paper(DOI='10.1609/aaai.v34i07.6812', crossref_json=None, google_schorlar_metadata=None, title='Real-time scene text detection with differentiable binarization', authors=['Minghui Liao', 'Zhaoyi Wan', 'Cong Yao', 'Kai Chen', 'Xiang Bai'], abstract='Recently, segmentation-based methods are quite popular in scene text detection, as the segmentation results can more accurately describe scene text of various shapes such as curve text. However, the post-processing of binarization is essential for segmentation-based detection, which converts probability maps produced by a segmentation method into bounding boxes/regions of text. In this paper, we propose a module named Differentiable Binarization (DB), which can perform the binarization process in a segmentation network. Optimized along with a DB module, a segmentation network can adaptively set the thresholds for binarization, which not only simplifies the post-processing but also enhances the performance of text detection. Based on a simple segmentation network, we validate the performance improvements of DB on five benchmark datasets, which consistently achieves state-of-the-art results, in terms of both detection accuracy and speed. In particular, with a light-weight backbone, the performance improvements by DB are significant so that we can look for an ideal tradeoff between detection accuracy and efficiency. Specifically, with a backbone of ResNet-18, our detector achieves an F-measure of 82.8, running at 62 FPS, on the MSRA-TD500 dataset. Code is available at: https://github. com/MhLiao/DB.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'OCR'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Path similarity skeleton graph matching': Paper(DOI='10.1109/tpami.2007.70769', crossref_json=None, google_schorlar_metadata=None, title='Path similarity skeleton graph matching', authors=['Xiang Bai', 'Longin Jan Latecki'], abstract='This paper proposes a novel graph matching algorithm and applies it to shape recognition based on object silhouettes. The main idea is to match skeleton graphs by comparing the geodesic paths between skeleton endpoints. In contrast to typical tree or graph matching methods, we do not consider the topological graph structure. Our approach is motivated by the fact that visually similar skeleton graphs may have completely different topological structures. The proposed comparison of geodesic paths between endpoints of skeleton graphs yields correct matching results in such cases. The skeletons are pruned by contour partitioning with discrete. Curve evolution, which implies that the endpoints of skeleton branches correspond to visual parts of the objects. The experimental results demonstrate that our method is able to produce correct results in the presence of articulations, stretching, and contour deformations.', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2000.855850', '10.1007/11907350_48', '10.1109/TPAMI.2007.59', '10.1016/0031-3203(94)00105-U', '10.1006/cviu.1998.0738', '10.1109/34.879802', '10.2140/pjm.1997.181.57', '10.1007/978-3-540-24670-1_25', '10.1109/TPAMI.2005.142', '10.1016/S0031-3203(02)00098-5', '10.1007/s11263-006-6993-y', '10.1145/956750.956777', '10.1006/cviu.1999.0802', '10.1007/3-540-63223-9_109', '10.1137/1.9781611972726.12', '10.1093/bioinformatics/17.6.495', '10.1109/TPAMI.2003.1159948', '10.1006/cviu.1997.0598', '10.1109/34.993558', '10.1007/BF00208719', '10.1016/j.sigpro.2004.10.016', '10.1016/0022-5193(73)90175-6', '10.1109/34.232073', '10.1016/S0042-6989(98)00043-1', '10.1109/TASSP.1978.1163055', '10.1109/ICCV.2001.937526', '10.1109/TPAMI.2003.1159951', '10.1109/ICDM.2005.118', '10.1142/S0218001408006405', '10.1109/34.809105', '10.1016/j.patcog.2003.07.004', '10.1007/BF01451741', '10.1109/TPAMI.2002.1046176', '10.1109/TPAMI.2007.41', '10.1109/34.75511', '10.1145/383259.383282', '10.1109/TPAMI.2005.146', '10.1016/j.cviu.2004.03.006', '10.1016/S0167-8655(02)00255-6', '10.1109/ICCV.1998.710722', '10.1016/S0262-8856(98)00130-9', '10.1109/TPAMI.2004.1273924', '10.1109/TPAMI.2004.1273924', '10.1023/A:1008102926703', '10.1109/34.368189'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Robot Perception', 'Machine Learning', 'Data Mining', 'Artificial Intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Spacetime constraints': Paper(DOI='10.1007/s10601-023-09356-1', crossref_json=None, google_schorlar_metadata=None, title='Spacetime constraints', authors=['Andrew Witkin', 'Michael Kass'], abstract='Spacetime constraints are a new method for creating character animation. The animator specifies what the character has to do, for instance, \"jump from here to there, clearing a hurdle in between;\" how the motion should be performed, for instance \"don\\'t waste energy,\" or \"come down hard enough to splatter whatever you land on;\" the character\\'s physical structure---the geometry, mass, connectivity, etc. of the parts; and the physical resources\\' available to the character to accomplish the motion, for instance the character\\'s muscles, a floor to push off from, etc. The requirements contained in this description, together with Newton\\'s laws, comprise a problem of constrained optimization. The solution to this problem is a physically valid motion satisfying the \"what\" constraints and optimizing the \"how\" criteria. We present as examples a Luxo lamp performing a variety of coordinated motions. These realistic motions conform\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Graphics', 'Computer Vision', 'Image Processing', 'Signal Processing', 'Artificial Intelligence'], conference_acronym='Constraints (Dordrecht)', publisher=None, query_handler=None),\n",
       " 'An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision': Paper(DOI='10.1007/3-540-44745-8_24', crossref_json=None, google_schorlar_metadata=None, title='An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision', authors=['Yuri Boykov', 'Vladimir Kolmogorov'], abstract=' After [10,15,12,2,4] minimum cut/maximum flow algorithms on graphs emerged as an increasingly useful tool for exact or approximate energy minimization in low-level vision. The combinatorial optimization literature provides many min-cut/max-flow algorithms with different polynomial time complexity. Their practical efficiency, however, has to date been studied mainly outside the scope of computer vision. The goal of this paper is to provide an experimental comparison of the efficiency of min-cut/max flow algorithms for energy minimization in vision. We compare the running times of several standard algorithms, as well as a new algorithm that we have recently developed. The algorithms we study include both Goldberg-style “push-relabel’ methods and algorithms based on Ford-Fulkerson style augmenting paths. We benchmark these algorithms on a number of typical graphs in the contexts of image\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2001.937505', '10.1109/ICCV.1999.791245', '10.1007/PL00009180', '10.1515/9781400875184', '10.1145/48014.61051', '10.1007/BFb0055670', '10.1109/CVPR.1998.698598', '10.1007/978-3-540-40899-4_7'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'biomedical image analysis'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Graph cuts and efficient ND image segmentation': Paper(DOI='10.1007/978-3-642-24088-1_24', crossref_json=None, google_schorlar_metadata=None, title='Graph cuts and efficient ND image segmentation', authors=['Yuri Boykov', 'Gareth Funka-Lea'], abstract=' Combinatorial graph cut algorithms have been successfully applied to a wide range of problems in vision and graphics. This paper focusses on possibly the simplest application of graph-cuts: segmentation of objects in image data. Despite its simplicity, this application epitomizes the best features of combinatorial graph cuts methods in vision: global optima, practical efficiency, numerical robustness, ability to fuse a wide range of visual cues and constraints, unrestricted topological properties of segments, and applicability to N-D problems. Graph cuts based approaches to object extraction have also been shown to have interesting connections with earlier segmentation methods such as snakes, geodesic active contours, and level-sets. The segmentation energies optimized by graph cuts combine boundary regularization with region-based properties in the same fashion as Mumford-Shah style functionals. We\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.1000236', '10.1109/CVPR.2005.332', '10.1109/34.946985', '10.1109/CVPR.2007.383231', '10.1023/B:VISI.0000022288.19776.77', '10.1007/3-540-47977-5_27', '10.1109/TSMCB.2004.837756', '10.1109/ICCV.2001.937655', '10.1006/cviu.1997.0573', '10.1007/978-3-540-45167-9_14', '10.1109/34.868688', '10.1109/TSMCB.2007.902249', '10.1016/S0165-1684(97)00060-1', '10.1109/TPAMI.2007.1046', '10.1109/ICCV.1999.790354', '10.1109/34.244673', '10.1109/ICCV.2003.1238361'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'biomedical image analysis'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Superpixels and supervoxels in an energy optimization framework': Paper(DOI='10.1007/978-3-642-15555-0_16', crossref_json=None, google_schorlar_metadata=None, title='Superpixels and supervoxels in an energy optimization framework', authors=['Olga Veksler', 'Yuri Boykov', 'Paria Mehrani'], abstract=' Many methods for object recognition, segmentation, etc., rely on a tessellation of an image into “superpixels”. A superpixel is an image patch which is better aligned with intensity edges than a rectangular patch. Superpixels can be extracted with any segmentation algorithm, however, most of them produce highly irregular superpixels, with widely varying sizes and shapes. A more regular space tessellation may be desired. We formulate the superpixel partitioning problem in an energy minimization framework, and optimize with graph cuts. Our energy function explicitly encourages regular superpixels. We explore variations of the basic energy, which allow a trade-off between a less regular tessellation but more accurate boundaries or better efficiency. Our advantage over previous work is computational efficiency, principled optimization, and applicability to 3D “supervoxel” segmentation. We achieve high\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2003.1238308', '10.1109/ICCV.2005.107', '10.1109/ICCV.2005.112', '10.1007/11744023_27', '10.5244/C.21.55', '10.1007/978-3-540-88690-7_36', '10.1109/ICCV.2009.5459175', '10.1145/1276377.1276485', '10.1109/34.1000236', '10.1023/B:VISI.0000022288.19776.77', '10.1109/TPAMI.2009.96', '10.1145/882262.882264', '10.1016/0021-9991(88)90002-2', '10.1109/34.969114', '10.1109/TPAMI.2007.70844', '10.1109/CVPR.2008.4587471', '10.1109/CVPR.2010.5539890', '10.1109/ICCV.2003.1238310', '10.1007/s11263-006-7934-5', '10.1145/1015706.1015763', '10.1109/CVPR.2007.383047', '10.1214/aos/1016218223'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Energy-based geometric multi-model fitting': Paper(DOI='10.1007/s11263-011-0474-7', crossref_json=None, google_schorlar_metadata=None, title='Energy-based geometric multi-model fitting', authors=['Hossam Isack', 'Yuri Boykov'], abstract=' Geometric model fitting is a typical chicken-&-egg problem: data points should be clustered based on geometric proximity to models whose unknown parameters must be estimated at the same time. Most existing methods, including generalizations of RANSAC, greedily search for models with most inliers (within a threshold) ignoring overall classification of points. We formulate geometric multi-model fitting as an optimal labeling problem with a global energy function balancing geometric errors and regularity of inlier clusters. Regularization based on spatial coherence (on some near-neighbor graph) and/or label costs is NP hard. Standard combinatorial algorithms with guaranteed approximation bounds (e.g. α-expansion) can minimize such regularization energies over a finite set of labels, but they are not directly applicable to a continuum of labels, e.g.  in line fitting. Our proposed approach (PEaRL\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-540-45243-0_31', '10.1007/s11263-011-0437-z', '10.1109/34.990138', '10.1109/34.601246', '10.1007/BF00054839', '10.1109/TPAMI.2006.130', '10.1006/cviu.1999.0832', '10.1098/rsta.1998.0224', '10.1109/34.537343', '10.1002/ima.20269'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'biomedical image analysis'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'On regularized losses for weakly-supervised cnn segmentation': Paper(DOI='10.1007/978-3-030-01270-0_31', crossref_json=None, google_schorlar_metadata=None, title='On regularized losses for weakly-supervised cnn segmentation', authors=['Meng Tang', 'Federico Perazzi', 'Abdelaziz Djelouah', 'Ismail Ben Ayed', 'Christopher Schroers', 'Yuri Boykov'], abstract=\"Minimization of regularized losses is a principled approach to weak supervision well-established in deep learning, in general. However, it is largely overlooked in semantic segmentation currently dominated by methods mimicking full supervision via``fake''fully-labeled masks (proposals) generated from available partial input. To obtain such full masks the typical methods explicitly use standard regularization techniques for``shallow''segmentation, eg graph cuts or dense CRFs. In contrast, we integrate such standard regularizers directly into the loss functions over partial input. This approach simplifies weakly-supervised training by avoiding extra MRF/CRF inference steps or layers explicitly generating full masks, while improving both the quality and efficiency of training. This paper proposes and experimentally compares different losses integrating MRF/CRF regularization terms. We juxtapose our regularized losses with earlier proposal-generation methods. Our approach achieves state-of-the-art accuracy in semantic segmentation with near full-supervision quality.\", conference=None, journal=None, year=None, reference_list=['10.1111/j.1467-8659.2009.01645.x', '10.1109/CVPR.2016.630', '10.1561/2200000016', '10.1109/34.969114', '10.1007/s11263-009-0238-9', '10.1137/040615286', '10.7551/mitpress/9780262033589.001.0001', '10.1109/TPAMI.2014.2345401', '10.1109/TPAMI.2010.200', '10.1109/ICCV.2015.191', '10.1007/978-3-319-46475-6_50', '10.1109/TPAMI.2006.233', '10.1109/CVPR.2017.181', '10.1007/978-3-319-46493-0_42', '10.1109/CVPR.2016.344', '10.1109/ICCV.2015.203', '10.1007/s11263-007-0110-8', '10.1109/CVPR.2009.5206604', '10.1109/TMI.2016.2621185', '10.1145/1186562.1015720', '10.1109/34.868688', '10.1109/CVPR.2018.00195', '10.1007/978-3-319-46475-6_46', '10.1109/CVPR.2017.315', '10.1007/978-3-642-35289-8_34', '10.1109/ICCV.2015.179'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'biomedical image analysis'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Constrained-CNN losses for weakly supervised segmentation': Paper(DOI='10.1016/j.media.2019.02.009', crossref_json=None, google_schorlar_metadata=None, title='Constrained-CNN losses for weakly supervised segmentation', authors=['Hoel Kervadec', 'Jose Dolz', 'Meng Tang', 'Eric Granger', 'Yuri Boykov', 'Ismail Ben Ayed'], abstract='Weakly-supervised learning based on, e.g., partially labelled images or image-tags, is currently attracting significant attention in CNN segmentation as it can mitigate the need for full and laborious pixel/voxel annotations. Enforcing high-order (global) inequality constraints on the network output (for instance, to constrain the size of the target region) can leverage unlabeled data, guiding the training process with domain-specific knowledge. Inequality constraints are very flexible because they do not assume exact prior knowledge. However, constrained Lagrangian dual optimization has been largely avoided in deep networks, mainly for computational tractability reasons. To the best of our knowledge, the method of Pathak et\\xa0al. (2015a) is the only prior work that addresses deep CNNs with linear constraints in weakly supervised segmentation. It uses the constraints to synthesize fully-labeled training masks (proposals\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2012.120', '10.1109/MSP.2017.2762355', '10.1016/j.neuroimage.2017.04.039', '10.1109/TMI.2017.2724070', '10.1109/TPAMI.2014.2306415', '10.1016/j.media.2017.07.005', '10.1016/j.media.2012.09.002', '10.1109/TMI.2016.2621185', '10.1109/TPAMI.2016.2636150', '10.1109/82.160169', '10.1109/WACV.2019.00020'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'biomedical image analysis'], conference_acronym='Medical image analysis (Print)', publisher=None, query_handler=None),\n",
       " 'A variable window approach to early vision': Paper(DOI='10.1109/34.735802', crossref_json=None, google_schorlar_metadata=None, title='A variable window approach to early vision', authors=['Yu Boykov', 'Olga Veksler', 'Ramin Zabih'], abstract='Early vision relies heavily on rectangular windows for tasks such as smoothing and computing correspondence. While rectangular windows are efficient, they yield poor results near object boundaries. We describe an efficient method for choosing an arbitrarily shaped connected window, in a manner that varies at each pixel. Our approach can be applied to several problems, including image restoration and visual correspondence. It runs in linear time, and takes a few seconds on traditional benchmark images. Performance on both synthetic and real imagery appears promising.', conference=None, journal=None, year=None, reference_list=['10.1109/34.310690', '10.1007/BF00127126', '10.1007/BF00054836', '10.1137/0201010', '10.21236/AD0786720', '10.1109/34.464558', '10.1002/0471725382', '10.1109/CVPR.1998.698598', '10.1017/S0305004100027419', '10.1016/0004-3702(81)90024-2', '10.1109/TPAMI.1984.4767596', '10.1109/ICIP.1995.537491', '10.1109/CCV.1988.589984', '10.1109/CCV.1988.590039', '10.1006/cviu.1996.0040', '10.1109/CVPR.1998.698673', '10.1109/ICCV.1993.378241', '10.1109/34.206955', '10.1038/317314a0'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Retina mosaicing using local features': Paper(DOI='10.1007/11866763_23', crossref_json=None, google_schorlar_metadata=None, title='Retina mosaicing using local features', authors=['Philippe C Cattin', 'Herbert Bay', 'Luc Van Gool', 'Gábor Székely'], abstract=' Laser photocoagulation is a proven procedure to treat various pathologies of the retina. Challenges such as motion compensation, correct energy dosage, and avoiding incidental damage are responsible for the still low success rate. They can be overcome with improved instrumentation, such as a fully automatic laser photocoagulation system. In this paper, we present a core image processing element of such a system, namely a novel approach for retina mosaicing. Our method relies on recent developments in region detection and feature description to automatically fuse retina images. In contrast to the state-of-the-art the proposed approach works even for retina images with no discernable vascularity. Moreover, an efficient scheme to determine the blending masks of arbitrarily overlapping images for multi-band blending is presented.', conference=None, journal=None, year=None, reference_list=['10.1097/00004397-199503540-00005', '10.1109/ICIP.1994.413740', '10.1109/10.650362', '10.1109/TITB.2004.826733', '10.1109/TMI.2003.819276', '10.1007/978-3-540-39899-8_92', '10.1023/B:VISI.0000029664.99615.94', '10.1109/ICCV.2003.1238630', '10.1109/CVPR.2005.235', '10.1007/11744023_32', '10.1109/TPAMI.2005.188', '10.1145/245.247'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Mixed Reality'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Computer vision–ECCV 2006': Paper(DOI='10.1007/11744085_5', crossref_json=None, google_schorlar_metadata=None, title='Computer vision–ECCV 2006', authors=['Herbert Bay', 'Tinne Tuytelaars', 'Luc Van Gool'], abstract=None, conference=None, journal=None, year=None, reference_list=['10.1145/566654.566573', '10.1016/0146-664X(74)90022-7', '10.1007/3-540-47969-4_11', '10.1364/JOSA.61.000001', '10.1142/3905', '10.1145/1201775.882269', '10.1017/CBO9780511526411', '10.1016/j.jphysparis.2003.10.010'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Mixed Reality'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Method and system for image-based information retrieval': Paper(DOI='10.17950/ijer/v3s6/607', crossref_json=None, google_schorlar_metadata=None, title='Method and system for image-based information retrieval', authors=['Till Quack', 'Herbert Bay'], abstract='For retrieving information based on images, a first image is taken (S1) using a digital camera associated with a communication terminal (1). Query data related to the first image is transmitted (S3) via a communication network (2) to a remote recognition server (3). In the remote recognition server (3) a reference image is identified (S4) based on the query data. Subsequently, in the remote recognition server (3), a Homography is computed (S5) based on the reference image and the query data, the Homography mapping the reference image to the first image. Moreover, in the remote recognition server (3), a second image is selected (S6) and a projection image is computed (S7) of the second image using the Homography. By replacing a part of the first image with at least a part of the projection image, an augmented image is generated (S8, S10) and displayed (S11) at the communication terminal (1). Efficient\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Mixed Reality'], conference_acronym='International journal of engineering research', publisher=None, query_handler=None),\n",
       " 'Object recognition for the internet of things': Paper(DOI='10.1007/978-3-540-78731-0_15', crossref_json=None, google_schorlar_metadata=None, title='Object recognition for the internet of things', authors=['Till Quack', 'Herbert Bay', 'Luc Van Gool'], abstract=' We present a system which allows to request information on physical objects by taking a picture of them. This way, using a mobile phone with integrated camera, users can interact with objects or ”things” in a very simple manner. A further advantage is that the objects themselves don’t have to be tagged with any kind of markers. At the core of our system lies an object recognition method, which identifies an object from a query image through multiple recognition stages, including local visual features, global geometry, and optionally also metadata such as GPS location. We present two applications for our system, namely a slide tagging application for presentation screens in smart meeting rooms and a cityguide on a mobile phone. Both systems are fully functional, including an application on the mobile phone, which allows simplest point-and-shoot interaction with objects. Experiments evaluate the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1147/sj.384.0508', '10.1007/11744023_32', '10.1145/1378063.1378068', '10.1007/11677482_3', '10.1109/CVPR.2006.107', '10.1145/358669.358692', '10.1145/1149488.1149490', '10.1017/CBO9780511811685', '10.1145/276698.276876', '10.1109/CVPR.2005.272', '10.1023/B:VISI.0000029664.99615.94', '10.1109/TPAMI.2005.188', '10.1109/IVL.1999.781134', '10.1109/CVPR.2006.264', '10.1109/ITSC.2006.1706801', '10.1109/TMM.2006.879870', '10.1038/scientificamerican0104-56'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Mixed Reality'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Surf: Speeded up robust features, Computer Vision: Ninth European Conference on Computer Vision, Graz, 7–13 May, 2006: proceedings': Paper(DOI='10.1007/11744023_32', crossref_json=None, google_schorlar_metadata=None, title='Surf: Speeded up robust features, Computer Vision: Ninth European Conference on Computer Vision, Graz, 7–13 May, 2006: proceedings', authors=['Herbert Bay', 'Tinne Tuytelaars', 'Luc Van Gool'], abstract=None, conference=None, journal=None, year=None, reference_list=['10.1023/A:1008045108935', '10.1023/B:VISI.0000029664.99615.94', '10.1007/3-540-47969-4_9', '10.5244/C.14.38', '10.5244/C.16.36', '10.1109/TPAMI.2005.188', '10.1007/s11263-005-3848-x', '10.5244/C.2.23', '10.1109/ICCV.1999.790410', '10.1023/A:1012460413855', '10.1023/B:VISI.0000027790.02288.f2', '10.1007/BF01249895', '10.1007/3-540-47969-4_28', '10.1109/34.93808', '10.1007/BF00336961', '10.5244/C.4.19', '10.1007/3-540-44935-3_11', '10.5244/C.16.23'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Mixed Reality'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'A Ess, and T Tuytelaars, SURF: speeded up robust features': Paper(DOI='10.1016/j.cviu.2007.09.014', crossref_json=None, google_schorlar_metadata=None, title='A Ess, and T Tuytelaars, SURF: speeded up robust features', authors=['Herbert Bay', 'Andreas Ess'], abstract=None, conference=None, journal=None, year=None, reference_list=['10.1007/11744023_32', '10.5244/C.16.23', '10.1007/11866763_23', '10.1145/997817.997857', '10.1007/BF01249895', '10.1109/34.93808', '10.1007/11559887_9', '10.1007/11612032_92', '10.5244/C.2.23', '10.1023/A:1012460413855', '10.1007/BF00336961', '10.1109/34.49051', '10.1023/A:1008045108935', '10.1007/3-540-44935-3_11', '10.1109/ICCV.1999.790410', '10.1023/B:VISI.0000029664.99615.94', '10.5244/C.16.36', '10.1007/3-540-47969-4_9', '10.1023/B:VISI.0000027790.02288.f2', '10.1109/TPAMI.2005.188', '10.1007/s11263-005-3848-x', '10.1109/ICPR.2006.479', '10.5244/C.14.38', '10.1007/s00138-006-0027-1'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Mixed Reality'], conference_acronym='Computer vision and image understanding (Print)', publisher=None, query_handler=None),\n",
       " 'Motion editing with prioritized constraints': Paper(DOI='10.1016/j.gmod.2005.03.001', crossref_json=None, google_schorlar_metadata=None, title='Motion editing with prioritized constraints', authors=['Ronan Boulic', 'Benoît Le Callennec', 'Martin Herren', 'Herbert Bay'], abstract='We propose an intuitive motion editing technique allowing the end-user to transform an original motion by applying position constraints on freely selected locations of the character body. The major innovation comes from the possibility to assign a priority level to each constraint. The resulting scale of user-defined priority levels allows to handle multiple asynchronously overlapping constraints. As a consequence the end user can enforce a larger range of natural behaviors where conflicting constraints compete to control a common set of joints. By default the joint angles of the original motion are preserved as the lowest priority constraint. However, in case a Cartesian constraint from the original motion is essential, it is straightforward to define a high priority constraint that will retain it before enforcing other lower priority constraints. Additional features are proposed to provide a more productive motion editing process like defining the constraints relative to a mobile frame and precisely specifying which joints are recruited by each constrain. The current limitation remains its computing cost which will be improved in future work.', conference=None, journal=None, year=None, reference_list=['10.1145/566654.566606', '10.1007/BF02021810', '10.1109/MCG.1987.276894', '10.1145/965105.807491', '10.1007/s00371-004-0244-4', '10.1109/2945.620491', '10.1016/S0097-8493(96)00043-X', '10.1145/218380.218421', '10.1002/1099-1778(200012)11:5<223::AID-VIS236>3.0.CO;2-5', '10.1145/301136.301152', '10.1006/gmod.2001.0549', '10.1145/253284.253321', '10.1145/364338.364400', '10.1145/280814.280820', '10.1109/38.486680', '10.1145/964965.808575', '10.1145/566654.566605', '10.1145/566654.566607', '10.1145/311535.311539', '10.1145/566654.566596', '10.1109/38.55154', '10.1111/1467-8659.00393', '10.1016/S0021-9290(96)00165-0', '10.1145/545261.545279', '10.1109/2945.468392', '10.1145/311535.311536', '10.1145/237170.237229', '10.1109/TSMCA.2004.832811', '10.1145/502122.502123', '10.1145/325165.325243', '10.1145/383259.383288', '10.1006/gmod.2000.0528', '10.1109/TMMS.1969.299896', '10.1145/37402.37429', '10.1145/378456.378507', '10.1145/218380.218422', '10.1109/TRA.2003.810579', '10.1109/TVCG.2003.1207443', '10.1145/195826.195827'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Mixed Reality'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Dense stereo by triangular meshing and cross validation': Paper(DOI='10.1007/11861898_71', crossref_json=None, google_schorlar_metadata=None, title='Dense stereo by triangular meshing and cross validation', authors=['Peter Wey', 'Bernd Fischer', 'Herbert Bay', 'Joachim M Buhmann'], abstract=' Dense depth maps can be estimated in a Bayesian sense from multiple calibrated still images of a rigid scene relative to a reference view [1]. This well-established probabilistic framework is extended by adaptively refining a triangular meshing procedure and by automatic cross-validation of model parameters. The adaptive refinement strategy locally adjusts the triangular meshing according to the measured image data. The new method substantially outperforms the competing techniques both in terms of robustness and accuracy.', conference=None, journal=None, year=None, reference_list=['10.1023/A:1014573219977', '10.1023/B:VISI.0000029664.99615.94', '10.1017/CBO9780511811685', '10.1109/TIP.2003.817240'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Mixed Reality'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale': Paper(DOI='10.1007/s11263-020-01316-z', crossref_json=None, google_schorlar_metadata=None, title='The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale', authors=['Alina Kuznetsova', 'Hassan Rom', 'Neil Alldrin', 'Jasper Uijlings', 'Ivan Krasin', 'Jordi Pont-Tuset', 'Shahab Kamali', 'Stefan Popov', 'Matteo Malloci', 'Alexander Kolesnikov', 'Tom Duerig', 'Vittorio Ferrari'], abstract=' We present Open Images V4, a dataset of 9.2M images with unified annotations for image classification, object detection and visual relationship detection. The images have a Creative Commons Attribution license that allows to share and adapt the material, and they have been collected from Flickr without a predefined list of class names or tags, leading to natural class statistics and avoiding an initial design bias. Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, we provide  more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images). The images often show complex scenes with several objects (8 annotated objects per image on average). We annotated visual relationships between them\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2010.5540226', '10.1109/TPAMI.2012.28', '10.1109/CVPR.2017.195', '10.1109/CVPR.2017.352', '10.1109/CVPR.2009.5206848', '10.1007/s11263-009-0275-4', '10.1007/s11263-014-0733-5', '10.1109/TPAMI.2006.79', '10.1109/CVPR.2010.5539906', '10.1109/TPAMI.2009.167', '10.1109/ICCV.2015.169', '10.1109/CVPR.2014.81', '10.1109/CVPR.2018.00872', '10.1109/TPAMI.2009.83', '10.1109/CVPR.2016.90', '10.1109/CVPR.2017.351', '10.1007/s11263-016-0981-7', '10.1109/CVPR.2017.766', '10.1609/aaai.v32i1.12274', '10.1109/CVPR.2017.469', '10.1109/ICCV.2017.324', '10.1007/978-3-319-10602-1_48', '10.1007/978-3-319-46448-0_2', '10.1007/978-3-319-46448-0_51', '10.1109/CVPR.2016.99', '10.1109/ICCV.2017.528', '10.1109/ICCV.2017.554', '10.1109/TPAMI.2011.158', '10.1016/S0893-6080(98)00116-6', '10.1109/CVPR.2017.690', '10.1109/CVPR.2016.91', '10.1007/s11263-015-0816-y', '10.1109/CVPR.2018.00474', '10.1109/ICCV.2017.97', '10.1109/CVPR.2015.7298594', '10.1109/CVPR.2016.308', '10.1609/aaai.v31i1.11231', '10.1109/CVPR.2018.00121', '10.1007/s11263-013-0620-5', '10.1109/CVPR.2017.330', '10.1109/CVPR.2010.5540235', '10.1109/CVPR.2018.00611', '10.1109/CVPR.2017.331', '10.1109/ICCV.2017.454'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Big Transfer (BiT): General Visual Representation Learning': Paper(DOI='10.1007/978-3-030-58558-7_29', crossref_json=None, google_schorlar_metadata=None, title='Big Transfer (BiT): General Visual Representation Learning', authors=['Alexander Kolesnikov*', 'Lucas Beyer*', 'Xiaohua Zhai*', 'Joan Puigcerver', 'Jessica Yung', 'Sylvain Gelly', 'Neil Houlsby', '*equal contribution'], abstract=' Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes—from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2017.195', '10.1109/CVPR.2009.5206848', '10.1109/CVPR42600.2020.00975', '10.1109/ICCV.2019.00502', '10.1109/CVPR.2016.90', '10.1007/978-3-319-46493-0_38', '10.1007/978-3-319-46478-7_5', '10.1109/ICCV.2017.449', '10.1109/ICCV.2017.324', '10.1007/978-3-319-10602-1_48', '10.1007/978-3-030-01216-8_12', '10.1109/ICVGIP.2008.47', '10.1109/TKDE.2009.191', '10.1109/CVPR.2012.6248092', '10.1007/s11263-015-0816-y', '10.1109/ICCV.2017.97', '10.1109/CVPR.2018.00131', '10.1109/CVPR.2015.7298594', '10.1109/CVPR.2016.308', '10.1145/2812802', '10.1109/CVPR42600.2020.01382', '10.1007/978-3-030-01261-8_1', '10.1109/CVPR42600.2020.01070', '10.1109/CVPR.2017.634', '10.1109/ICCV.2019.00612', '10.1109/ICCV.2019.00156', '10.1007/978-1-4899-7687-1_79'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Representation Learning', 'Vision and Language', 'Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation': Paper(DOI='10.1007/978-3-319-46493-0_42', crossref_json=None, google_schorlar_metadata=None, title='Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation', authors=['Alexander Kolesnikov', 'Christoph H. Lampert'], abstract=' We introduce a new loss function for the weakly-supervised training of semantic image segmentation models based on three guiding principles: to seed with weak localization cues, to expand objects based on the information about which classes can occur in an image, and to constrain the segmentations to coincide with object boundaries. We show experimentally that training a deep convolutional neural network using the proposed loss function leads to substantially better segmentations than previous state-of-the-art methods on the challenging PASCAL VOC\\xa02012 dataset. We furthermore give insight into the working mechanism of our method by a detailed experimental study that illustrates how the segmentation quality is affected by each term of the proposed loss function as well as their combinations.', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2014.49', '10.1109/WACV.2016.7477688', '10.1007/978-3-319-46478-7_34', '10.1109/TPAMI.2011.231', '10.1109/CVPR.2014.414', '10.1109/ICCV.2015.191', '10.1007/s11263-009-0275-4', '10.1109/ICCV.2011.6126343', '10.1109/CVPR.2016.349', '10.1145/2647868.2654889', '10.5244/C.30.92', '10.1007/978-3-319-46493-0_42', '10.1007/978-3-319-45886-1_31', '10.1007/978-3-642-15567-3_8', '10.1109/CVPR.2015.7298668', '10.1109/ICCV.2015.203', '10.1109/ICCV.2015.209', '10.1109/CVPR.2015.7298780', '10.1109/ICCV.2015.160', '10.1109/ICCV.2007.4408986', '10.1007/s11263-015-0816-y', '10.1109/TIT.1965.1053799', '10.1007/11744023_1', '10.1109/TPAMI.2008.105', '10.1109/CVPR.2007.383098', '10.1109/CVPR.2010.5540060', '10.1109/ICCV.2011.6126299', '10.1109/CVPR.2012.6247757', '10.1016/j.patcog.2016.01.015', '10.1145/2647868.2654910', '10.1109/CVPR.2014.408', '10.1109/CVPR.2015.7299002', '10.1109/CVPR.2013.249', '10.1109/TIP.2014.2344433', '10.1109/CVPR.2015.7298888', '10.1109/CVPR.2016.319'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['AI', 'Machine learning', 'Deep learning', 'Computer vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'PixelCNN models with Auxiliary Variables for Natural Image Modeling': Paper(DOI='10.3390/app131910941', crossref_json=None, google_schorlar_metadata=None, title='PixelCNN models with Auxiliary Variables for Natural Image Modeling', authors=['Alexander Kolesnikov', 'Christoph H. Lampert'], abstract='We study probabilistic models of natural images and extend the autoregressive family of PixelCNN models by incorporating auxiliary variables. Subsequently, we describe two new generative image models that exploit different image transformations as auxiliary variables: a quantized grayscale view of the image or a multi-resolution image pyramid. The proposed models tackle two known shortcomings of existing PixelCNN models: 1) their tendency to focus on low-level image details, while largely ignoring high-level image information, such as object shapes, and 2) their computationally costly procedure for image sampling. We experimentally demonstrate benefits of our models, in particular showing that they produce much more realistically looking image samples than previous state-of-the-art probabilistic models.', conference=None, journal=None, year=None, reference_list=['10.1109/MMM.2018.2802178', '10.1109/ICIP40778.2020.9190908', '10.1016/j.cageo.2022.105263', '10.1145/214762.214771', '10.1007/s11433-018-9354-y', '10.3847/2041-8213/ab8e47', '10.1088/1674-4527/19/9/133', '10.1093/mnras/stu1188'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['AI', 'Machine learning', 'Deep learning', 'Computer vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation': Paper(DOI='10.1049/cje.2020.09.013', crossref_json=None, google_schorlar_metadata=None, title='Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation', authors=['Boqing Gong', 'Kristen Grauman', 'Fei Sha'], abstract='Learning domain-invariant features is of vital importance to unsupervised domain adaptation, where classifiers trained on the source domain need to be adapted to a different target domain for which no labeled examples are available. In this paper, we propose a novel approach for learning such features. The central idea is to exploit the existence of landmarks, which are a subset of labeled data instances in the source domain that are distributed most similarly to the target domain. Our approach automatically discovers the landmarks and use them to bridge the source to the target by constructing provably easier auxiliary domain adaptation tasks. The solutions of those auxiliary tasks form the basis to compose invariant features for the original task. We show how this composition can be optimized discriminatively without requiring labels from the target domain. We validate the method on standard benchmark datasets for visual object recognition and sentiment analysis of text. Empirical results show the proposed method outperforms the state-of-the-art significantly.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision'], conference_acronym='Chinese journal of electronics (Print)', publisher=None, query_handler=None),\n",
       " 'LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop': Paper(DOI='10.3390/app10144913', crossref_json=None, google_schorlar_metadata=None, title='LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop', authors=['Fisher Yu', 'Ari Seff', 'Yinda Zhang', 'Shuran Song', 'Thomas Funkhouser', 'Jianxiong Xiao'], abstract='While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset.', conference=None, journal=None, year=None, reference_list=['10.1177/1745691610393980', '10.1109/ICCVW.2013.77', '10.1007/s11263-020-01316-z', '10.1109/ACCESS.2019.2956775', '10.1007/s11263-015-0816-y', '10.1177/0278364913491297', '10.1109/TPAMI.2018.2856256', '10.1109/ICCV.2017.167', '10.1162/neco.1995.7.1.108', '10.1109/ICCCN.2017.8038465', '10.1109/ICCV.2015.123', '10.1109/CVPR.2016.90', '10.1109/TPAMI.2016.2577031', '10.1007/978-3-319-10602-1_48', '10.1109/CVPR.2016.308'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Robotics', 'Autonomous Driving'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Skipnet: Learning dynamic routing in convolutional networks': Paper(DOI='10.1007/978-3-030-01261-8_25', crossref_json=None, google_schorlar_metadata=None, title='Skipnet: Learning dynamic routing in convolutional networks', authors=['Xin Wang', 'Fisher Yu', 'Zi-Yi Dou', 'Trevor Darrell', 'Joseph E Gonzalez'], abstract='While deeper convolutional networks are needed to achieve maximum accuracy in visual perception tasks, for many inputs shallower networks are sufficient. We exploit this observation by learning to skip convolutional layers on a per-input basis. We introduce SkipNet, a modified residual network, that uses a gating network to selectively skip convolutional blocks based on the activations of the previous layer. We formulate the dynamic skipping problem in the context of sequential decision making and propose a hybrid learning algorithm that combines supervised learning and reinforcement learning to address the challenges of non-differentiable skipping decisions. We show SkipNet reduces computation by 30-90% while preserving the accuracy of the original model on four benchmark datasets and outperforms the state-of-the-art dynamic networks and static compression methods. We also qualitatively evaluate the gating policy to reveal a relationship between image scale and saliency and the number of layers skipped.', conference=None, journal=None, year=None, reference_list=['10.1109/ICASSP.2016.7472621', '10.18653/v1/P17-1045', '10.1109/CVPR.2017.205', '10.1109/CVPR.2017.194', '10.1109/CVPR.2016.90', '10.1007/978-3-319-46493-0_38', '10.1109/ICCV.2017.155', '10.1162/neco.1997.9.8.1735', '10.1007/978-3-319-46493-0_39', '10.1007/s11263-015-0816-y', '10.1109/ICIP.2017.8296851', '10.1109/CVPR.2016.308', '10.1109/ICPR.2016.7900006'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Robotics', 'Autonomous Driving'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'An image is worth 16x16 words: Transformers for image recognition at scale. arXiv 2020': Paper(DOI='10.1016/j.bodyim.2020.03.003', crossref_json=None, google_schorlar_metadata=None, title='An image is worth 16x16 words: Transformers for image recognition at scale. arXiv 2020', authors=['Alexey Dosovitskiy', 'Lucas Beyer', 'Alexander Kolesnikov', 'Dirk Weissenborn', 'Xiaohua Zhai', 'Thomas Unterthiner', 'Mostafa Dehghani', 'Matthias Minderer', 'Georg Heigold', 'Sylvain Gelly', 'Jakob Uszkoreit', 'Neil Houlsby'], abstract=None, conference=None, journal=None, year=None, reference_list=['10.1016/j.bodyim.2015.07.005', '10.1177/1359105314520814', '10.1016/j.bodyim.2016.04.003', '10.1016/j.bodyim.2013.04.004', '10.1016/j.bodyim.2005.06.002', '10.1016/j.bodyim.2016.08.007', '10.1016/j.bodyim.2016.03.005', '10.1002/mar.21001', '10.1177/1461444819826530', '10.1016/j.bodyim.2017.10.002', '10.21810/strm.v8i2.203', '10.1177/1461444818771083', '10.1016/j.bodyim.2014.10.004', '10.1016/j.bodyim.2014.12.002', '10.1007/s11199-017-0796-1', '10.1177/001872675400700202', '10.1016/j.bodyim.2016.03.009', '10.1037/0022-3514.69.2.227', '10.1037/0033-2909.134.3.460', '10.1002/eat.10005', '10.1016/j.bodyim.2013.07.004', '10.1080/08838159709364422', '10.1521/jscp.1995.14.4.325', '10.1016/j.bodyim.2016.02.008', '10.1023/A:1014815725852', '10.1521/jscp.2009.28.1.9', '10.1348/135910704X15257', '10.1089/cyber.2013.0305', '10.3758/BRM.40.3.879', '10.1080/14680777.2014.883420', '10.1016/j.eatbeh.2014.01.001', '10.1111/j.1559-1816.2008.00351.x', '10.1016/j.bodyim.2019.03.001', '10.1016/j.bodyim.2017.06.004', '10.1016/j.bodyim.2006.07.004', '10.1111/j.1467-9450.2010.00836.x', '10.1016/j.bodyim.2014.09.006', '10.1016/j.bodyim.2015.04.001', '10.1016/j.bodyim.2018.02.010', '10.1521/jscp.23.1.23.26991', '10.1111/j.1471-6402.2010.01581.x', '10.1016/j.bodyim.2020.02.015', '10.1016/j.bodyim.2017.04.001', '10.1521/jscp.2009.28.1.73', '10.1016/j.bodyim.2012.08.001', '10.1016/j.bodyim.2015.06.003', '10.1016/j.bodyim.2009.07.008'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Artificial Intelligence', 'Computer Vision', 'Natural Language Processing'], conference_acronym='Body image', publisher=None, query_handler=None),\n",
       " 'Big transfer (bit): General visual representation learning': Paper(DOI='10.1007/978-3-030-58558-7_29', crossref_json=None, google_schorlar_metadata=None, title='Big transfer (bit): General visual representation learning', authors=['Alexander Kolesnikov', 'Lucas Beyer', 'Xiaohua Zhai', 'Joan Puigcerver', 'Jessica Yung', 'Sylvain Gelly', 'Neil Houlsby'], abstract=' Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes—from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2017.195', '10.1109/CVPR.2009.5206848', '10.1109/CVPR42600.2020.00975', '10.1109/ICCV.2019.00502', '10.1109/CVPR.2016.90', '10.1007/978-3-319-46493-0_38', '10.1007/978-3-319-46478-7_5', '10.1109/ICCV.2017.449', '10.1109/ICCV.2017.324', '10.1007/978-3-319-10602-1_48', '10.1007/978-3-030-01216-8_12', '10.1109/ICVGIP.2008.47', '10.1109/TKDE.2009.191', '10.1109/CVPR.2012.6248092', '10.1007/s11263-015-0816-y', '10.1109/ICCV.2017.97', '10.1109/CVPR.2018.00131', '10.1109/CVPR.2015.7298594', '10.1109/CVPR.2016.308', '10.1145/2812802', '10.1109/CVPR42600.2020.01382', '10.1007/978-3-030-01261-8_1', '10.1109/CVPR42600.2020.01070', '10.1109/CVPR.2017.634', '10.1109/ICCV.2019.00612', '10.1109/ICCV.2019.00156', '10.1007/978-1-4899-7687-1_79'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Artificial Intelligence', 'Computer Vision', 'Natural Language Processing'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Adaptive Bayesian quantum tomography': Paper(DOI='10.1103/physreva.85.052120', crossref_json=None, google_schorlar_metadata=None, title='Adaptive Bayesian quantum tomography', authors=['Ferenc Huszár', 'Neil MT Houlsby'], abstract=\"In this paper we revisit the problem of optimal design of quantum tomographic experiments. In contrast to previous approaches where an optimal set of measurements is decided in advance of the experiment, we allow for measurements to be adaptively and efficiently reoptimized depending on data collected so far. We develop an adaptive statistical framework based on Bayesian inference and Shannon's information, and demonstrate a significant reduction in the total number of measurements required as compared to nonadaptive methods, including mutually unbiased bases.\", conference=None, journal=None, year=None, reference_list=['10.1016/0003-4916(89)90322-9', '10.1103/PhysRevLett.105.030406', '10.1103/PhysRevA.83.062303', '10.1103/PhysRevA.82.044102', '10.1103/PhysRevA.81.042109', '10.1038/nrd1927', '10.1162/neco.2009.02-09-959', '10.1103/PhysRevA.61.032306', '10.1103/PhysRevA.65.050303', '10.1088/1367-2630/12/4/043034', '10.1103/PhysRevLett.105.150401', '10.1088/1751-8113/40/35/011', '10.1007/s10463-006-0099-8', '10.1214/009053606000001488'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Deep Learning', 'Artificial Intelligence', 'Computer Vision', 'Variational Methods'], conference_acronym='Physical review, A, Atomic, molecular, and optical physics', publisher=None, query_handler=None),\n",
       " 'Violence detection in video using computer vision techniques': Paper(DOI='10.22214/ijraset.2020.30788', crossref_json=None, google_schorlar_metadata=None, title='Violence detection in video using computer vision techniques', authors=['Enrique Bermejo Nievas', 'Oscar Deniz Suarez', 'Gloria Bueno García', 'Rahul Sukthankar'], abstract=' Whereas the action recognition community has focused mostly on detecting simple actions like clapping, walking or jogging, the detection of fights or in general aggressive behaviors has been comparatively less studied. Such capability may be extremely useful in some video surveillance scenarios like in prisons, psychiatric or elderly centers or even in camera phones. After an analysis of previous approaches we test the well-known Bag-of-Words framework used for action recognition in the specific problem of fight detection, along with two of the best action descriptors currently available: STIP and MoSIFT. For the purpose of evaluation and to foster research on violence detection in video we introduce a new video database containing 1000 sequences divided in two groups: fights and non-fights. Experiments on this database and another one with fights from action movies show that fights can be detected\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'An integer projected fixed point method for graph matching and map inference': Paper(DOI='10.1007/978-981-15-4409-5_86', crossref_json=None, google_schorlar_metadata=None, title='An integer projected fixed point method for graph matching and map inference', authors=['Marius Leordeanu', 'Martial Hebert', 'Rahul Sukthankar'], abstract='Graph matching and MAP inference are essential problems in computer vision and machine learning. We introduce a novel algorithm that can accommodate both problems and solve them efficiently. Recent graph matching algorithms are based on a general quadratic programming formulation, that takes in consideration both unary and second-order terms reflecting the similarities in local appearance as well as in the pairwise geometric relationships between the matched features. In this case the problem is NP-hard and a lot of effort has been spent in finding efficiently approximate solutions by relaxing the constraints of the original problem. Most algorithms find optimal continuous solutions of the modified problem, ignoring during the optimization the original discrete constraints. The continuous solution is quickly binarized at the end, but very little attention is put into this final discretization step. In this paper we argue that the stage in which a discrete solution is found is crucial for good performance. We propose an efficient algorithm, with climbing and convergence properties, that optimizes in the discrete domain the quadratic score, and it gives excellent results either by itself or by starting from the solution returned by any graph matching algorithm. In practice it outperforms state-or-the art algorithms and it also significantly improves their performance if used in combination. When applied to MAP inference, the algorithm is a parallel extension of Iterated Conditional Modes (ICM) with climbing and convergence properties that make it a compelling alternative to the sequential ICM. In our experiments on MAP inference our algorithm proved its\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2011.5995387', '10.1016/j.imavis.2004.02.006', '10.1007/978-3-642-15555-0_36'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning'], conference_acronym='Advances in intelligent systems and computing (Print)', publisher=None, query_handler=None),\n",
       " 'Surface shape and curvature scales': Paper(DOI='10.1016/0262-8856(92)90076-f', crossref_json=None, google_schorlar_metadata=None, title='Surface shape and curvature scales', authors=['Jan J Koenderink', 'Andrea J Van Doorn'], abstract='The classical surface curvature measures, such as the Gaussian and the mean curvature at a point of a surface, are not very indicative of local shape. The two principal curvatures (taken as a pair) are more informative, but one would prefer a single shape indicator rather than a pair of numbers. Moreover, the shape indicator should preferably be independent of the size i.e. the amount of curvature, as distinct from the type of curvature. We propose two novel measures of local shape, the ‘curvedness’ and the ‘shape index’. The curvedness is a positive number that specifies the amount of curvature, whereas the shape index is a number in the range [−1, +1] and is scale invariant. The shape index captures the intuitive notion of ‘local shape’ particularly well. The shape index can be mapped upon an intuitively natural colour scale. Two complementary shapes (like stamp and mould) map to complementary hues. The\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1080/713819112', '10.2307/1416110', '10.2307/1416741'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['human vision', 'consciousness', 'computer vision', 'experimental psychology', 'differential geometry'], conference_acronym='Image and vision computing', publisher=None, query_handler=None),\n",
       " 'Representation of local geometry in the visual system': Paper(DOI='10.1007/bf00318371', crossref_json=None, google_schorlar_metadata=None, title='Representation of local geometry in the visual system', authors=['Jan J Koenderink', 'Andrea J van Doorn'], abstract=' It is shown that a convolution with certain reasonable receptive field (RF) profiles yields the exact partial derivatives of the retinal illuminance blurred to a specified degree. Arbitrary concatenations of such RF profiles yield again similar ones of higher order and for a greater degree of blurring. By replacing the illuminance with its third order jet extension we obtain position dependent geometries. It is shown how such a representation can function as the substrate for “point processors” computing geometrical features such as edge curvature. We obtain a clear dichotomy between local and multilocal visual routines. The terms of the truncated Taylor series representing the jets are partial derivatives whose corresponding RF profiles closely mimic the well known units in the primary visual cortex. Hence this description provides a novel means to understand and classify these units. Taking the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-1-4612-5154-5', '10.1113/jphysiol.1968.sp008574', '10.1007/BF00319979', '10.1098/rspb.1977.0085', '10.1007/BF00336961', '10.1007/BF00318204'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['human vision', 'consciousness', 'computer vision', 'experimental psychology', 'differential geometry'], conference_acronym='Biological cybernetics', publisher=None, query_handler=None),\n",
       " 'The internal representation of solid shape with respect to vision': Paper(DOI='10.1007/bf00337644', crossref_json=None, google_schorlar_metadata=None, title='The internal representation of solid shape with respect to vision', authors=['Jan J Koenderink', 'Andrea J Van Doorn'], abstract=' It is argued that the internal model of any object must take the form of a function, such that for any intended action the resulting reafference is predictable. This function can be derived explicitly for the case of visual perception of rigid bodies by ambulant observers. The function depends on physical causation, not physiology; consequently, one can make a priori statements about possible internal models. A posteriori it seems likely that the orientation sensitive units described by Hubel and Wiesel constitute a physiological substrate subserving the extraction of the invariants of this function. The function is used to define a measure for the visual complexity of solid shape. Relations with Gestalt theories of perception are discussed.', conference=None, journal=None, year=None, reference_list=['10.1037/h0054663', '10.1152/jn.1965.28.2.229', '10.1068/p020429', '10.1080/713819112', '10.1364/JOSA.66.000717', '10.1007/BF00365595', '10.1007/BF02409853', '10.1126/science.171.3972.701', '10.1037/h0062156'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['human vision', 'consciousness', 'computer vision', 'experimental psychology', 'differential geometry'], conference_acronym='Biological cybernetics', publisher=None, query_handler=None),\n",
       " 'What does the occluding contour tell us about solid shape?': Paper(DOI='10.1068/p130321', crossref_json=None, google_schorlar_metadata=None, title='What does the occluding contour tell us about solid shape?', authors=['Jan J Koenderink'], abstract='A new theorem is discussed that relates the apparent curvature of the occluding contour of a visual shape to the intrinsic curvature of the surface and the radial curvature. This theorem allows the formulation of general laws for the apparent curvature, independent of viewing distance and regardless of the fact that the rim (the boundary between the visible and invisible parts of the object) is a general, thus twisted, space curve. Consequently convexities, concavities, or inflextions of contours in the retinal image allow the observer to draw inferences about local surface geometry with certainty. These results appear to be counterintuitive, witness to the treatment of the problem by recent authors. It is demonstrated how well-known examples, used to show how concavities and convexities of the contour have no obvious relation to solid shape, are actually good illustrations of the fact that convexities are due to local ovoid\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1515/9783112392348', '10.1007/978-3-662-36685-1', '10.1007/BF00365595', '10.1007/BF00337644', '10.1080/713820338', '10.1068/p110129'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['human vision', 'consciousness', 'computer vision', 'experimental psychology', 'differential geometry'], conference_acronym='Perception (London. Print)', publisher=None, query_handler=None),\n",
       " 'Scale and the differential structure of images': Paper(DOI='10.1007/978-1-4020-8840-7_6', crossref_json=None, google_schorlar_metadata=None, title='Scale and the differential structure of images', authors=['Luc MJ Florack', 'Bart M ter Haar Romeny', 'Jan J Koenderink', 'Max A Viergever'], abstract='Why and how one should study a scale-space is prescribed by the universal physical law of scale invariance, expressed by the so-called Pi-theorem. The fact that any image is a physical observable with an inner and outer scale bound, necessarily gives rise to a ‘scale-space representation’, in which a given image is represented by a one-dimensional family of images representing that image on various levels of inner spatial scale. An early vision system is completely ignorant of the geometry of its input. Its primary task is to establish this geometry at any available scale. The absence of geometrical knowledge poses additional constraints on the construction of a scale-space, notably linearity, spatial shift invariance and isotropy, thereby defining a complete hierarchical family of scaled partial differential operators: the Gaussian kernel (the lowest order, resettling operator) and its linear partial derivatives. They enable\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'image analysis', 'medical image analysis', 'brain-inspired computing', 'deep learning'], conference_acronym='Computational imaging and vision', publisher=None, query_handler=None),\n",
       " 'Invariant properties of the motion parallax field due to the movement of rigid bodies relative to an observer': Paper(DOI='10.1080/713819112', crossref_json=None, google_schorlar_metadata=None, title='Invariant properties of the motion parallax field due to the movement of rigid bodies relative to an observer', authors=['Jan J Koenderink', 'Andrea J Van Doorn'], abstract='It is shown that the time-change of the natural perspective, due to the movement of rigid bodies relative to an observer, conveys information about geometrical properties of the moving surfaces. Certain differential properties of the dynamic perspective are shown to be conserved. These properties are related to determinants of the shape of the surface elements of the moving bodies, as for instance the sign of the gaussian curvature, the asymptotic curves and the future contours. A kinematic analysis of the motion parallax field shows that the type of parallax field is related to the slant of the surface relative to the observer. The theory is of possible significance to visual kinesthesis.© 1975 Taylor and Francis Group, LLC.', conference=None, journal=None, year=None, reference_list=['10.2307/1419017', '10.1364/JOSA.55.001296', '10.1016/0022-2496(66)90005-8'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['human vision', 'consciousness', 'computer vision', 'experimental psychology', 'differential geometry'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Facts on optic flow': Paper(DOI='10.1007/bf00365219', crossref_json=None, google_schorlar_metadata=None, title='Facts on optic flow', authors=['Jan J Koenderink', 'Andrea J van Doorn'], abstract=' We employ an optimal solution to both the “shape from motion problem” and the related problem of the estimation of self-movement on a purely optical basis to deduce practical rules of thumb for the limits of the optic flow information content in the presence of perturbation of the motion parallax field. The results are illustrated and verified by means of a computer simulation. The results allow estimates of the accuracy of depth and egomotion estimates as a function of the accuracy of data sampling and the width of field of view, as well as estimates of the interaction between rotational and translational components of the movement.', conference=None, journal=None, year=None, reference_list=['10.1016/S0734-189X(83)80026-7', '10.1364/JOSA.66.000717', '10.1364/JOSA.71.000953', '10.1016/0042-6989(86)90078-7', '10.1098/rspb.1980.0057', '10.1016/0262-8856(86)90006-5', '10.1016/0042-6989(85)90171-3', '10.1098/rspb.1979.0006'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['human vision', 'consciousness', 'computer vision', 'experimental psychology', 'differential geometry'], conference_acronym='Biological cybernetics', publisher=None, query_handler=None),\n",
       " 'Local structure of movement parallax of the plane': Paper(DOI='10.1364/josa.66.000717', crossref_json=None, google_schorlar_metadata=None, title='Local structure of movement parallax of the plane', authors=['Jan J Koenderink', 'Andrea J van Doorn'], abstract='The movement parallax field due to the translation of an observer relative to a plane surface is studied in an infinitesimal neighborhood of a visual direction. The parallax field is decomposed into elementary transformations: a translation, a rigid rotation, a similarity, and a deformation. A topologically invariant classification based on critical-point analysis is also obtained. It is shown that the field is either that of a node or that of a saddle point. Numerical results for a general case are offered as illustration. We discuss the relevance of the local, as opposed to the global structure of the parallax field for visual perception and the visual space sense.', conference=None, journal=None, year=None, reference_list=['10.1364/JOSA.55.001296', '10.1068/p030063', '10.2307/1418521', '10.1038/225094a0', '10.1152/jn.1965.28.2.229'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['human vision', 'consciousness', 'computer vision', 'experimental psychology', 'differential geometry'], conference_acronym='Journal of the Optical Society of America (1930)', publisher=None, query_handler=None),\n",
       " 'The singularities of the visual mapping': Paper(DOI='10.1007/bf00365595', crossref_json=None, google_schorlar_metadata=None, title='The singularities of the visual mapping', authors=['Jan J Koenderink', 'Andrea J Van Doorn'], abstract=' In this article we treat purely metrical properties of the visual image, e.g. the time changes of the relative positions and orientations of image details. Self-induced movements of an observer relative to rigid bodies in his environment generate charactertistic motion parallax fields. The observer may regard those fields as proprioceptive and interprete the geometrical invariants of the fields as indicators of solid shape. In this way his perceptions become object-oriented, which is the normal case as the many constancy-phenomena show. Similar arguments apply to the disparity field of binocular vision. In this paper we treat the qualitative nature of such fields. [In this case the qualitative nature is basic. Compare the case of an equation with a single unknown. Often one is interested primarily in the qualitative solution (are there roots? How many?), and only slightly in the quantitative information (the numerical value\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1037/h0055232', '10.1007/978-1-4615-7904-5', '10.1007/BF00622503', '10.1068/p030313', '10.1068/p040391', '10.1080/713819112', '10.1364/JOSA.66.000717', '10.1007/BF00307859', '10.1007/BF00326670', '10.1364/JOSA.50.000838', '10.1068/p030063', '10.2307/1970070'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['human vision', 'consciousness', 'computer vision', 'experimental psychology', 'differential geometry'], conference_acronym='Biological cybernetics', publisher=None, query_handler=None),\n",
       " 'Photometric invariants related to solid shape': Paper(DOI='10.1080/713820338', crossref_json=None, google_schorlar_metadata=None, title='Photometric invariants related to solid shape', authors=['Jan J Koenderink', 'Andrea J van Doorn'], abstract='An attempt is made to identify the structural invariants of the field of isophotes on lambertian surfaces of arbitrary shape under arbitrary illumination. It is a priori clear that such invariants play an important part in the perception of solid shape by means of chiaroscuro. We have found several such invariants. The topological structure of the field of isophotes is determined by the collection of nested, closed parabolic curves on the surface. An important class of stationary points of the field clings to these parabolic lines. The isophotes cut the parabolic lines at a constant angle, irrespective of the location of the light sources. For certain canonical surface undulations the field of isophotes is explicitly given.', conference=None, journal=None, year=None, reference_list=['10.1002/sapm193918151', '10.2307/1970070', '10.1364/JOSA.50.000838', '10.1007/978-3-662-36685-1', '10.2307/1419017'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['human vision', 'consciousness', 'computer vision', 'experimental psychology', 'differential geometry'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Spatiotemporal modulation transfer in the human eye': Paper(DOI='10.1364/josa.57.001082', crossref_json=None, google_schorlar_metadata=None, title='Spatiotemporal modulation transfer in the human eye', authors=['FL Van Nes', 'JJ Koenderink', 'H Nas', 'MA Bouman'], abstract='The contrast sensitivity of the human eye for sinusoidal illuminance changes in space and time, obtained by means of traveling-wave stimuli, was measured as a function of spatial and temporal frequency for white light. The average retinal illuminance was varied between 0.85 and 850 trolands. The threshold modulation for perception of a moving grating is generally higher than that for detection of brightness changes, in space and/or time, that give rise to flicker phenomena. Flicker-fusion characteristics, as determined from the thresholds for the flicker phenomenon, are found to lose their band-pass-filter resemblance for spatial frequencies of more than 5 cycles per degree of visual angle. The thresholds at flicker fusion for spatial- and temporal-frequency combinations in which not both frequencies are very low, appear to be proportional to the inverse of the square root of mean retinal illuminance, in the investigated\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1364/JOSA.50.001115', '10.1364/JOSA.46.000721', '10.1364/JOSA.52.000328', '10.1364/JOSA.57.000401', '10.1364/JOSA.48.000777', '10.1111/j.1749-6632.1961.tb20181.x', '10.1364/JOSA.55.001086', '10.1364/JOSA.56.001141', '10.1364/JOSA.56.001628', '10.1016/S0031-8914(43)90575-0', '10.1364/JOSA.38.000196', '10.1364/JOSA.4.000035', '10.1098/rspb.1936.0072', '10.1364/JOSA.53.000121'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['human vision', 'consciousness', 'computer vision', 'experimental psychology', 'differential geometry'], conference_acronym='Journal of the Optical Society of America (1930)', publisher=None, query_handler=None),\n",
       " 'Generic neighborhood operators': Paper(DOI='10.1109/34.141551', crossref_json=None, google_schorlar_metadata=None, title='Generic neighborhood operators', authors=['Jan J.  Koenderink', 'Andrea J van Doorn'], abstract=\"A method that treats linear neighborhood operators within a unified framework that enables linear combinations, concatenations, resolution changes, or rotations of operators to be treated in a canonical manner is presented. Various families of operators with special kinds of symmetries (such as translation, rotation, magnification) are explicitly constructed in 1-D, 2-D, and 3-D. A concept of'order'is defined, and finite orthonormal bases of functions closely connected with the operators of various orders are constructed. Linear transformations between the various representations are considered. The method is based on two fundamental assumptions: a decrease of resolution should not introduce spurious detail, and the local operators should be self-similar under changes of resolution. These assumptions merely sum up the even more general need for homogeneity isotropy, scale invariance, and separability of\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1152/jn.1987.58.6.1233', '10.1007/BF00337144', '10.1007/BF00336961', '10.1007/BF00318371', '10.1007/BF00364136', '10.1364/JOSAA.5.001136', '10.1109/34.42861', '10.1364/JOSAA.4.002401', '10.1007/BF00318205', '10.1016/0169-2607(86)90095-7', '10.1016/0167-8655(85)90053-4', '10.1016/S0734-189X(87)80153-6', '10.1016/0042-6989(89)90008-4', '10.1109/TPAMI.1986.4767749', '10.1152/jn.1987.58.6.1187', '10.1364/JOSAA.2.000683'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['human vision', 'consciousness', 'computer vision', 'experimental psychology', 'differential geometry'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'The shape of smooth objects and the way contours end': Paper(DOI='10.1068/p110129', crossref_json=None, google_schorlar_metadata=None, title='The shape of smooth objects and the way contours end', authors=['Jan J Koenderink', 'Andrea J Van Doorn'], abstract='It is shown, by elementary mathematical reasoning, that visual contours can only be concave at their endpoints. This simple natural fact is contrasted with general mannerisms of draftsmen: in the great majority of cases contours are drawn to have convex ends. It is argued that this results from our general concept of solid shapes: a general shape is conceived of as a conglomerate of convex (‘ovoid’) elementary shapes, these shapes act as ‘figure’ and the way they are glued together is treated as the—relatively unimportant—‘ground’. The hypothesis is supported through citations from academic-art literature. An attempt is made to give a geometrical specification of just what draftsmen draw if they disregard the contour.', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-662-36685-1', '10.1080/713820338'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['human vision', 'consciousness', 'computer vision', 'experimental psychology', 'differential geometry'], conference_acronym='Perception (London. Print)', publisher=None, query_handler=None),\n",
       " 'Receptive field families': Paper(DOI='10.1007/bf00203452', crossref_json=None, google_schorlar_metadata=None, title='Receptive field families', authors=['Jan J Koenderink', 'AJ Van Doorn'], abstract=' It is generally agreed upon that size invariance and the absence of spurious resolution are two requirements that characterize well behaved spatial samping in visual systems. We show that these properties taken together constrain the structure of receptive fields to a very large degree. Only those field structures that arise as solutions of a certain linear partial differential equation of the second order prove to satisfy these general constraints. The equation admits of complete, orthonormal families of solutions. These families have to be regarded as the possible receptive field families subject to certain symmetry conditions. They can be transformed into each other via unitary transformations. Thus a single representation suffices to construct all the others. This theory permits us to classify the possible linear receptive field structures exhaustively, and to define their internal and external interrelations. This induces a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/0042-6989(89)90008-4', '10.1007/BF00318205', '10.1016/0167-8655(85)90053-4', '10.1016/S0734-189X(87)80153-6', '10.1152/jn.1987.58.6.1187', '10.1152/jn.1987.58.6.1233', '10.1007/BF00337144', '10.1007/BF00336961', '10.1007/BF00318371', '10.1007/BF00364136', '10.1364/JOSAA.5.001136', '10.1109/34.42861', '10.1117/12.969731', '10.1364/JOSAA.4.002401', '10.1364/JOSAA.2.000683'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['human vision', 'consciousness', 'computer vision', 'experimental psychology', 'differential geometry'], conference_acronym='Biological cybernetics', publisher=None, query_handler=None),\n",
       " 'Dynamic programming and graph algorithms in computer vision': Paper(DOI='10.1109/tpami.2010.135', crossref_json=None, google_schorlar_metadata=None, title='Dynamic programming and graph algorithms in computer vision', authors=['Pedro F Felzenszwalb', 'Ramin Zabih'], abstract='Optimization is a powerful paradigm for expressing and solving problems in a wide range of areas, and has been successfully applied to many vision problems. Discrete optimization techniques are especially interesting since, by carefully exploiting problem structure, they often provide nontrivial guarantees concerning solution quality. In this paper, we review dynamic programming and graph algorithms, and discuss representative examples of how these discrete optimization techniques have been applied to some classical vision problems. We focus on the low-level vision problem of stereo, the mid-level problem of interactive object segmentation, and the high-level problem of model-based recognition.', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2005.14', '10.1109/TPAMI.2006.193', '10.1109/TPAMI.2007.1031', '10.1109/ICCV.2005.81', '10.1109/TPAMI.2004.1262177', '10.1016/0020-0190(77)90002-3', '10.1109/TPAMI.2008.217', '10.1145/585265.585268', '10.1109/ICCV.2003.1238389', '10.1145/990308.990313', '10.1016/S0004-3702(83)80006-X', '10.1007/BF00133570', '10.1109/CVPR.1998.698598', '10.1109/TPAMI.2003.1233908', '10.1109/34.954599', '10.1016/0004-3702(81)90024-2', '10.1109/CVPR.2009.5206689', '10.1109/34.977562', '10.1117/12.280874', '10.1109/5.18626', '10.1145/1015706.1015720', '10.1109/CVPR.2005.130', '10.1109/34.598226', '10.1038/317314a0', '10.1109/TPAMI.2004.54', '10.1109/CVPR.2005.362', '10.1016/0020-0190(90)90109-B', '10.1109/TPAMI.2004.1273918', '10.1063/1.1699114', '10.1145/362588.362594', '10.1145/218380.218442', '10.1109/TPAMI.1985.4767639', '10.1145/1201775.882264', '10.1109/TPAMI.2009.143', '10.1109/CVPR.1998.698673', '10.1109/ICCV.1999.791245', '10.1007/s11263-006-7934-5', '10.1109/ICCV.2001.937505', '10.1109/TPAMI.2004.60', '10.1016/0166-218X(95)00103-X', '10.1109/TPAMI.2003.1217603', '10.1109/34.969114', '10.1145/502090.502096', '10.1109/ICCV.1998.710763', '10.1109/CVPR.2008.4587444', '10.1023/A:1014573219977', '10.1109/CVPR.2005.334', '10.1109/TPAMI.2003.1159951', '10.1109/TPAMI.2007.70844', '10.1109/34.868688', '10.1017/CBO9780511804441', '10.1016/S0166-218X(01)00341-9', '10.7551/mitpress/7132.001.0001', '10.1109/ICCV.1999.791261', '10.1109/T-C.1973.223602', '10.1109/34.368194', '10.1109/34.476006', '10.1109/TPAMI.1984.4767596', '10.1109/TIT.2005.856938', '10.1007/BF02612354', '10.1145/335305.335397', '10.1109/TPAMI.2009.131', '10.21236/AD0786720', '10.1002/net.3230150206', '10.1137/S0097539792225297', '10.1006/cviu.2000.0842', '10.1162/089976602760128072', '10.1109/ICCV.2001.937685', '10.1145/1015706.1015718', '10.1016/S0166-218X(01)00338-9', '10.1109/34.57681', '10.1109/34.485529', '10.1007/BF01386390', '10.1007/BF00054836', '10.1016/S0042-6989(98)00043-1', '10.1109/34.993558', '10.1002/jnm.433', '10.1023/B:VISI.0000042934.15159.49', '10.1109/TPAMI.2005.35'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Medical Imaging'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Digital image processing algorithms and applications': Paper(DOI='10.1016/1049-9660(92)90091-g', crossref_json=None, google_schorlar_metadata=None, title='Digital image processing algorithms and applications', authors=['Ioannis Pitas'], abstract='A unique collection of algorithms and lab experiments for practitioners and researchers of digital image processing technology With the field of digital image processing rapidly expanding, there is a growing need for a book that would go beyond theory and techniques to address the underlying algorithms. Digital Image Processing Algorithms and Applications fills the gap in the field, providing scientists and engineers with a complete library of algorithms for digital image processing, coding, and analysis. Digital image transform algorithms, edge detection algorithms, and image segmentation algorithms are carefully gleaned from the literature for compatibility and a track record of acceptance in the scientific community. The author guides readers through all facets of the technology, supplementing the discussion with detailed lab exercises in EIKONA, his own digital image processing software, as well as useful PDF transparencies. He covers in depth filtering and enhancement, transforms, compression, edge detection, region segmentation, and shape analysis, explaining at every step the relevant theory, algorithm structure, and its use for problem solving in various applications. The availability of the lab exercises and the source code (all algorithms are presented in C-code) over the Internet makes the book an invaluable self-study guide. It also lets interested readers develop digital image processing applications on ordinary desktop computers as well as on Unix machines.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'machine learning', 'autonomous systems', 'digital media'], conference_acronym='CVGIP. Image understanding (Print)', publisher=None, query_handler=None),\n",
       " 'Robust audio watermarking in the time domain': Paper(DOI='10.1109/6046.923822', crossref_json=None, google_schorlar_metadata=None, title='Robust audio watermarking in the time domain', authors=['Paraskevi Bassia', 'Ioannis Pitas', 'Nikos Nikolaidis'], abstract='The audio watermarking method proposed in this paper offers copyright protection to an audio signal by time domain processing. The strength of audio signal modifications is limited by the necessity to produce an output signal that is perceptually similar to the original one. The watermarking method presented here does not require the use of the original signal for watermark detection. The watermark signal is generated using a key, i.e., a single number known only to the copyright owner. Watermark embedding depends on the audio signal amplitude and frequency in a way that minimizes the audibility of the watermark signal. The embedded watermark is robust to common audio signal manipulations like MPEG audio coding, cropping, time shifting, filtering, resampling, and requantization.', conference=None, journal=None, year=None, reference_list=['10.1007/3-540-61996-8_48', '10.1147/sj.353.0313', '10.1109/MMCS.1999.779114', '10.1109/49.668979', '10.1109/35.256878', '10.1109/5.241504', '10.1103/RevModPhys.12.47', '10.1109/ICIP.1998.723411', '10.1109/5.687830', '10.1109/5.771066', '10.1109/MMCS.1996.535015', '10.1109/83.650120', '10.1109/5.771072', '10.1109/38.736465'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'machine learning', 'autonomous systems', 'digital media'], conference_acronym='IEEE transactions on multimedia', publisher=None, query_handler=None),\n",
       " 'Facial expression recognition in image sequences using geometric deformation features and support vector machines': Paper(DOI='10.1109/tip.2006.884954', crossref_json=None, google_schorlar_metadata=None, title='Facial expression recognition in image sequences using geometric deformation features and support vector machines', authors=['Irene Kotsia', 'Ioannis Pitas'], abstract='In this paper, two novel methods for facial expression recognition in facial image sequences are presented. The user has to manually place some of Candide grid nodes to face landmarks depicted at the first frame of the image sequence under examination. The grid-tracking and deformation system used, based on deformable models, tracks the grid in consecutive video frames over time, as the facial expression evolves, until the frame that corresponds to the greatest facial expression intensity. The geometrical displacement of certain selected Candide nodes, defined as the difference of the node coordinates between the first and the greatest facial expression intensity frame, is used as an input to a novel multiclass Support Vector Machine (SVM) system of classifiers that are used to recognize either the six basic facial expressions or a set of chosen Facial Action Units (FAUs). The results on the Cohn-Kanade\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/958468.958479', '10.1109/34.683777', '10.1109/TSP.2004.831018', '10.1109/72.788645', '10.1109/34.935847', '10.1117/12.402450', '10.1109/AFGR.2002.1004168', '10.1109/ICCV.2005.225', '10.1016/S0921-8890(99)00103-7', '10.1109/AFGR.1998.670980', '10.1109/AFGR.2002.1004159', '10.1109/AFGR.2000.840611', '10.1109/TSMCA.2003.817057', '10.1109/ICCV.1995.466916', '10.1016/j.image.2004.05.009', '10.1109/34.598232', '10.1109/34.908962', '10.1109/34.799905', '10.1109/ICASSP.1989.266481', '10.1109/ICIP.2005.1530305', '10.1109/34.817413', '10.1109/34.598235', '10.1109/TSMCB.2005.846658', '10.1109/AFGR.1998.670990', '10.1016/S0893-6080(03)00115-1', '10.1109/72.536309', '10.1109/TSMCB.2004.825930', '10.1016/S0923-5965(02)00076-0', '10.1016/S0167-8655(02)00371-9', '10.1109/34.895976', '10.1016/S0262-8856(00)00034-2', '10.1109/TPAMI.2005.93', '10.1016/S0031-3203(02)00052-3', '10.1016/S1077-3142(03)00081-X', '10.1109/AFGR.1998.670949', '10.1109/ICPR.1994.576879', '10.1023/A:1009715923555', '10.1109/72.991427', '10.1109/72.914517', '10.1109/72.788641'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'machine learning', 'autonomous systems', 'digital media'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Robust image watermarking in the spatial domain': Paper(DOI='10.7763/ijet.2009.v1.47', crossref_json=None, google_schorlar_metadata=None, title='Robust image watermarking in the spatial domain', authors=['Nikos Nikolaidis', 'Ioannis Pitas'], abstract='The rapid evolution of digital image manipulation and transmission techniques has created a pressing need for the protection of the intellectual property rights on images. A copyright protection method that is based on hiding an ‘invisible’ signal, known as digital watermark, in the image is presented in this paper. Watermark casting is performed in the spatial domain by slightly modifying the intensity of randomly selected image pixels. Watermark detection does not require the existence of the original image and is carried out by comparing the mean intensity value of the marked pixels against that of the pixels not marked. Statistical hypothesis testing is used for this purpose. Pixel modifications can be done in such a way that the watermark is resistant to JPEG compression and lowpass filtering. This is achieved by minimizing the energy content of the watermark signal at higher frequencies while taking into account\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'machine learning', 'autonomous systems', 'digital media'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Order statistics in digital image processing': Paper(DOI='10.1109/5.192071', crossref_json=None, google_schorlar_metadata=None, title='Order statistics in digital image processing', authors=['Ioannis Pitas', 'Anastasios N Venetsanopoulos'], abstract='A family of nonlinear filters based on order statistics is presented. A mathematical tool derived through robust estimation theory, order statistics has allowed engineers to develop nonlinear filters with excellent robustness properties. These filters are well suited to digital image processing because they preserve the edges and the fine details of an image much better than conventional linear filters. The probabilistic and deterministic properties of the best known and most widely used filter in this family, the median filter, are discussed. In addition, the authors consider filters that, while not based on order statistics, are related to them through robust estimation theory. A table that ranks nonlinear filters under a variety of performance criteria is included. Most of the topics treated are very active research areas, and the applications are varied, including HDTV, multichannel signal processing of geophysical and ECG/EEG data\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1002/9780470316641', '10.1016/S0146-664X(81)80018-4', '10.1109/TPAMI.1980.4766994', '10.1016/0734-189X(84)90106-3', '10.1109/31.1700', '10.1109/TPAMI.1986.4767819', '10.1109/TCS.1987.1086226', '10.1016/0734-189X(86)90184-2', '10.1109/76.97975', '10.1109/TASSP.1981.1163708', '10.1109/TASSP.1981.1163659', '10.1016/0146-664X(81)90057-5', '10.1109/TCOM.1984.1096099', '10.1109/TASSP.1975.1162749', '10.1109/TASSP.1987.1165153', '10.1109/TPAMI.1987.4767894', '10.1109/29.1660', '10.1109/ICSYSE.1989.48624', '10.1109/83.199924', '10.1109/29.106860', '10.1080/01621459.1972.10482402', '10.1109/TCS.1987.1086066', '10.1109/ASSP.1989.28066', '10.1109/29.31305', '10.1109/TCOM.1981.1095142', '10.1109/31.20212', '10.1016/0031-3203(86)90011-7', '10.1109/TASSP.1985.1164676', '10.1109/TIT.1984.1056875', '10.1049/ip-f-1.1982.0043', '10.1111/j.1467-9892.1982.tb00341.x', '10.1109/TCOM.1977.1093931', '10.1109/TIT.1977.1055790', '10.1109/TCS.1986.1085829', '10.1109/TASSP.1984.1164468', '10.1109/TIT.1986.1057162', '10.1117/12.55618', '10.1364/JOSAA.5.000019', '10.1016/0165-1684(86)90047-2', '10.1109/29.21708', '10.1109/TASSP.1984.1164363', '10.1109/5.54806', '10.1109/TASSP.1984.1164364', '10.1109/31.1801', '10.1109/ICASSP.1991.150913', '10.1109/29.56055', '10.1109/29.56056', '10.1109/29.45552', '10.1109/29.52706', '10.1109/29.1653', '10.1109/29.1489', '10.1109/TASSP.1986.1164871', '10.1109/TCS.1987.1086150', '10.1109/TASSP.1987.1165269', '10.2307/2285954', '10.1080/01621459.1969.10500997', '10.1214/aoms/1177731709', '10.2307/2344839', '10.1109/5.54807', '10.1109/ICASSP.1989.266552', '10.1109/ISCAS.1988.15274', '10.1007/978-1-4757-6017-0', '10.1002/0471725250', '10.1109/78.324725', '10.1109/TCOM.1983.1095885', '10.1007/978-1-4757-2769-2', '10.1109/76.97987', '10.1109/78.80781', '10.1109/TASSP.1985.1164543', '10.1109/78.134410', '10.1109/TCS.1986.1085911', '10.1109/ICASSP.1989.266679', '10.1109/TASSP.1982.1163980', '10.1109/31.16577', '10.1109/31.83870', '10.1109/78.80823', '10.1109/29.17503', '10.1109/78.80969', '10.1109/TASSP.1987.1165036', '10.1109/29.1591', '10.1109/ICASSP.1991.151013', '10.1109/29.57587', '10.1109/29.61536', '10.1109/TASSP.1982.1163951', '10.1109/TPAMI.1981.4767047', '10.1109/TASSP.1983.1164220', '10.1109/18.9767', '10.1109/31.7607', '10.1109/TASSP.1987.1165026', '10.1109/ICASSP.1984.1172478', '10.1145/358198.358222', '10.1109/MELCON.1991.161856', '10.1109/ICASSP.1991.150604', '10.1109/ICASSP.1988.196883', '10.1109/ICASSP.1990.115647', '10.1109/ICASSP.1990.115647', '10.1016/0165-1684(92)90119-H', '10.1007/978-1-4612-4978-8', '10.1016/0734-189X(86)90029-0', '10.1109/TCS.1987.1086165', '10.1109/78.80845', '10.1109/ISCAS.1988.15273', '10.1109/31.90399', '10.1016/0734-189X(86)90002-2', '10.1109/TPAMI.1987.4767941', '10.1109/29.1600', '10.1109/31.41299', '10.1109/TPAMI.1987.4767873', '10.1109/31.101277', '10.1109/TASSP.1987.1165198', '10.1109/TCS.1985.1085740', '10.1109/29.31294', '10.1109/29.61537', '10.1016/0146-664X(82)90105-8', '10.1109/29.60103', '10.1109/ICASSP.1989.266644', '10.1109/5.54808', '10.1117/12.7974127', '10.1109/ICASSP.1990.115594', '10.1109/ICASSP.1991.150763', '10.1016/0165-1684(91)90117-2', '10.1109/TASSP.1983.1164203', '10.1109/ICASSP.1990.115603', '10.1109/ICASSP.1991.150838', '10.1007/978-94-011-6991-2', '10.1109/29.45627', '10.1109/ISCAS.1988.15205', '10.1109/31.1852', '10.1109/29.31294', '10.1109/TASSP.1979.1163188', '10.1109/TCS.1987.1086059', '10.1109/31.62422', '10.1109/31.90400', '10.1117/12.968939', '10.1109/ICASSP.1988.196940', '10.1109/TASSP.1980.1163426', '10.1016/S0146-664X(81)80010-X', '10.1109/78.80784', '10.1016/0165-1684(88)90082-5', '10.1109/78.136561', '10.1109/29.31291', '10.1109/ISCAS.1988.15216', '10.1109/29.17561', '10.1109/ICASSP.1989.266753', '10.1109/ICASSP.1985.1168348', '10.1109/29.31280', '10.1109/29.31280', '10.1109/29.31291', '10.1117/12.932869', '10.1109/TSMC.1986.4308985', '10.1109/TASSP.1986.1164980', '10.1109/31.17580', '10.1142/S0218001488000194', '10.1109/ICASSP.1991.150918', '10.1109/TCOM.1987.1096834', '10.1117/12.7977031', '10.1109/TASSP.1984.1164361', '10.1109/29.1580', '10.1109/PROC.1968.6570', '10.1016/0165-1684(93)90075-L', '10.1109/29.1611', '10.1049/el:19860328', '10.1049/el:19870848', '10.1109/TASSP.1987.1165181', '10.1109/TASSP.1986.1164857', '10.1109/34.192465', '10.1109/TCS.1987.1086067', '10.1109/TASSP.1987.1165254', '10.1109/TASSP.1987.1165259', '10.1016/0734-189X(86)90004-6', '10.1109/MC.1983.1654163', '10.1109/ICASSP.1982.1171856', '10.1109/29.1603', '10.1109/TASSP.1985.1164591', '10.1109/TASSP.1984.1164279', '10.1137/0908071', '10.1109/ICASSP.1991.150641', '10.1109/ICASSP.1991.151009', '10.1109/78.80931', '10.1214/aop/1176994892', '10.1109/PROC.1982.12434', '10.1016/0005-1098(80)90086-2', '10.1109/ICASSP.1990.115593', '10.1109/PROC.1985.13167', '10.1109/TAC.1980.1102349', '10.1214/aos/1176345067', '10.1109/TASSP.1983.1164247', '10.1109/31.101318', '10.2307/2332466', '10.1109/ICASSP.1988.196711', '10.1016/0161-7346(92)90066-5', '10.1109/29.21690'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'machine learning', 'autonomous systems', 'digital media'], conference_acronym='Proceedings of the IEEE', publisher=None, query_handler=None),\n",
       " 'A novel method for automatic face segmentation, facial feature extraction and tracking': Paper(DOI='10.1016/s0923-5965(97)00042-8', crossref_json=None, google_schorlar_metadata=None, title='A novel method for automatic face segmentation, facial feature extraction and tracking', authors=['Karin Sobottka', 'Ioannis Pitas'], abstract='The present paper describes a novel method for the segmentation of faces, extraction of facial features and tracking of the face contour and features over time. Robust segmentation of faces out of complex scenes is done based on color and shape information. Additionally, face candidates are verified by searching for facial features in the interior of the face. As interesting facial features we employ eyebrows, eyes, nostrils, mouth and chin. We consider incomplete feature constellations as well. If a face and its features are detected once reliably, we track the face contour and the features over time. Face contour tracking is done by using deformable models like snakes. Facial feature tracking is performed by block matching. The success of our approach was verified by evaluating 38 different color image sequences, containing features as beard, glasses and changing facial expressions.', conference=None, journal=None, year=None, reference_list=['10.1109/34.244675', '10.1016/0923-5965(95)00028-U', '10.1109/34.216733', '10.1109/34.85660', '10.1016/1049-9660(92)90003-L'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'machine learning', 'autonomous systems', 'digital media'], conference_acronym='Signal processing. Image communication', publisher=None, query_handler=None),\n",
       " 'Video shot detection and condensed representation. a review': Paper(DOI='10.1109/msp.2006.1621446', crossref_json=None, google_schorlar_metadata=None, title='Video shot detection and condensed representation. a review', authors=['Costas Cotsaces', 'Nikos Nikolaidis', 'Ioannis Pitas'], abstract='There is an urgent need to develop techniques that organize video data into more compact forms or extract semantically meaningful information. Such operations can serve as a first step for a number of different data access tasks such as browsing, retrieval, genre classification, and event detection. In this paper, we focus not on the high-level video analysis task themselves but on the common basic techniques that have been developed to facilitate them. These basic tasks are shot boundary detection and condensed video representation', conference=None, journal=None, year=None, reference_list=['10.1007/BF01210504', '10.1109/ICASSP.2003.1199137', '10.1007/s005300050115', '10.1016/S0167-8655(01)00085-X', '10.1117/12.410931', '10.1016/S0031-3203(01)00134-0', '10.1109/TCSVT.2004.842603', '10.1109/TCSVT.2002.806813', '10.1109/76.988659', '10.1016/S0923-5965(00)00011-4', '10.1007/s00530-003-0076-5', '10.1142/S021946780100027X', '10.1109/76.825852', '10.1007/s005300050003', '10.1109/76.988656', '10.1109/TMM.2003.808819', '10.1109/76.927435', '10.1109/TMM.2005.843362', '10.1109/MMUL.2002.1022858', '10.1109/TCSVT.2005.854230', '10.1155/S1110865703211082', '10.1109/TMM.2003.811617', '10.1109/TCSVT.2004.841694', '10.1109/ICME.2003.1221726', '10.1117/12.476299', '10.1109/MSP.2006.1621445'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'machine learning', 'autonomous systems', 'digital media'], conference_acronym='IEEE signal processing magazine (Print)', publisher=None, query_handler=None),\n",
       " 'Information theory-based shot cut/fade detection and video summarization': Paper(DOI='10.1109/tcsvt.2005.856896', crossref_json=None, google_schorlar_metadata=None, title='Information theory-based shot cut/fade detection and video summarization', authors=['Zuzana Cernekova', 'Ioannis Pitas', 'Christophoros Nikou'], abstract='New methods for detecting shot boundaries in video sequences and for extracting key frames using metrics based on information theory are proposed. The method for shot boundary detection relies on the mutual information (MI) and the joint entropy (JE) between the frames. It can detect cuts, fade-ins and fade-outs. The detection technique was tested on the TRECVID2003 video test set having different types of shots and containing significant object and camera motion inside the shots. It is demonstrated that the method detects both fades and abrupt cuts with high accuracy. The information theory measure provides us with better results because it exploits the inter-frame information in a more compact way than frame subtraction. It was also successfully compared to other methods published in literature. The method for key frame extraction uses MI as well. We show that it captures satisfactorily the visual content of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/S0001-2998(78)80014-2', '10.1007/3-540-57233-3_114', '10.1109/ICIP.2002.1038995', '10.1109/ICIP.2001.958141', '10.1117/12.410931', '10.1109/ICIP.2000.899609', '10.1109/ICIP.2001.958137', '10.1007/s005300050115', '10.1109/79.888862', '10.1109/76.988656', '10.1109/TMM.2002.806532', '10.1145/192593.192699', '10.1117/12.229193', '10.1002/0471200611', '10.1016/S0031-3203(96)00114-8', '10.1007/978-1-4757-6017-0', '10.1006/jvci.1996.0004', '10.1109/76.974682', '10.1109/76.915358', '10.1117/12.333848', '10.1109/76.988656', '10.1007/978-1-4471-1597-7_36', '10.1109/ICASSP.1997.595429', '10.1109/ICASSP.1996.543588'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'machine learning', 'autonomous systems', 'digital media'], conference_acronym='IEEE transactions on circuits and systems for video technology (Print)', publisher=None, query_handler=None),\n",
       " 'The use of watermarks in the protection of digital multimedia products': Paper(DOI='10.1109/5.771072', crossref_json=None, google_schorlar_metadata=None, title='The use of watermarks in the protection of digital multimedia products', authors=['George Voyatzis', 'Ioannis Pitas'], abstract='The watermarking of digital images, audio, video, and multimedia products in general has been proposed for resolving copyright ownership and verifying originality of content. This paper studies the contribution of watermarking for developing protection schemes. A general watermarking framework (GWF) is studied and the fundamental demands are listed. The watermarking algorithms, namely watermark generation, embedding, and detection, are analyzed and necessary conditions for a reliable and efficient protection are stated. Although the GWF satisfies the majority of requirements for copyright protection and content verification, there are unsolved problems inside a pure watermarking framework. Particular solutions, based on product registration and related network services, are suggested to overcome such problems.', conference=None, journal=None, year=None, reference_list=['10.1117/12.274501', '10.1109/MMSP.1997.602662', '10.1109/ICIP.1996.560426', '10.1109/INFCOM.1994.337544', '10.1117/12.175304', '10.1109/83.650120', '10.1016/S0165-1684(98)00017-6', '10.1109/ICIP.1996.560422', '10.1016/S0165-1684(98)00014-0', '10.1147/sj.353.0313', '10.1109/6046.923822', '10.1109/5.387094', '10.1109/49.668980', '10.1016/S0097-8493(98)00030-2', '10.1117/1.482648', '10.1016/S0165-1684(98)00013-9', '10.1016/S0165-1684(98)00016-4', '10.1016/S0097-8493(98)00032-6', '10.1109/49.668979', '10.1109/ICIP.1997.647969', '10.1109/DSPWS.1996.555456', '10.1109/ICIP.1997.631957', '10.1007/3-540-49380-8_16', '10.1109/2.511977', '10.1109/MC.1998.4655281', '10.1109/30.267415', '10.1109/ICIP.1996.560428', '10.1109/ICIP.1996.560423', '10.1145/359340.359342', '10.1117/1.482647', '10.1109/ICIP.1996.560425', '10.1109/ICIP.1997.638587', '10.1145/73007.73011', '10.1109/ICIP.1994.413536', '10.1109/ICIP.1997.647970', '10.1117/1.482643', '10.1109/ICIP.1997.647964', '10.1016/S0165-1684(98)00012-7'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'machine learning', 'autonomous systems', 'digital media'], conference_acronym='Proceedings of the IEEE', publisher=None, query_handler=None),\n",
       " 'Exploiting discriminant information in nonnegative matrix factorization with application to frontal face verification': Paper(DOI='10.1109/tnn.2006.873291', crossref_json=None, google_schorlar_metadata=None, title='Exploiting discriminant information in nonnegative matrix factorization with application to frontal face verification', authors=['Stefanos Zafeiriou', 'Anastasios Tefas', 'Ioan Buciu', 'Ioannis Pitas'], abstract='In this paper, two supervised methods for enhancing the classification accuracy of the Nonnegative Matrix Factorization (NMF) algorithm are presented. The idea is to extend the NMF algorithm in order to extract features that enforce not only the spatial locality, but also the separability between classes in a discriminant manner. The first method employs discriminant analysis in the features derived from NMF. In this way, a two-phase discriminant feature extraction procedure is implemented, namely NMF plus Linear Discriminant Analysis (LDA). The second method incorporates the discriminant constraints inside the NMF decomposition. Thus, a decomposition of a face to its discriminant parts is obtained and new update rules for both the weights and the basis images are derived. The introduced methods have been applied to the problem of frontal face verification using the well-known XM2VTS database. Both\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/5.381842', '10.1109/TNN.2002.806647', '10.1109/6046.825791', '10.1109/MLSP.2004.1423017', '10.1109/NNSP.2002.1030067', '10.1016/0041-5553(67)90040-7', '10.1109/TNN.2004.836239', '10.1145/954339.954342', '10.1016/S0031-3203(99)00185-5', '10.1109/34.935847', '10.1109/ICPR.2000.903052', '10.1109/34.41390', '10.1038/44565', '10.1109/34.531802', '10.1109/34.598228', '10.1109/TNN.2003.813829', '10.1109/CVPR.2001.990477', '10.1109/TNN.2002.806629', '10.1109/TPAMI.2005.33', '10.1109/ICPR.2004.1334109', '10.1109/TNN.2003.820673', '10.1109/TNN.2004.841811', '10.1109/TNN.2005.849817', '10.1109/83.704308', '10.1109/TNN.2005.844909', '10.1109/TPAMI.2004.1273927', '10.1109/TNN.2002.804287', '10.1016/S0167-8655(03)00089-8', '10.1162/jocn.1991.3.1.71', '10.1016/S0167-8655(02)00399-9', '10.1109/34.990132', '10.1016/j.patrec.2004.02.002', '10.1016/j.patcog.2004.02.013'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Deep Learning', 'Statistical Machine Learning', 'Pattern Recognition', 'Biometrics'], conference_acronym='IEEE transactions on neural networks', publisher=None, query_handler=None),\n",
       " 'Using support vector machines to enhance the performance of elastic graph matching for frontal face authentication': Paper(DOI='10.1109/34.935847', crossref_json=None, google_schorlar_metadata=None, title='Using support vector machines to enhance the performance of elastic graph matching for frontal face authentication', authors=['Anastasios Tefas', 'Constantine Kotropoulos', 'Ioannis Pitas'], abstract=\"A novel method for enhancing the performance of elastic graph matching in frontal face authentication is proposed. The starting point is to weigh the local similarity values at the nodes of an elastic graph according to their discriminatory power. Powerful and well-established optimization techniques are used to derive the weights of the linear combination. More specifically, we propose a novel approach that reformulates Fisher's discriminant ratio to a quadratic optimization problem subject to a set of inequality constraints by combining statistical pattern recognition and support vector machines (SVM). Both linear and nonlinear SVM are then constructed to yield the optimal separating hyperplanes and the optimal polynomial decision surfaces, respectively. The method has been applied to frontal face authentication on the M2VTS database. Experimental results indicate that the performance of morphological elastic\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1109/12.210173', '10.1016/S0031-3203(96)00132-X', '10.1109/5.628712', '10.1109/34.41390', '10.1109/5.381842', '10.1109/83.704308', '10.1007/BF00994018', '10.1023/A:1009715923555', '10.1109/6046.825791', '10.1016/S0165-1684(98)00087-5', '10.1007/978-1-4757-2440-0', '10.1016/S0031-3203(99)00185-5', '10.1109/34.598234', '10.1109/83.841933', '10.1109/CVPR.1992.223162', '10.1109/34.598235', '10.1109/CVPR.1997.609310', '10.1023/A:1018946025316', '10.1109/78.650102', '10.1364/JOSAA.14.001724', '10.1109/83.753738', '10.1109/34.598233', '10.1016/0031-3203(94)90017-5', '10.1109/ICASSP.1997.595305', '10.1007/BF00332918', '10.1162/jocn.1991.3.1.71', '10.1007/BFb0016021', '10.1109/34.531802', '10.1109/34.598228'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'machine learning', 'autonomous systems', 'digital media'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'The CAS-PEAL large-scale Chinese face database and baseline evaluations': Paper(DOI='10.1109/tsmca.2007.909557', crossref_json=None, google_schorlar_metadata=None, title='The CAS-PEAL large-scale Chinese face database and baseline evaluations', authors=['Wen Gao', 'Bo Cao', 'Shiguang Shan', 'Xilin Chen', 'Delong Zhou', 'Xiaohua Zhang', 'Debin Zhao'], abstract='In this paper, we describe the acquisition and contents of a large-scale Chinese face database: the CAS-PEAL face database. The goals of creating the CAS-PEAL face database include the following: 1) providing the worldwide researchers of face recognition with different sources of variations, particularly pose, expression, accessories, and lighting (PEAL), and exhaustive ground-truth information in one uniform database; 2) advancing the state-of-the-art face recognition technologies aiming at practical applications by using off-the-shelf imaging equipment and by designing normal face variations in the database; and 3) providing a large-scale face database of Mongolian. Currently, the CAS-PEAL face database contains 99 594 images of 1040 individuals (595 males and 445 females). A total of nine cameras are mounted horizontally on an arc arm to simultaneously capture images across different poses. Each\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/12.210173', '10.1109/TPAMI.2002.1039204', '10.1109/ICIP.2003.1247052', '10.1109/34.598235', '10.1109/TPAMI.2002.1017623', '10.1109/TPAMI.2004.1265861', '10.1109/34.908964', '10.1109/34.598229', '10.1109/AMFG.2003.1240838', '10.1109/AFGR.2004.1301550', '10.1109/ICCV.2005.147', '10.1109/TPAMI.2004.57', '10.1145/954339.954342', '10.1109/34.982883', '10.1177/0146167289151002', '10.1207/s15516709cog2606_4', '10.1364/JOSAA.14.001724', '10.1109/34.531802', '10.1109/ICPR.2004.1334121', '10.1109/TIP.2002.999679', '10.1109/CVPR.2005.150', '10.1109/TPAMI.2002.1008382', '10.1109/ACV.1994.341300', '10.1007/978-3-642-72201-1_25', '10.1109/CVPR.1991.139758', '10.1109/34.598228', '10.1109/34.927464', '10.1109/TPAMI.2005.92', '10.1007/3-540-44887-X_74', '10.1007/0-387-27257-7_14', '10.1016/S0262-8856(97)00070-X', '10.1109/5.381842', '10.21236/ADA415962', '10.1109/34.879790', '10.1109/CVPR.2005.268', '10.6028/NIST.IR.6965', '10.1109/TPAMI.2003.1227983', '10.1109/TPAMI.2003.1251154', '10.1109/TNN.2005.849817', '10.1109/ICPR.2002.1047791', '10.1109/34.598230', '10.1109/CVPR.1994.323893', '10.1126/science.272.5270.1905', '10.1109/CVPR.1997.609431', '10.1016/S0042-6989(03)00079-8'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Pattern Recognition', 'Machine Learning', 'Face Recognition'], conference_acronym='IEEE transactions on systems, man and cybernetics. Part A. Systems and humans', publisher=None, query_handler=None),\n",
       " 'Object-contextual representations for semantic segmentation': Paper(DOI='10.1007/978-3-030-58539-6_11', crossref_json=None, google_schorlar_metadata=None, title='Object-contextual representations for semantic segmentation', authors=['Yuhui Yuan', 'Xilin Chen', 'Jingdong Wang'], abstract=' In this paper, we study the context aggregation problem in semantic segmentation. Motivated by that the label of a pixel is the category of the object that the pixel belongs to, we present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of the ground-truth segmentation. Second, we compute the object region representation by aggregating the representations of the pixels lying in the object region. Last, we compute the relation between each pixel and each object region, and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations. We empirically demonstrate our method achieves competitive performance on various benchmarks: Cityscapes, ADE20K, LIP\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2012.6248077', '10.1007/978-3-319-46448-0_23', '10.1109/CVPR.2018.00132', '10.1109/TPAMI.2017.2699184', '10.1007/978-3-030-01234-2_49', '10.1109/CVPR.2019.00052', '10.1109/ICCV.2019.00532', '10.1109/CVPR.2016.350', '10.1109/ICCV.2019.00692', '10.1109/CVPR.2019.00909', '10.1109/TPAMI.2012.231', '10.1109/CVPRW.2018.00058', '10.1109/CVPR.2019.00326', '10.1109/ICCV.2019.00685', '10.1109/CVPR.2017.760', '10.1109/CVPR.2017.715', '10.1109/ICCV.2009.5459211', '10.1109/CVPR.2019.00770', '10.1109/CVPR.2016.90', '10.1109/CVPRW.2018.00146', '10.1109/ICCV.2019.00069', '10.1007/978-3-030-01246-5_36', '10.1109/CVPR.2019.00656', '10.1109/CVPR.2019.00963', '10.1109/CVPR.2018.00106', '10.1109/ICCV.2019.00930', '10.1109/CVPR.2016.398', '10.1109/ICCV.2019.00926', '10.1109/CVPR.2017.684', '10.1109/TPAMI.2018.2820063', '10.1109/CVPR.2018.00085', '10.1109/CVPR.2019.00767', '10.1007/978-3-319-10602-1_48', '10.1109/CVPR.2019.00633', '10.1109/CVPR.2015.7298965', '10.1007/978-3-030-01240-3_26', '10.1109/CVPR.2014.119', '10.1109/ICCV.2017.534', '10.1109/WACV.2018.00168', '10.1109/ICCV.2019.00433', '10.1109/CVPR.2018.00591', '10.1109/CVPR.2019.00841', '10.1109/ICCV.2019.00533', '10.1109/CVPR.2019.00324', '10.1109/TPAMI.2009.186', '10.1007/s11263-013-0620-5', '10.1109/ICCV.2019.00580', '10.1109/CVPR.2018.00813', '10.1109/CVPR.2017.687', '10.1109/CVPR.2019.00902', '10.1109/CVPR.2018.00388', '10.1609/aaai.v34i07.6955', '10.1007/978-3-030-58610-2_29', '10.1109/ICCV.2019.00690', '10.1109/CVPR.2018.00747', '10.1109/CVPR.2019.00064', '10.1109/ICCV.2017.224', '10.1109/CVPR.2017.660', '10.1007/978-3-030-01240-3_17', '10.1109/CVPR.2017.544', '10.1109/CVPR.2019.00906', '10.1109/ICCV.2019.00068', '10.1109/3DV.2018.00083'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Pattern Recognition', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Histogram of Gabor phase patterns (HGPP): a novel object representation approach for face recognition': Paper(DOI='10.1109/tip.2006.884956', crossref_json=None, google_schorlar_metadata=None, title='Histogram of Gabor phase patterns (HGPP): a novel object representation approach for face recognition', authors=['Baochang Zhang', 'Shiguang Shan', 'Xilin Chen', 'Wen Gao'], abstract='A novel object descriptor, histogram of Gabor phase pattern (HGPP), is proposed for robust face recognition. In HGPP, the quadrant-bit codes are first extracted from faces based on the Gabor transformation. Global Gabor phase pattern (GGPP) and local Gabor phase pattern (LGPP) are then proposed to encode the phase variations. GGPP captures the variations derived from the orientation changing of Gabor wavelet at a given scale (frequency), while LGPP encodes the local neighborhood variations by using a novel local XOR pattern (LXP) operator. They are both divided into the nonoverlapping rectangular regions, from which spatial histograms are extracted and concatenated into an extended histogram feature to represent the original image. Finally, the recognition is performed by using the nearest-neighbor classifier with histogram intersection as the similarity measurement. The features of HGPP lie in two\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICPR.1996.546848', '10.1109/34.598228', '10.1109/CVPR.1991.139758', '10.1088/0954-898X/7/3/002', '10.1016/0893-6080(95)00051-8', '10.1109/TIP.2003.819223', '10.1016/0031-3203(92)90121-X', '10.1049/el:19990213', '10.1109/34.244676', '10.1109/TPAMI.2003.1227981', '10.1007/BF00130487', '10.1109/34.589215', '10.1117/12.234802', '10.1109/34.879790', '10.1109/34.598227', '10.1109/TPAMI.2002.1017623', '10.1098/rspb.1980.0020', '10.1109/34.598235', '10.1109/12.210173', '10.1006/cviu.2000.0897', '10.1109/TIP.2002.999679', '10.1006/cviu.1999.0830', '10.1109/TPAMI.2004.32', '10.1109/ICCV.2005.147', '10.1109/ICCV.1998.710780', '10.1109/IJCNN.2005.1556154', '10.1109/5.381842', '10.1145/954339.954342'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Pattern Recognition', 'Machine Learning'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Multi-view discriminant analysis': Paper(DOI='10.1007/s10489-022-04168-x', crossref_json=None, google_schorlar_metadata=None, title='Multi-view discriminant analysis', authors=['Meina Kan', 'Shiguang Shan', 'Haihong Zhang', 'Shihong Lao', 'Xilin Chen'], abstract='In many computer vision systems, the same object can be observed at varying viewpoints or even by different sensors, which brings in the challenging demand for recognizing objects from distinct even heterogeneous views. In this work we propose a Multi-view Discriminant Analysis (MvDA) approach, which seeks for a single discriminant common space for multiple views in a non-pairwise manner by jointly learning multiple view-specific linear transforms. Specifically, our MvDA is formulated to jointly solve the multiple linear transforms by optimizing a generalized Rayleigh quotient, i.e., maximizing the between-class variations and minimizing the within-class variations from both intra-view and inter-view in the common space. By reformulating this problem as a ratio trace problem, the multiple linear transforms are achieved analytically and simultaneously through generalized eigenvalue decomposition. Furthermore\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/354756.354805 10.1145/354756.354805', '10.1007/978-3-540-74958-5_42', '10.1016/j.neucom.2019.06.087', '10.1016/j.patcog.2020.107567', '10.1007/s10489-017-1043-8', '10.1007/s10489-021-02322-5', '10.5555/2976248.2976293', '10.1109/ICCV.2015.114 10.1109/ICCV.2015.114', '10.1145/279943.279962', '10.1109/CVPR.2005.317', '10.1109/TPAMI.2015.2435740', '10.1016/j.patcog.2019.03.008', '10.1016/j.patcog.2020.107660', '10.1016/j.knosys.2020.106567', '10.1007/s10489-020-01864-4', '10.1007/s10489-021-02365-8', '10.1016/j.neucom.2016.02.072', '10.1016/j.ins.2021.03.040', '10.1016/j.future.2019.03.015', '10.24963/ijcai.2019/284', '10.1109/TSMCB.2005.847744', '10.1109/TSMCB.2006.870645', '10.1109/ICPR.2018.8545325', '10.1109/TCYB.2020.3026741', '10.1016/j.inffus.2020.10.021', '10.1007/978-981-15-5341-7_51', '10.1109/JSTSP.2018.2873142', '10.1016/j.patcog.2018.05.023', '10.1016/j.eswa.2020.114154', '10.1007/s10586-018-1772-4', '10.1109/ICIP.2019.8803291', '10.1016/j.patcog.2018.11.015', '10.1007/978-3-030-31321-0_3', '10.23919/ICIf.2018.8455667', '10.1137/1.9781611975673.1 10.1137/1.9781611975673.1', '10.1016/j.knosys.2019.02.036', '10.1109/CVPR.2004.383', '10.1109/CVPR.2009.5206594'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Pattern Recognition', 'Machine Learning', 'Face Recognition'], conference_acronym='Applied intelligence (Boston)', publisher=None, query_handler=None),\n",
       " 'Coarse-to-fine auto-encoder networks (cfan) for real-time face alignment': Paper(DOI='10.1007/978-3-319-10605-2_1', crossref_json=None, google_schorlar_metadata=None, title='Coarse-to-fine auto-encoder networks (cfan) for real-time face alignment', authors=['Jie Zhang', 'Shiguang Shan', 'Meina Kan', 'Xilin Chen'], abstract=' Accurate face alignment is a vital prerequisite step for most face perception tasks such as face recognition, facial expression analysis and non-realistic face re-rendering. It can be formulated as the nonlinear inference of the facial landmarks from the detected face region. Deep network seems a good choice to model the nonlinearity, but it is nontrivial to apply it directly. In this paper, instead of a straightforward application of deep network, we propose a Coarse-to-Fine Auto-encoder Networks (CFAN) approach, which cascades a few successive Stacked Auto-encoder Networks (SANs). Specifically, the first SAN predicts the landmarks quickly but accurately enough as a preliminary, by taking as input a low-resolution version of the detected face holistically. The following SANs then progressively refine the landmark by taking as input the local features extracted around the current landmarks (output of the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2013.442', '10.1109/CVPR.2011.5995602', '10.1561/2200000006', '10.1109/ICCV.2013.191', '10.1109/34.927467', '10.1006/cviu.1995.1004', '10.5244/C.20.95', '10.5244/C.21.79', '10.1109/CVPR.2012.6247976', '10.1109/CVPR.2010.5540094', '10.1016/j.imavis.2005.07.009', '10.1007/978-3-540-88682-2_32', '10.1007/3-540-45344-X_14', '10.1007/978-3-642-33712-3_49', '10.1109/TPAMI.2008.238', '10.1023/B:VISI.0000029666.37597.d3', '10.1007/978-3-540-88693-8_37', '10.1109/CVPRW.2013.132', '10.1109/ICCV.2009.5459377', '10.1109/CVPR.2013.446', '10.1109/CVPR.2014.214', '10.1109/CVPR.2010.5539996', '10.1109/CVPR.2013.443', '10.1109/CVPR.2013.75', '10.1109/ICCV.2013.244', '10.1109/CVPR.2014.228', '10.1007/978-3-642-37331-2_48'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Pattern Recognition', 'Machine Learning', 'Face Recognition'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Occlusion aware facial expression recognition using CNN with attention mechanism': Paper(DOI='10.3233/idt-230047', crossref_json=None, google_schorlar_metadata=None, title='Occlusion aware facial expression recognition using CNN with attention mechanism', authors=['Yong Li', 'Jiabei Zeng', 'Shiguang Shan', 'Xilin Chen'], abstract='Facial expression recognition in the wild is challenging due to various unconstrained conditions. Although existing facial expression classifiers have been almost perfect on analyzing constrained frontal faces, they fail to perform well on partially occluded faces that are common in the wild. In this paper, we propose a convolution neutral network (CNN) with attention mechanism (ACNN) that can perceive the occlusion regions of the face and focus on the most discriminative un-occluded regions. ACNN is an end-to-end learning framework. It combines the multiple representations from facial regions of interest (ROIs). Each representation is weighed via a proposed gate unit that computes an adaptive weight from the region itself according to the unobstructedness and importance. Considering different RoIs, we introduce two versions of ACNN: patch-based ACNN (pACNN) and global-local-based ACNN (gACNN\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TCSVT.2021.3089724', '10.1109/TCSVT.2021.3063052', '10.1109/TCSVT.2021.3056098', '10.1109/TIP.2006.884954', '10.1109/TIP.2020.2972114', '10.1109/ACCESS.2017.2737821', '10.1109/ACCESS.2017.2784096', '10.1109/TCSVT.2021.3074032', '10.1109/TMM.2021.3096068', '10.1109/TAFFC.2017.2780838', '10.1109/TCYB.2015.2418092', '10.1109/TIM.2020.3031835', '10.1109/T-AFFC.2012.33', '10.1109/ACCESS.2020.3021994', '10.1109/ACCESS.2019.2901521', '10.1109/TIP.2020.2996086', '10.1109/ACCESS.2017.2737821', '10.1007/s00371-020-01988-1', '10.1007/s12193-020-00363-7', '10.1007/s11045-020-00752-x', '10.1007/s00371-022-02413-5', '10.1007/s11042-020-09566-2', '10.1109/TIP.2018.2886767', '10.1016/j.promfg.2020.07.005', '10.1007/s11042-013-1610-x', '10.1007/s12046-022-01943-x', '10.1093/comjnl/bxy133', '10.1109/ACCESS.2021.3067597', '10.1016/j.asoc.2015.10.034', '10.1155/2022/7450637'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Pattern Recognition', 'Machine Learning', 'Face Recognition'], conference_acronym='Intelligent decision technologies', publisher=None, query_handler=None),\n",
       " 'Fusing local patterns of gabor magnitude and phase for face recognition': Paper(DOI='10.1109/tip.2010.2041397', crossref_json=None, google_schorlar_metadata=None, title='Fusing local patterns of gabor magnitude and phase for face recognition', authors=['Shufu Xie', 'Shiguang Shan', 'Xilin Chen', 'Jie Chen'], abstract=\" Gabor features have been known to be effective for face recognition. However, only a few approaches utilize phase feature and they usually perform worse than those using magnitude feature. To investigate the potential of Gabor phase and its fusion with magnitude for face recognition, in this paper, we first propose local Gabor XOR patterns (LGXP), which encodes the Gabor phase by using the local XOR pattern (LXP) operator. Then, we introduce block-based Fisher's linear discriminant (BFLD) to reduce the dimensionality of the proposed descriptor and at the same time enhance its discriminative power. Finally, by using BFLD, we fuse local patterns of Gabor magnitude and phase for face recognition. We evaluate our approach on FERET and FRGC 2.0 databases. In particular, we perform comparative experimental studies of different local Gabor patterns. We also make a detailed comparison of their\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2007.250598', '10.1109/TIP.2006.877435', '10.1109/TPAMI.2008.70', '10.1007/978-3-540-75690-3_18', '10.1007/978-3-540-75690-3_13', '10.1109/ICCV.2007.4409060', '10.1109/34.598235', '10.1162/jocn.1991.3.1.71', '10.1126/science.290.5500.2319', '10.1109/ICPR.2006.163', '10.1109/ICPR.2004.1334652', '10.1007/s10044-006-0033-y', '10.1109/34.598228', '10.1109/TPAMI.2006.244', '10.1109/TPAMI.2006.90', '10.1016/S0031-3203(99)00179-X', '10.1016/j.cviu.2007.12.002', '10.1109/34.879790', '10.1109/TPAMI.2002.1017623', '10.1126/science.290.5500.2323', '10.1109/CVPR.2005.268', '10.1109/TPAMI.2005.55', '10.1109/ICCV.2001.937693', '10.1109/TPAMI.2004.1261097', '10.1152/jn.1987.58.6.1233', '10.1016/j.imavis.2005.02.005', '10.1109/34.667881', '10.1109/12.210173', '10.1016/S0031-3203(99)00200-9', '10.1109/TIP.2002.999679', '10.1109/TIP.2006.881945', '10.1109/34.254061', '10.1364/JOSAA.2.001160', '10.1109/TPAMI.2002.1114855', '10.1109/34.244676', '10.1109/TIP.2007.904421', '10.1023/A:1011183429707', '10.1109/ICCV.2005.147', '10.1145/954339.954342', '10.1007/s10044-008-0123-0', '10.1109/AFGR.2004.1301556', '10.1109/TPAMI.2007.1008', '10.1109/TPAMI.2003.1227981', '10.1109/TIP.2006.884956'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Pattern Recognition', 'Machine Learning', 'Face Recognition'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Automatic detection and recognition of signs from natural scenes': Paper(DOI='10.1109/tip.2003.819223', crossref_json=None, google_schorlar_metadata=None, title='Automatic detection and recognition of signs from natural scenes', authors=['Xilin Chen', 'Jie Yang', 'Jing Zhang', 'Alex Waibel'], abstract='In this paper, we present an approach to automatic detection and recognition of signs from natural scenes, and its application to a sign translation task. The proposed approach embeds multiresolution and multiscale edge detection, adaptive searching, color analysis, and affine rectification in a hierarchical framework for sign detection, with different emphases at each phase to handle the text in different sizes, orientations, color distributions and backgrounds. We use affine rectification to recover deformation of the text regions caused by an inappropriate camera view angle. The procedure can significantly improve text detection rate and optical character recognition (OCR) accuracy. Instead of using binary information for OCR, we extract features from an intensity image directly. We propose a local intensity normalization method to effectively handle lighting variations, followed by a Gabor transform to obtain local\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICPR.2002.1047830', '10.1109/ICPR.2000.906081', '10.1109/ICASSP.2002.5745049', '10.3115/1072133.1072223', '10.1016/0031-3203(95)00030-4', '10.1007/BF00318371', '10.1002/ecjb.20134', '10.1142/S0218001495000043', '10.1109/83.817607', '10.1145/244130.244137', '10.1109/ICPR.2000.902945', '10.1016/0031-3203(92)90121-X', '10.1109/IECON.1991.239252', '10.1109/34.273729', '10.1016/0167-8655(93)90097-W', '10.1109/ICIP.1994.413707', '10.1109/34.809116', '10.1109/CVPR.1997.609372', '10.1109/ICME.2000.871481', '10.1109/ICDAR.1995.602027', '10.1109/ISWC.1999.806662', '10.1109/TCSVT.2004.825538', '10.1016/S0031-3203(98)00067-3', '10.1109/CAIVD.1998.646033', '10.1109/ICPR.1998.711218', '10.1016/0893-6080(95)00051-8', '10.1109/34.254062', '10.1016/0004-3702(81)90019-9', '10.1109/34.598235'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Pattern Recognition', 'Machine Learning'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Dynamic R-CNN: Towards high quality object detection via dynamic training': Paper(DOI='10.1007/978-3-030-58555-6_16', crossref_json=None, google_schorlar_metadata=None, title='Dynamic R-CNN: Towards high quality object detection via dynamic training', authors=['Hongkai Zhang', 'Hong Chang', 'Bingpeng Ma', 'Naiyan Wang', 'Xilin Chen'], abstract=' Although two-stage object detectors have continuously advanced the state-of-the-art performance in recent years, the training process itself is far from crystal. In this work, we first point out the inconsistency problem between the fixed network settings and the dynamic training procedure, which greatly affects the performance. For example, the fixed label assignment strategy and regression loss function cannot fit the distribution change of proposals and thus are harmful to training high quality detectors. Consequently, we propose Dynamic R-CNN to adjust the label assignment criteria (IoU threshold) and the shape of regression loss function (parameters of SmoothL1 Loss) automatically based on the statistics of proposals during training. This dynamic design makes better use of the training samples and pushes the detector to fit more high quality samples. Specifically, our method improves upon ResNet-50-FPN\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1553374.1553380', '10.1109/ICCV.2017.593', '10.1109/CVPR.2018.00644', '10.1007/978-3-030-01267-0_28', '10.1109/ICCV.2017.89', '10.1109/CVPR.2009.5206848', '10.1109/ICCV.2015.169', '10.1109/CVPR.2014.81', '10.1007/978-3-030-58536-5_14', '10.1109/ICCV.2017.322', '10.1109/CVPR.2016.90', '10.1109/CVPR.2019.00300', '10.1109/CVPR.2017.351', '10.1007/978-3-030-01264-9_48', '10.1007/978-3-030-58517-4_2', '10.1007/978-3-030-01264-9_45', '10.1109/CVPR42600.2020.01060', '10.1109/ICCV.2019.00615', '10.1007/978-3-030-01240-3_21', '10.1109/CVPR.2017.106', '10.1109/ICCV.2017.324', '10.1007/978-3-319-10602-1_48', '10.1007/978-3-030-01252-6_24', '10.1007/978-3-319-46448-0_2', '10.1109/CVPR.2019.00091', '10.1109/CVPR.2018.00647', '10.1109/CVPR.2016.91', '10.1109/CVPR.2016.89', '10.1109/CVPR.2018.00377', '10.1109/ICCV.2019.00836', '10.1109/ICCV.2019.00972', '10.1109/CVPR.2019.00308', '10.1007/978-3-030-58548-8_24', '10.1007/978-3-030-01252-6_49', '10.1109/ICCV.2019.00975', '10.1109/CVPR42600.2020.00978', '10.1109/CVPR.2019.00953'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Autonomous Driving'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Deeply learning deformable facial action parts model for dynamic expression analysis': Paper(DOI='10.1007/978-3-319-16817-3_10', crossref_json=None, google_schorlar_metadata=None, title='Deeply learning deformable facial action parts model for dynamic expression analysis', authors=['Mengyi Liu', 'Shaoxin Li', 'Shiguang Shan', 'Ruiping Wang', 'Xilin Chen'], abstract=' Expressions are facial activities invoked by sets of muscle motions, which would give rise to large variations in appearance mainly around facial parts. Therefore, for visual-based expression analysis, localizing the action parts and encoding them effectively become two essential but challenging problems. To take them into account jointly for expression analysis, in this paper, we propose to adapt 3D Convolutional Neural Networks (3D CNN) with deformable action parts constraints. Specifically, we incorporate a deformable parts learning component into the 3D CNN framework, which can detect specific facial action parts under the structured spatial constraints, and obtain the discriminative part-based representation simultaneously. The proposed method is evaluated on two posed expression datasets, CK+, MMI, and a spontaneous dataset FERA. We show that, besides achieving state-of-the-art expression\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.895976', '10.1109/TPAMI.2008.52', '10.1109/5.726791', '10.1109/TPAMI.2012.59', '10.5244/C.22.99', '10.1109/TPAMI.2007.1110', '10.1109/TPAMI.2009.167', '10.1109/CVPRW.2010.5543262', '10.1109/FG.2011.5771373', '10.1109/34.927467', '10.5244/C.20.95', '10.1109/ICCV.2013.257', '10.1109/ICCV.2013.21', '10.1109/CVPR.2013.75', '10.1016/S0893-6080(98)00116-6', '10.1109/CVPR.2013.439', '10.1145/2522848.2531745'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Pattern Recognition', 'Machine Learning', 'Face Recognition'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Manifold discriminant analysis': Paper(DOI='10.3724/sp.j.1146.2012.01552', crossref_json=None, google_schorlar_metadata=None, title='Manifold discriminant analysis', authors=['Ruiping Wang', 'Xilin Chen'], abstract='This paper presents a novel discriminative learning method, called manifold discriminant analysis (MDA), to solve the problem of image set classification. By modeling each image set as a manifold, we formulate the problem as classification-oriented multi-manifolds learning. Aiming at maximizing “manifold margin”, MDA seeks to learn an embedding space, where manifolds with different class labels are better separated, and local data compactness within each manifold is enhanced. As a result, new testing manifold can be more reliably classified in the learned embedding space. The proposed method is evaluated on the tasks of object recognition with image sets, including face recognition and object categorization. Comprehensive comparisons and extensive experiments demonstrate the effectiveness of our method.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Pattern Recognition', 'Machine Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Deep expectation of real and apparent age from a single image without facial landmarks': Paper(DOI='10.1007/s11263-016-0940-3', crossref_json=None, google_schorlar_metadata=None, title='Deep expectation of real and apparent age from a single image without facial landmarks', authors=['Rasmus Rothe', 'Radu Timofte', 'Luc Van Gool'], abstract=' In this paper we propose a deep learning solution to age estimation from a single face image without the use of facial landmarks and introduce the IMDB-WIKI dataset, the largest public dataset of face images with age and gender labels. If the real age estimation research spans over decades, the study of apparent age estimation or the age as perceived by other humans from a face image is a recent endeavor. We tackle both tasks with our convolutional neural networks (CNNs) of VGG-16 architecture which are pre-trained on ImageNet for image classification. We pose the age estimation problem as a deep classification problem followed by a softmax expected value refinement. The key factors of our solution are: deep learned models from large data, robust face alignment, and expected value formulation for age regression. We validate our methods on standard benchmarks and achieve state-of-the-art\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2011.5995437', '10.1109/TMM.2015.2420374', '10.1109/WACV.2016.7477557', '10.1109/CVPR.2013.319', '10.1109/CVPR.2012.6248110', '10.1109/34.927467', '10.1109/TIFS.2014.2359646', '10.1109/ICCVW.2015.40', '10.1109/TPAMI.2009.167', '10.1109/TMM.2008.921847', '10.1109/TPAMI.2010.36', '10.1007/978-3-642-01793-3_14', '10.1016/0003-2670(86)80028-9', '10.1109/TPAMI.2007.70733', '10.1109/CVPR.2014.81', '10.1007/978-3-642-28598-1_4', '10.1109/CVPR.2011.5995404', '10.1016/j.imavis.2014.04.011', '10.1109/TIP.2008.924280', '10.1109/CVPR.2009.5206681', '10.1109/ICB.2013.6613022', '10.1109/TPAMI.2014.2362759', '10.1162/0899766042321814', '10.1145/2647868.2654889', '10.1109/CVPR.2014.241', '10.1006/cviu.1997.0549', '10.1109/TSMCB.2003.817091', '10.1109/5.726791', '10.1109/CVPRW.2015.7301352', '10.1109/ICCVW.2015.42', '10.1109/BTAS.2009.5339053', '10.1109/IJCB.2011.6117601', '10.1007/978-3-319-10593-2_47', '10.1049/iet-bmt.2014.0053', '10.1109/CVPR.2006.187', '10.1109/ICCVW.2015.41', '10.1109/CVPR.2016.599', '10.1007/s11263-015-0816-y', '10.1109/TPAMI.2009.39', '10.1109/CVPR.2015.7298594', '10.1109/CVPR.2014.220', '10.1109/CVPRW.2016.96', '10.1109/WACV.2015.77', '10.1109/TPAMI.2008.50', '10.1109/ICCV.2007.4409050', '10.1109/ICCVW.2015.53', '10.1007/978-3-540-74549-5_49', '10.1007/978-3-319-10590-1_53', '10.1109/CVPR.2010.5539975', '10.1109/ICCVW.2015.43'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'AI', 'Compression', 'Computational Photography'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'The 2018 pirm challenge on perceptual image super-resolution': Paper(DOI='10.1007/978-3-030-11021-5_21', crossref_json=None, google_schorlar_metadata=None, title='The 2018 pirm challenge on perceptual image super-resolution', authors=['Yochai Blau', 'Roey Mechrez', 'Radu Timofte', 'Tomer Michaeli', 'Lihi Zelnik-Manor'], abstract='This paper reports on the 2018 PIRM challenge on perceptual super-resolution (SR), held in conjunction with the Perceptual Image Restoration and Manipulation (PIRM) workshop at ECCV 2018. In contrast to previous SR challenges, our evaluation methodology jointly quantifies accuracy and perceptual quality, therefore enabling perceptualdriven methods to compete alongside algorithms that target PSNR maximization. Twenty-one participating teams introduced algorithms which well-improved upon the existing state-of-the-art methods in perceptual SR, as confirmed by a human opinion study. We also analyze popular image quality measures and draw conclusions regarding which of them correlates best with human opinion scores. We conclude with an analysis of the current trends in perceptual SR, as reflected from the leading submissions.', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2018.00652', '10.1007/978-3-030-11021-5_4', '10.1109/ICCV.2017.581', '10.1109/LSP.2018.2805809', '10.1007/978-3-319-10593-2_13', '10.1109/CVPR.2016.265', '10.1007/978-3-030-11021-5_6', '10.1109/CVPR.2018.00178', '10.1109/CVPR.2018.00179', '10.1109/CVPR.2015.7299156', '10.1007/978-3-319-46475-6_43', '10.1109/CVPR.2016.182', '10.1109/CVPRW.2018.00124', '10.1109/CVPR.2017.618', '10.1109/CVPR.2017.19', '10.1109/CVPRW.2017.151', '10.1109/ICCV.2017.324', '10.1007/978-3-030-11021-5_2', '10.1016/j.cviu.2016.12.009', '10.1007/978-3-030-01264-9_47', '10.1109/TIP.2012.2214050', '10.1109/LSP.2012.2227726', '10.1007/978-3-030-11021-5_1', '10.1007/978-3-030-11021-5_9', '10.1109/TIP.2012.2191563', '10.1109/ICCV.2017.481', '10.1109/TIP.2005.859378', '10.1109/TIP.2005.859389', '10.1109/CVPR.2018.00329', '10.1109/CVPRW.2017.150', '10.1109/ICCV.2013.241', '10.1109/ICCV.2017.514', '10.1007/978-3-030-11021-5_8', '10.1007/978-3-030-11021-5_7', '10.1109/CVPR.2018.00070', '10.1007/978-3-030-11021-5_5', '10.1109/CVPRW.2018.00131', '10.1109/TIP.2003.819861', '10.1109/TIP.2011.2109730', '10.1109/CVPR.2018.00068', '10.1007/978-3-030-01234-2_18', '10.1109/CVPR.2018.00262', '10.1109/TCI.2016.2644865'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'AI', 'Compression', 'Computational Photography'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Soft-to-hard vector quantization for end-to-end learning compressible representations': Paper(DOI='10.1016/j.asoc.2023.110136', crossref_json=None, google_schorlar_metadata=None, title='Soft-to-hard vector quantization for end-to-end learning compressible representations', authors=['Eirikur Agustsson', 'Fabian Mentzer', 'Michael Tschannen', 'Lukas Cavigelli', 'Radu Timofte', 'Luca Benini', 'Luc V Gool'], abstract='We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2017.2709749', '10.1016/j.sigpro.2021.108086', '10.1016/j.asoc.2021.108322', '10.1109/ICDAR.2011.213', '10.1109/ICDAR.2009.125', '10.1109/TKDE.2004.76', '10.1109/TPAMI.2008.89', '10.1109/ICPR.2010.836', '10.1109/ICASSP43922.2022.9747775', '10.1007/s10032-008-0066-4', '10.1109/ICDAR.2013.54', '10.1109/ICASSP39728.2021.9414341', '10.1007/978-3-642-33709-3_55', '10.1109/ICDAR.2015.7333956', '10.1109/ICCV.2003.1238663', '10.1007/978-3-642-15561-1_11', '10.1016/j.ins.2021.04.078', '10.1007/978-3-319-10590-1_38', '10.1109/ICASSP.2015.7178196', '10.1109/CVPR.2009.5206848', '10.1007/s11263-017-1016-8', '10.1016/j.asoc.2022.108942', '10.1109/TMM.2020.2974326', '10.1109/TPAMI.2021.3126648', '10.1016/j.imavis.2019.10.006', '10.1016/j.patcog.2022.108618', '10.1109/TIP.2021.3131042', '10.1016/j.knosys.2022.110128', '10.1109/CVPR42600.2020.00980', '10.1016/j.knosys.2022.109447', '10.1016/j.patcog.2021.108291', '10.1109/CVPRW.2014.131', '10.1109/TIP.2015.2400229', '10.1109/ICDAR.2017.342', '10.1109/ACCESS.2021.3056079', '10.1016/j.asoc.2020.106786'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'AI', 'Compression', 'Computational Photography'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Plug-and-play image restoration with deep denoiser prior': Paper(DOI='10.1109/tpami.2021.3088914', crossref_json=None, google_schorlar_metadata=None, title='Plug-and-play image restoration with deep denoiser prior', authors=['Kai Zhang', 'Yawei Li', 'Wangmeng Zuo', 'Lei Zhang', 'Luc Van Gool', 'Radu Timofte'], abstract='Recent works on plug-and-play image restoration have shown that a denoiser can implicitly serve as the image prior for model-based methods to solve many inverse problems. Such a property induces considerable advantages for plug-and-play image restoration (e.g., integrating the flexibility of model-based method and effectiveness of learning-based methods) when the denoiser is discriminatively learned via deep convolutional neural network (CNN) with large modeling capacity. However, while deeper and larger CNN models are rapidly gaining popularity, existing plug-and-play image restoration hinders its performance due to the lack of suitable denoiser prior. In order to push the limits of plug-and-play image restoration, we set up a benchmark deep denoiser prior by training a highly flexible and effective CNN denoiser. We then plug the deep denoiser prior as a modular part into a half quadratic splitting\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICPR.2018.8545043', '10.1109/TIP.2018.2875569', '10.1109/ICASSP.2019.8683057', '10.1109/LSP.2017.2763583', '10.1109/TIP.2003.818640', '10.1137/16M1102884', '10.1109/CVPR.2016.90', '10.1109/LSP.2020.2986643', '10.1109/CVPR.2018.00334', '10.1109/ICIP.2016.7533014', '10.1007/s10851-010-0251-1', '10.1109/TIT.2016.2556683', '10.1109/CVPR.2005.38', '10.1109/TIP.2006.881969', '10.1109/TIP.2011.2176954', '10.1109/TIP.2007.901238', '10.1109/TPAMI.2015.2439281', '10.1109/EUSIPCO.2015.7362905', '10.1109/CVPR.2019.00223', '10.1109/CVPR.2017.623', '10.1109/CVPR42600.2020.00277', '10.1109/ICCV.2019.00326', '10.1109/CVPR.2019.01129', '10.1109/CVPR.2019.00181', '10.1109/LSP.2019.2920250', '10.1109/ICCV.2011.6126278', '10.1007/s11263-008-0197-6', '10.1109/TIP.2009.2028254', '10.1109/CVPR.2007.383037', '10.1109/CVPR.2014.349', '10.1109/TIP.2012.2208981', '10.1109/ICCV.2001.937655', '10.1109/TPAMI.2016.2596743', '10.1109/TMI.2018.2865356', '10.1109/CVPR.2018.00196', '10.1088/1361-6420/ab460a', '10.1109/TPAMI.2018.2873610', '10.3390/app9061103', '10.1515/9783110524116', '10.1109/ICCV.2015.73', '10.1109/CVPR.2019.00621', '10.1109/CVPR.2009.5206815', '10.1109/ICCV.2017.491', '10.1007/978-3-030-58598-3_18', '10.1109/CVPR.2016.206', '10.1109/CVPR.2019.00613', '10.1109/ICCV.2013.352', '10.1007/978-3-030-01201-4_30', '10.1109/LGRS.2018.2802944', '10.1109/CVPRW.2017.151', '10.1109/TIP.2016.2631888', '10.1109/CVPRW.2017.150', '10.1364/JOSA.62.000055', '10.1109/TIP.2015.2482899', '10.1109/ICCV.2009.5459452', '10.1109/TIP.2019.2905991', '10.1109/TIP.2016.2574984', '10.1109/CVPR42600.2020.00231', '10.1109/TIP.2016.2518082', '10.1109/CVPR.2014.366', '10.1561/2200000016', '10.1109/83.392335', '10.1561/2400000003', '10.1145/2661229.2661260', '10.1109/GlobalSIP.2013.6737048', '10.1109/TIP.2016.2567075', '10.1109/CVPR.2017.300', '10.1006/jvci.1993.1030', '10.1109/TIP.2018.2839891', '10.1109/CVPR.2016.90', '10.1109/CVPR42600.2020.00328', '10.1007/978-3-319-24075-6_65', '10.1145/2980179.2982399', '10.1109/CVPR.2019.00399', '10.1109/CVPR42600.2020.00357', '10.1109/CVPR.2019.01132', '10.1109/ICASSP.2004.1326587'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'AI', 'Compression', 'Computational Photography'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Beyond correlation filters: Learning continuous convolution operators for visual tracking': Paper(DOI='10.1007/978-3-319-46454-1_29', crossref_json=None, google_schorlar_metadata=None, title='Beyond correlation filters: Learning continuous convolution operators for visual tracking', authors=['Martin Danelljan', 'Andreas Robinson', 'Fahad Shahbaz Khan', 'Michael Felsberg'], abstract=' Discriminative Correlation Filters (DCF) have demonstrated excellent performance for visual object tracking. The key to their success is the ability to efficiently exploit available negative data by including all shifted versions of a training sample. However, the underlying DCF formulation is restricted to single-resolution feature maps, significantly limiting its potential. In this paper, we go beyond the conventional DCF framework and introduce a novel formulation for training continuous convolution filters. We employ an implicit interpolation model to pose the learning problem in the continuous spatial domain. Our proposed formulation enables efficient integration of multi-resolution deep feature maps, leading to superior results on three object tracking benchmarks: OTB-2015 ( in mean OP), Temple-Color ( in mean OP), and VOT2015 ( relative reduction in failure rate). Additionally, our approach\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCVW.2013.37', '10.1023/B:VISI.0000011205.11775.fd', '10.1109/CVPR.2016.156', '10.1109/CVPR.2013.297', '10.1109/CVPR.2010.5539960', '10.1007/978-3-642-33783-3_44', '10.1109/CVPR.2015.7299007', '10.5244/C.28.65', '10.1109/ICCVW.2015.84', '10.1109/ICCV.2015.490', '10.1109/CVPR.2016.159', '10.1109/CVPR.2014.143', '10.1109/ICCVW.2013.22', '10.1109/TPAMI.2014.2375215', '10.1007/s100440050039', '10.1109/ICCV.2013.381', '10.1109/CVPR.2015.7299094', '10.1007/978-3-319-10578-9_13', '10.1109/ICPR.2016.7899807', '10.1109/ICCV.2011.6126251', '10.1109/CVPR.2013.314', '10.1007/978-3-642-33765-9_50', '10.1109/TPAMI.2014.2345390', '10.1109/CVPR.2010.5540231', '10.1109/ISMAR.2007.4538852', '10.1007/978-3-319-16181-5_14', '10.1007/978-3-319-16181-5_18', '10.1109/CVPR.2015.7299107', '10.1109/ICCV.2015.352', '10.1109/CVPR.2015.7299177', '10.1109/CVPR.2014.222', '10.1109/ICRA.2015.7139474', '10.1109/CVPR.2015.7298823', '10.1109/CVPR.2012.6247891', '10.1109/CVPR.2013.312', '10.1109/TPAMI.2014.2388226', '10.1007/978-3-319-10599-4_13', '10.1007/978-3-319-16817-3_44'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Color Attributes for Object Detection': Paper(DOI='10.1016/j.patcog.2015.07.005', crossref_json=None, google_schorlar_metadata=None, title='Color Attributes for Object Detection', authors=['Fahad Shahbaz Khan', 'Rao Muhammad Anwer', 'Joost van de Weijer', 'Andrew D Bagdanov', 'Maria Vanrell', 'Antonio M Lopez'], abstract='State-of-the-art object detectors typically use shape information as a low level feature representation to capture the local structure of an object. This paper shows that early fusion of shape and color, as is popular in image classification, leads to a significant drop in performance for object detection. Moreover, such approaches also yields suboptimal results for object categories with varying importance of color and shape. In this paper we propose the use of color attributes as an explicit color representation for object detection. Color attributes are compact, computationally efficient, and when combined with traditional shape features provide state-of-the-art results for object detection. Our method is tested on the PASCAL VOC 2007 and 2009 datasets and results clearly show that our method improves over state-of-the-art techniques despite its simplicity. We also introduce a new dataset consisting of cartoon character\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.730558', '10.1145/957013.957094', '10.1109/TPAMI.2010.70', '10.1109/ICCV.2009.5459296', '10.1109/CVPR.2006.54', '10.1109/ICCV.2011.6126499', '10.1023/A:1012460413855', '10.1109/CVPR.2011.5995344', '10.1109/CVPR.2012.6247743', '10.1109/CVPR.2007.383267', '10.1109/TPAMI.2012.89', '10.1109/CVPR.2013.151', '10.1109/TPAMI.2012.28', '10.1109/ICCV.2013.248', '10.1007/978-3-642-33709-3_30', '10.5244/C.27.78', '10.1007/978-3-642-33712-3_3', '10.1109/CVPR.2014.360', '10.1109/CVPR.2014.414', '10.1109/ICCV.2013.221', '10.1016/j.patcog.2012.02.009', '10.1167/13.13.27', '10.1109/TIP.2009.2019809', '10.1109/ICCV.2009.5459362', '10.1109/CVPR.2012.6248068', '10.1109/ICCV.2013.26', '10.1109/TPAMI.2011.272', '10.1145/1180639.1180824', '10.1007/978-3-540-79547-6_7', '10.1109/CVPR.2009.5206596', '10.1109/LSP.2013.2260737', '10.5244/C.25.110', '10.1109/ICCV.2013.193', '10.1109/CVPR.2013.271'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Object Recognition', 'Deep Learning'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'Deep projective 3D semantic segmentation': Paper(DOI='10.1007/978-3-319-64689-3_8', crossref_json=None, google_schorlar_metadata=None, title='Deep projective 3D semantic segmentation', authors=['Felix Järemo Lawin', 'Martin Danelljan', 'Patrik Tosteberg', 'Goutam Bhat', 'Fahad Shahbaz Khan', 'Michael Felsberg'], abstract=' Semantic segmentation of 3D point clouds is a challenging problem with numerous real-world applications. While deep learning has revolutionized the field of image semantic segmentation, its impact on point cloud data has been limited so far. Recent attempts, based on 3D deep learning approaches (3D-CNNs), have achieved below-expected results. Such methods require voxelizations of the underlying point cloud data, leading to decreased spatial resolution and increased memory consumption. Additionally, 3D-CNNs greatly suffer from the limited availability of annotated datasets. In this paper, we propose an alternative framework that avoids the limitations of 3D-CNNs. Instead of directly solving the problem in 3D, we first project the point cloud onto a set of synthetic 2D-images. These images are then used as input to a 2D-CNN, designed for semantic segmentation. Finally, the obtained\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2005.133', '10.1109/CVPR.2009.5206848', '10.1109/IROS.2015.7353446', '10.1109/CVPR.2016.213', '10.1007/978-3-319-10584-0_23', '10.5194/isprs-annals-III-3-177-2016', '10.5194/isprsannals-III-3-177-2016', '10.1109/ICPR.2016.7900038', '10.1109/34.765655', '10.1109/ICCV.2013.380', '10.1145/2487228.2487237', '10.1109/ICCV.2013.180', '10.1109/CVPR.2015.7298965', '10.1109/CVPR.2015.7299075', '10.1109/IROS.2015.7353481', '10.1007/978-3-319-11752-2_17', '10.5220/0006131500680076', '10.1109/CVPR.2016.609', '10.1016/j.cviu.2014.04.011', '10.1145/2733373.2807412', '10.1109/ICRA.2015.7139875', '10.1109/CVPR.2015.7298801', '10.1109/CVPR.2017.544', '10.1145/383259.383300'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Nyströmformer: A nyström-based algorithm for approximating self-attention': Paper(DOI='10.1609/aaai.v35i16.17664', crossref_json=None, google_schorlar_metadata=None, title='Nyströmformer: A nyström-based algorithm for approximating self-attention', authors=['Yunyang Xiong', 'Zhanpeng Zeng', 'Rudrasis Chakraborty', 'Mingxing Tan', 'Glenn Fung', 'Yin Li', 'Vikas Singh'], abstract='Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences-a topic being actively studied in the community. To address this limitation, we propose Nyströmformer-a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nyström method to approximate standard self-attention with O (n) complexity. The scalability of Nyströmformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nyströmformer performs comparably, or in a few cases, even slightly better, than standard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nyströmformer performs favorably relative to other efficient self-attention methods. Our code is available at https://github. com/mlpen/Nystromformer.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision', 'AutoML', 'Autonomous Driving'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Bignas: Scaling up neural architecture search with big single-stage models': Paper(DOI='10.1007/978-3-030-58571-6_41', crossref_json=None, google_schorlar_metadata=None, title='Bignas: Scaling up neural architecture search with big single-stage models', authors=['Jiahui Yu', 'Pengchong Jin', 'Hanxiao Liu', 'Gabriel Bender', 'Pieter-Jan Kindermans', 'Mingxing Tan', 'Thomas Huang', 'Xiaodan Song', 'Ruoming Pang', 'Quoc Le'], abstract=' Neural architecture search (NAS) has shown promising results discovering models that are both accurate and fast. For NAS, training a one-shot model has become a popular strategy to rank the relative quality of different architectures (child models) using a single set of shared weights. However, while one-shot model weights can effectively rank different network architectures, the absolute accuracies from these shared weights are typically far below those obtained from stand-alone training. To compensate, existing methods assume that the weights must be retrained, finetuned, or otherwise post-processed after the search is completed. These steps significantly increase the compute requirements and complexity of the architecture search and model deployment. In this work, we propose BigNAS, an approach that challenges the conventional wisdom that post-processing of the weights is necessary to get\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR42600.2020.01123', '10.1109/CVPR.2019.00020', '10.1007/978-3-030-58517-4_32', '10.1109/ICCV.2015.123', '10.1109/CVPR.2016.90', '10.1007/978-3-319-46493-0_38', '10.1109/CVPR.2019.00065', '10.1109/ICCV.2019.00140', '10.1109/CVPR.2018.00745', '10.1109/CVPR42600.2020.01210', '10.1007/978-3-030-01246-5_2', '10.1007/978-3-030-01264-9_8', '10.1609/aaai.v33i01.33014780', '10.1109/CVPR.2018.00474', '10.1007/978-3-030-46147-8_29', '10.1109/CVPR.2019.00293', '10.1109/CVPR.2019.01099', '10.1109/CVPR42600.2020.00190', '10.1109/ICCV.2019.00189', '10.1109/CVPR.2018.00907'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision', 'AutoML', 'Autonomous Driving'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Combined scaling for zero-shot transfer learning': Paper(DOI='10.1016/j.neucom.2023.126658', crossref_json=None, google_schorlar_metadata=None, title='Combined scaling for zero-shot transfer learning', authors=['Hieu Pham', 'Zihang Dai', 'Golnaz Ghiasi', 'Kenji Kawaguchi', 'Hanxiao Liu', 'Adams Wei Yu', 'Jiahui Yu', 'Yi-Ting Chen', 'Minh-Thang Luong', 'Yonghui Wu', 'Mingxing Tan', 'Quoc V Le'], abstract='Recent developments in multimodal training methodologies, including CLIP and ALIGN, obviate the necessity for individual data labeling. These approaches utilize pairs of data and corresponding textual information found online as a form of weak supervision signal. However, models employing this kind of weak supervision are not as competitive as their supervised and semi-supervised counterparts when sufficient labeled data is accessible. This performance gap constrains the applicability of weekly supervised models. In this paper, we narrow the gap by proposing a combined scaling method, named BASIC, that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best-published similar models, CLIP and ALIGN, by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR46437.2021.01501', '10.1109/ICCV48922.2021.00823', '10.1109/CVPR.2016.90', '10.18653/v1/W18-6301', '10.1007/978-3-319-46478-7_5', '10.1007/978-3-030-01216-8_12', '10.1109/ICCV.2017.97', '10.1007/978-3-030-58558-7_29', '10.1109/CVPR42600.2020.00975', '10.1109/CVPR.2010.5540112', '10.1162/tacl_a_00177', '10.1613/jair.3994', '10.1109/TIP.2014.2311377', '10.1109/TPAMI.2019.2932058', '10.1109/CVPR.2015.7298935', '10.1109/CVPR.2015.7298932', '10.1109/ICCV.2017.449', '10.1109/CVPR.2017.232', '10.1109/CVPR46437.2021.01553', '10.1109/ICCV.2019.00475', '10.1007/978-3-030-58598-3_10', '10.1109/CVPR46437.2021.01101', '10.1007/978-3-030-58577-8_7', '10.1109/CVPR52688.2022.01759', '10.1109/CVPR.2015.7298911', '10.1109/TPAMI.2015.2487986', '10.1109/CVPR.2017.321', '10.1109/CVPR.2016.15', '10.1109/CVPR.2017.328', '10.1109/CVPR.2019.00844', '10.1109/CVPR52688.2022.01042', '10.15607/RSS.2023.XIX.029', '10.1109/CVPRW59228.2023.00236', '10.1016/j.jcp.2018.10.045', '10.1137/21M1447039', '10.1109/TASLP.2023.3268730', '10.1145/347837.347846', '10.1109/CVPR.2018.00745', '10.18653/v1/D18-2012', '10.1109/TIP.2003.819861', '10.18653/v1/2021.emnlp-main.243', '10.1007/978-3-319-46493-0_39', '10.1109/CVPR.2014.259', '10.1109/CVPR.2014.461', '10.1109/ICVGIP.2008.47', '10.1007/978-3-319-10599-4_29', '10.1109/CVPR.2012.6248092', '10.1109/IGARSS.2018.8519248', '10.1109/JPROC.2017.2675998', '10.1109/CVPR.2010.5539970', '10.1109/CVPR42600.2020.01070', '10.1109/CVPR46437.2021.01139', '10.1145/3079856.3080246'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision', 'AutoML', 'Autonomous Driving'], conference_acronym='Neurocomputing (Amsterdam)', publisher=None, query_handler=None),\n",
       " 'Robust Stereo Matching Using Adaptive Normalized Cross Correlation': Paper(DOI='10.1109/tpami.2010.136', crossref_json=None, google_schorlar_metadata=None, title='Robust Stereo Matching Using Adaptive Normalized Cross Correlation', authors=['Y Heo', 'K Lee', 'S Lee'], abstract='A majority of the existing stereo matching algorithms assume that the corresponding color values are similar to each other. However, it is not so in practice as image color values are often affected by various radiometric factors such as illumination direction, illuminant color, and imaging device changes. For this reason, the raw color recorded by a camera should not be relied on completely, and the assumption of color consistency does not hold good between stereo images in real scenes. Therefore, the performance of most conventional stereo matching algorithms can be severely degraded under the radiometric variations. In this paper, we present a new stereo matching measure that is insensitive to radiometric variations between left and right images. Unlike most stereo matching measures, we use the color formation model explicitly in our framework and propose a new measure, called the Adaptive Normalized\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1023/A:1022314423998', '10.1364/JOSAA.3.001651', '10.1007/BF00056770', '10.1109/34.969113', '10.1109/ICCV.2005.110', '10.1109/TPAMI.2006.200', '10.1364/JOSA.61.000001', '10.1007/s11263-007-0087-3', '10.1109/ICCV.1998.710714', '10.1109/ICCV.2003.1238440', '10.1016/S0031-3203(98)00036-3', '10.1109/TPAMI.2004.1261083', '10.1016/S0167-8655(02)00324-0', '10.1109/34.977559', '10.1364/JOSAA.18.000253', '10.1023/A:1007958904918', '10.1109/ICCV.2007.4408933', '10.1109/CVPR.2006.260', '10.1109/34.713362', '10.1109/34.969114', '10.1023/A:1023713602895', '10.1109/TPAMI.2007.1171', '10.1145/258734.258884', '10.1109/CVPR.1999.786966', '10.1109/TPAMI.2004.88', '10.1109/ICCV.1998.710815', '10.1002/col.5080100409', '10.1007/BF00137441', '10.1109/ICCV.1993.378223', '10.1364/JOSAA.11.001553', '10.1109/ICCV.2007.4409102', '10.1007/BFb0028345', '10.1109/TPAMI.2007.1166', '10.1109/34.677269', '10.1023/A:1014554110407', '10.1109/34.868688', '10.1109/CVPR.2007.383248', '10.1109/TPAMI.2007.70844', '10.1109/83.597272', '10.1023/A:1014573219977', '10.1109/TPAMI.2006.70'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image': Paper(DOI='10.1007/978-3-030-58571-6_44', crossref_json=None, google_schorlar_metadata=None, title='I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image', authors=['Gyeongsik Moon', 'Kyoung Mu Lee'], abstract=' Most of the previous image-based 3D human pose and mesh estimation methods estimate parameters of the human mesh model from an input image. However, directly regressing the parameters from the input image is a highly non-linear mapping because it breaks the spatial relationship between pixels in the input image. In addition, it cannot model the prediction uncertainty, which can make training harder. To resolve the above issues, we propose I2L-MeshNet, an image-to-lixel (line+pixel) prediction network. The proposed I2L-MeshNet predicts the per-lixel likelihood on 1D heatmaps for each mesh vertex coordinate instead of directly regressing the parameters. Our lixel-based 1D heatmap preserves the spatial relationship in the input image and models the prediction uncertainty. We demonstrate the benefit of the image-to-lixel prediction and show that the proposed I2L-MeshNet outperforms previous\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2014.471', '10.1109/CVPR.2019.00351', '10.1109/CVPR.2019.00116', '10.1007/978-3-319-46454-1_34', '10.1109/CVPR.2019.01110', '10.1109/CVPR.2018.00742', '10.1007/978-3-030-58571-6_45', '10.1109/ICCV.2019.00090', '10.1109/CVPR.2019.01109', '10.1109/CVPR.2019.01208', '10.1109/ICCV.2017.322', '10.1109/CVPR.2016.90', '10.1109/TPAMI.2013.248', '10.5244/C.24.12', '10.1109/CVPR.2011.5995318', '10.1109/3DV53792.2021.00015', '10.1109/CVPR.2018.00744', '10.1109/CVPR.2019.00576', '10.1109/ICCV.2019.00234', '10.1109/CVPR.2019.00463', '10.1109/CVPR.2017.500', '10.1007/978-3-319-10602-1_48', '10.1145/2816795.2818013', '10.1007/978-3-030-01249-6_37', '10.1109/3DV.2017.00064', '10.1109/3DV.2018.00024', '10.1109/CVPR.2018.00533', '10.1109/ICCV.2019.01023', '10.1007/978-3-319-46484-8_29', '10.1109/3DV.2018.00062', '10.1109/CVPR.2019.01123', '10.1109/ICCV.2019.00089', '10.1109/CVPR.2017.139', '10.1109/CVPR.2018.00055', '10.1109/CVPR.2017.134', '10.1145/3130800.3130883', '10.1007/s11263-015-0816-y', '10.1109/ICCV.2017.284', '10.1007/978-3-030-01252-6_4', '10.1007/978-3-030-01231-1_29', '10.1109/ICCV.2019.00785'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Mobile robot and course adjusting method thereof': Paper(DOI='10.3724/sp.j.1218.2010.00166', crossref_json=None, google_schorlar_metadata=None, title='Mobile robot and course adjusting method thereof', authors=['Jeong-gon Song', 'Sang-yong Lee', 'Seung-bin Moon', 'Kyoung-mu Lee'], abstract='A mobile robot capable of recognizing its location and adjusting its direction in response to an obstacle in its way includes a running device, an obstacle detecting device for detecting the presence of an obstacle, a location recognizing device, a controlling portion, and a power supply. The location recognizing device includes a first vision camera directed toward the ceiling of a room and a first vision board. The first vision camera recognizes a base mark on the ceiling. The first vision board processes an image from the first vision camera and transmits the image data to the controlling portion. The obstacle detecting device includes a line laser for emitting a linear light beam toward the obstacle, a second vision camera for recognizing a reflective linear light beam from the obstacle, and a second vision board for processing image data captured by the second vision camera.', conference=None, journal=None, year=None, reference_list=['10.1016/j.robot.2004.08.005', '10.1023/B:VISI.0000029664.99615.94'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Pose2Mesh: Graph convolutional network for 3D human pose and mesh recovery from a 2D human pose': Paper(DOI='10.1007/978-3-030-58571-6_45', crossref_json=None, google_schorlar_metadata=None, title='Pose2Mesh: Graph convolutional network for 3D human pose and mesh recovery from a 2D human pose', authors=['Hongsuk Choi', 'Gyeongsik Moon', 'Kyoung Mu Lee'], abstract=' Most of the recent deep learning-based 3D human pose and mesh estimation methods regress the pose and shape parameters of human mesh models, such as SMPL and MANO, from an input image. The first weakness of these methods is the overfitting to image appearance, due to the domain gap between the training data captured from controlled settings such as a lab, and in-the-wild data in inference time. The second weakness is that the estimation of the pose parameters is quite challenging due to the representation issues of 3D rotations. To overcome the above weaknesses, we propose Pose2Mesh, a novel graph convolutional neural network (GraphCNN)-based system that estimates the 3D coordinates of human mesh vertices directly from the 2D human pose. The 2D human pose as input provides essential human body articulation information without image appearance. Also, the proposed\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2018.00542', '10.1109/CVPR.2014.471', '10.1109/CVPR.2019.00351', '10.1109/CVPR.2019.00116', '10.1007/978-3-319-46454-1_34', '10.1109/CVPR.2019.01110', '10.1109/ICCV.2019.00236', '10.1109/CVPR.2018.00742', '10.1109/TPAMI.2007.1115', '10.1109/CVPR.2019.01109', '10.1007/BF02291478', '10.1016/j.acha.2010.04.005', '10.1109/CVPR.2019.01208', '10.1109/ICCV.2017.322', '10.1109/CVPR.2016.90', '10.1109/TPAMI.2013.248', '10.5244/C.24.12', '10.1109/CVPR.2011.5995318', '10.1109/ICCV.2015.381', '10.1109/CVPR.2018.00744', '10.1109/CVPR.2019.00576', '10.1109/CVPR.2018.00411', '10.1109/ICCV.2019.00234', '10.1109/CVPR.2019.00463', '10.1109/CVPR.2017.500', '10.1007/978-3-319-10602-1_48', '10.1145/2661229.2661273', '10.1145/2816795.2818013', '10.1007/978-3-030-01249-6_37', '10.1109/ICCV.2017.288', '10.1109/3DV.2017.00064', '10.1109/3DV.2018.00024', '10.1109/ICCV.2019.01023', '10.1109/CVPR.2019.00796', '10.1007/978-3-030-58571-6_44', '10.1109/3DV.2018.00062', '10.1109/WACV.2018.00054', '10.1109/CVPR.2019.01123', '10.1109/CVPR.2017.139', '10.1109/CVPR.2018.00055', '10.1007/978-3-030-01219-9_43', '10.1145/3130800.3130883', '10.1109/ICCV.2017.48', '10.1109/ICCV.2019.00241', '10.1109/MSP.2012.2235192', '10.1109/CVPR.2019.00584', '10.1109/ICCV.2017.284', '10.1109/ICCV.2017.284', '10.1109/CVPR.2018.00275', '10.1109/CVPR.2019.00797', '10.1007/978-3-030-01252-6_4', '10.1007/978-3-030-01231-1_29', '10.1109/CVPR.2019.00354', '10.1109/CVPR.2019.00589', '10.1109/ICCV.2019.00090'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Volumetric descriptions of objects from multiple views': Paper(DOI='10.1109/tpami.1983.4767367', crossref_json=None, google_schorlar_metadata=None, title='Volumetric descriptions of objects from multiple views', authors=['Worthy N Martin', 'Jagdishkumar Keshoram Aggarwal'], abstract=\"Occluding contours from an image sequence with view-point specifications determine a bounding volume approximating the object generating the contours. The initial creation and continual refinement of the approximation requires a volumetric representation that facilitates modification yet is descriptive of surface detail. The ``volume segment'' representation presented in this paper is one such representation.\", conference=None, journal=None, year=None, reference_list=['10.1109/C-M.1981.220560', '10.1109/TPAMI.1980.6447703', '10.7551/mitpress/3877.001.0001', '10.1109/PROC.1981.12025', '10.1109/C-M.1981.220561', '10.1016/0146-664X(76)90028-9', '10.1145/563274.563323', '10.1145/965139.807384', '10.1016/S0146-664X(78)80003-3', '10.1109/TC.1976.1674626', '10.1145/356827.356833', '10.1007/978-3-642-87037-8_2', '10.1016/0004-3702(77)90006-6', '10.1145/360715.360727', '10.1109/TPAMI.1979.4766925', '10.1016/B978-0-444-86325-6.50013-8', '10.1016/0004-3702(71)90005-1', '10.1016/0146-664X(80)90055-6', '10.1016/0004-3702(73)90003-9', '10.1145/563858.563899', '10.1016/0004-3702(77)90020-0', '10.1038/236207a0', '10.1016/0004-3702(71)90015-4', '10.1016/S0146-664X(78)80018-5', '10.1016/0031-3203(72)90003-9', '10.1109/PROC.1977.10458'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'signal processing', 'control systems', 'machine learning', 'image processing'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Human activity recognition from 3d data: A review': Paper(DOI='10.1016/j.patrec.2014.04.011', crossref_json=None, google_schorlar_metadata=None, title='Human activity recognition from 3d data: A review', authors=['Jake K Aggarwal', 'Lu Xia'], abstract='Human activity recognition has been an important area of computer vision research since the 1980s. Various approaches have been proposed with a great portion of them addressing this issue via conventional cameras. The past decade has witnessed a rapid development of 3D data acquisition techniques. This paper summarizes the major techniques in human activity recognition from 3D data with a focus on techniques that use depth data. Broad categories of algorithms are identified based upon the use of different features. The pros and cons of the algorithms in each category are analyzed and the possible direction of future research is indicated.', conference=None, journal=None, year=None, reference_list=['10.1145/1922649.1922653', '10.1145/151254.151255', '10.1007/978-3-642-34274-5_17', '10.1007/BF01420984', '10.1109/TPAMI.2002.1023805', '10.1109/TGRS.2005.844731', '10.1109/CVPRW.2012.6239175', '10.1109/CVPR.2011.5995442', '10.1109/CVPR.2009.5206821', '10.1016/j.patrec.2013.02.006', '10.1007/978-3-642-33868-7_6', '10.1109/34.993559', '10.1109/34.400565', '10.1007/11744047_33', '10.1023/A:1008103604354', '10.1111/j.1467-8659.2011.02095.x', '10.1109/21.44067', '10.5772/45901', '10.1007/3-540-47967-8_49', '10.1109/ICPR.2008.4761290', '10.1109/34.677262', '10.2307/1418003', '10.1109/CVPR.2004.1315191', '10.1109/ISSPA.2003.1224850', '10.1109/3DIMPVT.2011.50', '10.1016/0004-3702(81)90024-2', '10.1109/ICCV.2007.4409000', '10.1007/3-540-47967-8_2', '10.1038/scientificamerican0675-76', '10.1109/CVPR.1996.517074', '10.5244/C.22.99', '10.1109/83.748890', '10.1177/0278364908091153', '10.1007/s11263-005-1838-7', '10.1145/2370216.2370248', '10.5244/C.25.46', '10.1109/AFGR.2000.840680', '10.1016/j.imavis.2006.07.012', '10.1109/TSMCB.2005.861864', '10.1007/s11263-005-3671-4', '10.1016/j.imavis.2009.11.014', '10.1016/j.patrec.2009.11.017', '10.1007/s001380100063', '10.1016/1049-9660(91)90032-K', '10.1006/cviu.1996.0017', '10.1016/0167-8655(95)00048-L', '10.1145/1291233.1291311', '10.1109/CVPR.2011.5995316', '10.1109/TCSVT.2008.2005594', '10.4218/etrij.11.0110.0314', '10.1007/978-3-642-33275-3_31', '10.5244/C.23.124', '10.1007/978-3-642-33709-3_62', '10.1007/s11263-010-0404-0', '10.1109/CVPR.2013.365', '10.1016/0262-8856(92)90066-C', '10.1109/CVPRW.2012.6239232', '10.1145/2393347.2396382', '10.5244/C.25.67', '10.1109/CVPR.2009.5204242', '10.1109/TIM.2007.911641', '10.1007/978-3-319-00711-3'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'signal processing', 'control systems', 'machine learning', 'image processing'], conference_acronym='Pattern recognition letters', publisher=None, query_handler=None),\n",
       " 'Determining the movement of objects from a sequence of images': Paper(DOI='10.1109/tpami.1980.6447703', crossref_json=None, google_schorlar_metadata=None, title='Determining the movement of objects from a sequence of images', authors=['John W Roach', 'JK Aggarwal'], abstract='Discusses the problem of determining the three-dimensional model and movement of an object from a sequence of two-dimensional images. A solution to this problem depends on solving a system of nonlinear equations using a modified least-squared error method. Two views of six points or three views of four points are needed to provide an overdetermined set of equations when the images are noisy. It is shown, however, that this numerical method is not very accurate unless the images of considerably more points are used.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'signal processing', 'control systems', 'machine learning', 'image processing'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Texture analysis using generalized co-occurrence matrices': Paper(DOI='10.1109/tpami.1979.4766921', crossref_json=None, google_schorlar_metadata=None, title='Texture analysis using generalized co-occurrence matrices', authors=['Larry S Davis', 'Steven A Johns', 'JK Aggarwal'], abstract=\"We present a new approach to texture analysis based on the spatial distribution of local features in unsegmented textures. The textures are described using features derived from generalized co-occurrence matrices (GCM). A GCM is determined by a spatial constraint predicate F and a set of local features P = {(Xi, Yi, di), i = 1,..., m} where (Xi, Yi) is the location of the ith feature, and di is a description of the ith feature. The GCM of P under F, GF, is defined by GF(i, j) = number of pairs, pk, pl such that F(pk, pl) is true and di and dj are the descriptions of pk and pl, respectively. We discuss features derived from GCM's and present an experimental study using natural textures.\", conference=None, journal=None, year=None, reference_list=['10.1109/T-C.1975.224169', '10.1098/rstb.1976.0090', '10.1016/0010-4809(71)90034-6', '10.1016/0146-664X(79)90049-2', '10.1109/TSMC.1973.4309314'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'signal processing', 'control systems', 'machine learning', 'image processing'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Tracking human motion in structured environments using a distributed-camera system': Paper(DOI='10.1109/34.809119', crossref_json=None, google_schorlar_metadata=None, title='Tracking human motion in structured environments using a distributed-camera system', authors=['Quin Cai', 'Jake K Aggarwal'], abstract='This paper presents a comprehensive framework for tracking coarse human models from sequences of synchronized monocular grayscale images in multiple camera coordinates. It demonstrates the feasibility of an end-to-end person tracking system using a unique combination of motion analysis on 3D geometry in different camera coordinates and other existing techniques in motion detection, segmentation, and pattern recognition. The system starts with tracking from a single camera view. When the system predicts that the active camera will no longer have a good view of the subject of interest, tracking will be switched to another camera which provides a better view and requires the least switching to continue tracking. The nonrigidity of the human body is addressed by matching points of the middle line of the human image, spatially and temporally, using Bayesian classification schemes. Multivariate normal\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICPR.1996.546796', '10.1109/ACV.1996.571994', '10.1145/217279.215267', '10.1109/CADVIS.1994.284490', '10.1109/ICCV.1998.710743', '10.1016/0734-189X(88)90115-6', '10.1016/0031-3203(93)90089-F', '10.1109/MNRAO.1994.346251', '10.1109/34.809119', '10.1109/ICIP.1995.529584'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'signal processing', 'control systems', 'machine learning', 'image processing'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Model-based object recognition in dense-range images—a review': Paper(DOI='10.1145/151254.151255', crossref_json=None, google_schorlar_metadata=None, title='Model-based object recognition in dense-range images—a review', authors=['Farshid Arman', 'Jake K Aggarwal'], abstract='The goal in computer vision systems is to analyze data collected from the environment and derive an interpretation to complete a specified task. Vision system tasks may be divided into data acquisition, low-level processing, representation, model construction, and matching subtasks. This paper presents a comprehensive survey of model-based vision systems using dense-range images. A comprehensive survey of the recent publications in each subtask pertaining to dense-range image object recognition is presented.', conference=None, journal=None, year=None, reference_list=['10.1006/ciun.1993.1030', '10.1109/TPAMI.1986.4767747', '10.1016/0031-3203(81)90009-1', '10.1109/MCG.1981.1673799', '10.1109/34.3881', '10.1016/0734-189X(86)90220-3', '10.1145/4078.4081', '10.1109/TPAMI.1984.4767527', '10.1109/MC.1987.1663657', '10.1016/0167-8655(87)90077-8', '10.1109/34.67626', '10.1109/TPAMI.1987.4767869', '10.1016/0734-189X(85)90001-5', '10.1016/S0734-189X(83)80049-8', '10.1109/TSMC.1981.4308619', '10.1109/TPAMI.1986.4767851', '10.1145/6462.6464', '10.1109/34.19034', '10.1109/34.42853', '10.1016/1049-9660(92)90012-R', '10.1109/34.99239', '10.1109/34.67642', '10.1109/34.24797', '10.1109/34.49052', '10.1109/TPAMI.1987.4767935', '10.1109/34.42856', '10.1109/TPAMI.1987.4767955', '10.1109/21.44066', '10.1016/0031-3203(84)90069-4', '10.1007/BF00123163', '10.1109/34.9102', '10.1016/0146-664X(80)90055-6', '10.1109/34.75511', '10.1016/0031-3203(87)90020-3', '10.1109/34.42854', '10.1109/TPAMI.1986.4767750', '10.1016/S0734-189X(85)80078-5', '10.1109/34.3895', '10.1142/S021800148700028X', '10.1109/34.35498', '10.1364/AO.23.003837', '10.1109/MC.1987.1663658', '10.1006/ciun.1993.1025', '10.1109/34.44401', '10.1016/0734-189X(89)90069-8', '10.1016/0262-8856(86)90029-6', '10.1145/965139.807400', '10.1109/34.44402', '10.1109/MCG.1987.276899', '10.1016/0734-189X(86)90077-0'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'signal processing', 'control systems', 'machine learning', 'image processing'], conference_acronym='ACM computing surveys', publisher=None, query_handler=None),\n",
       " 'A hierarchical Bayesian network for event recognition of human actions and interactions': Paper(DOI='10.1007/s00530-004-0148-1', crossref_json=None, google_schorlar_metadata=None, title='A hierarchical Bayesian network for event recognition of human actions and interactions', authors=['Sangho Park', 'Jake K Aggarwal'], abstract=' Recognizing human interactions is a challenging task due to the multiple body parts of interacting persons and the concomitant occlusions. This paper presents a method for the recognition of two-person interactions using a hierarchical Bayesian network (BN). The poses of simultaneously tracked body parts are estimated at the low level of the BN, and the overall body pose is estimated at the high level of the BN. The evolution of the poses of the multiple body parts are processed by a dynamic Bayesian network (DBN). The recognition of two-person interactions is expressed in terms of semantic verbal descriptions at multiple levels: individual body-part motions at low level, single-person actions at middle level, and two-person interactions at high level. Example sequences of interacting persons illustrate the success of the proposed framework.', conference=None, journal=None, year=None, reference_list=['10.1006/cviu.1998.0744', '10.1093/logcom/4.5.531', '10.1049/cp:19990433', '10.1145/982452.982454', '10.1109/ICPR.2002.1044748', '10.1109/ICCV.2001.937617', '10.1006/cviu.1998.0716', '10.1016/0020-0190(72)90045-2', '10.1109/34.868683', '10.1016/S0888-613X(96)00069-2', '10.1016/B978-1-55860-332-5.50050-X', '10.1023/A:1020346032608', '10.1006/cviu.2000.0897', '10.1109/34.868684', '10.1145/982452.982461', '10.1007/3-540-45113-7_39', '10.1109/5.18626', '10.1109/CVPR.2000.854946', '10.1109/MOT.2001.937986', '10.1007/3-540-45053-X_10', '10.1109/34.868687'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'signal processing', 'control systems', 'machine learning', 'image processing'], conference_acronym='Multimedia systems', publisher=None, query_handler=None),\n",
       " 'Structure from motion of rigid and jointed objects': Paper(DOI='10.1016/0004-3702(82)90023-6', crossref_json=None, google_schorlar_metadata=None, title='Structure from motion of rigid and jointed objects', authors=['Jon A Webb', 'Jake K Aggarwal'], abstract='A method for recovering the three-dimensional structure of moving rigid and jointed objects from several single camera views is presented. The method is based on the fixed axis assumption: all movement consists of translations and rotations about an axis that is fixed in direction for short periods of time. This assumption makes it possible to recover the structure of any group of two or more rigidly connected points. The structure of jointed objects is recovered by analyzing them as collections of rigid parts, and then unifying the structures proposed for the parts. The method presented here has been tested on several sets of data, including movies used to demonstrate human perception of structure from motion.', conference=None, journal=None, year=None, reference_list=['10.3758/BF03210375', '10.1037/h0045622', '10.3758/BF03212378', '10.1007/BF00309043', '10.1038/scientificamerican0675-76', '10.3758/BF03210461', '10.1109/TPAMI.1980.6447699', '10.1109/TPAMI.1980.6447705', '10.1109/TPAMI.1980.6447703', '10.1037/h0056880', '10.1109/C-M.1981.220561', '10.1037/h0044747'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'signal processing', 'control systems', 'machine learning', 'image processing'], conference_acronym='Artificial intelligence (General ed.)', publisher=None, query_handler=None),\n",
       " 'SPENCER: A Socially Aware Service Robot for Passenger Guidance and Help in Busy Airports': Paper(DOI='10.1007/978-3-319-27702-8_40', crossref_json=None, google_schorlar_metadata=None, title='SPENCER: A Socially Aware Service Robot for Passenger Guidance and Help in Busy Airports', authors=['L Palmieri', 'U Rafi', 'M van Rooij', 'B Okal', 'M Magnusson', 'T Linder', 'M Lohse', 'L Zhang', 'AJ Lilienthal', 'B Leibe', 'L Beyer', 'S Breuers', 'R Chatila', 'R Alami', 'K Arras', 'R Triebel', 'M Chetouani', 'D Cremers', 'M Joosse', 'H Khambhaita', 'HS Hung', 'V Evers', 'M Fiore', 'T Kucner'], abstract=None, conference=None, journal=None, year=None, reference_list=['10.1145/1015330.1015430', '10.1109/ROBOT.2008.4543447', '10.1109/34.481539', '10.1007/978-3-642-31479-7_14', '10.1007/978-3-319-23778-7_20', '10.1109/TASL.2010.2066267', '10.1109/ICRA.2014.6907688', '10.1145/2631488.2631499', '10.15607/RSS.2010.VI.034', '10.1145/2559636.2559662', '10.1109/IROS.2013.6696502', '10.1109/TPAMI.2008.170', '10.15607/RSS.2013.IX.001', '10.1002/rob.20204', '10.1109/ICRA.2012.6225241', '10.5244/C.26.8', '10.1109/ICRA.2015.7139368', '10.1287/opre.16.3.682', '10.7551/mitpress/8727.003.0027', '10.1109/IROS.2014.6942562', '10.1177/0278364911400640', '10.1109/ICRA.2014.6907397', '10.1177/0278364913499415', '10.1016/S0921-8890(02)00376-7', '10.1109/IROS.2013.6697033', '10.1109/TPAMI.2012.263', '10.1007/978-3-319-11752-2_53', '10.1109/IROS.2014.6942731', '10.1109/ICCVW.2011.6130339'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Representation Learning', 'Good Shit', 'Computer Vision', 'Robotics'], conference_acronym='Springer tracts in advanced robotics (Print)', publisher=None, query_handler=None),\n",
       " 'The STRANDS project: Long-term autonomy in everyday environments': Paper(DOI='10.1109/mra.2016.2636359', crossref_json=None, google_schorlar_metadata=None, title='The STRANDS project: Long-term autonomy in everyday environments', authors=['Nick Hawes', 'Christopher Burbridge', 'Ferdian Jovan', 'Lars Kunze', 'Bruno Lacerda', 'Lenka Mudrova', 'Jay Young', 'Jeremy Wyatt', 'Denise Hebesberger', 'Tobias Kortner', 'Rares Ambrus', 'Nils Bore', 'John Folkesson', 'Patric Jensfelt', 'Lucas Beyer', 'Alexander Hermans', 'Bastian Leibe', 'Aitor Aldoma', 'Thomas Faulhammer', 'Michael Zillich', 'Markus Vincze', 'Eris Chinellato', 'Muhannad Al-Omari', 'Paul Duckworth', 'Yiannis Gatsoulis', 'David C Hogg', 'Anthony G Cohn', 'Christian Dondrup', 'Jaime Pulido Fentanes', 'Tomas Krajnik', 'Joao M Santos', 'Tom Duckett', 'Marc Hanheide'], abstract='Thanks to the efforts of the robotics and autonomous systems community, the myriad applications and capacities of robots are ever increasing. There is increasing demand from end users for autonomous service robots that can operate in real environments for extended periods. In the Spatiotemporal Representations and Activities for Cognitive Control in Long-Term Scenarios (STRANDS) project (http://strandsproject.eu), we are tackling this demand head-on by integrating state-of-the-art artificial intelligence and robotics research into mobile service robots and deploying these systems for long-term installations in security and care environments. Our robots have been operational for a combined duration of 104 days over four deployments, autonomously performing end-user-defined tasks and traversing 116 km in the process. In this article, we describe the approach we used to enable long-term autonomous\\xa0…', conference=None, journal=None, year=None, reference_list=['10.3390/robotics4010063', '10.1109/ECMR.2015.7324192', '10.1109/ICRA.2014.6907688', '10.1109/IROS.2015.7353360', '10.1109/ICRA.2014.6907396', '10.1109/IROS.2014.6942756', '10.1109/LRA.2016.2516594', '10.1109/IROS.2015.7354183', '10.1109/ROBOT.1999.770401', '10.1109/LRA.2016.2522086', '10.1109/HRI.2016.7451730', '10.1109/IROS.2014.6942963', '10.1109/MIS.2016.53', '10.1109/ROBOT.2010.5509725'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Representation Learning', 'Good Shit', 'Computer Vision', 'Robotics'], conference_acronym='IEEE robotics & automation magazine', publisher=None, query_handler=None),\n",
       " 'Biternion Nets: Continuous Head Pose Regression from Discrete Training Labels': Paper(DOI='10.1007/978-3-319-24947-6_13', crossref_json=None, google_schorlar_metadata=None, title='Biternion Nets: Continuous Head Pose Regression from Discrete Training Labels', authors=['Lucas Beyer', 'Alexander Hermans', 'Bastian Leibe'], abstract=' While head pose estimation has been studied for some time, continuous head pose estimation is still an open problem. Most approaches either cannot deal with the periodicity of angular data or require very fine-grained regression labels. We introduce biternion nets, a CNN-based approach that can be trained on very coarse regression labels and still estimate fully continuous  head poses. We show state-of-the-art results on several publicly available datasets. Finally, we demonstrate how easy it is to record and annotate a new dataset with coarse orientation labels in order to obtain continuous head pose estimates using our biternion nets.', conference=None, journal=None, year=None, reference_list=['10.5244/C.23.76', '10.1109/LSP.2014.2364458', '10.1109/ICCV.2011.6126516', '10.1117/12.473032', '10.1109/CVPR.2012.6247976', '10.1007/978-3-319-10590-1_22', '10.1109/CVPR.2010.5540094', '10.1007/s11263-012-0549-0', '10.1016/j.imavis.2009.08.002', '10.1007/978-3-319-10605-2_36', '10.1007/978-3-319-10593-2_30', '10.1109/CVPR.2011.5995683', '10.1007/978-3-319-11752-2_39', '10.1109/ITSC.2007.4357803', '10.1109/TPAMI.2008.106', '10.1109/MCSE.2007.53', '10.1109/ICCV.2011.6126549', '10.1109/TPAMI.2012.263'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Representation Learning', 'Good Shit', 'Computer Vision', 'Robotics'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Convolutional LSTM network: A machine learning approach for precipitation nowcasting': Paper(DOI='10.1127/metz/2019/0977', crossref_json=None, google_schorlar_metadata=None, title='Convolutional LSTM network: A machine learning approach for precipitation nowcasting', authors=['S Xingjian', 'Z Chen', 'H Wang', 'D Yeung', 'W Wong', 'W Woo'], abstract=None, conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision', 'Natural Language Processing'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Learning a deep compact image representation for visual tracking': Paper(DOI='10.1016/j.jvcir.2019.102737', crossref_json=None, google_schorlar_metadata=None, title='Learning a deep compact image representation for visual tracking', authors=['Naiyan Wang', 'Dit-Yan Yeung'], abstract='In this paper, we study the challenging problem of tracking the trajectory of a moving object in a video with possibly very complex background. In contrast to most existing trackers which only learn the appearance of the tracked object online, we take a different approach, inspired by recent advances in deep learning architectures, by putting more emphasis on the (unsupervised) feature learning problem. Specifically, by using auxiliary natural images, we train a stacked denoising autoencoder offline to learn generic image features that are more robust against variations. This is then followed by knowledge transfer from offline training to the online tracking process. Online tracking involves a classification neural network which is constructed from the encoder part of the trained autoencoder as a feature extractor and an additional classification layer. Both the feature extractor and the classifier can be further tuned to adapt to appearance changes of the moving object. Comparison with the state-of-the-art trackers on some challenging benchmark video sequences shows that our deep learning tracker is very efficient as well as more accurate.', conference=None, journal=None, year=None, reference_list=['10.1109/TMM.2019.2891420', '10.1016/j.jvcir.2019.02.020', '10.1109/TSMC.2019.2899047', '10.1109/TIP.2017.2662206', '10.1109/TAFFC.2019.2954394', '10.1109/TPAMI.2015.2456908', '10.1109/TCSVT.2017.2731866', '10.1007/s11263-013-0620-5', '10.1109/TPAMI.2010.226', '10.1109/TPAMI.2011.239', '10.1109/TPAMI.2014.2315808', '10.1016/j.patcog.2012.07.013', '10.1109/TIP.2017.2737321'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Artificial Intelligence', 'Computer Vision'], conference_acronym='Journal of visual communication and image representation (Print)', publisher=None, query_handler=None),\n",
       " 'Constructive algorithms for structure learning in feedforward neural networks for regression problems': Paper(DOI='10.1109/72.572102', crossref_json=None, google_schorlar_metadata=None, title='Constructive algorithms for structure learning in feedforward neural networks for regression problems', authors=['Tin-Yau Kwok', 'Dit-Yan Yeung'], abstract='In this survey paper, we review the constructive algorithms for structure learning in feedforward neural networks for regression problems. The basic idea is to start with a small network, then add hidden units and weights incrementally until a satisfactory solution is found. By formulating the whole problem as a state-space search, we first describe the general issues in constructive algorithms, with special emphasis on the search strategy. A taxonomy, based on the differences in the state transition mapping, the training algorithm, and the network architecture, is then presented.', conference=None, journal=None, year=None, reference_list=['10.1007/978-1-4899-4541-9', '10.1007/BF00993164', '10.1080/01621459.1986.10478245', '10.1109/NAECON.1992.220489', '10.1007/978-1-4471-2097-1_188', '10.1007/978-3-642-79119-2_7', '10.1109/ICASSP.1992.226386', '10.1162/neco.1992.4.1.1', '10.1209/0295-5075/11/6/001', '10.1007/978-3-642-79119-2_1', '10.1080/01621459.1987.10478427', '10.1162/neco.1990.2.2.198', '10.1109/IJCNN.1990.137642', '10.1162/neco.1991.3.4.566', '10.1162/neco.1993.5.2.305', '10.1162/neco.1991.3.2.246', '10.1162/neco.1990.2.2.210', '10.1016/S0893-6080(09)80018-X', '10.1016/0893-6080(91)90009-T', '10.1016/0893-6080(89)90003-8', '10.1007/BF02551274', '10.1049/el:19890300', '10.1109/TAC.1974.1100705', '10.1080/00401706.1979.10489751', '10.1162/neco.1992.4.3.415', '10.1109/72.478392', '10.1109/72.80209', '10.1162/neco.1995.7.1.117', '10.1109/NNSP.1991.239541', '10.1016/0005-1098(78)90005-5', '10.1007/978-1-4471-3087-1_36', '10.1007/BF02506337', '10.1214/aos/1176344136', '10.1007/BF01404567', '10.1162/neco.1993.5.6.954', '10.1162/neco.1991.3.2.213', '10.1016/0893-6080(91)90005-P', '10.1162/neco.1992.4.2.141', '10.1109/ICNN.1994.374471', '10.1162/neco.1994.6.2.270', '10.1109/72.207612', '10.1016/0893-6080(94)90058-2', '10.1142/S0129065792000218', '10.1016/0893-6080(94)90040-X', '10.1117/12.23462', '10.1162/neco.1994.6.6.1262', '10.1109/NNSP.1992.253661', '10.1214/ss/1177010638', '10.1007/978-1-4899-3099-6_2', '10.1109/18.256500', '10.1109/18.382014', '10.1214/aos/1176348546', '10.1049/ip-cta:19951969', '10.1088/0954-898X/6/3/011', '10.1016/S0893-6080(05)80128-5', '10.1109/ICEC.1994.349941', '10.1002/int.4550080406', '10.1109/72.377967', '10.1109/12.257708', '10.1109/ICNN.1994.374191', '10.2307/1390909', '10.1109/72.536312', '10.1109/ICNN.1994.374476', '10.1214/aos/1176350382', '10.1109/72.286906', '10.1142/S0129065792000255', '10.1137/0904023', '10.1007/3-540-61510-5_95', '10.1162/neco.1993.5.3.443', '10.1109/IJCNN.1991.155142', '10.1109/72.329690', '10.1109/IJCNN.1992.226955', '10.1007/BF01299776', '10.1162/neco.1992.4.3.448', '10.7551/mitpress/4932.001.0001', '10.1016/S0893-6080(05)80010-3', '10.1007/BF00114777', '10.1162/neco.1994.6.5.842', '10.1109/72.80287', '10.1145/1968.1972', '10.1016/S0893-6080(05)80136-4', '10.1080/01621459.1981.10477729', '10.1162/neco.1989.1.2.201', '10.1007/978-1-4471-2097-1_189', '10.1080/09540098908915647', '10.1109/72.248452', '10.1109/72.80236', '10.1109/IJCNN.1991.155336', '10.1109/NNSP.1992.253707', '10.1109/72.363426', '10.1016/B978-1-55860-200-7.50049-0', '10.1016/0893-6080(94)00103-S', '10.1016/0031-3203(90)90083-W', '10.1007/BF01415010', '10.1162/neco.1993.5.1.105', '10.1016/0893-6080(94)90091-4', '10.1016/0893-6080(94)90061-2', '10.1016/0893-6080(91)90032-Z', '10.1109/ICNN.1994.374491', '10.1109/ICNN.1994.374217'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Artificial Intelligence', 'Computer Vision'], conference_acronym='IEEE transactions on neural networks', publisher=None, query_handler=None),\n",
       " 'Robust path-based spectral clustering': Paper(DOI='10.1016/j.patcog.2007.04.010', crossref_json=None, google_schorlar_metadata=None, title='Robust path-based spectral clustering', authors=['Hong Chang', 'Dit-Yan Yeung'], abstract='Spectral clustering and path-based clustering are two recently developed clustering approaches that have delivered impressive results in a number of challenging clustering tasks. However, they are not robust enough against noise and outliers in the data. In this paper, based on M-estimation from robust statistics, we develop a robust path-based spectral clustering method by defining a robust path-based similarity measure for spectral clustering under both unsupervised and semi-supervised settings. Our proposed method is significantly more robust than spectral clustering and path-based clustering. We have performed experiments based on both synthetic and real-world data, comparing our method with some other methods. In particular, color images from the Berkeley segmentation data set and benchmark are used in the image segmentation experiments. Experimental results show that our method consistently\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.868688', '10.1109/ICCV.1999.790354', '10.1109/TPAMI.2003.1190577', '10.1007/3-540-44745-8_16', '10.1214/aos/1176342503', '10.1109/34.1000236', '10.1109/TPAMI.2003.1240115', '10.1145/1015330.1015391', '10.2307/2284239', '10.1145/1015706.1015719', '10.1109/TPAMI.2004.1262179', '10.1109/TPAMI.2004.1262185', '10.1145/1015330.1015417'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Artificial Intelligence', 'Computer Vision'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'SVC2004: First international signature verification competition': Paper(DOI='10.1007/978-3-540-25948-0_3', crossref_json=None, google_schorlar_metadata=None, title='SVC2004: First international signature verification competition', authors=['Dit-Yan Yeung', 'Hong Chang', 'Yimin Xiong', 'Susan George', 'Ramanujan Kashi', 'Takashi Matsumoto', 'Gerhard Rigoll'], abstract=' sssHandwritten signature is the most widely accepted biometric for identity verification. To facilitate objective evaluation and comparison of algorithms in the field of automatic handwritten signature verification, we organized the First International Signature Verification Competition (SVC2004) recently as a\\xa0step towards establishing common benchmark databases and benchmarking rules. For each of the two tasks of the competition, a\\xa0signature database involving 100 sets of signature data was created, with 20 genuine signatures and 20 skilled forgeries for each set. Eventually, 13 teams competed for Task\\xa01 and eight teams competed for Task\\xa02. When evaluated on data with skilled forgeries, the best team for Task\\xa01 gives an equal error rate (EER) of 2.84% and that for Task\\xa02 gives an EER of 2.89%. We believe that SVC2004 has successfully achieved its goals and the experience gained from SVC2004 will\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/5.554220', '10.1007/b117227', '10.1016/S0031-3203(01)00240-0', '10.1016/0031-3203(89)90059-9', '10.1142/S0218001494000346'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Artificial Intelligence', 'Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Host-based intrusion detection using dynamic and static behavioral models': Paper(DOI='10.1016/s0031-3203(02)00026-2', crossref_json=None, google_schorlar_metadata=None, title='Host-based intrusion detection using dynamic and static behavioral models', authors=['Dit-Yan Yeung', 'Yuxin Ding'], abstract='Intrusion detection has emerged as an important approach to network security. In this paper, we adopt an anomaly detection approach by detecting possible intrusions based on program or user profiles built from normal usage data. In particular, program profiles based on Unix system calls and user profiles based on Unix shell commands are modeled using two different types of behavioral models for data mining. The dynamic modeling approach is based on hidden Markov models (HMM) and the principle of maximum likelihood, while the static modeling approach is based on event occurrence frequency distributions and the principle of minimum cross entropy. The novelty detection approach is adopted to estimate the model parameters using normal training data only, as opposed to the classification approach which has to use both normal and intrusion data for training. To determine whether or not a certain\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TSE.1987.232894', '10.1016/S1389-1286(98)00017-6', '10.1109/SECPRI.1996.502675', '10.1109/CSAC.1998.738647', '10.1109/IT.1998.713396', '10.1109/SECPRI.1999.766910', '10.1007/3-540-48412-4_32', '10.1145/322510.322526', '10.1016/S0020-0190(00)00122-8', '10.1126/science.1891718', '10.1049/ip-vis:19941330', '10.1145/288090.288122', '10.1109/5.18626', '10.1214/aoms/1177697196', '10.1109/TIT.1980.1056144', '10.1109/TIT.1983.1056747', '10.1214/aoms/1177729694'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Artificial Intelligence', 'Computer Vision'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'Mathematical expression recognition: a survey': Paper(DOI='10.1007/pl00013549', crossref_json=None, google_schorlar_metadata=None, title='Mathematical expression recognition: a survey', authors=['Kam-Fai Chan', 'Dit-Yan Yeung'], abstract='  Abstract. Automatic recognition of mathematical expressions is one of the key vehicles in the drive towards transcribing documents in scientific and engineering disciplines into electronic form. This problem typically consists of two major stages, namely, symbol recognition and structural analysis. In this survey paper, we will review most of the existing work with respect to each of the two major stages of the recognition process. In particular, we try to put emphasis on the similarities and differences between systems. Moreover, some important issues in mathematical expression recognition will be addressed in depth. All these together serve to provide a clear overall picture of how this research area has been developed to date.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Artificial Intelligence', 'Computer Vision'], conference_acronym='International journal on document analysis and recognition (Print)', publisher=None, query_handler=None),\n",
       " 'Enriching Word Vectors with Subword Information': Paper(DOI='10.1162/tacl_a_00051', crossref_json=None, google_schorlar_metadata=None, title='Enriching Word Vectors with Subword Information', authors=['Piotr Bojanowski', 'Edouard Grave', 'Armand Joulin', 'Tomas Mikolov'], abstract=' Continuous word representations, trained on large unlabeled corpora are useful                     for many natural language processing tasks. Popular models that learn such                     representations ignore the morphology of words, by assigning a distinct vector                     to each word. This is a limitation, especially for languages with large                     vocabularies and many rare words. In this paper, we propose a new approach based                     on the skipgram model, where each word is represented as a bag of character                         n-grams. A vector representation is associated                     to each character n-gram; words being represented                     as the sum of these representations. Our method is fast, allowing to train                     models on large corpora quickly and allows us to compute word representations                     for words that did not appear in the training data. We evaluate our word\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1162/coli_a_00016', '10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9', '10.1080/00437956.1954.11659520', '10.3758/BF03204766', '10.2307/1412159', '10.1613/jair.2934'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Unsupervised learning of visual features by contrasting cluster assignments': Paper(DOI='10.3390/vision3030047', crossref_json=None, google_schorlar_metadata=None, title='Unsupervised learning of visual features by contrasting cluster assignments', authors=['Mathilde Caron', 'Ishan Misra', 'Julien Mairal', 'Priya Goyal', 'Piotr Bojanowski', 'Armand Joulin'], abstract='Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the code of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.', conference=None, journal=None, year=None, reference_list=['10.1113/jphysiol.1968.sp008455', '10.1002/9783527680863.ch14', '10.1088/0954-898X_7_2_014', '10.1109/TSP.2018.2846226', '10.1038/srep11400', '10.1016/S0042-6989(97)00169-7', '10.1561/0600000058', '10.1038/nrn1949', '10.1523/JNEUROSCI.6284-11.2012', '10.1038/90526', '10.1038/nrn3136', '10.1152/jn.2002.88.1.455', '10.1007/s10827-006-0003-9', '10.1162/neco_a_00997', '10.1371/journal.pcbi.1005070', '10.1016/j.neucom.2004.01.133', '10.1038/4580', '10.1162/neco.2010.05-08-795', '10.1038/nature04485', '10.1007/BF00275687', '10.1214/009053604000000067', '10.1137/080716542', '10.1523/JNEUROSCI.23-21-07940.2003', '10.1103/PhysRevLett.90.088104', '10.1162/neco_a_01191', '10.1109/TAC.1974.1100705', '10.1109/TNN.2004.833303', '10.1155/2007/90727', '10.1137/1118101', '10.1515/znc-1981-9-1040', '10.1146/annurev.neuro.24.1.1193'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Deep Clustering for Unsupervised Learning of Visual Features': Paper(DOI='10.1007/978-3-030-01264-9_9', crossref_json=None, google_schorlar_metadata=None, title='Deep Clustering for Unsupervised Learning of Visual Features', authors=['Mathilde Caron', 'Piotr Bojanowski', 'Armand Joulin', 'Matthijs Douze'], abstract='Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large-scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2015.13', '10.7551/mitpress/7503.003.0024', '10.1007/978-3-642-35289-8_25', '10.1109/CVPR.2016.512', '10.1007/978-3-642-35289-8_30', '10.1109/CVPR.2009.5206848', '10.1109/ICCV.2015.167', '10.1109/ICCV.2017.226', '10.1109/CVPR.2015.7298761', '10.1007/978-0-387-21606-5', '10.1109/ICCV.2015.123', '10.1109/CVPR.2017.243', '10.1109/CVPR.2010.5539868', '10.1007/978-3-319-46478-7_5', '10.1561/0600000071', '10.1007/978-3-319-46493-0_35', '10.1109/5.726791', '10.1109/ICCV.2011.6126229', '10.1007/978-3-642-21735-7_7', '10.1109/CVPR.2016.320', '10.1007/978-3-319-46466-4_5', '10.1109/ICCV.2017.628', '10.1007/978-3-319-46448-0_48', '10.1109/CVPR.2017.638', '10.1109/CVPR.2016.278', '10.1109/ICCV.2015.19', '10.1109/CVPR.2007.383172', '10.1109/CVPR.2008.4587635', '10.1007/s11263-015-0816-y', '10.1109/CVPRW.2014.131', '10.1109/34.868688', '10.1109/TPAMI.2009.154', '10.1109/ICCV.2015.320', '10.1109/ICCV.2017.149', '10.1109/ICCV.2013.175', '10.1109/CVPR.2016.556', '10.1007/978-3-319-10590-1_53', '10.1007/978-3-319-46487-9_40', '10.1109/CVPR.2017.76'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Xcit: Cross-covariance image transformers': Paper(DOI='10.3390/math11081933', crossref_json=None, google_schorlar_metadata=None, title='Xcit: Cross-covariance image transformers', authors=['Alaaeldin Ali', 'Hugo Touvron', 'Mathilde Caron', 'Piotr Bojanowski', 'Matthijs Douze', 'Armand Joulin', 'Ivan Laptev', 'Natalia Neverova', 'Gabriel Synnaeve', 'Jakob Verbeek', 'Hervé Jégou'], abstract='Following their success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens, ie words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a “transposed” version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) is built upon XCA. It combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including image classification and self-supervised feature learning on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k. We will opensource our code and trained models to reproduce the reported results.', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-030-58452-8_13', '10.1109/ICCV48922.2021.00009', '10.1109/ICCV48922.2021.00060', '10.1109/TPAMI.2022.3206108', '10.1109/CVPR.2009.5206848', '10.1109/ICCVW.2013.77', '10.1109/ICCV48922.2021.00986', '10.1109/ICCV48922.2021.00062', '10.1088/1742-5468/ac9830', '10.1109/TPAMI.2022.3152247', '10.1145/3505244', '10.1109/CVPR.2014.461', '10.1109/CVPR52688.2022.00520', '10.1007/s41095-022-0274-8', '10.1109/ICCV48922.2021.00983', '10.1109/ICCV48922.2021.01172', '10.1109/ICCV48922.2021.00010', '10.1109/CVPR42600.2020.01044', '10.1109/CVPR42600.2020.00815', '10.1109/CVPR.2019.00277'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Weakly supervised action labeling in videos under ordering constraints': Paper(DOI='10.1007/978-3-319-10602-1_41', crossref_json=None, google_schorlar_metadata=None, title='Weakly supervised action labeling in videos under ordering constraints', authors=['Piotr Bojanowski', 'Rémi Lajugie', 'Francis Bach', 'Ivan Laptev', 'Jean Ponce', 'Cordelia Schmid', 'Josef Sivic'], abstract=' We are given a set of video clips, each one annotated with an ordered list of actions, such as “walk” then “sit” then “answer phone” extracted from, for example, the associated text script. We seek to temporally localize the individual actions in each clip as well as to learn a discriminative classifier for each action. We formulate the problem as a weakly supervised temporal assignment with ordering constraints. Each video clip is divided into small time intervals and each time interval of each video clip is assigned one action label, while respecting the order in which the action labels appear in the given annotations. We show that the action label assignment can be determined together with learning a classifier for each action in a discriminative manner. We evaluate the proposed model on a new and challenging dataset of 937 video clips with a total of 787720 frames containing sequences of 16 different actions\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2013.171', '10.1109/ICCV.2013.283', '10.1007/978-3-319-10602-1_41', '10.1109/ICCV.2009.5459279', '10.1002/nav.3800030109', '10.1002/9781118142882', '10.1007/978-0-387-84858-7', '10.1007/BF01908075', '10.1109/34.868686', '10.1111/j.1469-8137.1912.tb05611.x', '10.1109/CVPR.2010.5539868', '10.1109/CVPR.2012.6247719', '10.1007/978-3-642-33718-5_9', '10.1109/CVPR.2011.5995435', '10.1109/CVPR.2008.4587756', '10.1109/CVPR.2007.383074', '10.1109/CVPR.2011.5995353', '10.1007/978-3-642-15552-9_29', '10.1007/978-3-642-33718-5_11', '10.1109/CVPR.2012.6247806', '10.1109/CVPR.2009.5206513', '10.1109/CVPR.2012.6247808', '10.1007/3-540-36592-3_50', '10.1109/CVPR.2011.5995407', '10.1109/ICCV.2013.441'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'A geometric snake model for segmentation of medical imagery': Paper(DOI='10.1109/42.563665', crossref_json=None, google_schorlar_metadata=None, title='A geometric snake model for segmentation of medical imagery', authors=['Anthony Yezzi', 'Satyanad Kichenassamy', 'Arun Kumar', 'Peter Olver', 'Allen Tannenbaum'], abstract='We employ the new geometric active contour models, previously formulated, for edge detection and segmentation of magnetic resonance imaging (MRI), computed tomography (CT), and ultrasound medical imagery. Our method is based on defining feature-based metrics on a given image which in turn leads to a novel snake paradigm in which the feature of interest may be considered to lie at the bottom of a potential well. Thus, the snake is attracted very quickly and efficiently to the desired feature.', conference=None, journal=None, year=None, reference_list=['10.1137/0727053', '10.1016/0021-9991(88)90002-2', '10.1109/34.368173', '10.1007/978-3-0348-8629-1', '10.1137/0721016', '10.1109/ICCV.1995.466850', '10.1007/BF01451741', '10.1007/BFb0014889', '10.1016/0022-247X(92)90260-K', '10.1006/jcph.1995.1098', '10.4310/jdg/1214438998', '10.1007/BF00133570', '10.1090/S0273-0979-1992-00266-5', '10.1007/BF00379537', '10.1109/ICCV.1995.466855', '10.1007/978-1-4684-0152-3', '10.1016/0031-3203(94)90153-8', '10.1007/BF03025885', '10.1109/ICCV.1995.466853', '10.1016/0004-3702(88)90080-X', '10.1109/CVPR.1993.340975', '10.4310/jdg/1214439724', '10.1109/34.56205', '10.1016/1049-9660(91)90028-N', '10.1016/1049-9660(92)90041-Z', '10.1007/978-1-4757-2201-7', '10.1007/978-1-4613-9583-6_2', '10.4310/jdg/1214439902', '10.4310/jdg/1214441371', '10.1007/BF00375127', '10.1137/0729052', '10.1007/BF01385685', '10.1109/ICCV.1995.466871', '10.1007/BF01210742', '10.1016/0021-9991(92)90140-T', '10.4310/jdg/1214444092', '10.1109/42.387714', '10.1007/978-1-4612-5282-5', '10.1109/ICIP.1994.413615', '10.1512/iumj.1993.42.42046'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Image Processing', 'Shape Optimization', \"Geometric PDE's\"], conference_acronym='IEEE transactions on medical imaging (Print)', publisher=None, query_handler=None),\n",
       " 'Shapes, shocks, and deformations I: the components of two-dimensional shape and the reaction-diffusion space': Paper(DOI='10.1007/bf01451741', crossref_json=None, google_schorlar_metadata=None, title='Shapes, shocks, and deformations I: the components of two-dimensional shape and the reaction-diffusion space', authors=['Benjamin B Kimia', 'Allen R Tannenbaum', 'Steven W Zucker'], abstract=' We undertake to develop a general theory of two-dimensional shape by elucidating several principles which any such theory should meet. The principles are organized around two basic intuitions: first, if a boundary were changed only slightly, then, in general, its shape would change only slightly. This leads us to propose an operational theory of shape based on incremental contour deformations. The second intuition is that not all contours are shapes, but rather only those that can enclose “physical” material. A theory of contour deformation is derived from these principles, based on abstract conservation principles and Hamilton-Jacobi theory. These principles are based on the work of Sethian (1985a, c), the Osher-Sethian (1988), level set formulation the classical shock theory of Lax (1971; 1973), as well as curve evolution theory for a curve evolving as a function of the curvature and the relation to\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1137/0729052', '10.1109/ICCV.1993.378217', '10.1007/978-1-4757-2063-1', '10.1109/TPAMI.1986.4767747', '10.1037/h0054663', '10.1145/358645.358661', '10.1037/0033-295X.94.2.115', '10.1016/0004-3702(81)90025-4', '10.7551/mitpress/7132.001.0001', '10.1016/0022-5193(73)90175-6', '10.1177/027836498200100304', '10.1109/ICASSP.1992.226260', '10.1016/0004-3702(81)90028-X', '10.1073/pnas.89.1.60', '10.1090/S0273-0979-1992-00266-5', '10.1090/S0002-9947-1983-0690039-8', '10.1109/TPAMI.1987.4767937', '10.1016/0042-6989(93)90080-G', '10.1109/TPAMI.1982.4767244', '10.1112/S0025579300005714', '10.1109/T-C.1973.223602', '10.1145/356625.356627', '10.4310/jdg/1214439902', '10.1007/BF00355544', '10.1109/TC.1972.5008926', '10.2307/1971486', '10.1016/0010-0277(84)90022-2', '10.1002/cpa.3160030302', '10.1007/BF00133570', '10.1007/BFb0014889', '10.1016/0022-247X(92)90260-K', '10.1007/BF00336961', '10.1007/BF00318204', '10.1103/RevModPhys.52.1', '10.1002/cpa.3160100406', '10.1137/1.9781611970562', '10.1007/BF00054839', '10.1016/0734-189X(87)90117-4', '10.1016/0004-3702(88)90039-2', '10.1163/156856887X00187', '10.1098/rspb.1978.0020', '10.1109/TPAMI.1986.4767750', '10.1090/trans2/026/05', '10.1016/0021-9991(88)90002-2', '10.21236/ADA461783', '10.1162/neco.1989.1.1.82', '10.1109/34.56205', '10.1109/TSMC.1977.4309681', '10.1109/TPAMI.1987.4767938', '10.1016/S0146-664X(72)80017-0', '10.1364/JOSAA.3.001483', '10.1016/S0734-189X(85)80004-9', '10.1016/0010-0285(76)90013-X', '10.1145/358826.358836', '10.1016/0031-3203(93)90142-J', '10.1007/BF01420591', '10.1006/jfan.1994.1004', '10.1007/978-3-322-87869-4_53', '10.1007/BF01210742', '10.4310/jdg/1214444092', '10.1109/34.368189', '10.1016/0094-5765(77)90096-0', '10.1016/0021-9991(81)90140-6', '10.1109/34.44401', '10.1016/0165-1684(91)90025-E', '10.1016/S0146-664X(80)80035-9', '10.1109/TC.1972.5008949', '10.1162/neco.1989.1.1.68'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Medical Imaging', 'Systems and Control', 'Computer Vision', 'Applied Mathematics', 'Image Processing'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Robust control of linear time-invariant plants using periodic compensation': Paper(DOI='10.1109/tac.1985.1103841', crossref_json=None, google_schorlar_metadata=None, title='Robust control of linear time-invariant plants using periodic compensation', authors=['Poolla Khargonekar', 'Kameshwar Poolla', 'Allen Tannenbaum'], abstract='This paper considers the use and design of linear periodic time-varying controllers for the feedback control of linear time-invariant discrete-time plants. We will show that for a large class of robustness problems, periodic compensators are superior to time-invariant ones. We will give explicit design techniques which can be easily implemented. In the context of periodic controllers, we also consider the strong and simultaneous stabilization problems. Finally, we show that for the problem of weighted sensitivity minimization for linear time-invariant plants, time-varying controllers offer no advantage over the time-invariant ones.', conference=None, journal=None, year=None, reference_list=['10.1109/TAC.1983.1103299', '10.1109/TAC.1959.6429398', '10.1109/TAC.1985.1103805', '10.1109/TAC.1984.1103663', '10.1109/TAC.1981.1102565', '10.1109/TCS.1975.1084020', '10.1109/TAC.1982.1103005', '10.1080/00207178008922838', '10.1109/CDC.1983.269806', '10.1137/0310001', '10.1109/TAC.1981.1102555', '10.1016/0167-6911(84)90011-2', '10.1109/TAC.1984.1103409', '10.1109/TAC.1984.1103387', '10.1109/TAC.1981.1102770', '10.1109/TAC.1982.1103086', '10.1016/0005-1098(74)90021-1', '10.1109/TAC.1982.1102967', '10.1109/TAC.1983.1103275', '10.1109/TAC.1981.1102603', '10.1016/0016-0032(75)90161-1', '10.1016/0005-1098(85)90005-6'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Medical Imaging', 'Systems and Control', 'Computer Vision', 'Applied Mathematics', 'Image Processing'], conference_acronym='IEEE transactions on automatic control (Print)', publisher=None, query_handler=None),\n",
       " 'Behavioral analysis of anisotropic diffusion in image processing': Paper(DOI='10.1109/83.541424', crossref_json=None, google_schorlar_metadata=None, title='Behavioral analysis of anisotropic diffusion in image processing', authors=['Yu-Li You', 'Wenyuan Xu', 'Allen Tannenbaum', 'Mostafa Kaveh'], abstract='In this paper, we analyze the behavior of the anisotropic diffusion model of Perona and Malik (1990). The main idea is to express the anisotropic diffusion equation as coming from a certain optimization problem, so its behavior can be analyzed based on the shape of the corresponding energy surface. We show that anisotropic diffusion is the steepest descent method for solving an energy minimization problem. It is demonstrated that an anisotropic diffusion is well posed when there exists a unique global minimum for the energy functional and that the ill posedness of a certain anisotropic diffusion is caused by the fact that its energy functional has an infinite number of global minima that are dense in the image space. We give a sufficient condition for an anisotropic diffusion to be well posed and a sufficient and necessary condition for it to be ill posed due to the dense global minima. The mechanism of smoothing and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.4310/jdg/1214444092', '10.1007/BF01210742', '10.1007/BF01420591', '10.1109/ICIP.1994.413620', '10.1006/ciun.1993.1006', '10.1007/978-94-017-1699-4_4', '10.1090/S0273-0979-1992-00266-5', '10.4310/jdg/1214446559', '10.4310/jdg/1214439902', '10.4310/jdg/1214441371', '10.2307/1971486', '10.1007/978-94-009-7189-9_26', '10.1137/S003613999529558X', '10.2307/1971426', '10.1109/34.56205', '10.1016/0167-2789(92)90242-F', '10.2307/2944327', '10.1137/0729012', '10.1109/5.5962', '10.1007/BF01385685', '10.1137/0729052', '10.1007/978-3-0348-8629-1', '10.1016/0022-247X(92)90260-K', '10.1007/978-1-4612-5773-8', '10.1109/34.149591', '10.1016/0021-9991(88)90002-2', '10.1109/34.149593'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Medical Imaging', 'Systems and Control', 'Computer Vision', 'Applied Mathematics', 'Image Processing'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Conformal curvature flows: from phase transitions to active vision': Paper(DOI='10.1007/bf00379537', crossref_json=None, google_schorlar_metadata=None, title='Conformal curvature flows: from phase transitions to active vision', authors=['Satyanad Kichenassamy', 'Arun Kumar', 'Peter Olver', 'Allen Tannenbaum', 'Anthony Yezzi'], abstract=' In this paper, we analyze geometric active contour models from a curve evolution point of view and propose some modifications based on gradient flows relative to certain new feature-based Riemannian metrics. This leads to a novel edge-detection paradigm in which the feature of interest may be considered to lie at the bottom of a potential well. Thus an edge-seeking curve is attracted very naturally and efficiently to the desired feature. Comparison with the Allen-Cahn model clarifies some of the choices made in these models, and suggests inhomogeneous models which may in return be useful in phase transitions. We also consider some 3-dimensional active surface models based on these ideas. The justification of this model rests on the careful study of the viscosity solutions of evolution equations derived from a level-set approach.', conference=None, journal=None, year=None, reference_list=['10.1016/0001-6160(79)90196-2', '10.1137/0729052', '10.1007/BF01191340', '10.4310/jdg/1214446558', '10.1007/BF01041068', '10.1080/00029890.1980.11995034', '10.1016/0022-0396(91)90147-2', '10.1007/BF00375607', '10.1103/PhysRevA.39.5887', '10.4310/jdg/1214446564', '10.4310/jdg/1214439724', '10.1090/S0273-0979-1992-00266-5', '10.1007/978-1-4757-2201-7', '10.1007/978-1-4613-9583-6_2', '10.4310/jdg/1214446559', '10.1002/cpa.3160450903', '10.1007/BF01388602', '10.4310/jdg/1214439902', '10.4310/jdg/1214445048', '10.1215/S0012-7094-89-05825-0', '10.4310/jdg/1214441371', '10.2307/1971486', '10.1007/BF00251518', '10.1007/BF00281354', '10.4310/jdg/1214438998', '10.1512/iumj.1992.41.41036', '10.1007/BF00133570', '10.1007/BFb0014889', '10.1016/0022-247X(92)90260-K', '10.1007/978-3-0348-8629-1', '10.1109/34.368173', '10.1109/34.149591', '10.1063/1.1722742', '10.1137/0721016', '10.1016/0021-9991(88)90002-2', '10.1137/0727053', '10.1109/34.56205', '10.1007/978-1-4612-5282-5', '10.1137/0149007', '10.1007/BF01420591', '10.1512/iumj.1993.42.42046', '10.1007/BF01210742', '10.4310/jdg/1214444092', '10.1016/0021-9991(92)90140-T', '10.1007/978-1-4684-0152-3', '10.1017/CBO9780511753138', '10.1016/0004-3702(88)90080-X', '10.1007/BF03025885'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Image Processing', 'Shape Optimization', \"Geometric PDE's\"], conference_acronym='Archive for rational mechanics and analysis (Print)', publisher=None, query_handler=None),\n",
       " 'Affine invariant scale-space': Paper(DOI='10.1016/j.neucom.2016.06.094', crossref_json=None, google_schorlar_metadata=None, title='Affine invariant scale-space', authors=['Guillermo Sapiro', 'Allen Tannenbaum'], abstract=' A newaffine invariant scale-space for planar curves is presented in this work. The scale-space is obtained from the solution of a novel nonlinear curve evolution equation which admits affine invariant solutions. This flow was proved to be the affine analogue of the well knownEuclidean shortening flow. The evolution also satisfies properties such ascausality, which makes it useful in defining a scale-space. Using an efficient numerical algorithm for curve evolution, this continuous affine flow is implemented, and examples are presented. The affine-invariant progressive smoothing property of the evolution equation is demonstrated as well.', conference=None, journal=None, year=None, reference_list=['10.1016/j.neucom.2015.01.019', '10.1109/TIP.2012.2199502', '10.1016/j.ins.2014.04.053', '10.1016/j.ins.2014.11.014', '10.1109/2.410145', '10.1145/1126004.1126005', '10.1109/34.993558', '10.1109/TIP.2012.2202676', '10.1023/A:1008097225773', '10.1109/LSP.2012.2190060', '10.1007/BF01469346', '10.1109/TIP.2014.2364536', '10.1016/j.neucom.2015.05.056', '10.1023/B:VISI.0000029664.99615.94', '10.1016/B978-0-12-407701-0.00001-7', '10.1023/A:1008045108935', '10.1109/TPAMI.2005.188', '10.1016/j.ins.2014.03.016', '10.1016/j.patrec.2010.01.020', '10.1109/4.996', '10.1109/TIP.2014.2319735', '10.1023/A:1008300826001'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Medical Imaging', 'Systems and Control', 'Computer Vision', 'Applied Mathematics', 'Image Processing'], conference_acronym='Neurocomputing (Amsterdam)', publisher=None, query_handler=None),\n",
       " 'Optimal mass transport for registration and warping': Paper(DOI='10.54294/gd9piz', crossref_json=None, google_schorlar_metadata=None, title='Optimal mass transport for registration and warping', authors=['Steven Haker', 'Lei Zhu', 'Allen Tannenbaum', 'Sigurd Angenent'], abstract=' Image registration is the process of establishing a common geometric reference frame between two or more image data sets possibly taken at different times. In this paper we present a method for computing elastic registration and warping maps based on the Monge–Kantorovich theory of optimal mass transport. This mass transport method has a number of important characteristics. First, it is parameter free. Moreover, it utilizes all of the grayscale data in both images, places the two images on equal footing and is symmetrical: the optimal mapping from image A to image B being the inverse of the optimal mapping from B to A. The method does not require that landmarks be specified, and the minimizer of the distance functional involved is unique; there are no other local minimizers. Finally, optimal transport naturally takes into account changes in density that result from changes in area or volume. Although the\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Medical Imaging', 'Systems and Control', 'Computer Vision', 'Applied Mathematics', 'Image Processing'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Conformal surface parameterization for texture mapping': Paper(DOI='10.1109/2945.856998', crossref_json=None, google_schorlar_metadata=None, title='Conformal surface parameterization for texture mapping', authors=['Steven Haker', 'Sigurd Angenent', 'Allen Tannenbaum', 'Ron Kikinis', 'Guillermo Sapiro', 'Michael Halle'], abstract='We give an explicit method for mapping any simply connected surface onto the sphere in a manner which preserves angles. This technique relies on certain conformal mappings from differential geometry. Our method provides a new way to automatically assign texture coordinates to complex undulating surfaces. We demonstrate a finite element method that can be used to apply our mapping technique to a triangulated geometric description of a surface.', conference=None, journal=None, year=None, reference_list=['10.1109/MCG.1986.276672', '10.1145/218380.218440', '10.1007/BFb0082865', '10.1145/127719.122723', '10.1109/42.796283', '10.1007/978-1-4615-5223-9_20', '10.1145/122718.122744', '10.1109/MCG.1986.276545', '10.1145/133994.134071', '10.1007/978-1-4612-0953-9', '10.1109/42.650881', '10.1016/S1361-8415(96)80008-9', '10.1007/978-1-4757-2201-7', '10.1109/42.544496', '10.1145/280814.280828', '10.1007/BF00379537', '10.1145/800248.507101', '10.1145/360349.360353', '10.1145/280814.280930'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Medical Imaging', 'Systems and Control', 'Computer Vision', 'Applied Mathematics', 'Image Processing'], conference_acronym='IEEE transactions on visualization and computer graphics', publisher=None, query_handler=None),\n",
       " 'A lifting technique for linear periodic systems with applications to sampled-data control': Paper(DOI='10.1016/0167-6911(91)90033-b', crossref_json=None, google_schorlar_metadata=None, title='A lifting technique for linear periodic systems with applications to sampled-data control', authors=['Bassam Bamieh', 'J Boyd Pearson', 'Bruce A Francis', 'Allen Tannenbaum'], abstract='A lifting technique is developed for periodic linear systems and applied to the H∞ and H 2 sampled-data control problems.', conference=None, journal=None, year=None, reference_list=['10.1016/0167-6911(90)90114-A', '10.1109/9.29425', '10.1137/0325038', '10.1137/0519072', '10.1007/BFb0007371', '10.1080/00207178908559734', '10.1109/TAC.1985.1103841', '10.1016/0005-1098(91)90060-F', '10.1109/9.8655', '10.1109/9.53529'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['control theory', 'control', 'control systems', 'system theory', 'robotics'], conference_acronym='Systems & control letters (Print)', publisher=None, query_handler=None),\n",
       " 'Image segmentation using active contours driven by the Bhattacharyya gradient flow': Paper(DOI='10.1109/tip.2007.908073', crossref_json=None, google_schorlar_metadata=None, title='Image segmentation using active contours driven by the Bhattacharyya gradient flow', authors=['Oleg Michailovich', 'Yogesh Rathi', 'Allen Tannenbaum'], abstract='This paper addresses the problem of image segmentation by means of active contours, whose evolution is driven by the gradient flow derived from an energy functional that is based on the Bhattacharyya distance. In particular, given the values of a photometric variable (or of a set thereof), which is to be used for classifying the image pixels, the active contours are designed to converge to the shape that results in maximal discrepancy between the empirical distributions of the photometric variable inside and outside of the contours. The above discrepancy is measured by means of the Bhattacharyya distance that proves to be an extremely useful tool for solving the problem at hand. The proposed methodology can be viewed as a generalization of the segmentation methods, in which active contours maximize the difference between a finite number of empirical moments of the ldquoinsiderdquo and ldquooutsiderdquo\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.192463', '10.1006/gmip.1993.1040', '10.1007/978-1-4899-3324-9', '10.1007/b97541', '10.1109/ICCV.1999.790317', '10.1007/978-1-4757-1923-9', '10.1007/978-3-642-97522-6', '10.1080/01621459.1962.10480660', '10.1007/978-1-4612-4026-6', '10.1109/TIP.2003.821445', '10.1364/JOSAA.21.001231', '10.1109/83.902291', '10.1109/TMI.2004.824224', '10.1109/TMI.2002.808355', '10.1109/TIP.2005.847286', '10.1109/TCOM.1967.1089532', '10.1214/aoms/1177729694', '10.1002/0471200611', '10.1214/aoms/1177729330', '10.1137/0912004', '10.1002/0471221317', '10.2307/1268623', '10.1007/s11263-006-0635-2', '10.1109/TIP.2006.877317', '10.1109/TPAMI.2004.11', '10.1109/TIP.2005.854442', '10.1023/A:1020874308076', '10.1109/JOE.2002.808199', '10.1109/34.537343', '10.1109/83.469936', '10.1109/TPAMI.1986.4767851', '10.1109/42.563665', '10.1109/TPAMI.1984.4767596', '10.1002/cpa.3160420503', '10.1109/34.368173', '10.1109/34.44407', '10.1109/34.868683', '10.1109/83.817599', '10.1109/70.678450', '10.1109/56.788', '10.1109/TNET.2005.846901', '10.1109/TPAMI.2002.1033221', '10.1162/neco.1996.8.1.129', '10.1109/7.53409', '10.1109/34.368149', '10.1109/TIP.2003.819859', '10.1080/01621459.1996.10476701', '10.1016/0004-3702(81)90024-2', '10.1007/978-0-387-21540-2', '10.1016/0167-8655(94)90069-8', '10.1109/ICPR.1990.118074'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Medical Imaging', 'Systems and Control', 'Computer Vision', 'Applied Mathematics', 'Image Processing'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Hamilton-jacobi skeletons': Paper(DOI='10.1016/s1076-5670(04)35001-9', crossref_json=None, google_schorlar_metadata=None, title='Hamilton-jacobi skeletons', authors=['Kaleem Siddiqi', 'Sylvain Bouix', 'Allen Tannenbaum', 'Steven W Zucker'], abstract=\" The eikonal equation and variants of it are of significant interest for problems in computer vision and image processing. It is the basis for continuous versions of mathematical morphology, stereo, shape-from-shading and for recent dynamic theories of shape. Its numerical simulation can be delicate, owing to the formation of singularities in the evolving front and is typically based on level set methods. However, there are more classical approaches rooted in Hamiltonian physics which have yet to be widely used by the computer vision community. In this paper we review the Hamiltonian formulation, which offers specific advantages when it comes to the detection of singularities or shocks. We specialize to the case of Blum's grassfire flow and measure the average outward flux of the vector field that underlies the Hamiltonian system. This measure has very different limiting behaviors depending upon whether the\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1016/S0925-7721(01)00017-7', '10.1016/0167-8655(92)90074-A', '10.1109/TPAMI.1985.4767685', '10.1006/cviu.1997.0536', '10.1016/0167-8655(95)00034-E', '10.1016/0022-5193(73)90175-6', '10.1016/0734-189X(84)90035-5', '10.1016/S0031-3203(98)00082-X', '10.1023/A:1023761518825', '10.1109/TPAMI.2004.1262192', '10.1142/S0218195991000220', '10.1006/cviu.1995.1062', '10.1016/0734-189X(89)90147-3', '10.1006/cgip.1994.1042', '10.1109/34.107013', '10.1007/BF01420736', '10.1016/S0262-8856(97)00074-7', '10.1090/pspum/040.2/713249', '10.1007/BF01918532', '10.1016/0021-9991(88)90002-2', '10.1006/cviu.1998.0680', '10.1137/0729053', '10.1073/pnas.93.4.1591', '10.1109/2945.489387', '10.1109/2945.489386', '10.1023/A:1016376116653', '10.1006/jcph.1994.1155', '10.1109/34.42838'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'image analysis', 'biological shape'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'On affine plane curve evolution': Paper(DOI='10.1006/jfan.1994.1004', crossref_json=None, google_schorlar_metadata=None, title='On affine plane curve evolution', authors=['Guillermo Sapiro', 'Allen Tannenbaum'], abstract='An affine invariant curve evolution process is presented in this work. The evolution studied is the affine analogue of the Euclidean Curve Shortening flow. Evolution equations, for both affine and Euclidean invariants, are developed. An affine version of the classical (Euclidean) isoperimetric inequality is proved. This inequality is used to show that in the case of affine evolution of convex plane curves, the affine isoperimetric ratio is a non-decreasing function of time. Convergence of this affine isoperimetric ratio to the ellipse′s value (8π2), as well as convergence, in the Hausdorff metric, of the evolving curve to an ellipse, is also proved.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Medical Imaging', 'Systems and Control', 'Computer Vision', 'Applied Mathematics', 'Image Processing'], conference_acronym='Journal of functional analysis (Print)', publisher=None, query_handler=None),\n",
       " 'Non-Euclidian metrics and the robust stabilization of systems with parameter uncertainty': Paper(DOI='10.1109/tac.1985.1103805', crossref_json=None, google_schorlar_metadata=None, title='Non-Euclidian metrics and the robust stabilization of systems with parameter uncertainty', authors=['Pramod Khargonekar', 'Allen Tannenbaum'], abstract='This paper considers, from a complex function theoretic point of view, certain kinds of robust synthesis problems. In particular, we use a certain kind of metric on the disk (the \"hyperbolic\" metric) which allows us to reduce the problem of robust stabilization of systems with many types of real and complex parameter variations to an easily solvable problem in non-Euclidian geometry. It is shown that several apparently different problems can be treated in a unified general framework. A new result on the gain margin problem for multivariable plants is also given. Finally, we apply our methods to systems with real zero or pole variations.', conference=None, journal=None, year=None, reference_list=['10.1109/TAC.1981.1102603', '10.1016/0016-0032(67)90582-0', '10.1016/0005-1098(74)90021-1', '10.1109/TAC.1983.1103275', '10.1109/CDC.1981.269342', '10.1109/TAC.1983.1103172', '10.1109/TAC.1981.1102603', '10.1090/S0273-0979-1982-15001-7', '10.1080/00207727908941638', '10.1080/00207177808922376', '10.1109/TAC.1979.1102052', '10.1109/TAC.1984.1103663', '10.1090/mmono/050', '10.1080/00207178208932897', '10.1137/0136005', '10.1109/TAC.1976.1101301', '10.1109/TAC.1984.1103387', '10.1109/TCS.1978.1084408', '10.1109/CDC.1982.268218', '10.1109/TAC.1984.1103357', '10.1109/CDC.1982.268217', '10.1007/BF01691925', '10.1007/978-3-642-48895-5_5', '10.1007/BF01701500', '10.1007/BF01456817', '10.1080/00207178008922838'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Medical Imaging', 'Systems and Control', 'Computer Vision', 'Applied Mathematics', 'Image Processing'], conference_acronym='IEEE transactions on automatic control (Print)', publisher=None, query_handler=None),\n",
       " 'Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation': Paper(DOI='10.1016/j.media.2016.10.004', crossref_json=None, google_schorlar_metadata=None, title='Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation', authors=['Konstantinos Kamnitsas', 'Christian Ledig', 'Virginia FJ Newcombe', 'Joanna P Simpson', 'Andrew D Kane', 'David K Menon', 'Daniel Rueckert', 'Ben Glocker'], abstract='We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network’s soft segmentation, we use a 3D fully\\xa0…', conference=None, journal=None, year=None, reference_list=['10.2217/fnl.13.39', '10.1089/neu.2008.0683', '10.1109/TMI.2011.2138152', '10.1016/j.neurobiolaging.2008.04.008', '10.1016/j.nicl.2012.08.002', '10.1016/S0140-6736(07)61194-5', '10.1109/5.726791', '10.1016/j.media.2014.12.003', '10.1080/0269905040004310', '10.1227/NEU.0000000000000575', '10.1016/j.media.2016.07.009', '10.1109/TMI.2014.2377694', '10.1016/j.neuroimage.2014.04.056', '10.1136/jnnp-2012-302644', '10.1016/j.media.2004.06.007', '10.1016/j.ejrad.2008.02.044', '10.1016/j.neuroimage.2011.11.032', '10.1093/brain/awr175', '10.1038/nature16961', '10.1016/S0031-3203(98)00091-0', '10.1109/42.811270', '10.1089/neu.2010.1429', '10.1200/JCO.2009.26.3541', '10.1089/neu.2011.2008'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Medical Image Analysis'], conference_acronym='Medical image analysis (Print)', publisher=None, query_handler=None),\n",
       " 'Attention gated networks: Learning to leverage salient regions in medical images': Paper(DOI='10.1016/j.media.2019.01.012', crossref_json=None, google_schorlar_metadata=None, title='Attention gated networks: Learning to leverage salient regions in medical images', authors=['Jo Schlemper', 'Ozan Oktay', 'Michiel Schaap', 'Mattias Heinrich', 'Bernhard Kainz', 'Ben Glocker', 'Daniel Rueckert'], abstract='We propose a novel attention gate (AG) model for medical image analysis that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules when using convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN models such as VGG or U-Net architectures with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed AG models are evaluated on a variety of tasks, including medical image classification and segmentation. For classification, we demonstrate the use case of AGs in scan plane detection for fetal ultrasound screening. We show that the proposed attention\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2018.00636', '10.1109/TMI.2017.2712367', '10.18653/v1/D17-1151', '10.1038/nature21056', '10.1007/s11548-018-1797-4', '10.1109/CVPR.2018.00745', '10.1016/j.media.2016.10.004', '10.1016/j.media.2018.10.004', '10.18653/v1/D15-1166', '10.1038/s41746-017-0013-1', '10.1109/CVPR.2017.39', '10.1016/j.media.2018.01.006', '10.1016/j.media.2015.11.003', '10.1609/aaai.v32i1.11941', '10.1109/CVPR.2017.683', '10.1109/CVPR.2018.00813', '10.1109/TMI.2013.2265805', '10.1109/CVPR.2018.00864', '10.3174/ajnr.A5543', '10.1007/s11633-017-1053-3'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Medical Image Analysis', 'Computer Vision', 'Machine Learning'], conference_acronym='Medical image analysis (Print)', publisher=None, query_handler=None),\n",
       " 'Anatomically constrained neural networks (ACNNs): application to cardiac image enhancement and segmentation': Paper(DOI='10.1109/tmi.2017.2743464', crossref_json=None, google_schorlar_metadata=None, title='Anatomically constrained neural networks (ACNNs): application to cardiac image enhancement and segmentation', authors=['Ozan Oktay', 'Enzo Ferrante', 'Konstantinos Kamnitsas', 'Mattias Heinrich', 'Wenjia Bai', 'Jose Caballero', 'Stuart A Cook', 'Antonio De Marvao', 'Timothy Dawes', 'Declan P O‘Regan', 'Bernhard Kainz', 'Ben Glocker', 'Daniel Rueckert'], abstract='Incorporation of prior knowledge about organ shape and location is key to improve performance of image analysis approaches. In particular, priors can be useful in cases where images are corrupted and contain artefacts due to limitations in image acquisition. The highly constrained nature of anatomical objects can be well captured with learning-based techniques. However, in most recent and promising techniques such as CNN-based segmentation it is not obvious how to incorporate such prior knowledge. State-of-the-art methods operate as pixel-wise classifiers where the training objectives do not incorporate the structure and inter-dependencies of the output. To overcome this limitation, we propose a generic training strategy that incorporates anatomical prior knowledge into CNNs through a new regularisation model, which is trained end-to-end. The new framework encourages models to follow the global\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-319-24574-4_28', '10.1109/TMI.2016.2597270', '10.1007/978-3-642-21735-7_7', '10.1007/978-3-319-66182-7_24', '10.1007/978-3-319-59050-9_49', '10.1186/s12968-016-0227-4', '10.1007/978-3-319-46726-9_29', '10.1109/CVPR.2016.398', '10.1109/CVPR.2015.7298965', '10.1016/j.media.2016.01.005', '10.1093/comjnl/bxm075', '10.1016/j.media.2016.10.004', '10.1007/978-3-319-46475-6_43', '10.1016/j.jacc.2009.04.094', '10.1016/j.echo.2011.11.010', '10.1016/j.media.2016.11.004', '10.1007/978-3-319-46723-8_56', '10.1016/0262-8856(95)99727-I', '10.1109/TMI.2003.809688', '10.1186/1532-429X-16-16', '10.1007/978-3-319-46475-6_25', '10.1109/CVPR.2012.6247702', '10.1007/978-3-319-46466-4_29', '10.1016/j.media.2015.08.009', '10.1002/mrm.26631', '10.1016/j.ultrasmedbio.2012.08.008', '10.1109/TMI.2013.2256922', '10.1109/TMI.2015.2503890', '10.1007/978-3-319-46723-8_53', '10.1109/CVPR.2010.5539957', '10.1109/CVPR.2013.244', '10.1109/TPAMI.2009.186', '10.1109/TIP.2003.819861', '10.1109/CVPR.2016.207', '10.1109/CVPR.2008.4587633'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Biomedical Image Analysis', 'Computer Vision'], conference_acronym='IEEE transactions on medical imaging (Print)', publisher=None, query_handler=None),\n",
       " 'Automated cardiovascular magnetic resonance image analysis with fully convolutional networks': Paper(DOI='10.1186/s12968-018-0516-1', crossref_json=None, google_schorlar_metadata=None, title='Automated cardiovascular magnetic resonance image analysis with fully convolutional networks', authors=['Wenjia Bai', 'Matthew Sinclair', 'Giacomo Tarroni', 'Ozan Oktay', 'Martin Rajchl', 'Ghislain Vaillant', 'Aaron M Lee', 'Nay Aung', 'Elena Lukaschuk', 'Mihir M Sanghvi', 'Filip Zemrak', 'Kenneth Fung', 'Jose Miguel Paiva', 'Valentina Carapella', 'Young Jin Kim', 'Hideaki Suzuki', 'Bernhard Kainz', 'Paul M Matthews', 'Steffen E Petersen', 'Stefan K Piechnik', 'Stefan Neubauer', 'Ben Glocker', 'Daniel Rueckert'], abstract='Cardiovascular resonance (CMR) imaging is a standard imaging modality for assessing cardiovascular diseases (CVDs), the leading cause of death globally. CMR enables accurate quantification of the cardiac chamber volume, ejection fraction and myocardial mass, providing information for diagnosis and monitoring of CVDs. However, for years, clinicians have been relying on manual approaches for CMR image analysis, which is time consuming and prone to subjective errors. It is a major clinical challenge to automatically derive quantitative and clinically relevant information from CMR images. Deep neural networks have shown a great potential in image pattern recognition and segmentation for a variety of tasks. Here we demonstrate an automated analysis method for CMR images, which is based on a fully convolutional network (FCN). The network is trained and evaluated on a large-scale dataset from the UK Biobank, consisting of 4,875 subjects with 93,500 pixelwise annotated images. The performance of the method has been evaluated using a number of technical metrics, including the Dice metric, mean contour distance and Hausdorff distance, as well as clinically relevant measures, including left ventricle (LV) end-diastolic volume (LVEDV) and end-systolic volume (LVESV), LV mass (LVM); right ventricle (RV) end-diastolic volume (RVEDV) and end-systolic volume (RVESV). By combining FCN with a large-scale annotated dataset, the proposed automated method achieves a high performance in segmenting the LV and RV on short-axis CMR images and the left atrium (LA) and right atrium (RA) on long-axis CMR images. On a short\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1002/mrm.20110', '10.1186/1532-429X-12-69', '10.1002/mrm.24878', '10.1148/radiol.14140296', '10.1002/mrm.25387', '10.1186/s12968-017-0389-8', '10.1148/radiol.12112721', '10.1016/j.jcmg.2015.12.001', '10.1002/mrm.25285', '10.1186/s12968-015-0182-5', '10.1118/1.4764915', '10.1002/mrm.23153', '10.1002/mrm.25270', '10.1002/mrm.27068', '10.1002/jmri.25255', '10.1186/1532-429X-15-92', '10.1002/mrm.26841', '10.1016/j.media.2017.04.002', '10.1016/j.media.2016.01.005', '10.1097/RCT.0000000000000312', '10.1002/jmri.24338', '10.1049/iet-cvi.2016.0482', '10.1016/j.media.2016.05.009', '10.1002/mrm.26631', '10.1016/j.jcmg.2018.04.030.', '10.1146/annurev-bioeng-071516-044442', '10.1007/978-3-319-24574-4_28', '10.1109/TASSP.1986.1164959', '10.1109/TMI.2016.2528162', '10.1016/S1076-6332(03)00671-8', '10.1109/TMI.2016.2538465', '10.1109/TMI.2015.2482920', '10.1109/TMI.2016.2535302', '10.1109/TMI.2017.2743464', '10.1186/s12968-018-0471-x', '10.1016/j.media.2018.05.008', '10.1002/mrm.10394', '10.1118/1.3691905', '10.1177/0161734615613322', '10.1002/jmri.21586', '10.1186/1532-429X-13-40'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Medical Image Analysis', 'Computer Vision', 'Machine Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Dense image registration through MRFs and efficient linear programming': Paper(DOI='10.1016/j.media.2008.03.006', crossref_json=None, google_schorlar_metadata=None, title='Dense image registration through MRFs and efficient linear programming', authors=['Ben Glocker', 'Nikos Komodakis', 'Georgios Tziritas', 'Nassir Navab', 'Nikos Paragios'], abstract='In this paper, we introduce a novel and efficient approach to dense image registration, which does not require a derivative of the employed cost function. In such a context, the registration problem is formulated using a discrete Markov random field objective function. First, towards dimensionality reduction on the variables we assume that the dense deformation field can be expressed using a small number of control points (registration grid) and an interpolation strategy. Then, the registration cost is expressed using a discrete sum over image costs (using an arbitrary similarity measure) projected on the control points, and a smoothness term that penalizes local deviations on the deformation field according to a neighborhood system on the grid. Towards a discrete approach, the search space is quantized resulting in a fully discrete model. In order to account for large deformations and produce results on a high\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/S1077-3142(03)00002-X', '10.1006/gmod.2000.0531', '10.1007/BF00054995', '10.1109/42.481446', '10.1109/34.824822', '10.1109/TPAMI.1984.4767596', '10.1007/3-540-45468-3_62', '10.1007/978-3-540-75759-7_65', '10.1007/978-3-540-73273-0_34', '10.1109/CVPR.2008.4587562', '10.1109/TMI.2002.808365', '10.1023/A:1020830525823', '10.1126/science.220.4598.671', '10.1109/TIP.2007.909412', '10.1109/TPAMI.2004.1262177', '10.1109/TPAMI.2007.1061', '10.1109/CVPR.2007.383095', '10.1109/42.563664', '10.1016/B978-012077790-7/50037-0', '10.1007/BFb0056301', '10.1109/TMI.2003.814791', '10.1016/S1077-3142(03)00048-1', '10.1007/11866763_86', '10.1109/42.796284', '10.1145/15886.15903', '10.1023/A:1007958904918'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['deep learning', 'computer vision', 'machine learning', 'medical image analysis', 'artificial intelligence'], conference_acronym='Medical image analysis (Print)', publisher=None, query_handler=None),\n",
       " 'Ensembles of Multiple Models and Architectures for Robust Brain Tumour Segmentation': Paper(DOI='10.1007/978-3-319-75238-9_38', crossref_json=None, google_schorlar_metadata=None, title='Ensembles of Multiple Models and Architectures for Robust Brain Tumour Segmentation', authors=['Konstantinos Kamnitsas', 'Wenjia Bai', 'Enzo Ferrante', 'Steven McDonagh', 'Matthew Sinclair', 'Nick Pawlowski', 'Martin Rajchl', 'Matthew Lee', 'Bernhard Kainz', 'Daniel Rueckert', 'Ben Glocker'], abstract=' Deep learning approaches such as convolutional neural nets have consistently outperformed previous methods on challenging tasks such as dense, semantic segmentation. However, the various proposed networks perform differently, with behaviour largely influenced by architectural choices and training settings. This paper explores Ensembles of Multiple Models and Architectures (EMMA) for robust performance through aggregation of predictions from a wide range of methods. The approach reduces the influence of the meta-parameters of individual models and the risk of overfitting the configuration to a particular database. EMMA can be seen as an unbiased, generic deep learning model which is shown to yield excellent performance, winning the first position in the BRATS 2017 competition among 50+ participating teams.', conference=None, journal=None, year=None, reference_list=['10.1056/NEJM200101113440207', '10.1088/0031-9155/58/13/R97', '10.1007/s00401-016-1545-1', '10.1016/j.media.2004.06.007', '10.1007/978-3-642-23629-7_65', '10.1007/978-3-642-33418-4_80', '10.1007/978-3-319-30858-6_13', '10.1007/978-3-642-33454-2_46', '10.1007/978-3-319-55524-9_17', '10.1016/j.media.2016.10.004', '10.1016/0893-6080(89)90020-8', '10.1109/34.667881', '10.1017/S0269888997003123', '10.1109/CVPR.2014.77', '10.1007/978-3-319-46723-8_68', '10.1007/978-3-319-55524-9_14', '10.1109/CVPR.2015.7298965', '10.1007/978-3-319-24574-4_28', '10.1016/j.nicl.2017.12.022', '10.1038/sdata.2017.117', '10.1007/978-3-319-59050-9_47', '10.1145/1150402.1150464'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Biomedical Image Analysis', 'Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'ISLES 2015-A public evaluation benchmark for ischemic stroke lesion segmentation from multispectral MRI': Paper(DOI='10.29042/2018-3721-3725', crossref_json=None, google_schorlar_metadata=None, title='ISLES 2015-A public evaluation benchmark for ischemic stroke lesion segmentation from multispectral MRI', authors=['Oskar Maier', 'Bjoern H Menze', 'Janina Von der Gablentz', 'Levin Häni', 'Mattias P Heinrich', 'Matthias Liebrand', 'Stefan Winzeck', 'Abdul Basit', 'Paul Bentley', 'Liang Chen', 'Daan Christiaens', 'Francis Dutil', 'Karl Egger', 'Chaolu Feng', 'Ben Glocker', 'Michael Götz', 'Tom Haeck', 'Hanna-Leena Halme', 'Mohammad Havaei', 'Khan M Iftekharuddin', 'Pierre-Marc Jodoin', 'Konstantinos Kamnitsas', 'Elias Kellner', 'Antti Korvenoja', 'Hugo Larochelle', 'Christian Ledig', 'Jia-Hong Lee', 'Frederik Maes', 'Qaiser Mahmood', 'Klaus H Maier-Hein', 'Richard McKinley', 'John Muschelli', 'Chris Pal', 'Linmin Pei', 'Janaki Raman Rangarajan', 'Syed MS Reza', 'David Robben', 'Daniel Rueckert', 'Eero Salli', 'Paul Suetens', 'Ching-Wei Wang', 'Matthias Wilms', 'Jan S Kirschke', 'Ulrike M Krämer', 'Thomas F Münte', 'Peter Schramm', 'Roland Wiest', 'Heinz Handels', 'Mauricio Reyes'], abstract='Ischemic stroke is the most common cerebrovascular disease, and its diagnosis, treatment, and study relies on non-invasive imaging. Algorithms for stroke lesion segmentation from magnetic resonance imaging (MRI) volumes are intensely researched, but the reported results are largely incomparable due to different datasets and evaluation schemes. We approached this urgent problem of comparability with the Ischemic Stroke Lesion Segmentation (ISLES) challenge organized in conjunction with the MICCAI 2015 conference. In this paper we propose a common evaluation framework, describe the publicly available datasets, and present the results of the two sub-challenges: Sub-Acute Stroke Lesion Segmentation (SISS) and Stroke Perfusion Estimation (SPES). A total of 16 research groups participated with a wide range of state-of-the-art automatic segmentation algorithms. A thorough analysis of the obtained\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Biomedical Image Analysis', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Disease prediction using graph convolutional networks: application to autism spectrum disorder and Alzheimer’s disease': Paper(DOI='10.1016/j.media.2018.06.001', crossref_json=None, google_schorlar_metadata=None, title='Disease prediction using graph convolutional networks: application to autism spectrum disorder and Alzheimer’s disease', authors=['Sarah Parisot', 'Sofia Ira Ktena', 'Enzo Ferrante', 'Matthew Lee', 'Ricardo Guerrero', 'Ben Glocker', 'Daniel Rueckert'], abstract='Graphs are widely used as a natural framework that captures interactions between individual elements represented as nodes in a graph. In medical applications, specifically, nodes can represent individuals within a potentially large population (patients or healthy controls) accompanied by a set of features, while the graph edges incorporate associations between subjects in an intuitive manner. This representation allows to incorporate the wealth of imaging and non-imaging information as well as individual subject features simultaneously in disease classification tasks. Previous graph-based approaches for supervised or unsupervised learning in the context of disease prediction solely focus on pairwise similarities between subjects, disregarding individual characteristics and features, or rather rely on subject-specific imaging feature vectors and fail to model interactions between them. In this paper, we present a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.neuroimage.2016.10.045', '10.1016/j.neuroimage.2016.02.079', '10.1109/MSP.2017.2693418', '10.1016/j.neuroimage.2010.06.013', '10.1016/j.neuroimage.2006.01.021', '10.1038/mp.2013.78', '10.1109/TPAMI.2015.2408348', '10.1109/ICIP.2017.8296892', '10.1016/j.acha.2010.04.005', '10.1016/j.media.2016.05.004', '10.1016/j.nicl.2017.08.017', '10.3389/fnhum.2014.00349', '10.1016/j.neuroimage.2016.09.046', '10.1016/j.neuroimage.2017.12.052', '10.1016/j.media.2014.12.003', '10.1212/WNL.46.1.130', '10.1109/CVPR.2017.576', '10.1212/01.WNL.0000147469.18313.3B', '10.1371/journal.pone.0144200', '10.1111/j.1532-5415.2008.01684.x', '10.1016/j.nicl.2012.11.006', '10.1109/42.538937', '10.1109/TNN.2008.2005605', '10.1109/MSP.2012.2235192', '10.1109/CVPR.2017.11', '10.1007/BF01531727', '10.1007/s11682-013-9269-5', '10.1109/TBME.2016.2549363', '10.1016/j.patcog.2016.10.009', '10.1111/j.1469-7610.2008.02010.x', '10.1016/j.media.2017.05.003', '10.1097/WCO.0b013e32835ee548', '10.1016/j.media.2011.12.003', '10.1109/LSP.2014.2329056'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Medical Image Analysis', 'Computer Vision', 'Machine Learning'], conference_acronym='Medical image analysis (Print)', publisher=None, query_handler=None),\n",
       " 'Semi-supervised learning for network-based cardiac MR image segmentation': Paper(DOI='10.3724/sp.j.1004.2010.01527', crossref_json=None, google_schorlar_metadata=None, title='Semi-supervised learning for network-based cardiac MR image segmentation', authors=['Wenjia Bai', 'Ozan Oktay', 'Matthew Sinclair', 'Hideaki Suzuki', 'Martin Rajchl', 'Giacomo Tarroni', 'Ben Glocker', 'Andrew King', 'Paul M Matthews', 'Daniel Rueckert'], abstract=' Training a fully convolutional network for pixel-wise (or voxel-wise) image segmentation normally requires a large number of training images with corresponding ground truth label maps. However, it is a challenge to obtain such a large training set in the medical imaging domain, where expert annotations are time-consuming and difficult to obtain. In this paper, we propose a semi-supervised learning approach, in which a segmentation network is trained from both labelled and unlabelled data. The network parameters and the segmentations for the unlabelled data are alternately updated. We evaluate the method for short-axis cardiac MR image segmentation and it has demonstrated a high performance, outperforming a baseline supervised method. The mean Dice overlap metric is 0.92 for the left ventricular cavity, 0.85 for the myocardium and 0.89 for the right ventricular cavity. It also outperforms a state-of-the-art\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1015706.1015719', '10.1007/BF01759061', '10.1016/S0167-8655(02)00382-3', '10.1145/1327452.1327494'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Medical Image Analysis', 'Computer Vision', 'Machine Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Exploring visual relationship for image captioning': Paper(DOI='10.1016/j.imavis.2021.104146', crossref_json=None, google_schorlar_metadata=None, title='Exploring visual relationship for image captioning', authors=['Ting Yao', 'Yingwei Pan', 'Yehao Li', 'Tao Mei'], abstract='It is always well believed that modeling relationships between objects would be helpful for representing and eventually describing an image. Nevertheless, there has not been evidence in support of the idea on image description generation. In this paper, we introduce a new design to explore the connections between objects for image captioning under the umbrella of attention-based encoder-decoder framework. Specifically, we present Graph Convolutional Networks plus Long Short-Term Memory (dubbed as GCN-LSTM) architecture that novelly integrates both semantic and spatial object relationships into image encoder. Technically, we build graphs over the detected objects in an image based on their spatial and semantic connections. The representations of each region proposed on objects are then refined by leveraging graph structure through GCN. With the learnt region-level features, our GCN-LSTM capitalizes on LSTM-based captioning framework with attention mechanism for sentence generation. Extensive experiments are conducted on COCO image captioning dataset, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, GCN-LSTM increases CIDEr-D performance from 120.1% to 128.7% on COCO testing set.', conference=None, journal=None, year=None, reference_list=['10.1109/TNNLS.2018.2851077', '10.1016/j.imavis.2019.03.003'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Video Understanding', 'Vision and Language', 'Computer Vision'], conference_acronym='Image and vision computing', publisher=None, query_handler=None),\n",
       " 'A deep learning-based approach to progressive vehicle re-identification for urban surveillance': Paper(DOI='10.1007/978-3-319-46475-6_53', crossref_json=None, google_schorlar_metadata=None, title='A deep learning-based approach to progressive vehicle re-identification for urban surveillance', authors=['Xinchen Liu', 'Wu Liu', 'Tao Mei', 'Huadong Ma'], abstract=' While re-identification (Re-Id) of persons has attracted intensive attention, vehicle, which is a significant object class in urban video surveillance, is often overlooked by vision community. Most existing methods for vehicle Re-Id only achieve limited performance, as they predominantly focus on the generic appearance of vehicle while neglecting some unique identities of vehicle (e.g., license plate). In this paper, we propose a novel deep learning-based approach to PROgressive Vehicle re-ID, called “PROVID”. Our approach treats vehicle Re-Id as two specific progressive search processes: coarse-to-fine search in the feature space, and near-to-distant search in the real world surveillance environment. The first search process employs the appearance attributes of vehicle for a coarse filtering, and then exploits the Siamese Neural Network for license plate verification to accurately identify vehicles. The near-to\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TMM.2011.2170666', '10.1109/CVPR.2015.7299023', '10.1109/CVPR.2011.5995575', '10.1049/ip-vis:20041147', '10.1109/TITS.2011.2158001', '10.1109/TCSVT.2012.2203741', '10.1109/TITS.2011.2114346', '10.1109/ICME.2016.7553002', '10.1016/j.cviu.2007.01.003', '10.1109/TMM.2013.2281019', '10.1109/ICCV.2015.169', '10.1109/CVPR.2015.7298994', '10.1145/2536798', '10.1142/S0218001493000339', '10.1109/ICASSP.2016.7472194', '10.1023/B:VISI.0000029664.99615.94', '10.1109/ICCV.2003.1238663', '10.1109/TIP.2009.2019809', '10.1109/ICCV.2015.133', '10.1109/CVPR.2015.7298594', '10.1145/2647868.2654889', '10.1109/CVPR.2015.7298832'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia Analysis', 'Computer Vision', 'Generative AI', 'Artificial Intelligence'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Multiview spectral embedding': Paper(DOI='10.1117/1.jei.26.5.053002', crossref_json=None, google_schorlar_metadata=None, title='Multiview spectral embedding', authors=['Tian Xia', 'Dacheng Tao', 'Tao Mei', 'Yongdong Zhang'], abstract='In computer vision and multimedia search, it is common to use multiple features from different views to represent an object. For example, to well characterize a natural scene image, it is essential to find a set of visual features to represent its color, texture, and shape information and encode each feature into a vector. Therefore, we have a set of vectors in different spaces to represent the image. Conventional spectral-embedding algorithms cannot deal with such datum directly, so we have to concatenate these vectors together as a new vector. This concatenation is not physically meaningful because each feature has a specific statistical property. Therefore, we develop a new spectral-embedding algorithm, namely, multiview spectral embedding (MSE), which can encode different features in different ways, to achieve a physically meaningful embedding. In particular, MSE finds a low-dimensional embedding wherein\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ACCESS.2016.2607841', '10.1109/TPAMI.2015.2430329', '10.1016/j.patcog.2012.08.006', '10.1007/BF00130487', '10.1109/TSMCC.2011.2118750', '10.1109/TIP.2010.2042645', '10.1117/12.2002185', '10.1016/j.patcog.2011.02.003', '10.1117/1.JEI.25.2.023030', '10.1023/B:VISI.0000029664.99615.94', '10.1016/S0079-6123(06)55002-2', '10.1145/3065386', '10.1109/TIP.2017.2704661', '10.1109/TCSVT.2016.2539860', '10.1109/TCSVT.2015.2406194', '10.1109/NNSP.1999.788121', '10.1126/science.290.5500.2323', '10.1109/TSMCB.2009.2039566', '10.1109/CVPR.2012.6247923', '10.1109/TPAMI.2015.2435740', '10.1016/j.neucom.2016.07.044', '10.1109/TPAMI.2003.1227984', '10.1007/978-3-540-88682-2_24', '10.1117/1.JEI.25.2.023022', '10.1007/s11263-017-1016-8'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia Analysis', 'Computer Vision', 'Generative AI', 'Artificial Intelligence'], conference_acronym='Journal of electronic imaging (Print)', publisher=None, query_handler=None),\n",
       " 'Personalized recommendation combining user interest and social circle': Paper(DOI='10.1109/tkde.2013.168', crossref_json=None, google_schorlar_metadata=None, title='Personalized recommendation combining user interest and social circle', authors=['Xueming Qian', 'He Feng', 'Guoshuai Zhao', 'Tao Mei'], abstract=\"With the advent and popularity of social network, more and more users like to share their experiences, such as ratings, reviews, and blogs. The new factors of social network like interpersonal influence and interest based on circles of friends bring opportunities and challenges for recommender system (RS) to solve the cold start and sparsity problem of datasets. Some of the social factors have been used in RS, but have not been fully considered. In this paper, three social factors, personal interest, interpersonal interest similarity, and interpersonal influence, fuse into a unified personalized recommendation model based on probabilistic matrix factorization. The factor of personal interest can make the RS recommend items to meet users' individualities, especially for experienced users. Moreover, for cold start users, the interpersonal interest similarity and interpersonal influence can enhance the intrinsic link among\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1145/604050.604064', '10.1037/0033-2909.119.1.111', '10.1145/2043932.2043978', '10.1145/2063576.2063680', '10.1145/2009916.2010051', '10.1109/TSMCB.2011.2163711', '10.1145/2009916.2009945', '10.1007/11731139_44', '10.1145/1040830.1040870', '10.1145/1835449.1835555', '10.1016/j.neucom.2012.12.021', '10.1109/TMM.2013.2280127', '10.1145/1148170.1148257', '10.1145/1864708.1864721', '10.1145/963770.963772', '10.1145/1645953.1646050', '10.1145/2396761.2396771', '10.1145/2339530.2339728', '10.1145/1297231.1297235', '10.1145/775047.775057', '10.1145/1557019.1557072', '10.1145/1401890.1401944', '10.1145/245108.245124', '10.1609/aaai.v25i1.7915', '10.1145/2396761.2398448', '10.1007/978-3-319-03731-8_68', '10.1016/j.neucom.2013.09.018', '10.1109/TCSVT.2008.2005607', '10.1007/s00530-010-0223-8', '10.1145/2348283.2348346', '10.1145/1835804.1835893', '10.1145/1076034.1076056', '10.1145/1571941.1571978', '10.1145/1557019.1557067', '10.1109/INFCOM.2011.5935224', '10.1145/1458082.1458205', '10.1145/1935826.1935877', '10.1016/j.eswa.2009.12.061', '10.1145/1864708.1864736', '10.1111/1467-9868.00196', '10.1109/MC.2009.263', '10.1145/1281192.1281206', '10.1109/TKDE.2005.99', '10.1145/2009916.2009945', '10.1145/371920.372071', '10.1109/HICSS.2000.926814', '10.1145/1348549.1348556', '10.1145/2487575.2487668', '10.1145/2043932.2043975', '10.1145/1639714.1639794', '10.1109/WI.2006.186', '10.1145/1753326.1753503'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia Analysis', 'Computer Vision', 'Generative AI', 'Artificial Intelligence'], conference_acronym='IEEE transactions on knowledge and data engineering (Print)', publisher=None, query_handler=None),\n",
       " 'Provid: Progressive and multimodal vehicle reidentification for large-scale urban surveillance': Paper(DOI='10.1109/tmm.2017.2751966', crossref_json=None, google_schorlar_metadata=None, title='Provid: Progressive and multimodal vehicle reidentification for large-scale urban surveillance', authors=['Xinchen Liu', 'Wu Liu', 'Tao Mei', 'Huadong Ma'], abstract='Compared with person reidentification, which has attracted concentrated attention, vehicle reidentification is an important yet frontier problem in video surveillance and has been neglected by the multimedia and vision communities. Since most existing approaches mainly consider the general vehicle appearance for reidentification while overlooking the distinct vehicle identifier, such as the license plate number, they attain suboptimal performance. In this paper, we propose PROVID, a PROgressive Vehicle re-IDentification framework based on deep neural networks. In particular, our framework not only utilizes the multimodality data in large-scale video surveillance, such as visual features, license plates, camera locations, and contextual information, but also considers vehicle reidentification in two progressive procedures: coarse-to-fine search in the feature domain, and near-to-distant search in the physical space\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/T-C.1975.224208', '10.1109/CVPR.2016.139', '10.1109/TIP.2009.2019809', '10.1109/ICCV.2003.1238663', '10.1023/B:VISI.0000029664.99615.94', '10.1109/ICASSP.2016.7472194', '10.1016/j.patcog.2006.05.009', '10.1007/s11263-015-0816-y', '10.1109/CVPR.2015.7298594', '10.1109/ICCV.2015.133', '10.1109/TSMCC.2011.2109710', '10.1145/2647868.2654889', '10.1109/TMM.2015.2500734', '10.1109/MMUL.2017.265091429', '10.1109/TITS.2011.2114346', '10.1109/TCSVT.2012.2203741', '10.1109/ICME.2016.7553002', '10.1109/CVPR.2016.238', '10.1109/CVPRW.2016.195', '10.1109/CVPR.1999.784638', '10.1016/j.cviu.2007.01.003', '10.1142/S0218001493000339', '10.1109/TMM.2013.2241416', '10.1145/2536798', '10.1109/CVPR.2011.5995575', '10.1109/TITS.2011.2158001', '10.1109/CVPR.2005.202', '10.1049/ip-vis:20041147', '10.1109/TMM.2013.2271746', '10.1145/2629592', '10.1109/CVPR.2015.7299023', '10.1109/TMM.2015.2408566', '10.1109/TMM.2011.2170666', '10.1109/TMM.2015.2496139', '10.1007/978-3-319-46475-6_53', '10.1109/TMM.2013.2281019', '10.5244/C.28.6', '10.1109/ICCV.2015.169', '10.1109/CVPR.2015.7298832', '10.1145/3065386', '10.1109/CVPR.2015.7298994'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia Analysis', 'Computer Vision', 'Generative AI', 'Artificial Intelligence'], conference_acronym='IEEE transactions on multimedia', publisher=None, query_handler=None),\n",
       " 'Deep collaborative embedding for social image understanding': Paper(DOI='10.1109/tpami.2018.2852750', crossref_json=None, google_schorlar_metadata=None, title='Deep collaborative embedding for social image understanding', authors=['Zechao Li', 'Jinhui Tang', 'Tao Mei'], abstract='In this work, we investigate the problem of learning knowledge from the massive community-contributed images with rich weakly-supervised context information, which can benefit multiple image understanding tasks simultaneously, such as social image tag refinement and assignment, content-based image retrieval, tag-based image retrieval and tag expansion. Towards this end, we propose a Deep Collaborative Embedding (DCE) model to uncover a unified latent space for images and tags. The proposed method incorporates the end-to-end learning and collaborative factor analysis in one unified framework for the optimal compatibility of representation learning and latent space discovery. A nonnegative and discrete refined tagging matrix is learned to guide the end-to-end learning. To collaboratively explore the rich context information of social images, the proposed method integrates the weakly-supervised\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TMM.2012.2187435', '10.1109/TMM.2009.2030598', '10.1109/TPAMI.2016.2552172', '10.1109/TPAMI.2010.160', '10.1145/1873951.1874031', '10.1145/1873951.1874028', '10.1109/TMM.2015.2477035', '10.1145/1348246.1348248', '10.1109/34.895972', '10.1109/TPAMI.2016.2608882', '10.1109/TKDE.2007.48', '10.1145/2733373.2806294', '10.1109/CVPR.2016.503', '10.1109/TPAMI.2015.2400461', '10.1145/1899412.1899418', '10.1109/TMM.2011.2134078', '10.1109/TPAMI.2016.2598339', '10.1145/1873951.1874183', '10.1007/3-540-47979-1_7', '10.1109/TPAMI.2015.2487982', '10.1109/CVPR.2016.274', '10.1109/TPAMI.2015.2469281', '10.1109/TIP.2016.2549459', '10.1145/1835804.1835951', '10.1109/TPAMI.2010.231', '10.1145/1291233.1291447', '10.1007/978-3-319-10584-0_28', '10.1145/1646396.1646452', '10.1145/1460096.1460104', '10.1109/TPAMI.2016.2554555', '10.1109/TPAMI.2011.217', '10.1109/TIP.2016.2624140', '10.1109/TPAMI.2014.2366761', '10.1145/1873951.1873987', '10.1109/TPAMI.2007.61', '10.1109/TPAMI.2008.125', '10.1109/TMM.2010.2101051', '10.1109/ICCV.2015.525', '10.1109/TPAMI.2014.2343234', '10.1145/2906152', '10.1109/TPAMI.2011.191', '10.1007/s11263-011-0494-3', '10.1109/TPAMI.2012.124', '10.1007/s11263-013-0658-4', '10.1109/CVPR.2013.212', '10.1109/TNNLS.2017.2691725', '10.1109/TPAMI.2013.142', '10.1145/1390156.1390267', '10.1007/978-3-642-33786-4_10', '10.1038/44565', '10.1145/1459359.1459375', '10.1109/ICCV.2015.153', '10.1145/1631272.1631278'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia Analysis and Retrieval', 'Computer Vision', 'Aritificial Intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval': Paper(DOI='10.1109/tpami.2012.193', crossref_json=None, google_schorlar_metadata=None, title='Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval', authors=['Yunchao Gong', 'Svetlana Lazebnik', 'Albert Gordo', 'Florent Perronnin'], abstract='This paper addresses the problem of learning similarity-preserving binary codes for efficient similarity search in large-scale image collections. We formulate this problem in terms of finding a rotation of zero-centered data so as to minimize the quantization error of mapping this data to the vertices of a zero-centered binary hypercube, and propose a simple and efficient alternating minimization algorithm to accomplish this task. This algorithm, dubbed iterative quantization (ITQ), has connections to multiclass spectral clustering and to the orthogonal Procrustes problem, and it can be used both with unsupervised data embeddings such as PCA and supervised embeddings such as canonical correlation analysis (CCA). The resulting binary codes significantly outperform several other state-of-the-art methods. We also show that further performance improvements can result from transforming the data with a nonlinear kernel\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1327452.1327494', '10.1109/TPAMI.2009.166', '10.1109/CVPR.2010.5539928', '10.1109/CVPR.2008.4587353', '10.1109/CVPR.2011.5995505', '10.1109/CVPR.2011.5995432', '10.2307/2333955', '10.1109/CVPR.2011.5995518', '10.1109/TPAMI.2010.57', '10.1109/CVPR.2010.5540039', '10.1109/CVPR.2008.4587630', '10.1023/B:VISI.0000029664.99615.94', '10.1109/CVPR.2010.5540129', '10.1109/ICCV.2009.5459466', '10.1109/CVPR.2011.5995709', '10.1109/CVPR.2006.264', '10.1023/A:1011139631724', '10.1109/CVPR.2010.5539949', '10.1007/978-3-642-15561-1_11', '10.1109/TMM.2007.900138', '10.1109/CVPR.2010.5539994', '10.1109/CVPR.2007.383266', '10.1016/j.patrec.2010.04.004', '10.1109/TPAMI.2008.128', '10.1109/CVPR.2010.5539914', '10.1109/CVPR.2008.4587633', '10.1109/CVPR.2010.5540009', '10.1016/j.ijar.2008.11.006', '10.1109/ICCV.2003.1238361', '10.1007/BF02289451'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'artificial intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Multi-scale orderless pooling of deep convolutional activation features': Paper(DOI='10.1007/978-3-319-10584-0_26', crossref_json=None, google_schorlar_metadata=None, title='Multi-scale orderless pooling of deep convolutional activation features', authors=['Yunchao Gong', 'Liwei Wang', 'Ruiqi Guo', 'Svetlana Lazebnik'], abstract=' Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. However, global CNN activations lack geometric invariance, which limits their robustness for classification and matching of highly variable scenes. To improve the invariance of CNN activations without degrading their discriminative power, this paper presents a simple but effective scheme called multi-scale orderless pooling (MOP-CNN). This scheme extracts CNN activations for local patches at multiple scale levels, performs orderless VLAD pooling of these activations at each level separately, and concatenates the result. The resulting MOP-CNN representation can be used as a generic feature for either supervised or unsupervised recognition tasks, from image classification to instance-level retrieval; it consistently outperforms global CNN activations without requiring any joint training of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICASSP.2013.6639343', '10.1109/CVPR.2014.81', '10.1109/CVPR.2014.222', '10.1109/CVPRW.2014.131', '10.1109/CVPR.2009.5206848', '10.1109/CVPR.2007.383266', '10.1109/CVPR.2010.5540039', '10.1109/CVPR.2010.5540018', '10.1109/ICCV.2003.1238663', '10.1109/ICCV.2005.239', '10.1023/B:VISI.0000029664.99615.94', '10.1145/1553374.1553453', '10.1145/2647868.2654889', '10.1007/978-3-642-15561-1_11', '10.1109/CVPR.2010.5540009', '10.1109/ICCV.2011.6126383', '10.1109/CVPRW.2009.5206537', '10.1109/ICCV.2013.258', '10.1007/978-3-540-88682-2_24', '10.1109/CVPR.2012.6248090', '10.1007/s11263-013-0636-x', '10.1007/978-3-642-33709-3_6', '10.1109/CVPR.2013.124', '10.1007/978-3-642-33709-3_55', '10.1109/CVPR.2012.6248035', '10.1109/ICCV.2013.177'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'recognition'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Superparsing: scalable nonparametric image parsing with superpixels': Paper(DOI='10.1007/s11263-012-0574-z', crossref_json=None, google_schorlar_metadata=None, title='Superparsing: scalable nonparametric image parsing with superpixels', authors=['Joseph Tighe', 'Svetlana Lazebnik'], abstract=' This paper presents a simple and effective nonparametric approach to the problem of image parsing, or labeling image regions (in our case, superpixels produced by bottom-up segmentation) with their categories. This approach is based on lazy learning, and it can easily scale to datasets with tens of thousands of images and hundreds of labels. Given a test image, it first performs global scene-level matching against the training set, followed by superpixel-level matching and efficient Markov random field (MRF) optimization for incorporating neighborhood context. Our MRF setup can also compute a simultaneous labeling of image regions into semantic classes (e.g., tree, building, car) and geometric classes (sky, vertical, ground). Our system outperforms the state-of-the-art nonparametric method based on SIFT Flow on a dataset of 2,688 images and 33 labels. In addition, we report per-pixel rates on a larger\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2004.60', '10.1109/34.969114', '10.1016/j.cviu.2010.02.004', '10.1007/s11263-006-0031-y', '10.1109/TPAMI.2004.1262177', '10.1109/TPAMI.2011.131', '10.1109/TPAMI.2010.147', '10.1016/S0079-6123(06)55002-2', '10.1007/s11263-007-0090-8', '10.1109/TPAMI.2008.128'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'recognition'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'A multi-view embedding space for modeling internet images, tags, and their semantics': Paper(DOI='10.1007/s11263-013-0658-4', crossref_json=None, google_schorlar_metadata=None, title='A multi-view embedding space for modeling internet images, tags, and their semantics', authors=['Yunchao Gong', 'Qifa Ke', 'Michael Isard', 'Svetlana Lazebnik'], abstract=' This paper investigates the problem of modeling Internet images and associated text or tags for tasks such as image-to-image search, tag-to-image search, and image-to-tag search (image annotation). We start with canonical correlation analysis (CCA), a popular and successful approach for mapping visual and textual features to the same latent space, and incorporate a third view capturing high-level image semantics, represented either by a single category or multiple non-mutually-exclusive concepts. We present two ways to train the three-view embedding: supervised, with the third view coming from ground-truth labels or search keywords; and unsupervised, with semantic themes automatically obtained by clustering the tags. To ensure high accuracy for retrieval tasks while keeping the learning process scalable, we combine multiple strong visual features and use explicit nonlinear kernel mappings to\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2001.937654', '10.1017/S0305004100019897', '10.1109/CVPR.2006.57', '10.1109/CVPRW.2009.5204174', '10.1109/CVPR.2008.4587353', '10.1145/860435.860460', '10.1109/TPAMI.2007.61', '10.1109/ICCV.2011.6126323', '10.1145/1646396.1646452', '10.1109/CVPR.2005.177', '10.1145/1348246.1348248', '10.1109/CVPR.2009.5206848', '10.1007/3-540-47979-1_7', '10.1109/CVPR.2010.5540135', '10.1007/978-3-642-15561-1_2', '10.1109/ICCV.2009.5459169', '10.1109/CVPR.2011.5995432', '10.1109/TPAMI.2007.70791', '10.1109/CVPR.2012.6248035', '10.1109/ICCV.2009.5459266', '10.1109/CVPR.2010.5540120', '10.1162/0899766042321814', '10.1145/312624.312649', '10.1093/biomet/28.3-4.321', '10.5244/C.24.58', '10.1109/CVPR.2010.5540092', '10.7146/dpb.v27i537.7070', '10.1109/CVPR.2006.68', '10.1145/1631272.1631283', '10.1023/B:VISI.0000029664.99615.94', '10.1007/978-3-642-33718-5_10', '10.1109/ICCV.2009.5459203', '10.1007/978-3-540-88690-7_24', '10.1007/978-3-642-33709-3_35', '10.1145/1027527.1027608', '10.1023/A:1011139631724', '10.1109/CVPR.2010.5539914', '10.1109/CVPR.2007.383173', '10.1109/CVPRW.2008.4562959', '10.1109/TMM.2007.900138', '10.1145/1873951.1873987', '10.1007/BFb0020217', '10.1109/ICCV.2007.4409099', '10.1109/CVPR.2012.6247923', '10.1109/34.895972', '10.1007/978-3-642-15555-0_26', '10.1109/TPAMI.2009.154', '10.1109/CVPR.2010.5539949', '10.1007/978-3-642-33712-3_60', '10.1145/985692.985733', '10.1109/CVPR.2009.5206816', '10.1109/TPAMI.2008.127', '10.1145/1148170.1148204', '10.1109/CVPR.2010.5539970', '10.1145/860435.860485', '10.1109/CVPRW.2009.5204274', '10.1145/1076034.1076082'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'recognition'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Piggyback: Adapting a single network to multiple tasks by learning to mask weights': Paper(DOI='10.1007/978-3-030-01225-0_5', crossref_json=None, google_schorlar_metadata=None, title='Piggyback: Adapting a single network to multiple tasks by learning to mask weights', authors=['Arun Mallya', 'Dillon Davis', 'Svetlana Lazebnik'], abstract=\"This work presents a method for adapting a single, fixed deep neural network to multiple tasks without affecting performance on already learned tasks. By building upon ideas from network quantization and pruning, we learn binary masks that``piggyback''on an existing network, or are applied to unmodified weights of that network to provide good performance on a new task. These masks are learned in an end-to-end differentiable fashion, and incur a low overhead of 1 bit per network parameter, per task. Even though the underlying network is fixed, the ability to mask individual weights allows for the learning of a large number of filters. We show performance comparable to dedicated fine-tuned networks for a variety of classification tasks, including those with large domain shifts from the initial task (ImageNet), and a variety of network architectures. Unlike prior work, we do not suffer from catastrophic forgetting or competition between tasks, and our performance is agnostic to task ordering.\", conference=None, journal=None, year=None, reference_list=['10.1007/s11263-015-0816-y', '10.1016/S1364-6613(99)01294-2', '10.1073/pnas.1611835114', '10.1109/ICCV.2017.148', '10.1007/978-3-319-46493-0_37', '10.1109/CVPR.2018.00810', '10.1109/ICCVW.2013.77', '10.1109/ICVGIP.2008.47', '10.1145/2185520.2185540', '10.1109/CVPR.2016.90', '10.5244/C.30.87', '10.1109/CVPR.2017.243', '10.1007/978-1-4615-5529-2_5', '10.1109/CVPR.2017.579', '10.1109/ICCV.2017.368', '10.1167/17.10.296', '10.1109/CVPR.2015.7298965'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'recognition'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Learning two-branch neural networks for image-text matching tasks': Paper(DOI='10.1109/tpami.2018.2797921', crossref_json=None, google_schorlar_metadata=None, title='Learning two-branch neural networks for image-text matching tasks', authors=['Liwei Wang', 'Yin Li', 'Jing Huang', 'Svetlana Lazebnik'], abstract='Image-language matching tasks have recently attracted a lot of attention in the computer vision field. These tasks include image-sentence matching, i.e., given an image query, retrieving relevant sentences and vice versa, and region-phrase matching or visual grounding, i.e., matching a phrase to relevant regions. This paper investigates two-branch neural networks for learning the similarity between these two data modalities. We propose two network structures that produce different output representations. The first one, referred to as an  embedding network , learns an explicit shared latent embedding space with a maximum-margin ranking loss and novel neighborhood constraints. Compared to standard triplet sampling, we perform improved neighborhood sampling that takes neighborhood information into consideration while constructing mini-batches. The second network structure, referred to as a  similarity\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2014.242', '10.1109/CVPR.2015.7298878', '10.1162/tacl_a_00177', '10.1109/ICCV.2015.301', '10.1162/neco.1997.9.8.1735', '10.1109/CVPR.2017.375', '10.1093/biomet/28.3-4.321', '10.1162/0899766042321814', '10.1613/jair.3994', '10.1007/s11263-013-0658-4', '10.1109/CVPR.2015.7298966', '10.1109/ICCV.2015.483', '10.18653/v1/D16-1044', '10.1007/s11263-014-0733-5', '10.1109/ICCV.2015.169', '10.1109/ICCV.2015.303', '10.1007/s10994-009-5108-8', '10.1007/978-3-319-46448-0_49', '10.3115/v1/D14-1086', '10.1007/978-3-319-46475-6_5', '10.1109/ICCV.2015.303', '10.1162/tacl_a_00166', '10.1109/CVPR.2017.201', '10.1109/CVPR.2016.541', '10.1109/CVPR.2015.7298932', '10.1109/CVPR.2015.7298935', '10.1109/CVPR.2016.494', '10.1109/ICCV.2015.279', '10.1109/CVPR.2014.180', '10.1007/978-3-319-46484-8_44', '10.1109/CVPR.2005.202', '10.1109/CVPR.2015.7298682', '10.1145/1553374.1553494', '10.1142/S0218001493000339', '10.1109/CVPR.2015.7298767'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'recognition'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Modeling and recognition of landmark image collections using iconic scene graphs': Paper(DOI='10.1007/s11263-011-0445-z', crossref_json=None, google_schorlar_metadata=None, title='Modeling and recognition of landmark image collections using iconic scene graphs', authors=['Xiaowei Li', 'Changchang Wu', 'Christopher Zach', 'Svetlana Lazebnik', 'Jan-Michael Frahm'], abstract=' This paper presents an approach for modeling landmark sites such as the Statue of Liberty based on large-scale contaminated image collections gathered from the Internet. Our system combines 2D appearance and 3D geometric constraints to efficiently extract scene summaries, build 3D models, and recognize instances of the landmark in new test images. We start by clustering images using low-dimensional global “gist” descriptors. Next, we perform geometric verification to retain only the clusters whose images share a common 3D structure. Each valid cluster is then represented by a single iconic view, and geometric relationships between iconic views are captured by an iconic scene graph. In addition to serving as a compact scene summary, this graph is used to guide structure from motion to efficiently produce 3D models of the different aspects of the landmark. The set of iconic images is also used for\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/293347.293348', '10.1068/p2897', '10.1145/1526709.1526812', '10.1109/TPAMI.2008.121', '10.1023/B:VISI.0000029664.99615.94', '10.1109/TPAMI.2004.17', '10.1023/A:1011139631724', '10.1145/1386352.1386363', '10.1109/34.868688', '10.1007/s11263-007-0107-3'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'structure from motion', 'photo collection reconstruction', 'stereo', 'robust estimation'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Pixelwise View Selection for Unstructured Multi-View Stereo': Paper(DOI='10.1007/978-3-319-46487-9_31', crossref_json=None, google_schorlar_metadata=None, title='Pixelwise View Selection for Unstructured Multi-View Stereo', authors=['Johannes L Schönberger', 'Enliang Zheng', 'Marc Pollefeys', 'Jan-Michael Frahm'], abstract=' This work presents a Multi-View Stereo system for robust and efficient dense modeling from unstructured image collections. Our core contributions are the joint estimation of depth and normal information, pixelwise view selection using photometric and geometric priors, and a multi-view geometric consistency term for the simultaneous refinement and image-based depth and normal fusion. Experiments on benchmarks and large-scale Internet photo collections demonstrate state-of-the-art performance in terms of accuracy, completeness, and efficiency.', conference=None, journal=None, year=None, reference_list=['10.1007/3-540-47969-4_28', '10.1145/1141911.1141964', '10.1109/ICCV.2009.5459148', '10.1007/978-3-642-15561-1_27', '10.1109/CVPR.2015.7298949', '10.1109/ICCV.2015.240', '10.1109/CVPR.2015.7299148', '10.1109/CVPR.2016.445', '10.1109/CVPR.2007.383246', '10.1109/CVPR.2010.5539802', '10.1007/978-3-642-33712-3_29', '10.1109/3DV.2013.12', '10.1109/CVPR.2014.511', '10.1109/CVPR.2014.196', '10.1109/ICCV.2015.106', '10.1145/2398356.2398381', '10.1145/166117.166153', '10.1109/IROS.2013.6696924', '10.1109/CVPR.2008.4587706', '10.1007/BFb0028349', '10.1109/34.310690', '10.1023/A:1014573219977', '10.1109/CVPR.2011.5995372', '10.1007/978-3-540-88682-2_58', '10.1109/CVPR.2009.5206867', '10.1109/ICCV.2009.5459145', '10.1109/CVPR.2011.5995693', '10.1109/CVPR.2013.20', '10.1109/ICCV.2009.5459384', '10.1007/978-3-319-10590-1_10', '10.1109/ICCVW.2013.46', '10.1109/ICCV.2015.157', '10.1109/CVPR.2016.592', '10.1109/TPAMI.2008.99', '10.1109/34.865184', '10.1109/CVPR.2008.4587671', '10.1109/CVPR.2007.383245', '10.1109/ICCV.1999.791261', '10.5244/C.25.14', '10.1109/ICCV.2007.4408933', '10.1007/978-3-642-15986-2_1', '10.5244/C.26.34', '10.1109/ICCV.2015.107', '10.1109/TPAMI.2008.221', '10.1109/ICCV.2007.4408984', '10.1007/978-3-319-10602-1_54', '10.1145/2487228.2487237', '10.1109/3DIMPVT.2012.60', '10.20870/IJVR.2010.9.1.2761', '10.1109/TPAMI.2010.116'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'artificial intelligence', 'augmented reality', 'geometry'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'VSO: Visual Semantic Odometry': Paper(DOI='10.1007/978-3-030-01225-0_15', crossref_json=None, google_schorlar_metadata=None, title='VSO: Visual Semantic Odometry', authors=['Konstantinos-Nektarios Lianos', 'Johannes L Schönberger', 'Marc Pollefeys', 'Torsten Sattler'], abstract='Robust data association is a core problem of visual odometry, where image-to-image correspondences provide constraints for camera pose and map estimation. Current state-of-the-art direct and indirect methods use short-term tracking to obtain continuous frame-to-frame constraints, while long-term constraints are established using loop closures. In this paper, we propose a novel visual semantic odometry (VSO) framework to enable medium-term continuous tracking of points using semantics. Our proposed framework can be easily integrated into existing direct and indirect visual odometry pipelines. Experiments on challenging real-world datasets demonstrate a significant improvement over state-of-the-art baselines in the context of autonomous driving simply by integrating our semantic constraints.', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-319-54190-7_20', '10.15607/RSS.2014.X.043', '10.1109/CVPR.2011.5995462', '10.1109/ICRA.2017.7989203', '10.1007/978-3-030-01258-8_20', '10.1109/IROS.2011.6094648', '10.1109/ICCV.2015.246', '10.1109/CVPR.2016.350', '10.1109/TPAMI.2007.1049', '10.1109/CVPR.2017.380', '10.1109/TPAMI.2017.2658577', '10.1007/978-3-319-10605-2_54', '10.1109/TRO.2016.2623335', '10.1016/j.robot.2015.08.009', '10.1109/ICCV.2017.334', '10.1109/CVPR.2012.6248074', '10.1109/TPAMI.2016.2613051', '10.1177/0278364911434148', '10.1109/CVPR.2000.854954', '10.1109/ISMAR.2007.4538852', '10.15607/RSS.2013.IX.021', '10.1007/978-3-319-10599-4_45', '10.1109/CVPR.2007.383146', '10.1177/0278364914554813', '10.1090/qam/10666', '10.1109/ICRA.2016.7487260', '10.1137/0111030', '10.1007/978-3-319-10605-2_18', '10.1109/TPAMI.2005.188', '10.1007/s11263-005-3848-x', '10.1109/TRO.2009.2012342', '10.1109/TRO.2015.2463671', '10.1109/TRO.2017.2705103', '10.1109/ICCV.2011.6126513', '10.1109/ICRA.2017.7989525', '10.15607/RSS.2015.XI.034', '10.1109/ICARCV.2014.7064267', '10.1109/ICCV.2017.243', '10.1016/j.robot.2008.08.005', '10.1109/CVPR.2013.178', '10.1109/CVPR.2016.589', '10.1109/CVPR.2018.00721', '10.1016/j.imavis.2012.02.009', '10.1007/s11554-013-0379-5', '10.1007/978-3-642-15986-2_2', '10.1109/IROS.2012.6385773', '10.1109/3DIMPVT.2012.45', '10.1109/ICCVW.2017.83', '10.1109/ICRA.2015.7138983', '10.1109/ICCV.2017.421'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'artificial intelligence', 'augmented reality', 'geometry'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'A Vote-and-Verify Strategy for Fast Spatial Verification in Image Retrieval': Paper(DOI='10.1007/978-3-319-54181-5_21', crossref_json=None, google_schorlar_metadata=None, title='A Vote-and-Verify Strategy for Fast Spatial Verification in Image Retrieval', authors=['Johannes L Schönberger', 'True Price', 'Torsten Sattler', 'Jan-Michael Frahm', 'Marc Pollefeys'], abstract=' Spatial verification is a crucial part of every image retrieval system, as it accounts for the fact that geometric feature configurations are typically ignored by the Bag-of-Words representation. Since spatial verification quickly becomes the bottleneck of the retrieval process, runtime efficiency is extremely important. At the same time, spatial verification should be able to reliably distinguish between related and unrelated images. While methods based on RANSAC’s hypothesize-and-verify framework achieve high accuracy, they are not particularly efficient. Conversely, verification approaches based on Hough voting are extremely efficient but not as accurate. In this paper, we develop a novel spatial verification approach that uses an efficient voting scheme to identify promising transformation hypotheses that are subsequently verified and refined. Through comprehensive experiments, we show that our method is\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2003.1238663', '10.1109/CVPR.2012.6248018', '10.1007/978-3-319-16817-3_13', '10.1109/CVPR.2016.175', '10.1109/CVPR.2015.7298790', '10.5244/C.26.76', '10.1109/ICCV.2015.243', '10.1109/ICCV.2009.5459180', '10.1109/ICCV.2011.6126361', '10.1109/ICCV.2013.432', '10.1109/CVPR.2015.7299148', '10.1109/CVPR.2016.592', '10.1109/CVPR.2016.445', '10.1109/CVPR.2010.5540039', '10.1109/CVPR.2007.383266', '10.1109/CVPR.2014.417', '10.1109/CVPR.2016.572', '10.1007/978-3-319-46448-0_1', '10.1007/978-3-319-46466-4_15', '10.1109/CVPR.2011.5995601', '10.1007/s11263-013-0682-4', '10.1109/ICCV.2013.177', '10.1109/CVPR.2009.5206609', '10.1007/978-3-540-88682-2_24', '10.1109/CVPR.2007.383172', '10.1145/358669.358692', '10.1023/B:VISI.0000029664.99615.94', '10.1007/s11263-013-0659-3', '10.1109/ICCV.2015.218', '10.1109/ICCV.2007.4408891', '10.1007/978-3-319-16808-1_9', '10.1109/CVPR.2009.5206587', '10.1109/TPAMI.2008.111', '10.1109/ICCV.2009.5459459', '10.5244/C.29.25', '10.1007/978-3-540-45243-0_31', '10.5244/C.26.95', '10.1109/CVPR.2005.221', '10.1109/TPAMI.2012.257', '10.1109/CVPRW.2009.5206531', '10.1109/CVPR.2011.5995528', '10.1007/978-3-319-10605-2_33', '10.1016/j.cviu.2013.12.002', '10.1109/TPAMI.2013.237', '10.1109/CVPR.2008.4587635', '10.1145/2812802', '10.1109/CVPR.2015.7298949', '10.1109/CVPR.2009.5206529'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'artificial intelligence', 'augmented reality', 'geometry'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Learning to Fuse Proposals from Multiple Scanline Optimizations in Semi-Global Matching': Paper(DOI='10.1007/978-3-030-01261-8_45', crossref_json=None, google_schorlar_metadata=None, title='Learning to Fuse Proposals from Multiple Scanline Optimizations in Semi-Global Matching', authors=['Johannes L Schönberger', 'Sudipta N Sinha', 'Marc Pollefeys'], abstract='Semi-Global Matching (SGM) uses an aggregation scheme to combine costs from multiple 1D scanline optimizations that tends to hurt its accuracy in difficult scenarios. We propose replacing this aggregation scheme with a new learning-based method that fuses disparity proposals estimated using scanline optimization. Our proposed SGM-Forest algorithm solves this problem using per-pixel classification. SGM-Forest currently ranks 1st on the ETH3D stereo benchmark and is ranked competitively on the Middlebury 2014 and KITTI 2015 benchmarks. It consistently outperforms SGM in challenging settings and under difficult training protocols that demonstrate robust generalization, while adding only a small computational overhead to SGM.', conference=None, journal=None, year=None, reference_list=['10.1109/ICCVW.2011.6130286', '10.1109/CVPR.2011.5995581', '10.1109/ICCV.2015.117', '10.1007/978-3-319-11752-2_4', '10.5244/C.29.90', '10.1109/ICCVW.2013.36', '10.1007/978-3-642-04667-4_14', '10.1109/CVPR.2012.6248074', '10.1109/CVPR.2017.760', '10.1109/CVPR.2013.46', '10.1007/978-3-642-37431-9_36', '10.1007/978-3-540-92957-4_55', '10.1109/TPAMI.2007.1166', '10.5194/isprsannals-I-3-371-2012', '10.1109/IROS.2014.6943263', '10.1109/TPAMI.2011.283', '10.1109/ICCV.2017.17', '10.1109/CVPR.2017.159', '10.1109/LSP.2017.2778306', '10.1109/TPAMI.2009.143', '10.1109/CVPR.2016.614', '10.1109/TPAMI.2012.171', '10.1109/CVPR.2016.438', '10.1109/IVS.2013.6629629', '10.1109/TPAMI.1985.4767639', '10.1109/ICCVW.2017.108', '10.1109/3DV.2016.22', '10.1109/3DV.2016.61', '10.1007/978-3-319-11752-2_3', '10.1109/CVPR.2007.383191', '10.1023/A:1014573219977', '10.1109/IROS.2013.6696922', '10.1109/CVPR.2015.7299148', '10.1007/978-3-319-46487-9_31', '10.1109/CVPR.2017.272', '10.1109/CVPR.2017.703', '10.1109/3DV.2015.16', '10.1109/TPAMI.2017.2766072', '10.1109/CVPR.2017.729', '10.1023/A:1014562312225', '10.1007/978-3-319-10602-1_49', '10.1109/3DPVT.2006.124', '10.1109/TIP.2017.2752370'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'artificial intelligence', 'augmented reality', 'geometry'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Mapping on the Fly: Real-Time 3D Dense Reconstruction, Digital Surface Map and Incremental Orthomosaic Generation for Unmanned Aerial Vehicles': Paper(DOI='10.1007/978-3-319-67361-5_25', crossref_json=None, google_schorlar_metadata=None, title='Mapping on the Fly: Real-Time 3D Dense Reconstruction, Digital Surface Map and Incremental Orthomosaic Generation for Unmanned Aerial Vehicles', authors=['Timo Hinzmann', 'Johannes Lutz Schönberger', 'Marc Pollefeys', 'Roland Siegwart'], abstract=' The reduced operational cost and increased robustness of unmanned aerial vehicles has made them a ubiquitous tool in the commercial, industrial and scientific sector. Especially the ability to map and surveil a large area in a short amount of time makes them interesting for various applications. Generating a map in real-time is essential for first response teams in disaster scenarios such as, e.g. earthquakes, floods, or avalanches or may help other UAVs to localize without the need of Global Navigation Satellite Systems. For this application, we implemented a mapping framework that incrementally generates a dense georeferenced 3D point cloud, a digital surface model, and an orthomosaic and we support our design choices with respect to computational costs and its performance in diverse terrain. For accurate estimation of the camera poses, we employ a cost-efficient sensor setup consisting of a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/IROS.2014.6943040', '10.1016/j.isprsjprs.2014.07.015', '10.1007/978-3-319-26054-9_5', '10.1007/s11263-006-0002-3', '10.1109/ICCV.2005.86', '10.1145/1141911.1141966', '10.1007/s11633-014-0799-0', '10.1007/978-3-540-88458-3_9', '10.1007/978-3-319-26054-9_23', '10.1109/ICISA.2014.6847403', '10.1109/ICRA.2011.5979641', '10.15607/RSS.2013.IX.037', '10.15607/RSS.2015.XI.006', '10.1007/s001380050120', '10.1147/sj.41.0025', '10.1109/ICRA.2014.6906892', '10.1109/CCA.2014.6981466'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'artificial intelligence', 'augmented reality', 'geometry'], conference_acronym='Springer proceedings in advanced robotics (Print)', publisher=None, query_handler=None),\n",
       " 'Adaptive Covariance Matrix Estimation for Multi-Baseline InSAR Data Stacks': Paper(DOI='10.1109/tgrs.2014.2303516', crossref_json=None, google_schorlar_metadata=None, title='Adaptive Covariance Matrix Estimation for Multi-Baseline InSAR Data Stacks', authors=['Michael Schmitt', 'JL Schönberger', 'Uwe Stilla'], abstract='For many multidimensional applications of synthetic aperture radar (SAR) imaging, the estimation of the covariance matrix for each resolution cell is a critical processing step. The context of this work is the application of covariance matrix estimation for multi-baseline interferometric SAR data sets. In order to ensure local stationarity, which is needed for an unbiased estimation, adaptive techniques are necessary. In this paper, a new approach for adaptive covariance matrix estimation is proposed and evaluated based on measures known from the field of image processing. The procedure is centered around the idea of checking whether the neighboring pixels belong to the same statistical distribution as the currently investigated pixel by applying a threshold to the respective probability density function. All inlier pixels are then used to estimate the complex covariance matrix of the reference pixel. From this covariance\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-642-02020-9', '10.1109/MSP.2012.2183773', '10.1109/PIMRC.2003.1259213', '10.1127/1432-8364/2013/0199', '10.1109/IGARSS.2002.1026230', '10.1007/0-306-47633-9', '10.1109/IGARSS.1996.516435', '10.3390/s100100775', '10.1109/TPAMI.1980.4766994', '10.1109/TPAMI.1982.4767223', '10.1109/TPAMI.1985.4767641', '10.1109/36.718849', '10.1364/JOSAA.21.001455', '10.1109/TGRS.2005.864142', '10.1109/TGRS.2010.2076376', '10.1109/TGRS.2003.815240', '10.1007/BFb0111436', '10.1109/36.718859', '10.1109/LGRS.2009.2021489', '10.1016/j.isprsjprs.2012.06.007', '10.1109/MAES.2005.1499278', '10.1109/JSTARS.2012.2196758', '10.1201/9781420054989', '10.1098/rspa.1995.0059', '10.1364/AO.33.004361', '10.1364/AO.40.005954', '10.1109/LGRS.2010.2083631', '10.1109/TGRS.2011.2124465', '10.1109/TGRS.2013.2282406', '10.1109/TGRS.2012.2192937', '10.1109/TGRS.2013.2238947'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'artificial intelligence', 'augmented reality', 'geometry'], conference_acronym='IEEE transactions on geoscience and remote sensing', publisher=None, query_handler=None),\n",
       " '3-D Object Retrieval and Recognition With Hypergraph Analysis': Paper(DOI='10.1109/tip.2012.2199502', crossref_json=None, google_schorlar_metadata=None, title='3-D Object Retrieval and Recognition With Hypergraph Analysis', authors=['Yue Gao', 'Meng Wang', 'Dacheng Tao', 'Rongrong Ji', 'Qionghai Dai'], abstract='View-based 3-D object retrieval and recognition has become popular in practice, e.g., in computer aided design. It is difficult to precisely estimate the distance between two objects represented by multiple views. Thus, current view-based 3-D object retrieval and recognition methods may not perform well. In this paper, we propose a hypergraph analysis approach to address this problem by avoiding the estimation of the distance between objects. In particular, we construct multiple hypergraphs for a set of 3-D objects based on their 2-D views. In these hypergraphs, each vertex is an object, and each edge is a cluster of views. Therefore, an edge connects multiple vertices. We define the weight of each edge based on the similarities between any two views within the cluster. Retrieval and recognition are performed based on the hypergraphs. Therefore, our method can explore the higher order relationship among\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TCSVT.2009.2017400', '10.1111/1467-8659.00669', '10.1109/TIP.2011.2170081', '10.1016/j.image.2010.10.006', '10.1023/A:1026543900054', '10.1109/CVPR.2010.5540012', '10.1109/CVPR.2003.1211497', '10.1109/SMI.2008.4547955', '10.1109/LSP.2008.2010819', '10.1109/ICIP.2003.1247355', '10.1016/j.patcog.2007.03.018', '10.1016/0020-0190(83)90042-X', '10.1109/ICCV.2007.4408829', '10.1016/j.patcog.2006.04.034', '10.1109/ICPR.1994.576361', '10.1145/1118890.1118893', '10.1145/1126004.1126006', '10.1016/j.patcog.2010.08.022', '10.1109/TPAMI.2006.134', '10.1109/TNN.2011.2157359', '10.1109/SMI.2004.1314504', '10.1007/s11263-009-0277-2', '10.1109/TMM.2011.2160619', '10.1109/TMM.2006.886359', '10.1109/TSP.2012.2190406', '10.1145/582415.582418', '10.1109/TMM.2009.2012919', '10.1007/s11263-011-0472-9', '10.1016/S0923-5965(00)00019-9', '10.1109/34.55109', '10.1109/TIP.2011.2105496', '10.1109/34.765655', '10.1145/571647.571648', '10.1109/TMM.2010.2055045', '10.1016/S0262-8856(98)00119-X', '10.1007/s11042-007-0188-6', '10.1016/j.patcog.2009.07.012', '10.1109/TIP.2008.2003404', '10.1016/j.neucom.2009.11.050', '10.1145/1618452.1618505', '10.1109/CVPR.2011.5995344', '10.1109/CVPR.2010.5540118', '10.1109/TSMCC.2007.905756', '10.1007/s11042-007-0181-0', '10.1016/j.neucom.2011.06.002', '10.1007/s11042-009-0424-3', '10.1109/TCBB.2006.43', '10.1016/j.cad.2006.06.007', '10.1109/TIT.1982.1056489', '10.1006/cviu.2000.0889', '10.1109/34.21797', '10.1145/2072298.2072054', '10.1109/ICIG.2009.28', '10.1109/TPAMI.2008.176', '10.1007/s11263-009-0298-x', '10.1109/CVPR.2008.4587500'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Pattern Recognition'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Tag ranking': Paper(DOI='10.21884/ijmter.2017.4096.ambg3', crossref_json=None, google_schorlar_metadata=None, title='Tag ranking', authors=['Dong Liu', 'Xian-Sheng Hua', 'Linjun Yang', 'Meng Wang', 'Hong-Jiang Zhang'], abstract='Social media sharing web sites like Flickr allow users to annotate images with free tags, which significantly facilitate Web image search and organization. However, the tags associated with an image generally are in a random order without any importance or relevance information, which limits the effectiveness of these tags in search and other applications. In this paper, we propose a tag ranking scheme, aiming to automatically rank the tags associated with a given image according to their relevance to the image content. We first estimate initial relevance scores for the tags based on probability density estimation, and then perform a random walk over a tag similarity graph to refine the relevance scores. Experimental results on a 50, 000 Flickr photo collection show that the proposed tag ranking method is both effective and efficient. We also apply tag ranking into three applications: (1) tag-based image search, (2) tag\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['lidar'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Multimodal Deep Autoencoder for Human Pose Recovery': Paper(DOI='10.1109/tip.2015.2487860', crossref_json=None, google_schorlar_metadata=None, title='Multimodal Deep Autoencoder for Human Pose Recovery', authors=['Chaoqun Hong', 'Jun Yu', 'Jian Wan', 'Dacheng Tao', 'Meng Wang'], abstract='Video-based human pose recovery is usually conducted by retrieving relevant poses using image features. In the retrieving process, the mapping between 2D images and 3D poses is assumed to be linear in most of the traditional methods. However, their relationships are inherently non-linear, which limits recovery performance of these methods. In this paper, we propose a novel pose recovery method using non-linear mapping with multi-layered deep neural network. It is based on feature extraction with multimodal fusion and back-propagation deep learning. In multimodal fusion, we construct hypergraph Laplacian with low-rank representation. In this way, we obtain a unified feature description by standard eigen-decomposition of the hypergraph Laplacian matrix. In back-propagation deep learning, we learn a non-linear mapping from 2D images to 3D poses with parameter fine-tuning. The experimental results on\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1553374.1553463', '10.1109/CVPR.2014.303', '10.1007/s11263-009-0273-6', '10.1109/ICCV.2011.6126528', '10.1109/TPAMI.2005.220', '10.1109/TPAMI.2013.248', '10.1109/ICCV.2003.1238424', '10.1109/CVPR.2011.5995741', '10.1109/TIP.2014.2364113', '10.1109/TIP.2014.2361210', '10.1109/TIP.2014.2343458', '10.1007/s11263-008-0204-y', '10.1561/2200000006', '10.1109/CVPR.2014.214', '10.1109/TCYB.2015.2414920', '10.1109/TNNLS.2015.2461554', '10.1109/TIP.2014.2302677', '10.1109/TPAMI.2006.21', '10.1109/TPAMI.2008.35', '10.1109/ICCV.1999.790422', '10.1109/TIP.2014.2358082', '10.1109/CVPR.2000.854946', '10.1109/TIP.2014.2329765', '10.1145/1390156.1390294', '10.1109/CVPR.2013.471', '10.1109/CVPR.2005.177', '10.1109/TSMCB.2009.2039566', '10.1109/ICPR.2006.851', '10.1109/CVPR.2014.299', '10.1109/34.993558'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Pattern Recognition'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Crowded Scene Analysis: A Survey': Paper(DOI='10.1109/tcsvt.2014.2358029', crossref_json=None, google_schorlar_metadata=None, title='Crowded Scene Analysis: A Survey', authors=['Teng Li', 'Huan Chang', 'Meng Wang', 'Bingbing Ni', 'Richang Hong', 'Shuicheng Yan'], abstract='Automated scene analysis has been a topic of great interest in computer vision and cognitive science. Recently, with the growth of crowd phenomena in the real world, crowded scene analysis has attracted much attention. However, the visual occlusions and ambiguities in crowded scenes, as well as the complex behaviors and scene semantics, make the analysis a challenging task. In the past few years, an increasing number of works on the crowded scene analysis have been reported, which covered different aspects including crowd motion pattern learning, crowd behavior and activity analyses, and anomaly detection in crowds. This paper surveys the state-of-the-art techniques on this topic. We first provide the background knowledge and the available features related to crowded scenes. Then, existing models, popular algorithms, evaluation protocols, and system performance are provided corresponding to\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-7091-6874-5_3', '10.1109/ICCV.2009.5459301', '10.1007/978-3-642-41512-8_2', '10.1007/978-0-85729-670-2', '10.1109/TPAMI.2011.81', '10.1007/s00138-013-0491-3', '10.1109/CVPR.2013.328', '10.5244/C.26.21', '10.1109/TVCG.2012.317', '10.1109/TIFS.2013.2277773', '10.1109/CVPR.2009.5206641', '10.1155/2011/163682', '10.1016/j.cviu.2011.08.006', '10.1142/S0219525908001854', '10.1109/TPAMI.2013.137', '10.1088/1742-5468/2006/10/P10014', '10.1109/TSMCC.2011.2178594', '10.1016/0925-7535(96)81011-3', '10.1007/978-3-642-97651-3', '10.1109/TPAMI.2011.173', '10.1145/566570.566646', '10.1016/j.physd.2005.10.007', '10.1016/j.ijleo.2013.07.166', '10.1109/ICPR.2008.4761183', '10.1109/ICCV.2009.5459376', '10.1109/CVPR.2010.5539884', '10.1109/TSMCC.2004.829274', '10.1109/TPAMI.2006.176', '10.1109/CVPR.2011.5995459', '10.1007/978-3-642-25446-8_15', '10.1103/PhysRevE.51.4282', '10.1109/TSMCC.2012.2215319', '10.1109/VCIP.2011.6116003', '10.1145/1141911.1142008', '10.1016/S0191-2615(01)00015-7', '10.1145/322033.322044', '10.1109/ICME.2012.133', '10.1109/ICDE.2002.994784', '10.1007/978-3-642-33709-3_23', '10.1109/TPAMI.2006.184', '10.1109/ICCV.2011.6126374', '10.1109/CVPR.2007.383072', '10.1109/CVPR.2009.5206771', '10.1109/ICIP.2012.6467182', '10.1016/j.patcog.2012.11.021', '10.1007/978-3-642-38989-4_10', '10.1109/CVPR.2010.5540148', '10.1109/CC.2013.6506940', '10.1109/ICIP.2013.6738584', '10.1109/TSMCB.2012.2192267', '10.1109/FSKD.2012.6234226', '10.1109/ICPR.2008.4761655', '10.1109/JSTSP.2008.2001306', '10.1007/s11263-011-0510-7', '10.1145/2438653.2438670', '10.1109/ISCCSP.2012.6217836', '10.1109/TCSVT.2012.2226526', '10.1007/s12555-011-0511-x', '10.1109/TPAMI.2007.70738', '10.1145/2072508.2072515', '10.1007/3-540-32390-2_2', '10.1109/TCSVT.2013.2248239', '10.1109/ICCV.2013.22', '10.1109/CVPR.2010.5540143', '10.1109/ICINFA.2011.5949043', '10.1109/ROBIO.2011.6181342', '10.1109/CVPR.2007.383267', '10.1109/ICCVW.2011.6130235', '10.1109/TCSVT.2008.927109', '10.1109/CVPR.2010.5539882', '10.1109/CVPR.2007.382977', '10.1109/TPAMI.2012.123', '10.1007/s00138-008-0132-4', '10.1007/978-3-642-22170-5_59', '10.1016/j.cviu.2013.06.007', '10.1007/s00138-011-0341-0', '10.1109/CVPR.2007.383072', '10.1109/TIFS.2013.2272243', '10.1109/AVSS.2013.6636607', '10.1109/TCYB.2013.2242059', '10.1109/CVPR.2010.5539872', '10.1109/ICWAPR.2012.6294781', '10.1109/JSEN.2013.2245889', '10.1109/TCSVT.2013.2276151', '10.1109/TPAMI.2013.111'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Pattern Recognition'], conference_acronym='IEEE transactions on circuits and systems for video technology (Print)', publisher=None, query_handler=None),\n",
       " 'Unified video annotation via multigraph learning': Paper(DOI='10.1109/tcsvt.2009.2017400', crossref_json=None, google_schorlar_metadata=None, title='Unified video annotation via multigraph learning', authors=['Meng Wang', 'Xian-Sheng Hua', 'Richang Hong', 'Jinhui Tang', 'Guo-Jun Qi', 'Yan Song'], abstract='Learning-based video annotation is a promising approach to facilitating video retrieval and it can avoid the intensive labor costs of pure manual annotation. But it frequently encounters several difficulties, such as insufficiency of training data and the curse of dimensionality. In this paper, we propose a method named optimized multigraph-based semi-supervised learning (OMG-SSL), which aims to simultaneously tackle these difficulties in a unified scheme. We show that various crucial factors in video annotation, including multiple modalities, multiple distance functions, and temporal consistency, all correspond to different relationships among video units, and hence they can be represented by different graphs. Therefore, these factors can be simultaneously dealt with by learning with multiple graphs, namely, the proposed OMG-SSL approach. Different from the existing graph-based semi-supervised learning methods\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1291233.1291431', '10.1145/1290082.1290094', '10.1145/1178677.1178716', '10.1145/1101826.1101844', '10.1145/1101149.1101236', '10.1109/TPAMI.2006.212', '10.1145/1101149.1101337', '10.1109/ICME.2004.1394376', '10.1109/TMM.2008.921853', '10.1117/12.205308', '10.1145/1178677.1178722', '10.1109/34.879793', '10.1145/1148170.1148314', '10.1145/1282280.1282368', '10.1109/MMUL.2006.63', '10.1145/957052.957065', '10.1007/11526346_1', '10.1109/TMM.2007.900150', '10.1016/j.cviu.2008.08.003', '10.1145/1027527.1027531', '10.1145/1282280.1282352', '10.1109/ICME.2006.262544', '10.1109/ICME.2000.871495', '10.1145/1282280.1282366', '10.1145/860435.860522', '10.7551/mitpress/9780262033589.001.0001', '10.1109/CVPR.2004.1315274', '10.1145/1282280.1282308', '10.1145/1178677.1178685', '10.1145/1180639.1180768', '10.1109/CVPR.2006.310', '10.1145/1027527.1027665', '10.1145/1291233.1291303', '10.1109/CVPR.2005.317'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia Analysis and Retrieval', 'Computer Vision', 'Aritificial Intelligence'], conference_acronym='IEEE transactions on circuits and systems for video technology (Print)', publisher=None, query_handler=None),\n",
       " 'Bias and Debias in Recommender System: A Survey and Future Directions': Paper(DOI='10.1145/3564284', crossref_json=None, google_schorlar_metadata=None, title='Bias and Debias in Recommender System: A Survey and Future Directions', authors=['Jiawei Chen', 'Hande Dong', 'Xiang Wang', 'Fuli Feng', 'Meng Wang', 'Xiangnan He'], abstract='While recent years have witnessed a rapid growth of research papers on recommender system (RS), most of the papers focus on inventing machine learning models to better fit user behavior data. However, user behavior data is observational rather than experimental. This makes various biases widely exist in the data, including but not limited to selection bias, position bias, exposure bias, and popularity bias. Blindly fitting the data without considering the inherent biases will result in many serious issues, e.g., the discrepancy between offline evaluation and online metrics, hurting user satisfaction and trust on the recommendation service, and so on. To transform the large volume of research models into practical improvements, it is highly urgent to explore the impacts of the biases and perform debiasing when necessary. When reviewing the papers that consider biases in RS, we find that, to our surprise, the studies\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/3306618.3314309', '10.1145/3383313.3418487', '10.1145/3308558.3313697', '10.1145/3289600.3291017', '10.1145/3336191.3371832', '10.18653/v1/2020.acl-main.485', '10.1145/2792838.2800193', '10.1145/3404835.3462919', '10.1145/3308558.3313582', '10.1145/3460231.3473321', '10.1145/3289600.3290999', '10.1145/3404835.3462901', '10.1089/big.2016.0047', '10.1145/1341531.1341545', '10.1145/3460231.3474274', '10.1515/popets-2015-0007', '10.1145/3340531.3411962', '10.1145/3184558.3186905', '10.1145/230538.230561', '10.1145/3437963.3441824', '10.5555/902784', '10.1145/3442381.3449904', '10.1145/3289600.3290958', '10.1007/s11257-015-9165-3', '10.1145/3306618.3314288', '10.1145/1229179.1229181', '10.1145/2645710.2645754', '10.1145/3488560.3498380', '10.1145/985921.986115', '10.1287/mnsc.2018.3093', '10.1145/3437963.3441769', '10.1145/3219819.3220028', '10.1145/3442381.3449866', '10.1145/3404835.3462966', '10.1145/3404835.3462895', '10.1145/3394486.3403229', '10.1002/widm.1356', '10.1145/3404835.3462830', '10.1145/3366423.3380255', '10.1145/3404835.3463097', '10.1109/ICDM.2008.16', '10.1145/3308558.3313416', '10.1145/3366423.3380196', '10.5555/1642718', '10.1145/1401890.1401959', '10.1145/3041021.3054190', '10.1145/3383313.3412262', '10.1145/3409256.3409812', '10.1145/3336191.3371783', '10.1145/3383313.3412265', '10.1145/2835776.2835804', '10.1145/3308560.3317303', '10.5555/2789272.2886805', '10.1145/2124295.2124309', '10.1007/s10462-017-9539-5', '10.1145/3340531.3412031', '10.2307/146317', '10.1145/3460231.3474244', '10.1145/3437963.3441798', '10.1089/big.2014.0063', '10.1145/3366423.3380098', '10.1145/3437963.3441799', '10.1145/3447548.3467289', '10.1145/3442442.3453701', '10.1145/3442381.3450015', '10.1145/2835776.2835837', '10.1145/3404835.3462953', '10.1109/TKDE.2015.2405556', '10.1145/3459637.3482305', '10.1145/3158369', '10.1145/3366423.3380037', '10.1561/1500000066', '10.1145/3404835.3462875', '10.1145/3320496.3320500', '10.1145/3394486.3403384', '10.1145/3178876.3185994', '10.1145/3442381.3449788', '10.1145/3488560.3498427', '10.1145/3437963.3441820', '10.1145/3397271.3401177'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Pattern Recognition'], conference_acronym='ACM transactions on information systems', publisher=None, query_handler=None),\n",
       " 'Visual-textual joint relevance learning for tag-based social image search': Paper(DOI='10.1109/tip.2012.2202676', crossref_json=None, google_schorlar_metadata=None, title='Visual-textual joint relevance learning for tag-based social image search', authors=['Yue Gao', 'Meng Wang', 'Zheng-Jun Zha', 'Jialie Shen', 'Xuelong Li', 'Xindong Wu'], abstract='Due to the popularity of social media websites, extensive research efforts have been dedicated to tag-based social image search. Both visual information and tags have been investigated in the research field. However, most existing methods use tags and visual characteristics either separately or sequentially in order to estimate the relevance of images. In this paper, we propose an approach that simultaneously utilizes both visual and textual information to estimate the relevance of user tagged images. The relevance estimation is determined with a hypergraph learning approach. In this method, a social image hypergraph is constructed, where vertices represent images and hyperedges represent visual or textual terms. Learning is achieved with use of a set of pseudo-positive images, where the weights of hyperedges are updated throughout the learning process. In this way, the impact of different tags and visual words\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.21797', '10.1109/CVPR.2010.5540012', '10.1109/TMM.2010.2055045', '10.1109/ICME.2009.5202833', '10.1145/1631144.1631150', '10.1109/CVPR.2008.4587500', '10.1007/978-3-540-89689-0_36', '10.1109/CVPR.2009.5206795', '10.1109/TMM.2009.2012919', '10.1145/582415.582418', '10.1016/j.cviu.2008.08.003', '10.1145/1873951.1874005', '10.1145/1631272.1631359', '10.1109/TMM.2008.2009719', '10.1145/1631272.1631278', '10.1109/34.895972', '10.1145/1823746.1823747', '10.1109/TMM.2011.2160619', '10.1145/1816041.1816044', '10.1109/TMM.2008.2004914', '10.1109/TMM.2009.2030598', '10.1145/1873951.1874028', '10.1023/B:VISI.0000029664.99615.94', '10.1007/3-540-45113-7_24', '10.1109/TPAMI.2011.170', '10.1109/TCSVT.2009.2017400', '10.1145/1646396.1646452', '10.1145/1459359.1459364', '10.1145/1291233.1291446', '10.1109/CVPR.2009.5206729', '10.1145/1873951.1873970', '10.1109/TMM.2011.2134078', '10.1007/978-3-540-89689-0_16', '10.1145/1367497.1367542', '10.1007/978-3-540-85481-4_19', '10.1109/CVPR.2010.5539988', '10.1145/1460096.1460126', '10.1109/ICME.2009.5202506', '10.1145/2072298.2072054', '10.1109/TMM.2011.2174782', '10.1109/CVPR.2011.5995344', '10.1109/TCSVT.2008.2005607', '10.1109/TIP.2011.2170081', '10.1109/TMM.2008.917359', '10.1007/s00530-010-0223-8', '10.1109/TIP.2011.2176950', '10.1145/1873951.1874013', '10.1109/TMM.2010.2087744', '10.1145/1290082.1290111', '10.1109/CVPR.2010.5540118', '10.1145/1282280.1282352', '10.1007/s11263-011-0472-9', '10.1016/j.patcog.2010.07.014', '10.1109/ICIG.2009.28', '10.1145/1390334.1390495'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Pattern Recognition'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Adaptive hypergraph learning and its application in image classification': Paper(DOI='10.1109/tip.2012.2190083', crossref_json=None, google_schorlar_metadata=None, title='Adaptive hypergraph learning and its application in image classification', authors=['Jun Yu', 'Dacheng Tao', 'Meng Wang'], abstract='Recent years have witnessed a surge of interest in graph-based transductive image classification. Existing simple graph-based transductive learning methods only model the pairwise relationship of images, however, and they are sensitive to the radius parameter used in similarity calculation. Hypergraph learning has been investigated to solve both difficulties. It models the high-order relationship of samples by using a hyperedge to link multiple samples. Nevertheless, the existing hypergraph learning methods face two problems, i.e., how to generate hyperedges and how to handle a large set of hyperedges. This paper proposes an adaptive hypergraph learning method for transductive image classification. In our method, we generate hyperedges by linking images and their nearest neighbors. By varying the size of the neighborhood, we are able to generate a set of hyperedges for each image and its visual\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1873951.1874005', '10.1109/34.21797', '10.1109/TKDE.2007.190672', '10.1162/089976603321780317', '10.1109/CVPR.2006.68', '10.1109/ACV.1994.341300', '10.1145/279943.279962', '10.1109/TSMCC.2004.843247', '10.1109/CVPR.2005.89', '10.1145/1401890.1401971', '10.1109/CVPR.2008.4587500', '10.1093/bioinformatics/btp467', '10.1109/ACVMOT.2005.107', '10.1109/CVPR.2010.5540012', '10.1007/11871842_39', '10.1109/TCSVT.2009.2017400', '10.1109/TPAMI.2007.70765', '10.1109/CVPR.2011.5995487', '10.1109/TPAMI.2008.235', '10.1145/1143844.1143847', '10.1109/TIP.2011.2158225', '10.1109/TIP.2010.2051632', '10.1109/TIP.2011.2105496', '10.1109/TIP.2009.2032939', '10.1109/ICASSP.2004.1326716', '10.1109/TMM.2010.2055045', '10.7551/mitpress/9780262033589.001.0001', '10.1109/ICCAD.1996.569592', '10.1109/TPAMI.2011.25', '10.1016/0012-365X(93)90322-K', '10.1080/0308108031000084374', '10.1109/CVPR.2010.5540018', '10.1109/34.993558', '10.1109/TMM.2009.2012919', '10.1109/5.726791'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Pattern Recognition'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Learning to Rank Using User Clicks and Visual Features for Image Retrieval': Paper(DOI='10.1109/tcyb.2014.2336697', crossref_json=None, google_schorlar_metadata=None, title='Learning to Rank Using User Clicks and Visual Features for Image Retrieval', authors=['Jun Yu', 'Dacheng Tao', 'Meng Wang', 'Yong Rui'], abstract='The inconsistency between textual features and visual contents can cause poor image search results. To solve this problem, click features, which are more reliable than textual information in justifying the relevance between a query and clicked images, are adopted in image ranking model. However, the existing ranking model cannot integrate visual features, which are efficient in refining the click-based search results. In this paper, we propose a novel ranking model based on the learning to rank framework. Visual features and click features are simultaneously utilized to obtain the ranking model. Specifically, the proposed approach is based on large margin structured output learning and the visual consistency is integrated with the click features through a hypergraph regularizer term. In accordance with the fast alternating linearization method, we design a novel algorithm to optimize the objective function. This\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TMM.2012.2190387', '10.1109/TIP.2012.2202676', '10.1016/j.cviu.2013.03.007', '10.1145/582415.582418', '10.1109/TSMCB.2009.2039566', '10.1016/j.neucom.2013.02.017', '10.1016/j.neucom.2012.09.001', '10.1145/1015330.1015341', '10.1109/TIP.2013.2255302', '10.1145/1150402.1150429', '10.1109/TIP.2012.2207397', '10.1007/978-1-4471-2099-5_24', '10.2200/S00348ED1V01Y201104HLT012', '10.1007/s10107-012-0530-2', '10.1137/0802028', '10.1109/TMM.2012.2187181', '10.1109/TIP.2012.2190083', '10.1145/1148170.1148205', '10.1145/1273496.1273513', '10.1109/CVPR.2011.5995315', '10.1145/1390156.1390306', '10.1145/1459359.1459379', '10.1145/1180639.1180654', '10.1145/1459359.1459378', '10.1145/1291233.1291446', '10.1145/1718487.1718510', '10.1109/TIT.2008.929939', '10.1145/1277741.1277809', '10.1561/1500000016', '10.1145/582415.582418', '10.1109/TMM.2012.2199970', '10.1145/1645953.1646301', '10.1109/TCYB.2014.2307862', '10.1109/TIP.2014.2311377', '10.1109/TNNLS.2014.2330900', '10.1109/TMM.2013.2284755', '10.1007/s11263-014-0703-y', '10.1109/TCSVT.2008.2005607', '10.1109/TIP.2014.2328894', '10.1109/TNNLS.2013.2293418'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Pattern Recognition'], conference_acronym='IEEE transactions on cybernetics (Print)', publisher=None, query_handler=None),\n",
       " 'Multimodal graph-based reranking for web image search': Paper(DOI='10.1109/tip.2012.2207397', crossref_json=None, google_schorlar_metadata=None, title='Multimodal graph-based reranking for web image search', authors=['Meng Wang', 'Hao Li', 'Dacheng Tao', 'Ke Lu', 'Xindong Wu'], abstract='This paper introduces a web image search reranking approach that explores multiple modalities in a graph-based learning scheme. Different from the conventional methods that usually adopt a single modality or integrate multiple modalities into a long feature vector, our approach can effectively integrate the learning of relevance scores, weights of modalities, and the distance metric and its scaling for each modality into a unified scheme. In this way, the effects of different modalities can be adaptively modulated and better reranking performance can be achieved. We conduct experiments on a large dataset that contains more than 1000 queries and 1 million images to evaluate our approach. Experimental results demonstrate that the proposed reranking approach is more robust than using each individual modality, and it also performs better than many existing methods.', conference=None, journal=None, year=None, reference_list=['10.1109/TIP.2011.2134107', '10.1109/TMM.2011.2160619', '10.1109/CVPR.2009.5206695', '10.1109/TCSVT.2009.2017400', '10.1145/957052.957065', '10.1145/1180639.1180768', '10.1145/1027527.1027531', '10.1109/ICPR.2008.4761472', '10.1109/34.879793', '10.1109/TIP.2009.2035866', '10.1126/science.290.5500.2323', '10.1145/1291233.1291446', '10.1145/1571941.1572027', '10.1109/TCSVT.2009.2026951', '10.1145/1459359.1459378', '10.1145/1101149.1101236', '10.1007/3-540-45113-7_24', '10.1109/TIP.2012.2190083', '10.1016/j.cviu.2008.08.003', '10.1145/1963405.1963447', '10.1145/1027527.1027665', '10.1109/TIP.2011.2159227', '10.1145/2072298.2072367', '10.1109/CVPR.2010.5540092', '10.1109/TMM.2009.2012919', '10.1109/MMUL.2011.36', '10.1145/2009916.2010078', '10.1145/582415.582418', '10.1145/1101149.1101288', '10.1109/ICDMW.2009.46', '10.1109/TMM.2010.2055045', '10.1145/1282280.1282331', '10.1145/2072298.2072346', '10.1109/TMM.2010.2103931', '10.1109/TKDE.2007.190672', '10.1145/1873951.1873977', '10.1109/TKDE.2009.126', '10.1109/TPAMI.2008.121', '10.1109/TIP.2006.877491', '10.1007/s11063-005-2192-z', '10.1145/1816041.1816048'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Pattern Recognition'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Aspect ranking: identifying important product aspects from online consumer reviews': Paper(DOI='10.21884/ijmter.2017.4019.2ztzg', crossref_json=None, google_schorlar_metadata=None, title='Aspect ranking: identifying important product aspects from online consumer reviews', authors=['Jianxing Yu', 'Zheng-Jun Zha', 'Meng Wang', 'Tat-Seng Chua'], abstract='In this paper, we dedicate to the topic of aspect ranking, which aims to automatically identify important product aspects from online consumer reviews. The important aspects are identified according to two observations:(a) the important aspects of a product are usually commented by a large number of consumers; and (b) consumers’ opinions on the important aspects greatly influence their overall opinions on the product. In particular, given consumer reviews of a product, we first identify the product aspects by a shallow dependency parser and determine consumers’ opinions on these aspects via a sentiment classifier. We then develop an aspect ranking algorithm to identify the important aspects by simultaneously considering the aspect frequency and the influence of consumers’ opinions given to each aspect on their overall opinions. The experimental results on 11 popular products in four domains demonstrate the effectiveness of our approach. We further apply the aspect ranking results to the application of documentlevel sentiment classification, and improve the performance significantly.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Pattern Recognition'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Revisiting Graph based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach': Paper(DOI='10.1609/aaai.v34i01.5330', crossref_json=None, google_schorlar_metadata=None, title='Revisiting Graph based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach', authors=['Lei Chen', 'Le Wu', 'Richang Hong', 'Kun Zhang', 'Meng Wang'], abstract='Graph Convolutional Networks~(GCNs) are state-of-the-art graph based representation learning models by iteratively stacking multiple layers of convolution aggregation operations and non-linear activation operations. Recently, in Collaborative Filtering~(CF) based Recommender Systems~(RS), by treating the user-item interaction behavior as a bipartite graph, some researchers model higher-layer collaborative signals with GCNs. These GCN based recommender models show superior performance compared to traditional works. However, these models suffer from training difficulty with non-linear activations for large user-item graphs. Besides, most GCN based models could not model deeper layers due to the over smoothing effect with the graph convolution operation. In this paper, we revisit GCN based CF models from two aspects. First, we empirically show that removing non-linearities would enhance recommendation performance, which is consistent with the theories in simple graph convolutional networks. Second, we propose a residual network structure that is specifically designed for CF with user-item interaction modeling, which alleviates the over smoothing problem in graph convolution aggregation operation with sparse user-item interaction data. The proposed model is a linear model and it is easy to train, scale to large datasets, and yield better efficiency and effectiveness on two real datasets. We publish the source code at https://github. com/newlei/LR-GCCF.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Pattern Recognition'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Beyond distance measurement: Constructing neighborhood similarity for video annotation': Paper(DOI='10.1109/tmm.2009.2012919', crossref_json=None, google_schorlar_metadata=None, title='Beyond distance measurement: Constructing neighborhood similarity for video annotation', authors=['Meng Wang', 'Xian-Sheng Hua', 'Jinhui Tang', 'Richang Hong'], abstract='In the past few years, video annotation has benefited a lot from the progress of machine learning techniques. Recently, graph-based semi-supervised learning has gained much attention in this domain. However, as a crucial factor of these algorithms, the estimation of pairwise similarity has not been sufficiently studied. Generally, the similarity of two samples is estimated based on the Euclidean distance between them. But we will show that the similarity between two samples is not merely related to their distance but also related to the distribution of surrounding samples and labels. It is shown that the traditional distance-based similarity measure may lead to high classification error rates even on several simple datasets. To address this issue, we propose a novel neighborhood similarity measure, which explores the local sample and label distributions. We show that the neighborhood similarity between two samples\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TIT.1964.1053633', '10.1145/1291233.1291303', '10.1145/1291233.1291431', '10.1145/1180639.1180768', '10.1109/TPAMI.2007.70714', '10.1214/ss/1028905973', '10.1007/11526346_1', '10.1109/TMM.2007.900150', '10.1145/1027527.1027531', '10.1109/TMM.2008.921853', '10.1145/1101826.1101844', '10.1145/1180639.1180855', '10.1109/ICASSP.2004.1326716', '10.7551/mitpress/9780262033589.001.0001', '10.1145/1961189.1961199', '10.1109/TPAMI.2004.127', '10.1109/34.879793', '10.1023/A:1026543900054', '10.1145/1178677.1178722', '10.1145/1101149.1101236', '10.1109/TPAMI.2006.212'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia Analysis and Retrieval', 'Computer Vision', 'Aritificial Intelligence'], conference_acronym='IEEE transactions on multimedia', publisher=None, query_handler=None),\n",
       " 'Event Driven Web Video Summarization by Tag Localization and Key-Shot Identification': Paper(DOI='10.1109/tmm.2012.2185041', crossref_json=None, google_schorlar_metadata=None, title='Event Driven Web Video Summarization by Tag Localization and Key-Shot Identification', authors=['Meng Wang', 'Richang Hong', 'Guangda Li', 'Z Zha', 'Shuicheng Yan', 'T Chua'], abstract='With the explosive growth of web videos on the Internet, it becomes challenging to efficiently browse hundreds or even thousands of videos. When searching an event query, users are often bewildered by the vast quantity of web videos returned by search engines. Exploring such results will be time consuming and it will also degrade user experience. In this paper, we present an approach for event driven web video summarization by tag localization and key-shot mining. We first localize the tags that are associated with each video into its shots. Then, we estimate the relevance of the shots with respect to the event query by matching the shot-level tags with the query. After that, we identify a set of key-shots from the shots that have high relevance scores by exploring the repeated occurrence characteristic of key sub-events. Following the scheme in [6] and [22], we provide two types of summaries, i.e., threaded video\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TMM.2007.911830', '10.1109/TCSVT.2008.2005607', '10.1145/1816041.1816055', '10.1023/B:VISI.0000029664.99615.94', '10.1145/1646396.1646452', '10.1109/34.946990', '10.1109/TPAMI.2011.265', '10.1145/1991996.1992033', '10.1109/JCDL.2004.1336123', '10.1007/s00530-010-0223-8', '10.1007/978-3-642-11301-7_55', '10.1145/1631272.1631411', '10.1007/978-3-540-33215-2_2', '10.1145/1298306.1298309', '10.1145/1291233.1291278', '10.1007/s00530-003-0076-5', '10.1145/1873951.1874159', '10.1145/1459359.1459448', '10.1145/1291233.1291280', '10.1126/science.1136800', '10.1109/MSP.2006.1621449', '10.1145/1631144.1631154', '10.1109/TMM.2008.2009703', '10.1145/860435.860444', '10.1109/TMM.2009.2012919', '10.1145/1291233.1291448', '10.1109/TCSVT.2009.2017400', '10.1145/2043612.2043613', '10.1016/j.cviu.2008.08.003', '10.1109/CBMI.2010.5529899', '10.1145/1386352.1386358', '10.1155/S1110865703210052', '10.1145/1571941.1572010', '10.1109/TMM.2007.898928', '10.1016/j.jvcir.2007.04.002', '10.1016/j.cviu.2009.08.002'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Pattern Recognition'], conference_acronym='IEEE transactions on multimedia', publisher=None, query_handler=None),\n",
       " 'Histogram of gabor phase patterns (hgpp): A novel object representation approach for face recognition': Paper(DOI='10.1109/tip.2006.884956', crossref_json=None, google_schorlar_metadata=None, title='Histogram of gabor phase patterns (hgpp): A novel object representation approach for face recognition', authors=['Baochang Zhang', 'Shiguang Shan', 'Xilin Chen', 'Wen Gao'], abstract='A novel object descriptor, histogram of Gabor phase pattern (HGPP), is proposed for robust face recognition. In HGPP, the quadrant-bit codes are first extracted from faces based on the Gabor transformation. Global Gabor phase pattern (GGPP) and local Gabor phase pattern (LGPP) are then proposed to encode the phase variations. GGPP captures the variations derived from the orientation changing of Gabor wavelet at a given scale (frequency), while LGPP encodes the local neighborhood variations by using a novel local XOR pattern (LXP) operator. They are both divided into the nonoverlapping rectangular regions, from which spatial histograms are extracted and concatenated into an extended histogram feature to represent the original image. Finally, the recognition is performed by using the nearest-neighbor classifier with histogram intersection as the similarity measurement. The features of HGPP lie in two\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICPR.1996.546848', '10.1109/34.598228', '10.1109/CVPR.1991.139758', '10.1088/0954-898X/7/3/002', '10.1016/0893-6080(95)00051-8', '10.1109/TIP.2003.819223', '10.1016/0031-3203(92)90121-X', '10.1049/el:19990213', '10.1109/34.244676', '10.1109/TPAMI.2003.1227981', '10.1007/BF00130487', '10.1109/34.589215', '10.1117/12.234802', '10.1109/34.879790', '10.1109/34.598227', '10.1109/TPAMI.2002.1017623', '10.1098/rspb.1980.0020', '10.1109/34.598235', '10.1109/12.210173', '10.1006/cviu.2000.0897', '10.1109/TIP.2002.999679', '10.1006/cviu.1999.0830', '10.1109/TPAMI.2004.32', '10.1109/ICCV.2005.147', '10.1109/ICCV.1998.710780', '10.1109/IJCNN.2005.1556154', '10.1109/5.381842', '10.1145/954339.954342'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Pattern Recognition', 'Machine Learning', 'Face Recognition'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Shift-net: Image inpainting via deep feature rearrangement': Paper(DOI='10.1007/978-3-030-01264-9_1', crossref_json=None, google_schorlar_metadata=None, title='Shift-net: Image inpainting via deep feature rearrangement', authors=['Zhaoyi Yan', 'Xiaoming Li', 'Mu Li', 'Wangmeng Zuo', 'Shiguang Shan'], abstract='Deep convolutional networks (CNNs) have exhibited their potential in image inpainting for producing plausible results. However, in most existing methods, eg, context encoder, the missing parts are predicted by propagating the surrounding convolutional features through a fully connected layer, which intends to produce semantically plausible but blurry result. In this paper, we introduce a special shift-connection layer to the U-Net architecture, namely Shift-Net, for filling in missing regions of any shape with sharp structures and fine-detailed textures. To this end, the encoder feature of the known region is shifted to serve as an estimation of the missing parts. A guidance loss is introduced on decoder feature to minimize the distance between the decoder feature after fully connected layer and the ground-truth encoder feature of the missing parts. With such constraint, the decoder feature in missing region can be used to guide the shift of encoder feature in known region. An end-to-end learning algorithm is further developed to train the Shift-Net. Experiments on the Paris StreetView and Places datasets demonstrate the efficiency and effectiveness of our Shift-Net in producing sharper, fine-detailed, and visually plausible results. The codes and pre-trained models are available at https://github. com/Zhaoyi-Yan/Shift-Net.', conference=None, journal=None, year=None, reference_list=['10.1145/1531326.1531330', '10.1007/978-3-642-15558-1_3', '10.1145/2185520.2185597', '10.1145/882262.882267', '10.1109/ICCV.1999.790383', '10.1167/16.12.326', '10.1109/CVPR.2017.397', '10.1109/ICCV.2017.167', '10.1145/3072959.3073659', '10.1109/CVPR.2017.632', '10.1109/TPAMI.2004.10', '10.1007/978-3-319-46475-6_43', '10.1007/978-3-319-11752-2_43', '10.1109/TIP.2007.906269', '10.1109/ICIP.2011.6116441', '10.1109/CVPR.2017.19', '10.1109/CVPR.2016.272', '10.1109/CVPR.2017.624', '10.1109/CVPR.2017.740', '10.1109/CVPR.2015.7299155', '10.1109/CVPR.2016.278', '10.1109/ICCV.2009.5459159', '10.1109/CVPR.2008.4587842', '10.1145/1073204.1073274', '10.1109/TPAMI.2007.60', '10.1109/TIP.2010.2042098', '10.1109/CVPR.2017.434', '10.1109/CVPR.2017.728', '10.1109/TPAMI.2017.2723009', '10.1109/ICCV.2017.244'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Pattern Recognition', 'Machine Learning', 'Face Recognition'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Biometric recognition: Security and privacy concerns': Paper(DOI='10.1109/msecp.2003.1193209', crossref_json=None, google_schorlar_metadata=None, title='Biometric recognition: Security and privacy concerns', authors=['Salil Prabhakar', 'Sharath Pankanti', 'Anil K Jain'], abstract='Biometrics offers greater security and convenience than traditional methods of personal recognition. In some applications, biometrics can replace or supplement the existing technology. In others, it is the only viable approach. But how secure is biometrics? And what are the privacy implications?.', conference=None, journal=None, year=None, reference_list=['10.1109/5.628723', '10.1145/310930.310988', '10.1109/ISIT.2002.1023680', '10.1109/ICPR.2002.1048144', '10.1117/12.462719'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Machine Learning', 'Image processing', 'Pattern Recognition', 'Computer Vision'], conference_acronym='IEEE security & privacy', publisher=None, query_handler=None),\n",
       " 'An identity-authentication system using fingerprints': Paper(DOI='10.1109/jproc.1997.628673', crossref_json=None, google_schorlar_metadata=None, title='An identity-authentication system using fingerprints', authors=['Anil K Jain', 'Lin Hong', 'Sharath Pankanti', 'Ruud Bolle'], abstract='Fingerprint verification is an important biometric technique for personal identification. We describe the design and implementation of a prototype automatic identity-authentication system that uses fingerprints to authenticate the identity of an individual. We have developed an improved minutiae-extraction algorithm that is faster and more accurate than our earlier algorithm (1995). An alignment-based minutiae-matching algorithm has been proposed. This algorithm is capable of finding the correspondences between input minutiae and the stored template without resorting to exhaustive search and has the ability to compensate adaptively for the nonlinear deformations and inexact transformations between an input and a template. To establish an objective assessment of our system, both the Michigan State University and the National Institute of Standards and Technology NIST 9 fingerprint data bases have been used\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Image Processing', 'Machine Learning', 'Medical Imaging'], conference_acronym='Proceedings of the IEEE', publisher=None, query_handler=None),\n",
       " 'Fingerprint-based fuzzy vault: Implementation and performance': Paper(DOI='10.1109/tifs.2007.908165', crossref_json=None, google_schorlar_metadata=None, title='Fingerprint-based fuzzy vault: Implementation and performance', authors=['Karthik Nandakumar', 'Anil K Jain', 'Sharath Pankanti'], abstract='Reliable information security mechanisms are required to combat the rising magnitude of identity theft in our society. While cryptography is a powerful tool to achieve information security, one of the main challenges in cryptosystems is to maintain the secrecy of the cryptographic keys. Though biometric authentication can be used to ensure that only the legitimate user has access to the secret keys, a biometric system itself is vulnerable to a number of threats. A critical issue in biometric systems is to protect the template of a user which is typically stored in a database or a smart card. The fuzzy vault construct is a biometric cryptosystem that secures both the secret key and the biometric template by binding them within a cryptographic framework. We present a fully automatic implementation of the fuzzy vault scheme based on fingerprint minutiae. Since the fuzzy vault stores only a transformed version of the template\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.121791', '10.1109/TIP.2004.834659', '10.1109/ICPR.2002.1048144', '10.1109/ICPR.2002.1047997', '10.1109/SECPRI.2001.924299', '10.1145/319709.319714', '10.1109/TC.2006.138', '10.1147/sj.403.0614', '10.1109/JPROC.2004.827372', '10.1145/982507.982516', '10.1109/TCSVT.2003.818349', '10.1145/1128817.1128845', '10.1109/TPAMI.2007.1004', '10.1007/3-540-45344-X_32', '10.1109/SECPRI.1998.674831', '10.1109/ISIT.2002.1023680', '10.1007/11599548_31', '10.1007/11527923_32', '10.1117/12.665875', '10.1109/34.587996', '10.1109/CVPRW.2006.185'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometric recognition', 'computer vision', 'machine learning', 'applied cryptography'], conference_acronym='IEEE transactions on information forensics and security', publisher=None, query_handler=None),\n",
       " 'Deep learning ensembles for melanoma recognition in dermoscopy images': Paper(DOI='10.18178/ijmlc.2018.8.1.664', crossref_json=None, google_schorlar_metadata=None, title='Deep learning ensembles for melanoma recognition in dermoscopy images', authors=['Noel CF Codella', 'Q-B Nguyen', 'Sharath Pankanti', 'David A Gutman', 'Brian Helba', 'Allan C Halpern', 'John R Smith'], abstract='Melanoma is the deadliest form of skin cancer. While curable with early detection, only highly trained specialists are capable of accurately recognizing the disease. As expertise is in limited supply, automated systems capable of identifying disease could save lives, reduce unnecessary biopsies, and reduce costs. Toward this goal, we propose a system that combines recent developments in deep learning with established machine learning approaches, creating ensembles of methods that are capable of segmenting skin lesions, as well as analyzing the detected area and surrounding tissue for melanoma detection. The system is evaluated using the largest publicly available benchmark dataset of dermoscopic images, containing 900 training and 379 testing images. New state-of-the-art performance levels are demonstrated, leading to an improvement in the area under receiver operating characteristic curve of 7.5\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Machine Learning', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'A Prototype Hand Geometrybased Verification System': Paper(DOI='10.4028/www.scientific.net/amr.756-759.4188', crossref_json=None, google_schorlar_metadata=None, title='A Prototype Hand Geometrybased Verification System', authors=['A Jain', 'A Ross', 'S Pankanti'], abstract=None, conference=None, journal=None, year=None, reference_list=['10.1109/icse.2001.919142', '10.1007/978-3-7091-9170-5_1', '10.1145/226241.226244', '10.1109/apsec.2002.1183010'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'driver assistance', 'public safety', 'surveillance', 'security'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Smart video surveillance: exploring the concept of multiscale spatiotemporal tracking': Paper(DOI='10.1109/msp.2005.1406476', crossref_json=None, google_schorlar_metadata=None, title='Smart video surveillance: exploring the concept of multiscale spatiotemporal tracking', authors=['Arun Hampapur', 'Lisa Brown', 'Jonathan Connell', 'Ahmet Ekin', 'Norman Haas', 'Max Lu', 'Hans Merkl', 'Sharath Pankanti'], abstract='Situation awareness is the key to security. Awareness requires information that spans multiple scales of space and time. Smart video surveillance systems are capable of enhancing situational awareness across multiple scales of space and time. However, at the present time, the component technologies are evolving in isolation. To provide comprehensive, nonintrusive situation awareness, it is imperative to address the challenge of multiscale, spatiotemporal tracking. This article explores the concepts of multiscale spatiotemporal tracking through the use of real-time video analysis, active cameras, multiple object models, and long-term pattern analysis to provide comprehensive situation awareness.', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-540-45243-0_39', '10.1109/5.959339', '10.1007/978-1-4615-0913-4', '10.1109/JRA.1987.1087109', '10.1016/j.imavis.2005.06.007', '10.1109/34.868677', '10.1109/AVSS.2003.1217896', '10.1109/5.959343', '10.1109/34.868683', '10.1109/TPAMI.2003.1227983'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Computer Vision', 'Biometrics'], conference_acronym='IEEE signal processing magazine (Print)', publisher=None, query_handler=None),\n",
       " 'Click prediction for web image reranking using multimodal sparse coding': Paper(DOI='10.1109/tip.2014.2311377', crossref_json=None, google_schorlar_metadata=None, title='Click prediction for web image reranking using multimodal sparse coding', authors=['Jun Yu', 'Yong Rui', 'Dacheng Tao'], abstract='Image reranking is effective for improving the performance of a text-based image search. However, existing reranking algorithms are limited for two main reasons: 1) the textual meta-data associated with images is often mismatched with their actual visual content and 2) the extracted visual features do not accurately describe the semantic similarities between images. Recently, user click information has been used in image reranking, because clicks have been shown to more accurately describe the relevance of retrieved images to search queries. However, a critical problem for click-based methods is the lack of click data, since only a small number of web images have actually been clicked on by users. Therefore, we aim to solve this problem by predicting image clicks. We propose a multimodal hypergraph learning-based sparse coding method for image click prediction, and apply the obtained click data to the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1229179.1229181', '10.1109/TMM.2011.2111363', '10.1109/CVPR.2009.5206695', '10.1109/TCSVT.2009.2017400', '10.1145/1291233.1291446', '10.1145/1835449.1835546', '10.1109/CVPR.2010.5540012', '10.1093/bioinformatics/btp467', '10.1007/978-3-540-85760-0_71', '10.1109/TMM.2011.2160619', '10.1145/1101149.1101236', '10.1109/CVPR.2008.4587500', '10.1162/089976603321780317', '10.1145/1401890.1401971', '10.1007/s00521-013-1362-6', '10.1145/1963405.1963447', '10.1002/cpa.20124', '10.1109/ICCV.2009.5459452', '10.1109/TPAMI.2008.79', '10.1109/TPAMI.2012.63', '10.1109/CVPR.2010.5540003', '10.1109/TIP.2011.2159227', '10.1145/1718487.1718510', '10.1007/978-3-540-78646-7_61', '10.1137/S1064827596304010', '10.1109/CVPR.2010.5540018', '10.1023/B:VISI.0000029664.99615.94', '10.1214/009053604000000067', '10.1145/582415.582418', '10.1023/A:1011139631724', '10.1109/CVPR.2006.68'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia analysis', 'computer vision', 'AI', 'machine learning'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Hierarchical deep click feature prediction for fine-grained image recognition': Paper(DOI='10.1109/tpami.2019.2932058', crossref_json=None, google_schorlar_metadata=None, title='Hierarchical deep click feature prediction for fine-grained image recognition', authors=['Jun Yu', 'Min Tan', 'Hongyuan Zhang', 'Yong Rui', 'Dacheng Tao'], abstract='The click feature of an image, defined as the user click frequency vector of the image on a predefined word vocabulary, is known to effectively reduce the semantic gap for fine-grained image recognition. Unfortunately, user click frequency data are usually absent in practice. It remains challenging to predict the click feature from the visual feature, because the user click frequency vector of an image is always noisy and sparse. In this paper, we devise a  H ierarchical  D eep  W ord  E mbedding (HDWE) model by integrating sparse constraints and an improved RELU operator to address click feature prediction from visual features. HDWE is a coarse-to-fine click feature predictor that is learned with the help of an auxiliary image dataset containing click information. It can therefore discover the hierarchy of word semantics. We evaluate HDWE on three dog and one bird image datasets, in which Clickture-Dog and Clickture\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.patrec.2016.11.004', '10.1109/TPAMI.2015.2505311', '10.1109/ICME.2017.8019407', '10.1109/TIP.2014.2311377', '10.24963/ijcai.2018/87', '10.1109/CVPR.2016.541', '10.1109/TASLP.2014.2377580', '10.1145/3123266.3123326', '10.1109/ICCV.2017.557', '10.1109/TMM.2017.2648498', '10.1109/CVPR.2017.775', '10.1145/3209666', '10.1109/TITS.2015.2506182', '10.1109/TIP.2017.2688133', '10.1007/978-3-319-10590-1_54', '10.1007/978-3-030-01258-8_5', '10.1145/3240508.3240550', '10.1109/CVPR.2017.106', '10.1016/j.neucom.2013.09.054', '10.1109/CVPR.2017.476', '10.1109/TPAMI.2017.2703082', '10.1109/TPAMI.2017.2723400', '10.1109/CVPR.2016.41', '10.1109/CVPR.2017.743', '10.1007/978-3-030-01270-0_35', '10.5244/C.30.24', '10.1007/s11042-018-5703-4', '10.1145/2502081.2502283', '10.1145/2733373.2806243', '10.1109/TIP.2019.2921861', '10.1109/ICCV.2017.208', '10.1109/TPAMI.2017.2699960', '10.1007/978-3-319-46675-0_14', '10.1109/ICME.2017.8019312', '10.1016/j.neucom.2015.04.123', '10.1007/978-3-319-51811-4_11'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia analysis', 'computer vision', 'AI', 'machine learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Learning to rank using user clicks and visual features for image retrieval': Paper(DOI='10.1109/tcyb.2014.2336697', crossref_json=None, google_schorlar_metadata=None, title='Learning to rank using user clicks and visual features for image retrieval', authors=['Jun Yu', 'Dacheng Tao', 'Meng Wang', 'Yong Rui'], abstract='The inconsistency between textual features and visual contents can cause poor image search results. To solve this problem, click features, which are more reliable than textual information in justifying the relevance between a query and clicked images, are adopted in image ranking model. However, the existing ranking model cannot integrate visual features, which are efficient in refining the click-based search results. In this paper, we propose a novel ranking model based on the learning to rank framework. Visual features and click features are simultaneously utilized to obtain the ranking model. Specifically, the proposed approach is based on large margin structured output learning and the visual consistency is integrated with the click features through a hypergraph regularizer term. In accordance with the fast alternating linearization method, we design a novel algorithm to optimize the objective function. This\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TMM.2012.2190387', '10.1109/TIP.2012.2202676', '10.1016/j.cviu.2013.03.007', '10.1145/582415.582418', '10.1109/TSMCB.2009.2039566', '10.1016/j.neucom.2013.02.017', '10.1016/j.neucom.2012.09.001', '10.1145/1015330.1015341', '10.1109/TIP.2013.2255302', '10.1145/1150402.1150429', '10.1109/TIP.2012.2207397', '10.1007/978-1-4471-2099-5_24', '10.2200/S00348ED1V01Y201104HLT012', '10.1007/s10107-012-0530-2', '10.1137/0802028', '10.1109/TMM.2012.2187181', '10.1109/TIP.2012.2190083', '10.1145/1148170.1148205', '10.1145/1273496.1273513', '10.1109/CVPR.2011.5995315', '10.1145/1390156.1390306', '10.1145/1459359.1459379', '10.1145/1180639.1180654', '10.1145/1459359.1459378', '10.1145/1291233.1291446', '10.1145/1718487.1718510', '10.1109/TIT.2008.929939', '10.1145/1277741.1277809', '10.1561/1500000016', '10.1145/582415.582418', '10.1109/TMM.2012.2199970', '10.1145/1645953.1646301', '10.1109/TCYB.2014.2307862', '10.1109/TIP.2014.2311377', '10.1109/TNNLS.2014.2330900', '10.1109/TMM.2013.2284755', '10.1007/s11263-014-0703-y', '10.1109/TCSVT.2008.2005607', '10.1109/TIP.2014.2328894', '10.1109/TNNLS.2013.2293418'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia analysis', 'computer vision', 'AI', 'machine learning'], conference_acronym='IEEE transactions on cybernetics (Print)', publisher=None, query_handler=None),\n",
       " 'Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs': Paper(DOI='10.1007/978-3-319-46478-7_25', crossref_json=None, google_schorlar_metadata=None, title='Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs', authors=['Siddhartha Chandra', 'Iasonas Kokkinos'], abstract=' In this work we propose a structured prediction technique that combines the virtues of Gaussian Conditional Random Fields (G-CRF) with Deep Learning: (a) our structured prediction task has a unique global optimum that is obtained exactly from the solution of a linear system (b) the gradients of our model parameters are analytically computed using closed form expressions, in contrast to the memory-demanding contemporary deep structured prediction approaches [1, 2] that rely on back-propagation-through-time, (c) our pairwise terms do not have to be simple hand-crafted expressions, as in the line of works building on the DenseCRF\\xa0[1, 3], but can rather be ‘discovered’ from data through deep architectures, and (d) out system can trained in an end-to-end manner. Building on standard tools from numerical analysis we develop very efficient algorithms for inference and learning, as well as a customized\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2015.179', '10.1109/CVPR.2016.351', '10.1109/TPAMI.2012.231', '10.1109/CVPR.2015.7298959', '10.1109/CVPR.2015.7298642', '10.1109/CVPR.2015.7298965', '10.1109/CVPR.2016.519', '10.1109/ICCV.2015.339', '10.1109/CVPR.2016.348', '10.1109/ICCV.2015.162', '10.1109/CVPR.2007.382979', '10.1109/CVPR.2012.6247950', '10.1109/ICCV.2015.331', '10.1109/TPAMI.2006.233', '10.1201/9780203492024', '10.1109/CVPR.2016.396', '10.1007/978-3-319-10602-1_48', '10.1109/ICCV.2015.203', '10.1109/CVPR.2016.90', '10.1109/ICCV.2015.304'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Evaluation of combined artificial intelligence and radiologist assessment to interpret screening mammograms': Paper(DOI='10.1007/s00330-023-10423-7', crossref_json=None, google_schorlar_metadata=None, title='Evaluation of combined artificial intelligence and radiologist assessment to interpret screening mammograms', authors=['Thomas Schaffter', 'Diana SM Buist', 'Christoph I Lee', 'Yaroslav Nikulin', 'Dezső Ribli', 'Yuanfang Guan', 'William Lotter', 'Zequn Jie', 'Hao Du', 'Sijia Wang', 'Jiashi Feng', 'Mengling Feng', 'Hyo-Eun Kim', 'Francisco Albiol', 'Alberto Albiol', 'Stephen Morrell', 'Zbigniew Wojna', 'Mehmet Eren Ahsen', 'Umar Asif', 'Antonio Jimeno Yepes', 'Shivanthan Yohanandan', 'Simona Rabinovici-Cohen', 'Darvin Yi', 'Bruce Hoff', 'Thomas Yu', 'Elias Chaibub Neto', 'Daniel L Rubin', 'Peter Lindholm', 'Laurie R Margolies', 'Russell Bailey McBride', 'Joseph H Rothstein', 'Weiva Sieh', 'Rami Ben-Ari', 'Stefan Harrer', 'Andrew Trister', 'Stephen Friend', 'Thea Norman', 'Berkman Sahiner', 'Fredrik Strand', 'Justin Guinney', 'Gustavo Stolovitzky', 'Lester Mackey', 'Joyce Cahoon', 'Li Shen', 'Jae Ho Sohn', 'Hari Trivedi', 'Yiqiu Shen', 'Ljubomir Buturovic', 'Jose Costa Pereira', 'Jaime S Cardoso', 'Eduardo Castro', 'Karl Trygve Kalleberg', 'Obioma Pelka', 'Imane Nedjar', 'Krzysztof J Geras', 'Felix Nensa', 'Ethan Goan', 'Sven Koitka', 'Luis Caballero', 'David D Cox', 'Pavitra Krishnaswamy', 'Gaurav Pandey', 'Christoph M Friedrich', 'Dimitri Perrin', 'Clinton Fookes', 'Bibo Shi', 'Gerard Cardoso Negrie', 'Michael Kawczynski', 'Kyunghyun Cho', 'Can Son Khoo', 'Joseph Y Lo', 'A Gregory Sorensen', 'Hwejin Jung', 'DM DREAM Consortium'], abstract='ImportanceMammography screening currently relies on subjective human interpretation. Artificial intelligence (AI) advances could be used to increase mammography screening accuracy by reducing missed cancers and false positives.ObjectiveTo evaluate whether AI can overcome human mammography interpretation limitations with a rigorous, unbiased evaluation of machine learning algorithms.Design, Setting, and ParticipantsIn this diagnostic accuracy study conducted between September 2016 and November 2017, an international, crowdsourced challenge was hosted to foster AI algorithm development focused on interpreting screening mammography. More than 1100 participants comprising 126 teams from 44 countries participated. Analysis began November 18, 2016.Main Outcomes and MeasurementsAlgorithms used images alone (challenge 1) or combined images, previous examinations (if available\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1001/jamaoncol.2016.5688', '10.1177/0969141321993866', '10.1016/j.canep.2012.02.007', '10.1148/radiol.2020192212', '10.1148/radiol.13122581', '10.1016/j.jacr.2018.06.016', '10.2214/ajr.08.1665', '10.1136/bmj.j4683', '10.1136/bmj.n1872', '10.1148/radiol.2021210391', '10.1148/radiol.222639', '10.1016/j.crad.2019.02.006', '10.1007/s00330-022-08909-x', '10.1016/j.jacr.2021.11.008', '10.1148/radiol.2015151516', '10.2147/clep.S99457', '10.2147/clep.S99467', '10.1016/j.ejca.2017.04.018', '10.1007/s00330-020-07276-9', '10.1001/jamaoncol.2020.3321', '10.1038/s41586-019-1799-6', '10.1093/jnci/djy222', '10.1038/s41591-020-01174-9', '10.1001/jamanetworkopen.2020.0265', '10.1148/radiol.210948', '10.1016/s2589-7500(22)00070-x', '10.1016/s2589-7500(22)00088-7', '10.1016/j.ebiom.2023.104498', '10.1159/000515698', '10.1186/s13244-022-01322-4', '10.1016/S1470-2045(23)00298-X', '10.1016/S2589-7500(23)00153-X'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision'], conference_acronym='European radiology (Internet)', publisher=None, query_handler=None),\n",
       " 'The devil is in the decoder: Classification, regression and gans': Paper(DOI='10.1007/s11263-019-01170-8', crossref_json=None, google_schorlar_metadata=None, title='The devil is in the decoder: Classification, regression and gans', authors=['Zbigniew Wojna', 'Vittorio Ferrari', 'Sergio Guadarrama', 'Nathan Silberman', 'Liang-Chieh Chen', 'Alireza Fathi', 'Jasper Uijlings'], abstract=' Many machine vision applications, such as semantic segmentation and depth prediction, require predictions for every pixel of the input image. Models for such problems usually consist of encoders which decrease spatial resolution while learning a high-dimensional representation, followed by decoders who recover the original input resolution and result in low-dimensional predictions. While encoders have been studied rigorously, relatively few studies address the decoder side. This paper presents an extensive comparison of a variety of decoders for a variety of pixel-wise tasks ranging from classification, regression to synthesis. Our contributions are: (1) decoders matter: we observe significant variance in results between different types of decoders on various problems. (2) We introduce new residual-like connections for decoders. (3) We introduce a novel decoder: bilinear additive upsampling. (4) We\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2014.471', '10.1109/TPAMI.2010.161', '10.1109/TPAMI.2017.2699184', '10.1109/CVPR.2017.195', '10.1109/TPAMI.2015.2439281', '10.1109/CVPR.2016.522', '10.1109/ICCV.2015.316', '10.1109/CVPR.2015.7298761', '10.1109/ICCV.2015.304', '10.1109/ICCV.2011.6126343', '10.1109/CVPR.2016.90', '10.1109/CVPR.2017.243', '10.1007/978-3-319-46487-9_22', '10.1145/2897824.2925974', '10.1109/CVPR.2017.179', '10.1109/CVPR.2016.27', '10.1109/CVPR.2016.182', '10.1109/3DV.2016.32', '10.1162/neco.1989.1.4.541', '10.1109/CVPR.2017.19', '10.1109/CVPR.2017.549', '10.1109/CVPR.2017.106', '10.1109/ICCV.2015.425', '10.1109/CVPR.2015.7298965', '10.1109/ICCV.2001.937655', '10.1007/978-3-319-46484-8_29', '10.1109/CVPR.2017.374', '10.23915/distill.00003', '10.1109/CVPR.2016.71', '10.1007/978-3-319-46448-0_5', '10.1017/CBO9780511812651', '10.1109/TITS.2017.2750080', '10.1007/978-3-319-24574-4_28', '10.1007/s11263-015-0816-y', '10.1109/CVPR.2016.207', '10.1007/978-3-642-33715-4_54', '10.1109/CVPR.2016.308', '10.1007/978-3-642-15555-0_26', '10.1109/CVPR.2015.7299103', '10.1007/978-3-319-46454-1_20', '10.1109/CVPR.2010.5539957', '10.1007/978-3-319-46487-9_40'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Reinforcement Learning', 'Computer Vision', 'Machine Learning', 'NLP', 'Computing with Words'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Large-scale mammography CAD with deformable conv-nets': Paper(DOI='10.1007/978-3-030-00946-5_7', crossref_json=None, google_schorlar_metadata=None, title='Large-scale mammography CAD with deformable conv-nets', authors=['Stephen Morrell', 'Zbigniew Wojna', 'Can Son Khoo', 'Sebastien Ourselin', 'Juan Eugenio Iglesias'], abstract=' State-of-the-art deep learning methods for image processing are evolving into increasingly complex meta-architectures with a growing number of modules. Among them, region-based fully convolutional networks (R-FCN) and deformable convolutional nets (DCN) can improve CAD for mammography: R-FCN optimizes for speed and low consumption of memory, which is crucial for processing the high resolutions of to $$50\\\\,\\\\upmu \\\\hbox {m}$$ used by radiologists. Deformable convolution and pooling can model a wide range of mammographic findings of different morphology and scales, thanks to their versatility. In this study, we present a neural net architecture based on R-FCN/DCN, that we have adapted from the natural image domain\\xa0to suit mammograms—particularly their larger image size—without compromising resolution. We trained the network on a large, recently released dataset (Optimam\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1001/jamainternmed.2015.5231', '10.1001/jamainternmed.2013.1397', '10.1007/978-3-319-46723-8_5', '10.1109/42.538938', '10.1007/978-3-319-46723-8_13', '10.1016/j.media.2016.07.007', '10.1007/s11263-015-0816-y', '10.1007/978-3-319-10602-1_48', '10.1109/ICCV.2017.89', '10.1109/CVPR.2017.106', '10.1109/ICCV.2017.322', '10.1109/CVPR.2016.89', '10.1109/CVPR.2016.308', '10.1007/978-3-319-24574-4_78', '10.1109/CVPR.2018.00745', '10.1109/CVPR.2017.243', '10.1109/CVPR.2017.634', '10.1109/CVPR.2016.90'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'A crowdsourcing approach to develop machine learning models to quantify radiographic joint damage in rheumatoid arthritis': Paper(DOI='10.2139/ssrn.4051459', crossref_json=None, google_schorlar_metadata=None, title='A crowdsourcing approach to develop machine learning models to quantify radiographic joint damage in rheumatoid arthritis', authors=['Dongmei Sun', 'Thanh M Nguyen', 'Robert J Allaway', 'Jelai Wang', 'Verena Chung', 'V Yu Thomas', 'Michael Mason', 'Isaac Dimitrovsky', 'Lars Ericson', 'Hongyang Li', 'Yuanfang Guan', 'Ariel Israel', 'Alex Olar', 'Balint Armin Pataki', 'Gustavo Stolovitzky', 'Justin Guinney', 'Percio S Gulko', 'Mason B Frazier', 'Jake Y Chen', 'James C Costello', 'S Louis Bridges', 'Zbigniew Wojna', 'Anna Krason', 'YanMing Tan', 'RaphaelHaoChong Quek', 'Neelambuj Chaturvedi', 'Michael Stadler', 'Chenfu Shi', 'Krishnakumar Vaithinathan', 'Julian Benadit', 'Duc Tran', 'Tin Nguyen', 'Alexander Biehl', 'Mehrad Mahmoudian', 'Sami Pietilä', 'Tomi Suomi', 'Mikko S Venäläinen', 'Laura L Elo', 'Chenguang Xue', 'Akshat Shreemali', 'Srinivas Chilukuri', 'Khanh-Tung Nguyen-Ba', 'Jay Ji-Hyung Ryu', 'Rui Bai', 'Yilin Wu', 'Yingnan Wu', 'Xiaofu He'], abstract='ImportanceAn automated, accurate method is needed for unbiased assessment quantifying accrual of joint space narrowing and erosions on radiographic images of the hands and wrists, and feet for clinical trials, monitoring of joint damage over time, assisting rheumatologists with treatment decisions. Such a method has the potential to be directly integrated into electronic health records.ObjectivesTo design and implement an international crowdsourcing competition to catalyze the development of machine learning methods to quantify radiographic damage in rheumatoid arthritis (RA).Design, Setting, and ParticipantsThis diagnostic/prognostic study describes the Rheumatoid Arthritis 2–Dialogue for Reverse Engineering Assessment and Methods (RA2-DREAM Challenge), which used existing radiographic images and expert-curated Sharp-van der Heijde (SvH) scores from 2 clinical studies (674 radiographic sets\\xa0…', conference=None, journal=None, year=None, reference_list=['10.2165/00019053-200422001-00002', '10.1109/TPAMI.2019.2913372', '10.1109/TMI.2008.2004401', '10.1016/j.ejrad.2009.04.046', '10.1038/clpt.2013.36', '10.1038/nbt.2877', '10.1200/CCI.17.00018', '10.1016/S1470-2045(16)30560-5', '10.1080/01621459.1952.10483441', '10.1016/j.compag.2019.01.012', '10.1145/2939672.2939785', '10.3389/fnins.2019.00095', '10.1145/1143844.1143864'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Holistic multi-view building analysis in the wild with projection pooling': Paper(DOI='10.1609/aaai.v35i4.16393', crossref_json=None, google_schorlar_metadata=None, title='Holistic multi-view building analysis in the wild with projection pooling', authors=['Zbigniew Wojna', 'Krzysztof Maziarz', 'Łukasz Jocz', 'Robert Pałuba', 'Robert Kozikowski', 'Iasonas Kokkinos'], abstract='We address six different classification tasks related to fine-grained building attributes: construction type, number of floors, pitch and geometry of the roof, facade material, and occupancy class. Tackling such a remote building analysis problem became possible only recently due to growing large-scale datasets of urban scenes. To this end, we introduce a new benchmarking dataset, consisting of 49426 images (top-view and street-view) of 9674 buildings. These photos are further assembled, together with the geometric metadata. The dataset showcases various real-world challenges, such as occlusions, blur, partially visible objects, and a broad spectrum of buildings. We propose a new\\\\emph {projection pooling layer}, creating a unified, top-view representation of the top-view and the side views in a high-dimensional space. It allows us to utilize the building and imagery metadata seamlessly. Introducing this layer improves classification accuracy--compared to highly tuned baseline models--indicating its suitability for building analysis.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Extracting structured information from 2D images': Paper(DOI='10.2139/ssrn.1719046', crossref_json=None, google_schorlar_metadata=None, title='Extracting structured information from 2D images', authors=['Zbigniew Wojna'], abstract='Convolutional neural networks can handle an impressive array of supervised learning tasks while relying on a single backbone architecture, suggesting that one solution fits all vision problems. But for many tasks, we can directly make use of the problem structure within neural networks to deliver more accurate predictions. In this thesis, we propose novel deep learning components that exploit the structured output space of an increasingly complex set of problems. We start from Optical Character Recognition (OCR) in natural scenes and leverage the constraints imposed by a spatial outline of letters and language requirements. Conventional OCR systems do not work well in natural scenes due to distortions, blur, or letter variability. We introduce a new attention-based model, equipped with extra information about the neuron positions to guide its focus across characters sequentially. It beats the previous state-of-the-art benchmark by a significant margin. We then turn to dense labeling tasks employing encoder-decoder architectures. We start with an experimental study that documents the drastic impact that decoder design can have on task performance. Rather than optimizing one decoder per task separately, we propose new robust layers for the upsampling of high-dimensional encodings. We show that these better suit the structured per pixel output across the board of all tasks. Finally, we turn to the problem of urban scene understanding. There is an elaborate structure in both the input space (multi-view recordings, aerial and street-view scenes) and the output space (multiple fine-grained attributes for holistic building understanding). We design\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-0-8176-4545-8_14', '10.1017/cbo9780511809781', '10.2139/ssrn.77429', '10.1086/260062'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'VIPS: a visionbased page segmentation algorithm': Paper(DOI='10.1515/phys-2019-0010', crossref_json=None, google_schorlar_metadata=None, title='VIPS: a visionbased page segmentation algorithm', authors=['Deng Cai', 'Shipeng Yu', 'Ji-Rong Wen', 'Wei-Ying Ma'], abstract='A new web content structure analysis based on visual representation is proposed in this paper. Many web applications such as information retrieval, information extraction and automatic page adaptation can benefit from this structure. This paper presents an automatic top-down, tag-tree independent approach to detect web content structure. It simulates how a user understands web layout structure based on his visual perception. Comparing to other existing techniques, our approach is independent to underlying documentation representation such as HTML and works well even when the HTML structure is far different from layout structure. Experiments show satisfactory results.', conference=None, journal=None, year=None, reference_list=['10.1155/2015/283629', '10.1016/j.measurement.2012.06.005', '10.1109/19.836317', '10.1117/12.2202805', '10.1109/ICCSNT.2013.6967053', '10.1109/ICIST.2014.6920593', '10.1109/ICIEA.2013.6566539', '10.1016/j.rinp.2017.02.027', '10.21042/AMNS.2017.1.00001', '10.1080/15567036.2018.1454552', '10.1002/pc.23772', '10.1109/TFUZZ.2018.2796074', '10.21042/AMNS.2018.1.00004', '10.4064/cm6959-8-2016', '10.3934/ipi.2007.1.457'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine learning', 'Computer vision', 'Data mining', 'Information retrieval'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Semi-supervised discriminant analysis': Paper(DOI='10.3724/sp.j.1004.2009.01513', crossref_json=None, google_schorlar_metadata=None, title='Semi-supervised discriminant analysis', authors=['Deng Cai', 'Xiaofei He', 'Jiawei Han'], abstract=None, conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2003.1251154', '10.1109/TIT.2007.911294', '10.1016/j.jvcir.2008.11.009', '10.1109/TPAMI.2005.55', '10.1109/34.598228'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine learning', 'Computer vision', 'Data mining', 'Information retrieval'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Graph regularized sparse coding for image representation': Paper(DOI='10.1016/j.neucom.2014.11.067', crossref_json=None, google_schorlar_metadata=None, title='Graph regularized sparse coding for image representation', authors=['Miao Zheng', 'Jiajun Bu', 'Chun Chen', 'Can Wang', 'Lijun Zhang', 'Guang Qiu', 'Deng Cai'], abstract='Sparse coding has received an increasing amount of interest in recent years. It is an unsupervised learning algorithm, which finds a basis set capturing high-level semantics in the data and learns sparse coordinates in terms of the basis set. Originally applied to modeling the human visual cortex, sparse coding has been shown useful for many applications. However, most of the existing approaches to sparse coding fail to consider the geometrical structure of the data space. In many real applications, the data is more likely to reside on a low-dimensional submanifold embedded in the high-dimensional ambient space. It has been shown that the geometrical information of the data is important for discrimination. In this paper, we propose a graph based algorithm, called graph regularized sparse coding, to learn the sparse representations that explicitly take into account the local manifold structure of the data. By using\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.neucom.2014.01.043', '10.1016/j.patcog.2013.03.007', '10.1109/TPAMI.2012.274', '10.1016/j.cviu.2013.07.004', '10.1109/TPAMI.2007.70844', '10.1109/INFCOMW.2014.6849291', '10.1109/TIP.2007.911828', '10.1109/TIP.2010.2050625', '10.1109/TIP.2012.2192127', '10.1109/TIP.2012.2205006', '10.1109/MSP.2007.914731', '10.1109/TPAMI.2008.79', '10.1109/TPAMI.2012.215', '10.1109/TIP.2010.2090535', '10.1109/TPAMI.2012.63', '10.1016/j.knosys.2013.09.004', '10.1016/j.cviu.2013.03.007', '10.1109/TIP.2014.2311377', '10.1126/science.290.5500.2319', '10.1126/science.290.5500.2323', '10.1137/S1064827502419154', '10.1109/78.258082', '10.1109/ACSSC.1993.342465', '10.1137/S1064827596304010', '10.1198/016214506000000735', '10.1007/s00041-008-9045-x', '10.1109/JPROC.2010.2040551', '10.1109/TSP.2009.2036477', '10.1109/TPAMI.2007.70755', '10.1109/TIP.2013.2290593', '10.1137/070697653', '10.1137/07070156X', '10.1109/TIP.2011.2176743', '10.1109/LGRS.2013.2290531', '10.1109/TIP.2012.2215620', '10.1109/TPAMI.2012.57', '10.1007/s11634-011-0090-y', '10.1109/34.868688'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine learning', 'Computer vision', 'Data mining', 'Information retrieval'], conference_acronym='Neurocomputing (Amsterdam)', publisher=None, query_handler=None),\n",
       " 'Pixellink: Detecting scene text via instance segmentation': Paper(DOI='10.1609/aaai.v32i1.12269', crossref_json=None, google_schorlar_metadata=None, title='Pixellink: Detecting scene text via instance segmentation', authors=['Dan Deng', 'Haifeng Liu', 'Xuelong Li', 'Deng Cai'], abstract='Most state-of-the-art scene text detection algorithms are deep learning based methods that depend on bounding box regression and perform at least two kinds of predictions: text/non-text classification and location regression. Regression plays a key role in the acquisition of bounding boxes in these methods, but it is not indispensable because text/non-text prediction can also be considered as a kind of semantic segmentation that contains full location information in itself. However, text instances in scene images often lie very close to each other, making them very difficult to separate via semantic segmentation. Therefore, instance segmentation is needed to address this problem. In this paper, PixelLink, a novel scene text detection algorithm based on instance segmentation, is proposed. Text instances are first segmented out by linking pixels within the same instance together. Text bounding boxes are then extracted directly from the segmentation result without location regression. Experiments show that, compared with regression-based methods, PixelLink can achieve better or comparable performance on several benchmarks, while requiring many fewer training iterations and less training data.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine learning', 'Computer vision', 'Data mining', 'Information retrieval'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Extracting content structure for web pages based on visual representation': Paper(DOI='10.1007/3-540-36901-5_42', crossref_json=None, google_schorlar_metadata=None, title='Extracting content structure for web pages based on visual representation', authors=['Deng Cai', 'Shipeng Yu', 'Ji-Rong Wen', 'Wei-Ying Ma'], abstract=' A new web content structure based on visual representation is proposed in this paper. Many web applications such as information retrieval, information extraction and automatic page adaptation can benefit from this structure. This paper presents an automatic top-down, tag-tree independent approach to detect web content structure. It simulates how a user understands web layout structure based on his visual perception. Comparing to other existing techniques, our approach is independent to underlying documentation representation such as HTML and works well even when the HTML structure is far different from layout structure. Experiments show satisfactory results.', conference=None, journal=None, year=None, reference_list=['10.1016/S0169-7552(98)00110-X', '10.1007/3-540-62222-5_55', '10.1145/371920.372054', '10.1145/511446.511466', '10.1145/371920.372161', '10.1145/304182.304223', '10.1007/3-540-47952-X_18', '10.1016/S1389-1286(00)00041-4', '10.1145/775047.775134', '10.1108/EUM0000000007186', '10.1142/9789812384737_0019', '10.1145/775152.775155'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine learning', 'Computer vision', 'Data mining', 'Information retrieval'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Constrained nonnegative matrix factorization for image representation': Paper(DOI='10.1007/s11277-018-5325-1', crossref_json=None, google_schorlar_metadata=None, title='Constrained nonnegative matrix factorization for image representation', authors=['Haifeng Liu', 'Zhaohui Wu', 'Xuelong Li', 'Deng Cai', 'Thomas S Huang'], abstract='Nonnegative matrix factorization (NMF) is a popular technique for finding parts-based, linear representations of nonnegative data. It has been successfully applied in a wide range of applications such as pattern recognition, information retrieval, and computer vision. However, NMF is essentially an unsupervised method and cannot make use of label information. In this paper, we propose a novel semi-supervised matrix decomposition method, called Constrained Nonnegative Matrix Factorization (CNMF), which incorporates the label information as additional constraints. Specifically, we show how explicitly combining label information improves the discriminating power of the resulting matrix decomposition. We explore the proposed CNMF method with two cost function formulations and provide the corresponding update solutions for the optimization problems. Empirical experiments demonstrate the effectiveness of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1038/44565', '10.1109/TPAMI.2006.60', '10.1109/TPAMI.2010.231', '10.1109/TIP.2010.2090535', '10.1016/j.patcog.2008.09.002', '10.1016/j.neucom.2016.09.052', '10.1016/j.neucom.2015.01.103', '10.1109/TPAMI.2011.217', '10.1109/JSTARS.2017.2684132', '10.1109/TGRS.2012.2213825', '10.1109/TGRS.2013.2265322', '10.1109/LGRS.2014.2325874', '10.1109/JSTARS.2015.2401603', '10.1109/JSTARS.2015.2508448', '10.1109/JSTARS.2015.2427656', '10.1109/TGRS.2014.2365953', '10.1109/TASLP.2015.2427520', '10.1109/LSP.2014.2362556', '10.1109/TASLP.2017.2656805', '10.1049/iet-spr.2016.0414', '10.1109/TASLP.2017.2774925', '10.1109/LSP.2016.2532903', '10.1109/TCYB.2015.2512852', '10.1016/j.asoc.2012.04.001', '10.1109/TPAMI.2009.187'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine learning', 'Computer vision', 'Data mining', 'Information retrieval'], conference_acronym='Wireless personal communications', publisher=None, query_handler=None),\n",
       " 'Vse++: Improved visual-semantic embeddings': Paper(DOI='10.1609/aaai.v32i1.11279', crossref_json=None, google_schorlar_metadata=None, title='Vse++: Improved visual-semantic embeddings', authors=['Fartash Faghri', 'David J Fleet', 'Jamie Ryan Kiros', 'Sanja Fidler'], abstract=None, conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Natural Language Processing', 'Computer Vision'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'A multiplicative model for learning distributed text-based attribute representations': Paper(DOI='10.1007/s11042-017-4967-4', crossref_json=None, google_schorlar_metadata=None, title='A multiplicative model for learning distributed text-based attribute representations', authors=['Ryan Kiros', 'Richard Zemel', 'Ruslan R Salakhutdinov'], abstract='In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to a wide variety of concepts, such as document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.', conference=None, journal=None, year=None, reference_list=['10.1007/s00365-007-9003-x', '10.1016/j.cviu.2012.10.008', '10.1109/TPAMI.2013.50', '10.1109/TIT.2005.862083', '10.1016/j.neucom.2014.05.011', '10.1007/s11042-010-0635-7', '10.1145/1327452.1327492', '10.1109/TIT.2006.871582', '10.1109/CVPR.2010.5539926', '10.1007/978-3-642-33765-9_38', '10.1109/CVPR.2014.81', '10.1007/978-3-540-88682-2_21', '10.1109/HPCC.and.EUC.2013.106', '10.1007/978-3-540-25948-0_99', '10.1109/TPAMI.2011.48', '10.1109/CVPR.2009.5206594', '10.1007/978-3-642-33863-2_40', '10.5244/C.26.24', '10.1007/978-1-4471-6296-4_5', '10.1007/978-1-4471-6296-4_6', '10.1007/978-3-642-33863-2_41', '10.1109/ICCV.2013.257', '10.1109/CVPR.2013.414', '10.5244/C.24.21', '10.1109/CVPR.2010.5540121', '10.1007/978-3-642-33863-2_45', '10.1109/CVPR.2013.465', '10.1109/CVPR.2011.5995329', '10.1186/s40537-014-0008-6', '10.1007/978-3-319-27674-8_24', '10.1109/WACV.2009.5403131', '10.1145/2733373.2806232', '10.1145/2733373.2806400', '10.1002/jip.68', '10.1109/TPAMI.2008.79', '10.1109/CVPR.2012.6248101', '10.1145/2671188.2749347', '10.1561/2200000006', '10.1109/CVPR.2013.460', '10.1109/TPAMI.2011.164', '10.1109/TPAMI.2012.138'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Natural Language Processing', 'Computer Vision'], conference_acronym='Multimedia tools and applications', publisher=None, query_handler=None),\n",
       " 'Stacked multiscale feature learning for domain independent medical image segmentation': Paper(DOI='10.1007/978-3-319-10581-9_4', crossref_json=None, google_schorlar_metadata=None, title='Stacked multiscale feature learning for domain independent medical image segmentation', authors=['Ryan Kiros', 'Karteek Popuri', 'Dana Cobzas', 'Martin Jagersand'], abstract=' In this work we propose a feature-based segmentation approach that is domain independent. While most existing approaches are based on application-specific hand-crafted features, we propose a framework for learning features from data itself at multiple scales and depth. Our features can be easily integrated into classifiers or energy-based segmentation algorithms. We test the performance of our proposed method on two MICCAI grand challenges, obtaining the top score on VESSEL12 and competitive performance on BRATS2012.', conference=None, journal=None, year=None, reference_list=['10.1023/A:1014080923068', '10.1109/TMI.2008.2004421', '10.1016/j.media.2013.01.001', '10.1007/BFb0056195', '10.1145/1273496.1273592', '10.1007/978-3-319-00065-7_27', '10.1007/978-3-642-33415-3_24', '10.1007/978-3-642-40811-3_66', '10.1007/978-3-642-40811-3_92', '10.1006/cviu.2000.0866'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Natural Language Processing', 'Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Region covariance: A fast descriptor for detection and classification': Paper(DOI='10.1007/11744047_45', crossref_json=None, google_schorlar_metadata=None, title='Region covariance: A fast descriptor for detection and classification', authors=['Oncel Tuzel', 'Fatih Porikli', 'Peter Meer'], abstract=' We describe a new region descriptor and apply it to two problems, object detection and texture classification. The covariance of d-features, e.g., the three-dimensional color vector, the norm of first and second derivatives of intensity with respect to x and y, etc., characterizes a region of interest. We describe a fast method for computation of covariances based on integral images. The idea presented here is more general than the image sums or histograms, which were already published before, and with a series of integral images the covariances are obtained by a few arithmetic operations. Covariance matrices do not lie on Euclidean space, therefore we use a distance metric involving generalized eigenvalues which also follows from the Lie group structure of positive definite matrices. Feature matching is a simple nearest neighbor search under the distance metric and performed extremely rapidly using the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TSMC.1977.4309663', '10.1109/34.254061', '10.1023/A:1007939232436', '10.1109/CVPR.2005.188', '10.1023/A:1011126920638', '10.1109/TPAMI.2004.2', '10.1023/B:VISI.0000029664.99615.94', '10.1109/ICCV.2003.1238382'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Deep Learning', 'Computer Vision', 'Robotics'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Pedestrian detection via classification on riemannian manifolds': Paper(DOI='10.1109/tpami.2008.75', crossref_json=None, google_schorlar_metadata=None, title='Pedestrian detection via classification on riemannian manifolds', authors=['Oncel Tuzel', 'Fatih Porikli', 'Peter Meer'], abstract='We present a new algorithm to detect pedestrian in still images utilizing covariance matrices as object descriptors. Since the descriptors do not form a vector space, well known machine learning techniques are not well suited to learn the classifiers. The space of d-dimensional nonsingular covariance matrices can be represented as a connected Riemannian manifold. The main contribution of the paper is a novel approach for classifying points lying on a connected Riemannian manifold using the geometry of the space. The algorithm is tested on INRIA and DaimlerChrysler pedestrian datasets where superior detection rates are observed over the previous approaches.', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2005.226', '10.1109/CVPR.2004.1315241', '10.1109/CVPR.2006.210', '10.1109/34.655648', '10.1364/JOSAA.4.000519', '10.1109/ICCV.2003.1238422', '10.1109/CVPR.2001.990517', '10.1016/j.sigpro.2005.12.018', '10.1214/aos/1016218223', '10.1088/1751-8113/40/36/009', '10.1109/ICCV.2003.1238407', '10.1109/CVPR.2003.1211479', '10.1023/B:VISI.0000042934.15159.49', '10.5244/C.2.23', '10.1109/CVPR.2006.119', '10.1109/ICCV.1999.791202', '10.1109/CVPR.2005.272', '10.1002/cpa.3160300502', '10.1023/A:1012460413855', '10.1023/A:1011179004708', '10.1109/ICCV.2001.937561', '10.1109/CVPR.2006.202', '10.1023/B:VISI.0000029664.99615.94', '10.1109/CVPR.2005.252', '10.1007/BF00994018', '10.1109/34.1000236', '10.1109/CVPR.2005.177', '10.1137/S0895479803436937', '10.1109/34.993558', '10.1109/CVPR.2006.50', '10.1109/CVPR.2005.320', '10.1109/34.917571', '10.1109/TPAMI.2006.217', '10.1023/A:1008162616689', '10.1109/CVPR.2006.153', '10.1109/CVPR.2005.188', '10.1007/s11263-005-3222-z', '10.1109/CVPR.2006.94', '10.1109/34.655647', '10.1109/CVPR.2007.383134'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Deep Learning', 'Computer Vision', 'Robotics'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Going deeper into action recognition: A survey': Paper(DOI='10.1016/j.imavis.2017.01.010', crossref_json=None, google_schorlar_metadata=None, title='Going deeper into action recognition: A survey', authors=['Samitha Herath', 'Mehrtash Harandi', 'Fatih Porikli'], abstract='Understanding human actions in visual data is tied to advances in complementary research areas including object recognition, human dynamics, domain adaptation and semantic segmentation. Over the last decade, human action analysis evolved from earlier schemes that are often limited to controlled environments to nowadays advanced solutions that can learn from millions of videos and apply to almost all daily activities. Given the broad range of applications from video surveillance to human–computer interaction, scientific milestones in action recognition are achieved more rapidly, eventually leading to the demise of what used to be good in a short time. This motivated us to provide a comprehensive review of the notable steps taken towards recognizing human actions. To this end, we start our discussion with the pioneering methods that use handcrafted representations, and then, navigate into the realm of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s00138-010-0298-4', '10.1109/72.279181', '10.1109/34.910878', '10.1109/LSP.2013.2284196', '10.1109/TIT.2005.862083', '10.1109/TPAMI.2014.2361137', '10.1016/j.eswa.2012.03.005', '10.1109/TIT.2006.871582', '10.1023/A:1021669406132', '10.1109/TPAMI.2013.57', '10.1007/s11263-013-0677-1', '10.1561/2000000004', '10.1109/TPAMI.2011.253', '10.1162/neco.1997.9.8.1735', '10.1007/s11263-014-0719-3', '10.1016/0262-8856(83)90003-3', '10.1109/TPAMI.2012.59', '10.1109/TIP.2015.2456412', '10.1007/s11263-005-1838-7', '10.1109/5.726791', '10.1109/TIP.2007.911828', '10.1098/rspb.1982.0024', '10.1016/j.imavis.2013.03.005', '10.1016/j.cviu.2006.08.002', '10.1109/TPAMI.2002.1017623', '10.1016/S0042-6989(97)00169-7', '10.1016/j.imavis.2009.11.014', '10.1109/TPAMI.2007.1124', '10.1109/5.18626', '10.1007/s00138-012-0450-4', '10.1016/0005-1098(78)90005-5', '10.1006/ciun.1994.1006', '10.1007/s11263-015-0816-y', '10.1109/TCYB.2013.2273174', '10.1016/j.cviu.2014.01.002', '10.1109/TSMCC.2011.2149519', '10.1109/TCSVT.2008.2005594', '10.1109/TPAMI.2008.75', '10.1007/s00371-012-0752-6', '10.1109/TPAMI.2010.214', '10.1016/j.cviu.2006.07.013', '10.1016/j.cviu.2010.10.002', '10.1109/TPAMI.2008.79', '10.1145/1177352.1177355', '10.1007/s00138-008-0132-4', '10.1109/TPAMI.2007.1110'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Image Processing', 'Manifold Learning', 'Dictionary Learning', 'Deep Learning'], conference_acronym='Image and vision computing', publisher=None, query_handler=None),\n",
       " 'Underwater scene prior inspired deep underwater image and video enhancement': Paper(DOI='10.1016/j.patcog.2019.107038', crossref_json=None, google_schorlar_metadata=None, title='Underwater scene prior inspired deep underwater image and video enhancement', authors=['Chongyi Li', 'Saeed Anwar', 'Fatih Porikli'], abstract='In underwater scenes, wavelength-dependent light absorption and scattering degrade the visibility of images and videos. The degraded underwater images and videos affect the accuracy of pattern recognition, visual understanding, and key feature extraction in underwater scenes. In this paper, we propose an underwater image enhancement convolutional neural network (CNN) model based on underwater scene prior, called UWCNN. Instead of estimating the parameters of underwater imaging model, the proposed UWCNN model directly reconstructs the clear latent underwater image, which benefits from the underwater scene prior which can be used to synthesize underwater image training data. Besides, based on the light-weight network structure and effective training data, our UWCNN model can be easily extended to underwater videos for frame-by-frame enhancement. Specifically, combining an underwater\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.patcog.2006.05.036', '10.1016/j.patcog.2016.06.008', '10.1016/j.patcog.2018.08.015', '10.1109/TPAMI.2018.2819173', '10.1109/TIP.2018.2887029', '10.1016/j.patcog.2019.01.006', '10.1109/TIP.2017.2787612', '10.1016/j.patcog.2017.10.013', '10.1117/1.JEI.24.3.033023', '10.1016/j.patrec.2017.05.023', '10.1109/LSP.2018.2792050', '10.1109/TIP.2017.2759252', '10.1109/TIP.2011.2179666', '10.1016/j.jvcir.2014.11.006', '10.1109/MCG.2016.26', '10.1109/TPAMI.2010.168', '10.1109/TIP.2016.2612882', '10.1109/TIP.2017.2663846', '10.1016/j.patcog.2010.02.007', '10.1016/j.patcog.2016.07.026', '10.1016/j.patcog.2018.08.018', '10.1109/TIP.2003.819861', '10.1109/TIP.2015.2491020', '10.1109/JOE.2015.2469915'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Image Processing', 'Manifold Learning', 'Dictionary Learning', 'Deep Learning'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'Less is more: Towards compact cnns': Paper(DOI='10.1007/978-3-319-46493-0_40', crossref_json=None, google_schorlar_metadata=None, title='Less is more: Towards compact cnns', authors=['Hao Zhou', 'Jose M Alvarez', 'Fatih Porikli'], abstract=' To attain a favorable performance on large-scale datasets, convolutional neural networks (CNNs) are usually designed to have very high capacity involving millions of parameters. In this work, we aim at optimizing the number of neurons in a network, thus the number of parameters. We show that, by incorporating sparse constraints into the objective function, it is possible to decimate the number of neurons during the training stage. As a result, the number of parameters and the memory footprint of the neural network are also reduced, which is also desirable at the test time. We evaluated our method on several well-known CNN structures including AlexNet, and VGG over different datasets including ImageNet. Extensive experimental results demonstrate that our method leads to compact networks. Taking first fully connected layer as an example, our compact CNN contains only  of the original neurons\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1150402.1150464', '10.1109/CVPR.2009.5206848', '10.1117/12.2024410', '10.1109/CVPR.2014.81', '10.5244/C.28.88', '10.1109/5.726791', '10.1109/TPAMI.2012.39', '10.1007/s11263-015-0816-y', '10.5244/C.29.31', '10.3115/1687878.1687946', '10.1145/2733373.2807412', '10.1109/ICASSP.2012.6288897', '10.1109/ICASSP.2013.6638312'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Image Processing', 'Manifold Learning', 'Dictionary Learning', 'Deep Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'LightenNet: A convolutional neural network for weakly illuminated image enhancement': Paper(DOI='10.1016/j.patrec.2018.01.010', crossref_json=None, google_schorlar_metadata=None, title='LightenNet: A convolutional neural network for weakly illuminated image enhancement', authors=['Chongyi Li', 'Jichang Guo', 'Fatih Porikli', 'Yanwei Pang'], abstract='Weak illumination or low light image enhancement as pre-processing is needed in many computer vision tasks. Existing methods show limitations when they are used to enhance weakly illuminated images, especially for the images captured under diverse illumination circumstances. In this letter, we propose a trainable Convolutional Neural Network (CNN) for weakly illuminated image enhancement, namely LightenNet, which takes a weakly illuminated image as input and outputs its illumination map that is subsequently used to obtain the enhanced image based on Retinex model. The proposed method produces visually pleasing results without over or under-enhanced regions. Qualitative and quantitative comparisons are conducted to evaluate the performance of the proposed method. The experimental results demonstrate that the proposed method achieves superior performance than existing methods\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TIP.2016.2598681', '10.1109/TPAMI.2015.2439281', '10.1109/TIP.2015.2474701', '10.1016/j.sigpro.2016.05.031', '10.1145/3072959.3073592', '10.1109/TIP.2016.2639450', '10.1109/TPAMI.2012.213', '10.1109/83.597272', '10.1073/pnas.83.10.3078', '10.1016/j.patcog.2016.06.008', '10.1137/100806588', '10.1016/S0734-189X(87)80186-X', '10.1109/TIP.2014.2324813', '10.1109/TIP.2013.2261309', '10.1109/TIP.2003.819861', '10.1109/TIP.2013.2293423'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Image Processing', 'Pattern Recognition', 'Machine Learning'], conference_acronym='Pattern recognition letters', publisher=None, query_handler=None),\n",
       " 'Ultra-resolving face images by discriminative generative networks': Paper(DOI='10.1007/978-3-319-46454-1_20', crossref_json=None, google_schorlar_metadata=None, title='Ultra-resolving face images by discriminative generative networks', authors=['Xin Yu', 'Fatih Porikli'], abstract=' Conventional face super-resolution methods, also known as face hallucination, are limited up\\xa0to  scaling factors where  additional pixels are estimated for each given pixel. Besides, they become very fragile when the input low-resolution image size is too small that only little information is available in the input image. To address these shortcomings, we present a discriminative generative network that can ultra-resolve a very low resolution face image of size  pixels to its  larger version by reconstructing 64 pixels from a single pixel. We introduce a pixel-wise  regularization term to the generative model and exploit the feedback of the discriminative network to make the upsampled face images more similar to real ones. In our framework, the discriminative network learns the essential constituent parts of the faces and the generative network blends these parts in the most accurate fashion\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2002.1033210', '10.1109/TSMCC.2005.848171', '10.1007/s11263-006-0029-5', '10.1109/TIP.2008.922421', '10.1109/TIP.2010.2050625', '10.1016/j.patcog.2009.12.019', '10.1007/978-3-642-33786-4_18', '10.1109/CVPR.2013.146', '10.1609/aaai.v29i1.9795', '10.1007/s11263-013-0645-9', '10.1007/978-3-319-10593-2_25', '10.1109/ICCV.2015.425', '10.1109/TPAMI.2015.2439281', '10.1109/CVPR.2016.182', '10.1109/TIP.2014.2305844', '10.1109/ICCV.2013.75', '10.1109/38.988747', '10.1109/ICCV.2009.5459271', '10.1109/CVPR.2015.7299003', '10.1109/CVPR.2015.7299156', '10.1109/TPAMI.2006.105', '10.1109/CVPR.2014.364', '10.1109/ICCV.2015.212', '10.1016/j.patcog.2013.09.012', '10.1109/CVPR.2015.7299121', '10.1109/ICCV.2011.6126474'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Image Processing', 'Manifold Learning', 'Dictionary Learning', 'Deep Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'LIME: Low-light image enhancement via illumination map estimation': Paper(DOI='10.1109/tip.2016.2639450', crossref_json=None, google_schorlar_metadata=None, title='LIME: Low-light image enhancement via illumination map estimation', authors=['Xiaojie Guo', 'Yu Li', 'Haibin Ling'], abstract='When one captures images in low-light conditions, the images often suffer from low visibility. Besides degrading the visual aesthetics of images, this poor quality may also significantly degenerate the performance of many computer vision and multimedia algorithms that are primarily designed for high-quality inputs. In this paper, we propose a simple yet effective low-light image enhancement (LIME) method. More concretely, the illumination of each pixel is first estimated individually by finding the maximum value in R, G, and B channels. Furthermore, we refine the initial illumination map by imposing a structure prior on it, as the final illumination map. Having the well-constructed illumination map, the enhancement can be achieved accordingly. Experiments on a number of challenging low-light images are present to reveal the efficacy of our LIME and show its superiority over several state-of-the-arts in terms of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TIP.2011.2158229', '10.1145/1141911.1142005', '10.1145/1141911.1141936', '10.1145/1015706.1015780', '10.1109/CVPR.2014.366', '10.1109/CVPR.2012.6247952', '10.1109/TIP.2007.901238', '10.1109/83.597272', '10.1109/ICME.2016.7552893', '10.1109/TIP.2013.2261309', '10.1016/j.sigpro.2016.05.031', '10.1109/CVPR.2016.304', '10.1109/ICIP.2015.7351501', '10.1109/ICCV.2009.5459428', '10.1109/CVPR.2014.346', '10.1145/1360612.1360666', '10.1016/j.dsp.2003.07.002', '10.1007/BF03178082', '10.1109/TIP.2011.2157513', '10.1145/2070781.2024211', '10.1109/TCE.2007.381734', '10.1038/scientificamerican1277-108', '10.1109/TIP.2013.2284059', '10.1109/83.557356', '10.1109/TPAMI.2010.168', '10.1145/2366145.2366222', '10.1109/ICCV.2013.82', '10.1109/TPAMI.2003.1201821', '10.1109/TIP.2007.891788', '10.1109/ICCV.2013.358', '10.1109/TIP.2011.2171353', '10.1016/0016-0032(80)90058-7', '10.2352/CIC.2012.20.1.art00008'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'augmented reality', 'medical image analysis', 'human-computer interaction', 'machine learning'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Shape classification using the inner-distance': Paper(DOI='10.1109/tpami.2007.41', crossref_json=None, google_schorlar_metadata=None, title='Shape classification using the inner-distance', authors=['Haibin Ling', 'David W Jacobs'], abstract='Part structure and articulation are of fundamental importance in computer and human vision. We propose using the inner-distance to build shape descriptors that are robust to articulation and capture part structure. The inner-distance is defined as the length of the shortest path between landmark points within the shape silhouette. We show that it is articulation insensitive and more effective at capturing part structures than the Euclidean distance. This suggests that the inner-distance can be used as a replacement for the Euclidean distance to build more accurate descriptors for complex shapes, especially for those with articulated parts. In addition, texture information along the shortest path can be used to further improve shape classification. With this idea, we propose three approaches to using the inner-distance. The first method combines the inner-distance and multidimensional scaling (MDS) to build articulation\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2005.362', '10.1109/ICCV.2005.67', '10.1109/TPAMI.2005.208', '10.1109/CVPR.2003.1211497', '10.1109/TPAMI.2005.151', '10.1109/CVPR.2000.855850', '10.1006/jvci.1998.0396', '10.1109/TIP.2005.860606', '10.1023/A:1008102926703', '10.1007/BF01451741', '10.1007/978-3-540-39966-7_36', '10.1016/0010-0277(84)90022-2', '10.1109/CVPR.2003.1211346', '10.1109/TPAMI.2004.1273924', '10.1109/TPAMI.2003.1233902', '10.1109/TPAMI.2002.1046166', '10.1145/571647.571648', '10.1109/CVPR.2000.855827', '10.1109/TPAMI.2003.1159951', '10.1023/B:VISI.0000011202.85607.00', '10.1109/TPAMI.2005.188', '10.1109/TPAMI.2004.108', '10.1109/CVPR.2003.1211479', '10.2307/25065637', '10.1109/34.817410', '10.1109/34.993558', '10.1037/0033-295X.112.1.243', '10.1109/CVPR.2003.1211347', '10.1016/S0042-6989(98)00043-1', '10.1023/B:VISI.0000042934.15159.49', '10.1109/CVPR.2005.45', '10.1109/TPAMI.2006.253', '10.1007/3-540-62909-2_87', '10.1109/TIP.2003.816010', '10.1016/0022-5193(73)90175-6', '10.1037//0033-295X.94.2.115', '10.1007/978-1-4757-2711-1', '10.1109/34.24792'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Robust visual tracking and vehicle classification via sparse representation': Paper(DOI='10.1109/tpami.2011.66', crossref_json=None, google_schorlar_metadata=None, title='Robust visual tracking and vehicle classification via sparse representation', authors=['Xue Mei', 'Haibin Ling'], abstract='In this paper, we propose a robust visual tracking method by casting tracking as a sparse approximation problem in a particle filter framework. In this framework, occlusion, noise, and other challenging issues are addressed seamlessly through a set of trivial templates. Specifically, to find the tracking target in a new frame, each target candidate is sparsely represented in the space spanned by target templates and trivial templates. The sparsity is achieved by solving an ℓ 1 -regularized least-squares problem. Then, the candidate with the smallest projection error is taken as the tracking target. After that, tracking is continued using a Bayesian state inference framework. Two strategies are used to further improve the tracking performance. First, target templates are dynamically updated to capture appearance changes. Second, nonnegativity constraints are enforced to filter out clutter which negatively resembles tracking\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2008.79', '10.1007/s11263-006-0027-7', '10.1109/TPAMI.2005.167', '10.1023/A:1008078328650', '10.1109/TPAMI.2003.1233903', '10.1109/JSTSP.2007.910971', '10.1109/CVPR.2008.4587583', '10.1109/CVPR.2010.5540231', '10.1109/CVPR.2005.144', '10.1109/CVPR.2003.1211434', '10.1109/TIP.2004.836152', '10.1023/B:VISI.0000011205.11775.fd', '10.1109/TSMCB.2010.2041662', '10.1145/1177352.1177355', '10.1109/ICCV.2009.5459278', '10.1109/CVPR.2005.260', '10.1023/A:1007939232436', '10.1002/cpa.20124', '10.1109/CVPR.2008.4587652', '10.1109/TPAMI.2004.16', '10.1109/ICCV.2003.1238365', '10.1109/TPAMI.2003.1195991', '10.1007/s11263-007-0075-7', '10.1109/TIT.2006.871582', '10.1109/CVPR.2007.383176', '10.1007/978-1-4757-3437-9', '10.1109/CVPR.1999.786982', '10.1109/34.722606', '10.1109/CVPR.2004.1315111', '10.1016/j.patcog.2008.09.026', '10.1109/CVPR.2006.94'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'augmented reality', 'medical image analysis', 'human-computer interaction', 'machine learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'M2det: A single-shot object detector based on multi-level feature pyramid network': Paper(DOI='10.1609/aaai.v33i01.33019259', crossref_json=None, google_schorlar_metadata=None, title='M2det: A single-shot object detector based on multi-level feature pyramid network', authors=['Qijie Zhao', 'Tao Sheng', 'Yongtao Wang', 'Zhi Tang', 'Ying Chen', 'Ling Cai', 'Haibin Ling'], abstract='Feature pyramids are widely exploited by both the state-of-the-art one-stage object detectors (eg, DSSD, RetinaNet, RefineDet) and the two-stage object detectors (eg, Mask RCNN, DetNet) to alleviate the problem arising from scale variation across object instances. Although these object detectors with feature pyramids achieve encouraging results, they have some limitations due to that they only simply construct the feature pyramid according to the inherent multiscale, pyramidal architecture of the backbones which are originally designed for object classification task. Newly, in this work, we present Multi-Level Feature Pyramid Network (MLFPN) to construct more effective feature pyramids for detecting objects of different scales. First, we fuse multi-level features (ie multiple layers) extracted by backbone as the base feature. Second, we feed the base feature into a block of alternating joint Thinned U-shape Modules and Feature Fusion Modules and exploit the decoder layers of each Ushape module as the features for detecting objects. Finally, we gather up the decoder layers with equivalent scales (sizes) to construct a feature pyramid for object detection, in which every feature map consists of the layers (features) from multiple levels. To evaluate the effectiveness of the proposed MLFPN, we design and train a powerful end-to-end one-stage object detector we call M2Det by integrating it into the architecture of SSD, and achieve better detection performance than state-of-the-art one-stage detectors. Specifically, on MSCOCO benchmark, M2Det achieves AP of 41.0 at speed of 11.8 FPS with single-scale inference strategy and AP of 44.2 with multi\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'augmented reality', 'medical image analysis', 'human-computer interaction', 'machine learning'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Encoding color information for visual tracking: Algorithms and benchmark': Paper(DOI='10.1109/tip.2015.2482905', crossref_json=None, google_schorlar_metadata=None, title='Encoding color information for visual tracking: Algorithms and benchmark', authors=['Pengpeng Liang', 'Erik Blasch', 'Haibin Ling'], abstract='While color information is known to provide rich discriminative clues for visual inference, most modern visual trackers limit themselves to the grayscale realm. Despite recent efforts to integrate color in tracking, there is a lack of comprehensive understanding of the role color information can play. In this paper, we attack this problem by conducting a systematic study from both the algorithm and benchmark perspectives. On the algorithm side, we comprehensively encode 10 chromatic models into 16 carefully selected state-of-the-art visual trackers. On the benchmark side, we compile a large set of 128 color sequences with ground truth and challenge factor annotations (e.g., occlusion). A thorough evaluation is conducted by running all the color-encoded trackers, together with two recently proposed color trackers. A further validation is conducted on an RGBD tracking benchmark. The results clearly show the benefit of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2013.230', '10.1109/ICCVW.2013.20', '10.1145/1177352.1177355', '10.1109/CVPR.2010.5539821', '10.1109/TPAMI.2011.239', '10.1109/TPAMI.2008.146', '10.1109/TIP.2009.2019809', '10.1109/TPAMI.2003.1195991', '10.1109/CVPR.2012.6247882', '10.1109/TPAMI.2005.205', '10.1109/CVPR.2014.143', '10.1109/ICCV.2011.6126251', '10.1109/CVPR.2012.6247895', '10.1109/TPAMI.2011.66', '10.1109/CVPR.2011.5995730', '10.5244/C.20.6', '10.1007/s11263-012-0582-z', '10.1109/CVPR.2014.443', '10.1023/B:VISI.0000013087.49260.fb', '10.1109/TPAMI.2010.226', '10.1109/CVPR.2005.188', '10.1023/A:1008078328650', '10.1109/ICCVW.2009.5457445', '10.1109/CVPR.2011.5995733', '10.1109/CVPR.2012.6247891', '10.1109/TPAMI.2014.2315808', '10.1109/CVPR.2006.256', '10.1007/s11263-007-0075-7', '10.1109/TPAMI.2014.2345390', '10.1016/j.cviu.2008.07.003', '10.1109/TPAMI.2009.154', '10.1109/CVPR.2013.312', '10.1109/CVPR.2013.367', '10.1109/ICCV.2013.36', '10.1109/ICCV.2013.346', '10.1002/col.5080100409', '10.1007/978-3-319-10578-9_13', '10.1007/s11263-011-0495-2', '10.1007/s11263-013-0633-0'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'augmented reality', 'medical image analysis', 'human-computer interaction', 'machine learning'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'U2Fusion: A unified unsupervised image fusion network': Paper(DOI='10.1109/tpami.2020.3012548', crossref_json=None, google_schorlar_metadata=None, title='U2Fusion: A unified unsupervised image fusion network', authors=['Han Xu', 'Jiayi Ma', 'Junjun Jiang', 'Xiaojie Guo', 'Haibin Ling'], abstract='This study proposes a novel  unified  and  unsupervised  end-to-end image  fusion  network, termed as  U2Fusion , which is capable of solving different fusion problems, including multi-modal, multi-exposure, and multi-focus cases. Using feature extraction and information measurement, U2Fusion automatically estimates the importance of corresponding source images and comes up with adaptive information preservation degrees. Hence, different fusion tasks are unified in the same framework. Based on the adaptive degrees, a network is trained to preserve the adaptive similarity between the fusion result and source images. Therefore, the stumbling blocks in applying deep learning for image fusion, e.g., the requirement of ground-truth and specifically designed metrics, are greatly mitigated. By avoiding the loss of previous fusion capabilities when training a single model for different tasks sequentially, we obtain a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2017.2773081', '10.1109/TIP.2003.819861', '10.1109/TPAMI.2015.2491929', '10.1109/TAFFC.2015.2512598', '10.1109/AVSS.2019.8909828', '10.1109/TPAMI.2018.2827389', '10.1109/TPAMI.2018.2828817', '10.1109/TMM.2019.2895292', '10.1016/j.inffus.2018.02.004', '10.1016/j.inffus.2016.05.004', '10.1109/TCI.2017.2786138', '10.1016/j.inffus.2019.07.005', '10.1016/j.inffus.2014.05.004', '10.1109/TIP.2020.2977573', '10.24963/ijcai.2019/549', '10.1016/j.inffus.2019.07.011', '10.1016/j.inffus.2016.12.001', '10.1016/j.inffus.2014.05.004', '10.1016/j.inffus.2011.08.002', '10.1109/CVPR.2017.243', '10.1109/CVPRW.2018.00135', '10.1609/aaai.v34i07.6936', '10.1073/pnas.1611835114', '10.1109/LSP.2014.2354534', '10.1016/j.inffus.2017.05.006', '10.1109/LSP.2016.2618776', '10.23919/ICIF.2017.8009769', '10.1109/TIP.2018.2887342', '10.1016/j.inffus.2016.02.001', '10.1109/TPAMI.2011.109', '10.1109/TPAMI.2005.93', '10.1109/ACCESS.2019.2898111', '10.1016/j.inffus.2018.09.004', '10.1016/j.image.2018.12.004', '10.1109/ICCV.2017.505', '10.1109/LSP.2018.2877893', '10.1016/j.inffus.2017.10.007', '10.1109/TIP.2013.2244222', '10.1109/TIM.2018.2838778', '10.1142/S0218126616501231', '10.1016/j.jvcir.2015.06.021', '10.1016/j.inffus.2015.11.003', '10.1109/TIP.2018.2794218', '10.1109/TBME.2013.2282461', '10.1016/j.aeue.2015.09.004'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Processing', 'Computer Vision', 'Machine Learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Deepsaliency: Multi-task deep neural network model for salient object detection': Paper(DOI='10.1109/tip.2016.2579306', crossref_json=None, google_schorlar_metadata=None, title='Deepsaliency: Multi-task deep neural network model for salient object detection', authors=['Xi Li', 'Liming Zhao', 'Lina Wei', 'Ming-Hsuan Yang', 'Fei Wu', 'Yueting Zhuang', 'Haibin Ling', 'Jingdong Wang'], abstract='A key problem in salient object detection is how to effectively model the semantic properties of salient objects in a data-driven manner. In this paper, we propose a multi-task deep saliency model based on a fully convolutional neural network with global input (whole raw images) and global output (whole saliency maps). In principle, the proposed saliency model takes a data-driven strategy for encoding the underlying saliency prior information, and then sets up a multi-task learning scheme for exploring the intrinsic correlations between saliency detection and semantic image segmentation. Through collaborative feature learning from such two correlated tasks, the shared fully convolutional layers produce effective features for object perception. Moreover, it is capable of capturing the semantic information on salient objects across different levels using the fully convolutional layers, which investigate the feature-sharing\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2015.304', '10.1109/TPAMI.2010.70', '10.1145/957013.957094', '10.1109/CVPR.2015.7298731', '10.1109/CVPR.2013.271', '10.1109/CVPR.2015.7298938', '10.1109/TPAMI.2014.2345401', '10.1109/ICCV.2013.413', '10.1109/ICCV.2015.38', '10.1109/CVPR.2009.5206596', '10.1109/ICCV.2009.5459240', '10.1109/CVPR.2013.153', '10.1109/ICCV.2011.6126499', '10.1109/CVPR.2015.7298594', '10.1109/CVPRW.2010.5543739', '10.1109/TPAMI.2004.1273918', '10.1109/ICCV.2015.179', '10.1109/CVPR.2015.7298798', '10.1109/ICCV.2013.370', '10.1109/TIP.2015.2487833', '10.1145/2647868.2654889', '10.1007/s11263-009-0275-4', '10.1109/CVPR.2015.7298603', '10.5244/C.25.110', '10.1109/ICCV.2015.169', '10.1007/978-3-642-33712-3_3', '10.1109/CVPR.2015.7298868', '10.1109/CVPR.2013.407', '10.1109/CVPR.2014.360', '10.1109/ICCV.2013.193', '10.1109/ICCV.2013.209', '10.1109/TIP.2015.2403241', '10.1109/TIP.2012.2216276', '10.1109/TIP.2014.2383320', '10.1109/TIP.2015.2456497', '10.1109/TPAMI.2011.130', '10.1109/34.730558', '10.1109/TPAMI.2012.120', '10.1007/s00371-013-0867-4', '10.1109/CVPR.2014.43', '10.1038/35058500', '10.1007/s11263-015-0816-y'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'augmented reality', 'medical image analysis', 'human-computer interaction', 'machine learning'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " \"An efficient earth mover's distance algorithm for robust histogram comparison\": Paper(DOI='10.1109/tpami.2007.1058', crossref_json=None, google_schorlar_metadata=None, title=\"An efficient earth mover's distance algorithm for robust histogram comparison\", authors=['Haibin Ling', 'Kazunori Okada'], abstract=\"We propose EMD-L_{1}: a fast and exact algorithm for computing the Earth Mover's Distance (EMD) between a pair of histograms. The efficiency of the new algorithm enables its application to problems that were previously prohibitive due to high time complexities. The proposed EMD-L_{1} significantly simplifies the original linear programming formulation of EMD. Exploiting the L_{1} metric structure, the number of unknown variables in EMD-L_{1} is reduced to O(N) from O(N^{2}) of the original EMD for a histogram with N bins. In addition, the number of constraints is reduced by half and the objective function of the linear program is simplified. Formally, without any approximation, we prove that the EMD-L_{1} formulation is equivalent to the original EMD with a L_{1} ground distance. To perform the EMD-L_{1} computation, we propose an efficient tree-based algorithm, Tree-EMD. Tree-EMD exploits the fact that a\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1023/A:1026501619075', '10.1109/CVPR.2004.1315035', '10.1109/ICCV.2005.239', '10.1109/TIP.2003.816010', '10.1006/cviu.1999.0767', '10.1109/34.993558', '10.1109/ICCV.1999.790393', '10.1109/CVPR.2003.1211355', '10.1016/0734-189X(83)90112-3', '10.1007/BF00130487', '10.1023/A:1026543900054', '10.1109/TPAMI.2003.1159951', '10.1016/0734-189X(85)90055-6', '10.1109/CVPR.2003.1211346', '10.1007/978-1-4757-3343-3', '10.1006/cviu.2001.0934', '10.1137/1129093', '10.1109/TIP.2005.860606', '10.1109/34.765655', '10.1117/12.143648', '10.1016/S0262-8856(02)00060-4', '10.1145/571647.571648', '10.1002/sapm1941201224', '10.1109/34.192468', '10.1016/S0262-8856(02)90005-3', '10.1109/TPAMI.2005.188', '10.5244/C.2.23', '10.1109/CVPR.2003.1211347', '10.1109/CVPR.2005.45', '10.1109/34.391417', '10.1214/aoms/1177692631', '10.1007/11744078_26', '10.1023/B:VISI.0000029664.99615.94', '10.1109/CVPR.2005.362', '10.1109/ICCV.2005.67', '10.1109/ICCV.2001.937632', '10.1109/18.61115', '10.1109/TPAMI.2005.151', '10.1109/CVPR.2000.855850', '10.1145/800057.808695'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'augmented reality', 'medical image analysis', 'human-computer interaction', 'machine learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Feature pyramid and hierarchical boosting network for pavement crack detection': Paper(DOI='10.1109/tits.2019.2910595', crossref_json=None, google_schorlar_metadata=None, title='Feature pyramid and hierarchical boosting network for pavement crack detection', authors=['Fan Yang', 'Lei Zhang', 'Sijia Yu', 'Danil Prokhorov', 'Xue Mei', 'Haibin Ling'], abstract='Pavement crack detection is a critical task for insuring road safety. Manual crack detection is extremely time-consuming. Therefore, an automatic road crack detection method is required to boost this progress. However, it remains a challenging task due to the intensity inhomogeneity of cracks and complexity of the background, e.g., the low contrast with surrounding pavements and possible shadows with a similar intensity. Inspired by recent advances of deep learning in computer vision, we propose a novel network architecture, named feature pyramid and hierarchical boosting network (FPHBN), for pavement crack detection. The proposed network integrates context information to low-level features for crack detection in a feature pyramid way, and it balances the contributions of both easy and hard samples to loss by nested sample reweighting in a hierarchical way during training. In addition, we propose a novel\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1061/9780784480823.036', '10.1109/IFSA-NAFIPS.2013.6608558', '10.1109/WACV.2014.6836111', '10.1109/TITS.2015.2482222', '10.1109/SMC.2013.516', '10.1109/TPAMI.2011.267', '10.1007/BF00133570', '10.1155/2008/861701', '10.1109/ICIP.2013.6738843', '10.1109/IMCCC.2015.364', '10.1155/2011/989354', '10.1109/TITS.2016.2552248', '10.1109/ICIP.2016.7533052', '10.22260/ISARC2017/0066', '10.1109/ICCV.2013.231', '10.1109/IJCNN.2017.7966101', '10.1109/CVPR.2015.7298965', '10.1109/CVPR.2017.622', '10.1109/ICIP.2006.313007', '10.1109/TPAMI.2010.161', '10.1109/ICIP.2014.7025156', '10.1117/1.2172917', '10.1109/MIXDES.2015.7208590', '10.13176/11.167', '10.1109/TITS.2015.2477675', '10.1109/ICIP.2014.7025159', '10.1109/ICIP.2014.7025155', '10.1109/ICCV.2015.164', '10.1109/TITS.2012.2208630', '10.1109/ICIP.2011.6115610', '10.1016/j.patrec.2011.11.004', '10.1117/1.2177650', '10.1016/0968-090X(93)90002-W', '10.1109/WACV.2017.121', '10.1109/KAM.2008.29', '10.1007/s00138-011-0394-0', '10.1145/2647868.2654889', '10.1007/978-3-319-11656-3_18', '10.1109/CVPR.2001.990517', '10.1109/CVPR.2016.89', '10.1109/ICCV.2017.324', '10.1109/CVPR.2017.749', '10.1109/TITS.2017.2775628'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'augmented reality', 'medical image analysis', 'human-computer interaction', 'machine learning'], conference_acronym='IEEE transactions on intelligent transportation systems (Print)', publisher=None, query_handler=None),\n",
       " 'Learning cross-media joint representation with sparse and semisupervised regularization': Paper(DOI='10.1109/tcsvt.2013.2276704', crossref_json=None, google_schorlar_metadata=None, title='Learning cross-media joint representation with sparse and semisupervised regularization', authors=['Xiaohua Zhai', 'Yuxin Peng', 'Jianguo Xiao'], abstract='Cross-media retrieval has become a key problem in both research and application, in which users can search results across all of the media types (text, image, audio, video, and 3-D) by submitting a query of any media type. How to measure the content similarity among different media is the key challenge. Existing cross-media retrieval methods usually focus on modeling the pairwise correlation or semantic information separately. In fact, these two kinds of information are complementary to each other and optimizing them simultaneously can further improve the accuracy. In this paper, we propose a novel feature learning algorithm for cross-media data, called joint representation learning (JRL), which is able to explore jointly the correlation and semantic information in a unified optimization framework. JRL integrates the sparse and semisupervised regularization for different media types into one unified optimization\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICASSP.2007.366215', '10.1109/CVPR.2005.274', '10.1111/1467-8659.00669', '10.1504/IJMIS.2010.035970', '10.1145/1631272.1631298', '10.1145/957142.957143', '10.1145/1873951.1873987', '10.1145/2072298.2072336', '10.1109/ICDM.2011.22', '10.1145/2072298.2072318', '10.1109/TMM.2007.911822', '10.1007/978-3-642-27355-1_30', '10.1145/1816041.1816057', '10.1109/TCSVT.2008.918763', '10.1145/1991996.1992040', '10.1109/TPAMI.2004.1262334', '10.1093/biomet/28.3-4.321', '10.1145/860458.860459', '10.1109/TCSVT.2006.873157', '10.1145/1460096.1460125', '10.1109/TPAMI.2007.70791', '10.1145/1126004.1126005', '10.1109/TMM.2008.917359', '10.1109/CVPR.2008.4587353', '10.1109/ICASSP.2012.6288383', '10.1145/1877972.1877991', '10.1109/TMM.2009.2032676', '10.1007/s11042-008-0250-z', '10.1117/12.708204'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Representation Learning', 'Vision and Language', 'Computer Vision'], conference_acronym='IEEE transactions on circuits and systems for video technology (Print)', publisher=None, query_handler=None),\n",
       " 'A large-scale study on regularization and normalization in GANs': Paper(DOI='10.1145/3569928', crossref_json=None, google_schorlar_metadata=None, title='A large-scale study on regularization and normalization in GANs', authors=['Karol Kurach', 'Mario Lučić', 'Xiaohua Zhai', 'Marcin Michalski', 'Sylvain Gelly'], abstract='Generative adversarial networks (GANs) are a class of deep generative models which aim to learn a target distribution in an unsupervised fashion. While they were successfully applied to many problems, training a GAN is a notoriously challenging task and requires a significant number of hyperparameter tuning, neural architecture engineering, and a non-trivial amount of “tricks\". The success in many practical applications coupled with the lack of a measure to quantify the failure modes of GANs resulted in a plethora of proposed losses, regularization and normalization schemes, as well as neural architectures. In this work we take a sober view of the current state of GANs from a practical perspective. We discuss and evaluate common pitfalls and reproducibility issues, open-source our code on Github, and provide pre-trained models on TensorFlow Hub.', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR46437.2021.01405', '10.1109/CVPR.2019.00233', '10.1109/TNN.2009.2014161', '10.1109/TNNLS.2013.2275077', '10.1109/TNN.2009.2034144', '10.1109/TKDE.2010.26', '10.1109/CVPR.2019.01243', '10.1109/CVPR.2018.00916', '10.1109/CVPR42600.2020.00821', '10.1016/j.patcog.2021.108098', '10.1109/ICCV.2015.167', '10.4310/CDM.1997.v1997.n1.a2', '10.1145/3474838', '10.1109/CVPR42600.2020.00975', '10.1080/00401706.1970.10488634', '10.1145/3314203', '10.1109/ICCV.2017.167', '10.1049/iet-cvi.2018.5623', '10.1145/3309541', '10.1109/CVPR42600.2020.00782', '10.1109/CVPR.2019.00453', '10.1109/CVPR.2019.00202', '10.1145/3065386', '10.3389/fpubh.2020.00164', '10.1109/ICPR48806.2021.9412045', '10.1109/TAI.2021.3071642', '10.1109/TETCI.2022.3193373', '10.1109/ICCV.2017.304', '10.1016/0024-3795(90)90403-Y', '10.1109/TPAMI.2018.2858821', '10.1198/016214508000000337', '10.1109/CVPR.2019.00244', '10.1007/s11263-019-01265-2', '10.1007/978-3-030-30671-7_3', '10.1109/CVPR42600.2020.00926', '10.1007/978-981-15-9735-0_5', '10.1109/TNNLS.2012.2214488', '10.1109/ACCESS.2019.2941272', '10.1109/TIP.2018.2836316', '10.1109/tevc.2019.2895748', '10.1016/j.neunet.2018.08.007', '10.1016/j.patcog.2022.108552', '10.1109/CVPR.2018.00143', '10.1109/ICIP40778.2020.9191083', '10.1016/j.media.2019.101552', '10.1109/CVPR.2018.00577', '10.1109/ICCV.2019.00612', '10.1109/ICCV.2019.00156', '10.1109/ICCV.2017.629', '10.1109/TCSI.2019.2959886', '10.1016/j.knosys.2018.08.004'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Representation Learning', 'Vision and Language', 'Computer Vision'], conference_acronym='ACM computing surveys', publisher=None, query_handler=None),\n",
       " 'Semantic understanding of scenes through the ade20k dataset': Paper(DOI='10.1007/s11263-018-1140-0', crossref_json=None, google_schorlar_metadata=None, title='Semantic understanding of scenes through the ade20k dataset', authors=['Bolei Zhou', 'Hang Zhao', 'Xavier Puig', 'Tete Xiao', 'Sanja Fidler', 'Adela Barriuso', 'Antonio Torralba'], abstract=' Semantic understanding of visual scenes is one of the holy grails of computer vision. Despite efforts of the community in data collection, there are still few image datasets covering a wide range of scenes and object categories with pixel-wise annotations for scene understanding. In this work, we present a densely annotated dataset ADE20K, which spans diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. Totally there are 25k images of the complex everyday scenes containing a variety of objects in their natural spatial context. On average there are 19.5 instances and 10.5 object classes per image. Based on ADE20K, we construct benchmarks for scene parsing and instance segmentation. We provide baseline performances on both of the benchmarks and re-implement state-of-the-art models for open source. We further evaluate the effect of synchronized batch\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2016.2644615', '10.1145/2461912.2462002', '10.1109/CVPR.2015.7298970', '10.1109/CVPR.2018.00132', '10.1109/CVPR.2014.254', '10.1109/CVPR.2016.350', '10.1109/CVPR.2015.7299025', '10.1109/CVPR.2016.343', '10.1007/s11263-009-0275-4', '10.1109/CVPR.2012.6248074', '10.1109/ICCV.2017.322', '10.1007/978-3-030-01264-9_48', '10.1109/CVPR.2017.106', '10.1007/978-3-319-10602-1_48', '10.1109/CVPR.2015.7298965', '10.1109/ICCV.2001.937655', '10.1109/CVPR.2014.119', '10.1007/978-3-642-33715-4_54', '10.1109/ICCV.2015.178', '10.1007/978-3-319-46466-4_6', '10.1109/CVPR.2018.00647', '10.1007/s11263-015-0816-y', '10.1007/s11263-007-0090-8', '10.1109/CVPR.2015.7298655', '10.1007/s11263-010-0376-0', '10.1109/CVPR.2010.5539970', '10.1007/978-3-030-01228-1_26', '10.1109/ICCV.2017.221', '10.1109/CVPR.2017.660', '10.1109/CVPR.2017.544'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimodal Learning', 'Autonomous Driving', 'Embodied AI', 'Computer Vision'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Vse++: Improving visual-semantic embeddings with hard negatives': Paper(DOI='10.1016/j.patcog.2022.109272', crossref_json=None, google_schorlar_metadata=None, title='Vse++: Improving visual-semantic embeddings with hard negatives', authors=['Fartash Faghri', 'David J Fleet', 'Jamie Ryan Kiros', 'Sanja Fidler'], abstract='We present a new technique for learning visual-semantic embeddings for cross-modal retrieval. Inspired by hard negative mining, the use of hard negatives in structured prediction, and ranking loss functions, we introduce a simple change to common loss functions used for multi-modal embeddings. That, combined with fine-tuning and use of augmented data, yields significant gains in retrieval performance. We showcase our approach, VSE++, on MS-COCO and Flickr30K datasets, using ablation studies and comparisons with existing methods. On MS-COCO our approach outperforms state-of-the-art methods by 8.8% in caption retrieval and 11.3% in image retrieval (at R@1).', conference=None, journal=None, year=None, reference_list=['10.3390/jimaging7080125', '10.1016/j.patcog.2021.107905', '10.1016/j.patcog.2020.107734', '10.1162/tacl_a_00166', '10.1016/j.patcog.2021.108084', '10.1186/s40649-019-0069-y', '10.1016/j.patcog.2022.108676', '10.1109/TPAMI.2019.2940446', '10.1016/j.patcog.2018.07.001', '10.1016/j.patcog.2021.108217', '10.1016/j.patcog.2019.05.008', '10.1016/j.patcog.2020.107359', '10.1016/j.patcog.2008.06.025', '10.1016/j.patcog.2021.108271', '10.1016/j.procs.2013.05.005', '10.1109/ACCESS.2019.2932868', '10.1007/s11263-016-0981-7'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'Scaling egocentric vision: The epic-kitchens dataset': Paper(DOI='10.1007/s11263-021-01531-2', crossref_json=None, google_schorlar_metadata=None, title='Scaling egocentric vision: The epic-kitchens dataset', authors=['Dima Damen', 'Hazel Doughty', 'Giovanni Maria Farinella', 'Sanja Fidler', 'Antonino Furnari', 'Evangelos Kazakos', 'Davide Moltisanti', 'Jonathan Munro', 'Toby Perrett', 'Will Price', 'Michael Wray'], abstract='First-person vision is gaining interest as it offers a unique viewpoint on people’s interaction with objects, their attention, and even intention. However, progress in this challenging domain has been relatively slow due to the lack of sufficiently large datasets. In this paper, we introduce EPIC-KITCHENS, a large-scale egocentric video benchmark recorded by 32 participants in their native kitchen environments. Our videos depict non-scripted daily activities: we simply asked each participant to start recording every time they entered their kitchen. Recording took place in 4 cities (in North America and Europe) by participants belonging to 10 different nationalities, resulting in highly diverse cooking styles. Our dataset features 55 hours of video consisting of 11.5 M frames, which we densely labelled for a total of 39.6 K action segments and 454.3 K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos (after recording), thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens.', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-319-46478-7_34', '10.1007/978-3-319-10602-1_41', '10.1109/CVPR42600.2020.01164', '10.5244/C.31.128', '10.1007/978-3-319-11382-1_18', '10.1177/0278364915614638', '10.1109/CVPR.2017.502', '10.1109/CVPR.2019.00366', '10.1109/ICCV.2019.00642', '10.1109/CVPR.2016.350', '10.1007/978-3-030-01225-0_44', '10.5244/C.28.30', '10.1007/978-3-319-46454-1_17', '10.1109/CVPR.2009.5206848', '10.1007/978-3-642-33718-5_23', '10.1109/ICCV.2019.00630', '10.1109/TPAMI.2020.2992889', '10.1007/978-3-030-11021-5_24', '10.1007/978-3-319-58347-1_10', '10.1109/CVPR.2012.6248074', '10.1109/ICCV.2017.622', '10.1109/CVPR.2018.00633', '10.1007/s11263-019-01255-4', '10.1109/ICCV.2019.00502', '10.1109/ICCV.2017.322', '10.2307/1912352', '10.1109/CVPR.2015.7298698', '10.1007/978-3-030-01252-6_13', '10.1109/WACV45572.2020.9093358', '10.1109/CVPRW.2018.00141', '10.1007/978-3-319-46493-0_9', '10.1145/582415.582418', '10.1109/TMM.2015.2390499', '10.1109/CVPR.2015.7298932', '10.1109/ICCV.2019.00559', '10.1109/TPAMI.2015.2430335', '10.1109/ICCV.2017.83', '10.1109/CVPR.2014.105', '10.1109/ICCV.2011.6126543', '10.1109/CVPR.2017.113', '10.1109/CVPR.2017.233', '10.1109/ICCV.2019.00634', '10.1109/CVPR.2015.7298625', '10.1109/ICCV.2019.00718', '10.1109/ICCV.2019.00399', '10.1007/978-3-319-10602-1_48', '10.1109/CVPR.2019.00139', '10.1109/CVPR42600.2020.01242', '10.1177/0278364916679498', '10.1109/CVPR.2009.5206557', '10.21105/joss.00861', '10.1145/2911996.2912036', '10.1007/978-3-319-46454-1_27', '10.1109/ICCV.2019.00272', '10.1109/CVPR.2019.01015', '10.1109/ICCV.2017.314', '10.1109/TPAMI.2019.2901464', '10.1109/CVPR42600.2020.00020', '10.1109/ICCV.2019.00877', '10.1109/ICCV.2017.534', '10.1109/CVPR.2018.00706', '10.1109/ICCV.2019.00560', '10.1007/978-3-319-46466-4_5', '10.1109/CVPRW50498.2020.00172', '10.1609/aaai.v34i07.6854', '10.5244/C.24.50', '10.1109/ICCV.2019.00149', '10.1109/CVPR.2016.85', '10.1109/CVPR.2012.6248010', '10.1145/3240508.3240633', '10.1109/CVPR.2018.00771', '10.1109/CVPR.2012.6247801', '10.1109/CVPR.2015.7298940', '10.1109/ICCVW.2019.00460', '10.1007/978-3-642-15561-1_16', '10.1007/978-3-642-15561-1_16', '10.1109/CVPR42600.2020.00989', '10.1007/978-3-319-46448-0_31', '10.1007/978-3-642-33715-4_54', '10.1109/ICCV.2017.381', '10.1145/2493432.2493482', '10.1145/2493432.2493482', '10.1109/CVPR.2011.5995347', '10.1109/CVPR.2017.572', '10.1007/978-3-030-01261-8_24', '10.1109/CVPR.2017.678', '10.1007/978-3-319-46484-8_2', '10.1109/ICCV.2019.00054', '10.1109/ICRA.2018.8460982', '10.1109/CVPR.2016.571', '10.1016/j.imavis.2016.01.001', '10.1007/s11263-017-1013-y', '10.1109/ICCV.2019.00940', '10.24963/ijcai.2019/871', '10.1109/CVPR.2017.440', '10.1109/ICCV.2019.00876', '10.1007/978-3-030-01246-5_49', '10.1109/CVPR.2019.00674', '10.1126/scirobotics.aaw6661', '10.1609/aaai.v32i1.12342', '10.1109/CVPR.2017.544'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d': Paper(DOI='10.1007/978-3-030-58568-6_12', crossref_json=None, google_schorlar_metadata=None, title='Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d', authors=['Jonah Philion', 'Sanja Fidler'], abstract=' The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single “bird’s-eye-view” coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird’s-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to “lift” each image individually into a frustum of features for each camera, then “splat” all frustums into a rasterized bird’s-eye-view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird’s-eye-view tasks such as object segmentation and map segmentation, our\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2019.00895', '10.1109/CVPR.2016.236', '10.1109/ICCV.2017.169', '10.1109/CVPR.2019.01298', '10.1109/5.726791', '10.1145/3306346.3323020', '10.1109/WACV45572.2020.9093519', '10.1109/CVPR.2019.01185', '10.1109/CVPR42600.2020.01407', '10.1109/3DV.2016.78', '10.1609/aaai.v33i01.33018851', '10.1109/CVPR42600.2020.01115', '10.1109/ICCV.2019.00208', '10.1109/CVPR42600.2020.00810', '10.1109/CVPR42600.2020.00252', '10.1109/ICCV.2019.00533', '10.1109/CVPR42600.2020.00063', '10.1109/CVPR.2019.00864', '10.1109/CVPR.2019.00886'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " '3d object proposals using stereo imagery for accurate object class detection': Paper(DOI='10.1109/tpami.2017.2706685', crossref_json=None, google_schorlar_metadata=None, title='3d object proposals using stereo imagery for accurate object class detection', authors=['Xiaozhi Chen', 'Kaustav Kundu', 'Yukun Zhu', 'Huimin Ma', 'Sanja Fidler', 'Raquel Urtasun'], abstract='The goal of this paper is to perform 3D object detection in the context of autonomous driving. Our method aims at generating a set of high-quality 3D object proposals by exploiting stereo imagery. We formulate the problem as minimizing an energy function that encodes object size priors, placement of objects on the ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground. We then exploit a CNN on top of these proposals to perform object detection. In particular, we employ a convolutional neural net (CNN) that exploits context and depth information to jointly regress to 3D bounding box coordinates and object pose. Our experiments show significant performance gains over existing RGB and RGB-D object proposal methods on the challenging KITTI benchmark. When combined with the CNN, our approach outperforms all existing results in\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-319-10599-4_42', '10.1109/CVPR.2015.7299022', '10.1109/TPAMI.2015.2465908', '10.1109/TITS.2015.2409889', '10.1007/s11263-014-0780-y', '10.1109/TPAMI.2015.2408347', '10.1007/978-3-642-33786-4_32', '10.1007/978-3-319-10602-1_47', '10.1109/CVPR.2013.423', '10.1109/TPAMI.2011.231', '10.1109/ICCV.2013.179', '10.1109/ICRA.2013.6630857', '10.1109/ICCV.2013.315', '10.1109/CVPR.2015.7299035', '10.5244/C.28.6', '10.1109/IROS.2013.6696957', '10.1109/CVPR.2015.7298784', '10.1109/IROS.2014.6943141', '10.1109/IVS.2015.7225711', '10.15607/RSS.2016.XII.042', '10.15607/RSS.2015.XI.035', '10.1109/CVPR.2014.414', '10.1109/TITS.2015.2496795', '10.1109/ICCV.2015.196', '10.1109/ICCV.2015.296', '10.1109/CVPR.2012.6248074', '10.1109/ICCV.2015.169', '10.1007/s10994-009-5108-8', '10.1007/978-3-319-10584-0_23', '10.1109/CVPR.2014.81', '10.1109/TPAMI.2009.167', '10.1007/s11263-009-0275-4', '10.1109/CVPR.2014.49', '10.1109/ICCV.2011.6126456', '10.1109/ICCV.2013.51', '10.1109/TPAMI.2012.28', '10.1109/ICCV.2015.384', '10.1145/1015330.1015341', '10.1109/TPAMI.2014.2300479', '10.1109/CVPR.2015.7298800', '10.1109/ICCV.2015.221', '10.1109/CVPR.2015.7299034'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Motion analysis for image enhancement: Resolution, occlusion, and transparency': Paper(DOI='10.1006/jvci.1993.1030', crossref_json=None, google_schorlar_metadata=None, title='Motion analysis for image enhancement: Resolution, occlusion, and transparency', authors=['Michal Irani', 'Shmuel Peleg'], abstract='Accurate computation of image motion enables the enhancement of image sequences. In scenes having multiple moving objects the motion computation is performed together with object segmentation by using a unique temporal integration approach. After the motion for the different image regions is computed, these regions can be enhanced by fusing several successive frames covering the same region. Enhancements treated here include improvement of image resolution, filling-in occluded regions, and reconstruction of transparent objects.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computational Photography'], conference_acronym='Journal of visual communication and image representation (Print)', publisher=None, query_handler=None),\n",
       " 'Computing occluding and transparent motions': Paper(DOI='10.1007/bf01420982', crossref_json=None, google_schorlar_metadata=None, title='Computing occluding and transparent motions', authors=['Michal Irani', 'Benny Rousso', 'Shmuel Peleg'], abstract=' Computing the motions of several moving objects in image sequences involves simultaneous motion analysis and segmentation. This task can become complicated when image motion changes significantly between frames, as with camera vibrations. Such vibrations make tracking in longer sequences harder, as temporal motion constancy cannot be assumed. The problem becomes even more difficult in the case of transparent motions. A method is presented for detecting and tracking occluding and transparent moving objects, which uses temporal integration without assuming motion constancy. Each new frame in the sequence is compared to a dynamic internal representation image of the tracked object. The internal representation image is constructed by temporally integrating frames after registration based on the motion computation. The temporal integration maintains sharpness of the tracked\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.1985.4767678', '10.1007/3-540-55426-2_27', '10.1109/34.161348', '10.1109/WVM.1991.212808', '10.1109/WVM.1991.212810', '10.1007/BF00056772', '10.1007/BF00133568', '10.1016/0004-3702(81)90024-2', '10.1016/0734-189X(84)90131-2', '10.1109/CVPR.1992.223272', '10.1007/3-540-55426-2_53', '10.1109/ICPR.1990.118074', '10.1007/3-540-55426-2_46', '10.1109/ICPR.1990.118111', '10.1109/WVM.1991.212811'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Computational Photography'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Video indexing based on mosaic representations': Paper(DOI='10.1109/5.664279', crossref_json=None, google_schorlar_metadata=None, title='Video indexing based on mosaic representations', authors=['Michal Irani', 'Prabu Anandan'], abstract='Video is a rich source of information. It provides visual information about scenes. This information is implicitly buried inside the raw video data, however, and is provided with the cost of very high temporal redundancy. While the standard sequential form of video storage is adequate for viewing in a movie mode, it fails to support rapid access to information of interest that is required in many of the emerging applications of video. This paper presents an approach for efficient access, use and manipulation of video data. The video data are first transformed from their sequential and redundant frame-based representation, in which the information about the scene is distributed over many frames, to an explicit and compact scene-based representation, to which each frame can be directly related. This compact reorganization of the video data supports nonlinear browsing and efficient indexing to provide rapid access directly\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF00137442', '10.1145/166266.166270', '10.1109/ACV.1994.341287', '10.1109/WVM.1991.212811', '10.1007/BF01210504', '10.1109/CVPR.1993.341105', '10.1109/ICCV.1995.466820', '10.1007/BFb0028365', '10.1109/ICPR.1996.546117', '10.1016/0923-5965(95)00055-0', '10.1016/0923-5965(95)00022-1', '10.1007/BF01420982', '10.1109/CVPR.1994.323866', '10.1109/ICCV.1995.466883', '10.1007/BF00318371', '10.1109/ICPR.1994.576402', '10.1109/ICPR.1994.576402', '10.1109/CVPR.1994.323927', '10.1109/ICCV.1995.466859', '10.1109/WVM.1991.212808', '10.1109/34.161348', '10.1109/CVPR.1994.323870', '10.1109/WVM.1991.212810', '10.1109/2.410146', '10.1109/TPAMI.1985.4767678', '10.1109/WVRS.1995.476847', '10.1109/TCSI.2004.836856', '10.1007/3-540-57956-7_24', '10.1109/ICIP.1994.413336', '10.1109/CVPR.1997.609346'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Image Processing', 'Video Information Analysis'], conference_acronym='Proceedings of the IEEE', publisher=None, query_handler=None),\n",
       " 'High confidence visual recognition of persons by a test of statistical independence': Paper(DOI='10.1109/34.244676', crossref_json=None, google_schorlar_metadata=None, title='High confidence visual recognition of persons by a test of statistical independence', authors=['John G Daugman'], abstract='A method for rapid visual recognition of personal identity is described, based on the failure of a statistical test of independence. The most unique phenotypic feature visible in a person\\'s face is the detailed texture of each eye\\'s iris. The visible texture of a person\\'s iris in a real-time video image is encoded into a compact sequence of multi-scale quadrature 2-D Gabor wavelet coefficients, whose most-significant bits comprise a 256-byte \"iris code\". Statistical decision theory generates identification decisions from Exclusive-OR comparisons of complete iris codes at the rate of 4000 per second, including calculation of decision confidence levels. The distributions observed empirically in such comparisons imply a theoretical \"cross-over\" error rate of one in 131000 when a decision criterion is adopted that would equalize the false accept and false reject error rates. In the typical recognition case, given the mean observed\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/0165-1684(80)90024-9', '10.1016/0734-189X(85)90130-6', '10.1109/29.1644', '10.1007/978-1-349-09997-9', '10.2307/2842415', '10.1109/PROC.1979.11328', '10.1109/TSMC.1973.4309314', '10.1016/0031-3203(81)90008-X', '10.1016/0031-3203(91)90143-S', '10.1002/j.1538-7305.1977.tb00522.x', '10.3758/BF03206215', '10.1037/h0058700', '10.1016/0031-3203(89)90007-1', '10.3758/BF03203047', '10.1016/0042-6989(80)90065-6', '10.1016/0167-8655(85)90053-4', '10.1109/34.41384', '10.1364/JOSAA.2.001160', '10.1109/34.192463', '10.1109/TIT.1954.1057460', '10.1109/10.16457', '10.1016/0031-3203(92)90007-6'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'information theory', 'iris recognition'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters': Paper(DOI='10.1364/josaa.2.001160', crossref_json=None, google_schorlar_metadata=None, title='Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters', authors=['John G Daugman'], abstract='Two-dimensional spatial linear filters are constrained by general uncertainty relations that limit their attainable information resolution for orientation, spatial frequency, and two-dimensional (2D) spatial position. The theoretical lower limit for the joint entropy, or uncertainty, of these variables is achieved by an optimal 2D filter family whose spatial weighting functions are generated by exponentiated bivariate second-order polynomials with complex coefficients, the elliptic generalization of the one-dimensional elementary functions proposed in Gabor’s famous theory of communication [ J. Inst. Electr. Eng.93,  429 ( 1946)]. The set includes filters with various orientation bandwidths, spatial-frequency bandwidths, and spatial dimensions, favoring the extraction of various kinds of information from an image. Each such filter occupies an irreducible quantal volume (corresponding to an independent datum) in a four\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1113/jphysiol.1962.sp006837', '10.1002/cne.901580304', '10.1113/jphysiol.1968.sp008574', '10.1126/science.173.3991.74', '10.1016/0042-6989(73)90201-0', '10.1113/jphysiol.1979.sp012827', '10.1016/0042-6989(74)90157-6', '10.1016/0042-6989(78)90088-3', '10.1126/science.6765993', '10.1364/JOSA.70.001297', '10.1038/289117a0', '10.1007/BF01963207', '10.1126/science.7233231', '10.1007/BF00336972', '10.1007/BF00319978', '10.1016/0042-6989(80)90065-6', '10.1113/jphysiol.1978.sp012488', '10.1113/jphysiol.1975.sp011025', '10.1113/jphysiol.1979.sp012652', '10.1016/0042-6989(65)90033-7', '10.1152/jn.1976.39.3.512', '10.1152/jn.1984.52.2.372', '10.1016/0042-6989(82)90113-4', '10.1152/jn.1974.37.6.1394', '10.1007/BF00239014', '10.1007/BF00234465', '10.1016/0042-6989(78)90037-8', '10.1016/0042-6989(82)90112-2', '10.1126/science.7292014', '10.1364/JOSA.69.001519'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'information theory', 'iris recognition'], conference_acronym='Journal of the Optical Society of America. A, Optics, image science, and vision', publisher=None, query_handler=None),\n",
       " 'Complete discrete 2-D Gabor transforms by neural networks for image analysis and compression': Paper(DOI='10.1109/29.1644', crossref_json=None, google_schorlar_metadata=None, title='Complete discrete 2-D Gabor transforms by neural networks for image analysis and compression', authors=['John G Daugman'], abstract='A three-layered neural network is described for transforming two-dimensional discrete signals into generalized nonorthogonal 2-D Gabor representations for image analysis, segmentation, and compression. These transforms are conjoint spatial/spectral representations, which provide a complete image description in terms of locally windowed 2-D spectral coordinates embedded within global 2-D spatial coordinates. In the present neural network approach, based on interlaminar interactions involving two layers with fixed weights and one layer with adjustable weights, the network finds coefficients for complete conjoint 2-D Gabor transforms without restrictive conditions. In wavelet expansions based on a biologically inspired log-polar ensemble of dilations, rotations, and translations of a single underlying 2-D Gabor wavelet template, image compression is illustrated with ratios up to 20:1. Also demonstrated is image\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/PROC.1984.12947', '10.1117/12.7972986', '10.1109/TSMC.1973.4309314', '10.1364/AO.26.001892', '10.1364/JOSAA.2.001160', '10.1146/annurev.ne.02.030179.001303', '10.1113/jphysiol.1962.sp006837', '10.1002/cne.901580304', '10.1126/science.7233231', '10.1016/0042-6989(80)90065-6', '10.1364/JOSA.70.001297', '10.1109/TSMC.1983.6313083', '10.1364/OL.10.000098', '10.1109/ICASSP.1987.1169721', '10.1073/pnas.79.8.2554', '10.1109/PROC.1980.11686', '10.1109/TSMC.1983.6313076', '10.1152/jn.1987.58.6.1233', '10.1007/978-3-642-96384-1', '10.1137/0515056', '10.1190/1.1441329', '10.1063/1.527388', '10.1063/1.526761', '10.1109/34.192463'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'information theory', 'iris recognition'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Two-dimensional spectral analysis of cortical receptive field profiles': Paper(DOI='10.1016/0042-6989(80)90065-6', crossref_json=None, google_schorlar_metadata=None, title='Two-dimensional spectral analysis of cortical receptive field profiles', authors=['John G Daugman'], abstract='Most vision research embracing the spatial frequency paradigm has been conceptually and mathematically a one-dimensional analysis of two-dimensional mechanisms. Spatial vision models and the experiments sustaining them have generally treated spatial frequency as a one-dimensional variable, even though receptive fields and retinal images are two-dimensional and linear transform theory obliges any frequency analysis to preserve dimension. Four models of cortical receptive fields are introduced and studied here in 2D form, in order to illustrate the relationship between their excitatory/inhibitory spatial structure and their resulting 2D spectral properties. It emerges that only a very special analytic class of receptive fields possess independent tuning functions for spatial frequency and orientation; namely, those profiles whose two-dimensional Fourier Transforms are expressible as the separable product of a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1113/jphysiol.1979.sp012652', '10.1152/jn.1974.37.4.621', '10.1113/jphysiol.1973.sp010218', '10.1113/jphysiol.1971.sp009374', '10.1113/jphysiol.1968.sp008604', '10.1113/jphysiol.1968.sp008574', '10.1007/BF00233183', '10.1113/jphysiol.1979.sp012827', '10.1113/jphysiol.1966.sp008107', '10.1016/0042-6989(79)90182-2', '10.1016/0042-6989(73)90061-8', '10.1152/jn.1974.37.6.1394', '10.1113/jphysiol.1962.sp006837', '10.1007/BF00234673', '10.1016/0042-6989(76)90227-3', '10.1113/jphysiol.1975.sp010868', '10.1016/0042-6989(65)90033-7', '10.1016/0042-6989(79)90138-X', '10.1364/JOSA.61.001176', '10.1152/jn.1976.39.6.1334', '10.1016/0042-6989(78)90088-3'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'information theory', 'iris recognition'], conference_acronym='Vision research (Oxford)', publisher=None, query_handler=None),\n",
       " 'New methods in iris recognition': Paper(DOI='10.1007/978-1-4471-4402-1_13', crossref_json=None, google_schorlar_metadata=None, title='New methods in iris recognition', authors=['John Daugman'], abstract='This paper presents the following four advances in iris recognition: 1) more disciplined methods for detecting and faithfully modeling the iris inner and outer boundaries with active contours, leading to more flexible embedded coordinate systems; 2) Fourier-based methods for solving problems in iris trigonometry and projective geometry, allowing off-axis gaze to be handled by detecting it and ldquorotatingrdquo the eye into orthographic perspective; 3) statistical inference methods for detecting and excluding eyelashes; and 4) exploration of score normalizations, depending on the amount of iris data that is available in images and the required scale of database search. Statistical results are presented based on 200 billion iris cross-comparisons that were generated from 632 500 irises in the United Arab Emirates database to analyze the normalization issues raised in different regions of receiver operating\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.cviu.2010.06.002', '10.1016/j.cviu.2007.08.005', '10.1511/2001.28.737', '10.1016/S0031-3203(02)00030-4', '10.1109/TCSVT.2003.818350', '10.1109/TSMCB.2007.903540', '10.5244/C.17.16', '10.1016/S0734-189X(88)80033-1', '10.1117/12.666448', '10.1109/BTAS.2009.5339062', '10.1364/AO.43.000391', '10.1109/TIP.2004.827237', '10.1109/TIP.2004.827237', '10.1109/JPROC.2006.884091', '10.1109/2.820042', '10.1109/TSMCA.2008.2008210', '10.1007/978-3-540-89639-5_70', '10.1109/TPAMI.2009.140', '10.1109/CVPRW.2008.4563108', '10.1109/ICIP.2009.5414156', '10.1016/j.optlaseng.2010.09.011', '10.1109/TSMCB.2007.904831', '10.1109/5.628669', '10.1109/ICIP.2006.313181', '10.1109/TSMCB.2009.2015426', '10.1109/BCC.2006.4341623'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'information theory', 'iris recognition'], conference_acronym='Advances in computer vision and pattern recognition (Print)', publisher=None, query_handler=None),\n",
       " 'The importance of being random: statistical principles of iris recognition': Paper(DOI='10.1016/s0031-3203(02)00030-4', crossref_json=None, google_schorlar_metadata=None, title='The importance of being random: statistical principles of iris recognition', authors=['John Daugman'], abstract='The statistical variability that is the basis of iris recognition is analysed in this paper using new large databases. The principle underlying the recognition algorithm is the failure of a test of statistical independence on iris phase structure encoded by multi-scale quadrature wavelets. Combinatorial complexity of this phase information across different persons spans about 249 degrees-of-freedom and generates a discrimination entropy of about 3.2 bits/mm 2 over the iris, enabling real-time identification decisions with great enough accuracy to support exhaustive searches through very large databases. This paper presents the results of 9.1 million comparisons among several thousand eye images acquired in trials in Britain, the USA, Japan and Korea.', conference=None, journal=None, year=None, reference_list=['10.1109/34.598228', '10.1109/34.598229', '10.1109/34.879790', '10.1109/2.820039', '10.1109/34.244676', '10.1364/JOSAA.12.000641', '10.1016/0042-6989(80)90065-6', '10.1364/JOSAA.2.001160', '10.1109/29.1644', '10.2307/2684728', '10.1111/j.1755-3768.1985.tb05205.x', '10.1001/jama.242.13.1385'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'information theory', 'iris recognition'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'Combining crypto with biometrics effectively': Paper(DOI='10.1109/tc.2006.138', crossref_json=None, google_schorlar_metadata=None, title='Combining crypto with biometrics effectively', authors=['Feng Hao', 'Ross Anderson', 'John Daugman'], abstract=\"We propose the first practical and secure way to integrate the iris biometric into cryptographic applications. A repeatable binary string, which we call a biometric key, is generated reliably from genuine iris codes. A well-known difficulty has been how to cope with the 10 to 20 percent of error bits within an iris code and derive an error-free key. To solve this problem, we carefully studied the error patterns within iris codes and devised a two-layer error correction technique that combines Hadamard and Reed-Solomon codes. The key is generated from a subject's iris image with the aid of auxiliary error-correction data, which do not reveal the key and can be saved in a tamper-resistant token, such as a smart card. The reproduction of the key depends on two factors: the iris biometric and the token. The attacker has to procure both of them to compromise the key. We evaluated our technique using iris samples from 70\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1017/CBO9780511606267', '10.1145/319709.319714', '10.1147/sj.302.0206', '10.1109/ISIT.2002.1023680', '10.1016/S0031-3203(02)00030-4', '10.1108/09685220210436949', '10.1109/SECPRI.2001.924299', '10.1145/319709.319720', '10.1145/982507.982516', '10.1109/JPROC.2004.827372', '10.1007/978-3-540-24676-3_31', '10.1108/09685220110408022', '10.1145/1030083.1030096', '10.1109/ICARCV.2002.1238560', '10.1007/BFb0101073'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'information theory', 'iris recognition'], conference_acronym='I.E.E.E. transactions on computers (Print)', publisher=None, query_handler=None),\n",
       " 'Probing the uniqueness and randomness of iriscodes: Results from 200 billion iris pair comparisons': Paper(DOI='10.1109/jproc.2006.884092', crossref_json=None, google_schorlar_metadata=None, title='Probing the uniqueness and randomness of iriscodes: Results from 200 billion iris pair comparisons', authors=['John Daugman'], abstract='Recent large-scale deployments of iris recognition for border-crossing controls enable critical assessment of the robustness of this technology against making false matches, since vast numbers of cross comparisons become possible within large databases. This paper presents results from the 200 billion iris cross comparisons that could be performed within a database of 632 500 different iris images, spanning 152 nationalities. Each iris pattern was encoded into a phase sequence of 2048 bits using the Daugman algorithms. Empirically analyzing the tail of the resulting distribution of similarity scores enables specification of decision thresholds, and prediction of performance, of the iris recognition algorithms if deployed in identification mode on national scales', conference=None, journal=None, year=None, reference_list=['10.1098/rspb.2001.1696', '10.1023/A:1012365806338', '10.1109/TCSVT.2003.818350', '10.1016/S0031-3203(02)00030-4', '10.1109/34.244676', '10.1007/BF00133570', '10.1007/978-1-4471-1555-7'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'information theory', 'iris recognition'], conference_acronym='Proceedings of the IEEE', publisher=None, query_handler=None),\n",
       " 'Epigenetic randomness, complexity and singularity of human iris patterns': Paper(DOI='10.1098/rspb.2001.1696', crossref_json=None, google_schorlar_metadata=None, title='Epigenetic randomness, complexity and singularity of human iris patterns', authors=['John Daugman', 'Cathryn Downing'], abstract='We investigated the randomness and uniqueness of human iris patterns by mathematically comparing 2.3 million different pairs of eye images. The phase structure of each iris pattern was extracted by demodulation with quadrature wavelets spanning several scales of analysis. The resulting distribution of phase sequence variation among different eyes was precisely binomial, revealing 244 independent degrees of freedom. This amount of statistical variability corresponds to an entropy (information density) of about 3.2 bits mm−2 over the iris. It implies that the probability of two different irides agreeing by chance in more than 70% of their phase sequence is about one in 7 billion. We also compared images of genetically identical irides, from the left and right eyes of 324 persons, and from monozygotic twins. Their relative phase sequence variation generated the same statistical distribution as did unrelated eyes. This\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1002/0471200611', '10.1016/0042-6989(80)90065-6', '10.1109/29.1644', '10.1109/34.244676', '10.1126/science.26.670.589-b', '10.1016/S0039-6257(97)80018-5', '10.1016/B978-1-4832-3090-0.50007-1', '10.1080/00031305.1994.10476068'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'information theory', 'iris recognition'], conference_acronym='Proceedings - Royal Society. Biological Sciences (Print)', publisher=None, query_handler=None),\n",
       " 'Spatial visual channels in the Fourier plane': Paper(DOI='10.1016/0042-6989(84)90065-8', crossref_json=None, google_schorlar_metadata=None, title='Spatial visual channels in the Fourier plane', authors=['John G Daugman'], abstract='Properties of human spatial visual channels were studied in two-dimensional form by a signal detection masking paradigm.Tuning surfaces of contrast threshold elevation induced by a sinusoidal mask were generated for four Subjects, interpolated from an 11 × 11 Cartesian grid over the Fourier plane, and numerically Fourier transformed in two dimensions to infer putative filter profiles in the 2D space domain. Among the main findings in the 2D frequency domain were:(1) Threshold elevation surfaces are highlypolar nonseparable—they cannot be described as the product of a spatial frequency tuning curve times an orientation tuning curve.(2) Iso-half-amplitude contours of the spectral tuning surfaces have a length/width elongation ratio of about 2:1.(3) Necessarily, resolution for spatial frequency and for orientation are in fundamental competition with 2D spatial resolution. By calculating the occupied area of the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/0042-6989(80)90070-X', '10.1068/p010371', '10.1113/jphysiol.1969.sp008862', '10.1038/228477a0', '10.1016/0042-6989(83)90135-9', '10.1113/jphysiol.1969.sp008861', '10.1113/jphysiol.1966.sp008101', '10.1113/jphysiol.1968.sp008574', '10.1016/0042-6989(77)90023-2', '10.1016/0042-6989(80)90065-6', '10.1016/0042-6989(82)90113-4', '10.1113/jphysiol.1979.sp012827', '10.1113/jphysiol.1975.sp010952', '10.1007/BF00337286', '10.1016/0042-6989(71)90189-1', '10.1016/0042-6989(78)90122-0', '10.1016/0022-2496(66)90005-8', '10.1113/jphysiol.1962.sp006837', '10.1007/BF00234672', '10.1364/JOSA.50.001115', '10.1016/0042-6989(75)90230-8', '10.1016/0042-6989(75)90282-5', '10.1016/0042-6989(76)90111-5', '10.1007/BF01963207', '10.1016/0042-6989(73)90006-0', '10.1016/0042-6989(78)90024-X', '10.1364/JOSA.70.001458', '10.1038/289117a0', '10.1038/240479a0', '10.1016/0042-6989(73)90201-0', '10.1364/JOSA.70.001297', '10.1016/0042-6989(76)90227-3', '10.1126/science.173.3991.74', '10.1016/0042-6989(82)90172-9', '10.1037/0033-295X.89.4.407', '10.1364/JOSA.61.001176', '10.1007/BF00336972', '10.1007/BF00360909', '10.1146/annurev.ps.25.020174.001211', '10.1016/0042-6989(78)90232-8', '10.1016/0042-6989(82)90162-6', '10.1016/0042-6989(77)90022-0', '10.1111/j.2044-8317.1965.tb00689.x', '10.1016/0042-6989(79)90117-2', '10.1016/0042-6989(82)90175-4'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'information theory', 'iris recognition'], conference_acronym='Vision research (Oxford)', publisher=None, query_handler=None),\n",
       " 'Entropy reduction and decorrelation in visual coding by oriented neural receptive fields': Paper(DOI='10.1109/10.16456', crossref_json=None, google_schorlar_metadata=None, title='Entropy reduction and decorrelation in visual coding by oriented neural receptive fields', authors=['John G Daugman'], abstract=\"In biological visual systems, it is not obvious whether coding efficiency as measured by mutual information among the neurons is a factor that explains any of their properties. The center/surround receptive field profiles of neurons in the retina and geniculate are far from an orthogonal set, but a given neuron can still be regarded as a decorrelator of the incoming signal in the sense that it responds primarily to changes in the image. At the level of the brain's visual cortex, the introduction of the new variable of orientation selectivity can be regarded not only as a means for providing orientation labels for image structure, but also more basically as an effective decorrelator of the neural representation. The present image coding simulations, based on quantitative neurobiological data about the code primitives, provide measures of the bit-rate efficiency of such oriented, quadrature, neural codes. Demonstrations of data\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1109/PROC.1986.13504', '10.1109/TPAMI.1987.4767871', '10.1113/jphysiol.1962.sp006837', '10.1002/cne.901580304', '10.1126/science.7233231', '10.1152/jn.1987.58.6.1233', '10.1016/0042-6989(80)90065-6', '10.1364/JOSA.70.001297', '10.1364/JOSAA.2.001160', '10.1007/BF00337011', '10.1109/29.1644', '10.1113/jphysiol.1969.sp008836', '10.1364/ON.13.8.000016', '10.1109/TIT.1962.1057701', '10.1109/TPAMI.1984.4767596', '10.1007/978-3-642-46345-7', '10.1016/0042-6989(84)90065-8', '10.1190/1.1441329', '10.1063/1.526761', '10.1137/0515056', '10.1109/34.192463', '10.1063/1.527388'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'information theory', 'iris recognition'], conference_acronym='IEEE transactions on biomedical engineering (Print)', publisher=None, query_handler=None),\n",
       " 'An information-theoretic view of analog representation in striate cortex': Paper(DOI='10.1093/cercor/10.9.840', crossref_json=None, google_schorlar_metadata=None, title='An information-theoretic view of analog representation in striate cortex', authors=['John G Daugman'], abstract='An information-theoretic view of analog representation in striate cortex | Computational \\nneuroscience ACM Digital Library home ACM home Google, Inc. (search) Advanced Search \\nBrowse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs \\nConferences People More Search ACM Digital Library SearchSearch Advanced Search \\nBrowse Browse Digital Library Collections More HomeBrowse by TitleBooksComputational \\nneuroscienceAn information-theoretic view of analog representation in striate cortex chapter \\nShare on An information-theoretic view of analog representation in striate cortex Author: John \\nG. Daugman View Profile Authors Info & Claims Computational neuroscienceOctober 1993 \\nPages 403–423 Online:29 October 1993Publication History 5citation 0 Downloads Metrics Total \\nCitations5 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New Citation …', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'information theory', 'iris recognition'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Enlightengan: Deep light enhancement without paired supervision': Paper(DOI='10.1109/tip.2021.3051462', crossref_json=None, google_schorlar_metadata=None, title='Enlightengan: Deep light enhancement without paired supervision', authors=['Yifan Jiang', 'Xinyu Gong', 'Ding Liu', 'Yu Cheng', 'Chen Fang', 'Xiaohui Shen', 'Jianchao Yang', 'Pan Zhou', 'Zhangyang Wang'], abstract='Deep learning-based methods have achieved remarkable success in image restoration and enhancement, but are they still competitive when there is a lack of paired training data? As one such example, this paper explores the low-light image enhancement problem, where in practice it is extremely challenging to simultaneously take a low-light and a normal-light photo of the same visual scene. We propose a highly effective unsupervised generative adversarial network, dubbed  EnlightenGAN , that can be trained without low/normal-light image pairs, yet proves to generalize very well on various real-world test images. Instead of supervising the learning using ground truth data, we propose to regularize the unpaired training using the information extracted from the input itself, and benchmark a series of innovations for the low-light image enhancement problem, including a global-local discriminator structure, a self\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2018.00916', '10.1109/CVPR.2019.00396', '10.1007/978-3-030-01219-9_11', '10.1145/3072959.3073609', '10.1016/j.cviu.2018.10.010', '10.1109/TIP.2020.2981922', '10.1109/ICCV.2017.511', '10.1109/TIP.2018.2867951', '10.1109/ICCV.2019.00897', '10.1007/978-3-030-01216-8_8', '10.1109/CVPR.2018.00347', '10.1109/TIP.2013.2261309', '10.1109/83.597272', '10.1093/biomet/39.3-4.324', '10.1109/LSP.2012.2227726', '10.24963/ijcai.2018/117', '10.1109/TIP.2015.2442920', '10.1145/2713168.2713194', '10.1109/TPAMI.2018.2849989', '10.1109/CVPR.2017.437', '10.1109/ICIP.2012.6467022', '10.1109/CVPR.2018.00853', '10.1515/9783110524116', '10.1109/ICCV.2017.244', '10.1016/j.patcog.2016.06.008', '10.1016/S0734-189X(87)80186-X', '10.1109/CVPR.2016.182', '10.1109/CVPR.2018.00660', '10.1109/CVPRW.2018.00113', '10.1109/CVPR.2018.00577', '10.1109/CVPR.2018.00263', '10.1007/978-3-030-58555-6_36', '10.1609/aaai.v32i1.12317', '10.1038/scientificamerican1277-108', '10.1109/ICCV.2017.304', '10.1145/3072959.3073592', '10.1109/TIP.2018.2810539', '10.1109/TIP.2018.2794218', '10.1109/CVPR.2016.304', '10.1109/ISCAS.2018.8351427', '10.1109/TIP.2016.2639450', '10.1109/CVPR.2017.19', '10.1109/ICCV.2019.00332', '10.1109/ICCV.2019.00454'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Robust image sentiment analysis using progressively trained and domain transferred deep networks': Paper(DOI='10.1609/aaai.v29i1.9179', crossref_json=None, google_schorlar_metadata=None, title='Robust image sentiment analysis using progressively trained and domain transferred deep networks', authors=['Quanzeng You', 'Jiebo Luo', 'Hailin Jin', 'Jianchao Yang'], abstract='Sentiment analysis of online user generated content is important for many social media analytics tasks. Researchers have largely relied on textual sentiment analysis to develop systems to predict political elections, measure economic indicators, and so on. Recently, social media users are increasingly using images and videos to express their opinions and share their experiences. Sentiment analysis of such large scale visual content can help better extract user sentiments toward events or topics, such as those in image tweets, so that prediction of sentiment from visual content is complementary to textual sentiment analysis. Motivated by the needs in leveraging large scale yet noisy training data to solve the extremely challenging problem of image sentiment analysis, we employ Convolutional Neural Networks (CNN). We first design a suitable CNN architecture for image sentiment analysis. We obtain half a million training samples by using a baseline sentiment algorithm to label Flickr images. To make use of such noisy machine labeled data, we employ a progressive strategy to fine-tune the deep network. Furthermore, we improve the performance on Twitter images by inducing domain transfer with a small number of manually labeled Twitter images. We have conducted extensive experiments on manually labeled Twitter images. The results show that the proposed CNN can achieve better performance in image sentiment analysis than competing algorithms.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Deep Learning'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Youtube-vos: Sequence-to-sequence video object segmentation': Paper(DOI='10.1007/978-3-030-01228-1_36', crossref_json=None, google_schorlar_metadata=None, title='Youtube-vos: Sequence-to-sequence video object segmentation', authors=['Ning Xu', 'Linjie Yang', 'Yuchen Fan', 'Jianchao Yang', 'Dingcheng Yue', 'Yuchen Liang', 'Brian Price', 'Scott Cohen', 'Thomas Huang'], abstract='Learning long-term spatial-temporal features are critical for many video analysis tasks. However, existing video segmentation methods predominantly rely on static image segmentation techniques, and methods capturing temporal dependency for segmentation have to depend on pretrained optical flow models, leading to suboptimal solutions for the problem. End-to-end sequential learning to explore spatial-temporal features for video segmentation is largely limited by the scale of available video segmentation datasets, ie, even the largest video segmentation dataset only contains 90 short video clips. To solve this problem, we build a new large-scale video object segmentation dataset called YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains 3,252 YouTube video clips and 78 categories including common objects and human activities. This is by far the largest video object segmentation dataset to our knowledge and we have released it at https://youtube-vos. org. Based on this dataset, we propose a novel sequence-to-sequence network to fully exploit long-term spatial-temporal information in videos for segmentation. We demonstrate that our method is able to achieve the best results on our YouTube-VOS test set and comparable results on DAVIS 2016 compared to the current state-of-the-art methods. Experiments show that the large scale dataset is indeed a key factor to the success of our model.', conference=None, journal=None, year=None, reference_list=['10.1109/72.279181', '10.1007/978-3-642-15555-0_21', '10.1109/CVPR.2017.565', '10.1109/TPAMI.2017.2699184', '10.1109/ICCV.2017.81', '10.3115/v1/D14-1179', '10.21236/ADA623249', '10.1109/CVPR.2017.228', '10.5244/C.28.21', '10.1145/2816795.2818105', '10.1109/ICCV.2013.438', '10.1109/CVPR.2015.7298698', '10.1162/neco.1997.9.8.1735', '10.1109/CVPR.2017.179', '10.1007/978-3-319-10593-2_43', '10.1109/CVPR.2017.336', '10.1109/ICCV.2013.273', '10.1109/CVPR.2016.87', '10.1109/ICCV.2015.370', '10.1109/TPAMI.2013.242', '10.1109/ICCV.2013.223', '10.1109/CVPR.2017.372', '10.1109/CVPR.2016.85', '10.1109/CVPR.2015.7298720', '10.1109/TPAMI.2016.2572683', '10.1109/CVPR.2017.64', '10.1109/ICCV.2017.480', '10.1109/ICCV.2015.510', '10.1109/CVPR.2016.423', '10.5244/C.31.116', '10.1162/neco.1989.1.2.270', '10.1109/CVPR.2017.376', '10.5244/C.31.182', '10.1109/CVPR.2016.47', '10.1109/CVPR.2018.00680'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Youtube-vos: A large-scale video object segmentation benchmark': Paper(DOI='10.1007/978-3-030-58555-6_13', crossref_json=None, google_schorlar_metadata=None, title='Youtube-vos: A large-scale video object segmentation benchmark', authors=['Ning Xu', 'Linjie Yang', 'Yuchen Fan', 'Dingcheng Yue', 'Yuchen Liang', 'Jianchao Yang', 'Thomas Huang'], abstract='Learning long-term spatial-temporal features are critical for many video analysis tasks. However, existing video segmentation methods predominantly rely on static image segmentation techniques, and methods capturing temporal dependency for segmentation have to depend on pretrained optical flow models, leading to suboptimal solutions for the problem. End-to-end sequential learning to explore spatialtemporal features for video segmentation is largely limited by the scale of available video segmentation datasets, i.e., even the largest video segmentation dataset only contains 90 short video clips. To solve this problem, we build a new large-scale video object segmentation dataset called YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains 4,453 YouTube video clips and 94 object categories. This is by far the largest video object segmentation dataset to our knowledge and has been released at http://youtube-vos.org. We further evaluate several existing state-of-the-art video object segmentation algorithms on this dataset which aims to establish baselines for the development of new algorithms in the future.', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2017.565', '10.1109/CVPR.2018.00130', '10.1109/CVPR.2019.00210', '10.1109/CVPR.2018.00624', '10.1007/978-3-319-46448-0_7', '10.1109/ICCV.2013.396', '10.1007/978-3-030-11018-5_2', '10.1109/CVPR.2019.00656', '10.1109/CVPR.2018.00602', '10.1007/978-3-030-01219-9_13', '10.1007/978-3-030-01219-9_6', '10.1109/CVPR.2017.777', '10.1109/ICCV.2017.143', '10.1109/CVPR.2018.00071', '10.1007/978-3-030-01252-6_39', '10.1109/CVPR.2019.00675', '10.1007/978-3-319-46493-0_48', '10.1109/CVPR.2019.00539', '10.1109/ICCV.2019.00932', '10.1109/CVPR.2019.00091', '10.1109/CVPR.2017.372', '10.1109/CVPR.2016.85', '10.1109/ICCV.2017.480', '10.1109/CVPR.2019.00971', '10.1109/ICCV.2019.00404', '10.1109/CVPR.2019.00318', '10.1109/CVPR.2018.00770', '10.1109/CVPR.2015.7298839', '10.1007/978-3-030-01228-1_36', '10.1109/CVPR.2018.00680', '10.1109/CVPR.2019.01075', '10.1109/CVPR.2018.00142', '10.1609/aaai.v34i07.7008'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'image processing', 'machine learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Multimodal machine learning: A survey and taxonomy': Paper(DOI='10.1109/tpami.2018.2798607', crossref_json=None, google_schorlar_metadata=None, title='Multimodal machine learning: A survey and taxonomy', authors=['Tadas Baltrušaitis', 'Chaitanya Ahuja', 'Louis-Philippe Morency'], abstract='Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/JPROC.2003.817150', '10.1109/TPAMI.2007.1124', '10.1145/1873951.1873987', '10.1007/978-3-642-24571-8_51', '10.1109/ICCV.2015.303', '10.18653/v1/D15-1303', '10.1145/2808196.2811638', '10.1145/2663204.2666277', '10.1145/2939672.2939812', '10.1109/TPAMI.2013.212', '10.1613/jair.4135', '10.3115/v1/P15-1006', '10.1109/ICASSP.2016.7472621', '10.1007/978-3-540-85099-1_8', '10.1007/978-3-319-24947-6_17', '10.1109/ICME.2007.4284731', '10.3115/1073336.1073359', '10.1109/TMM.2007.906583', '10.1007/s11263-016-0987-1', '10.1145/2522848.2531741', '10.18653/v1/N16-1020', '10.1109/ICCV.2011.6126545', '10.1007/978-3-642-24571-8_53', '10.1109/CVPR.2010.5539928', '10.1145/258734.258880', '10.1162/tacl_a_00207', '10.1613/jair.4900', '10.1145/279943.279962', '10.1145/1866029.1866080', '10.1109/ICCV.2015.507', '10.1109/CVPR.1997.609450', '10.3115/v1/W14-3348', '10.3115/v1/P15-2017', '10.1613/jair.3540', '10.1109/TPAMI.2015.2461544', '10.1109/ICASSP.2015.7178347', '10.3115/v1/N15-1017', '10.1007/978-3-540-74048-3_4', '10.1007/978-3-662-44415-3_16', '10.1109/CVPR.2016.213', '10.1007/978-3-642-15561-1_2', '10.1109/CVPR.2009.5206772', '10.1109/TMM.2013.2267205', '10.3115/v1/P14-2074', '10.1145/2682899', '10.1109/CVPR.2016.497', '10.1109/CVPR.2016.264', '10.1109/CVPR.2014.299', '10.1109/TPAMI.2011.47', '10.1007/s10462-012-9368-5', '10.18653/v1/D16-1203', '10.1109/CVPR.2016.12', '10.1109/CVPR.2013.434', '10.1145/2993148.2993176', '10.1007/978-3-642-10331-5_9', '10.1109/ICCV.2015.279', '10.1109/T-AFFC.2011.9', '10.1109/ICASSP.1994.389596', '10.1007/978-0-85729-997-0_19', '10.1145/383259.383316', '10.1145/1180995.1181013', '10.21236/ADA307097', '10.1109/ICCV.2013.337', '10.1109/ICASSP.2013.6638947', '10.1145/1452392.1452442', '10.1162/0899766042321814', '10.1016/j.neucom.2014.12.020', '10.1145/2647868.2654902', '10.18653/v1/D16-1044', '10.1109/JPROC.2003.817119', '10.1109/TPAMI.2017.2648793', '10.1109/ICCV.2009.5459169', '10.1109/CVPR.2010.5540112', '10.1162/tacl_a_00177', '10.1023/B:MTAP.0000046380.27575.a5', '10.1109/ICCV.2015.277', '10.1016/j.inffus.2013.12.002', '10.3115/v1/P14-1068', '10.1109/ICASSP.1996.541110', '10.18653/v1/N16-1147', '10.1145/2911996.2912043', '10.1080/00401706.1991.10484833', '10.1109/CVPR.2017.348', '10.1016/j.patrec.2014.08.005', '10.1109/MSP.2012.2205597', '10.1109/CVPR.2016.8', '10.1162/neco.2006.18.7.1527', '10.1109/ICME.2007.4284627', '10.1109/ICASSP.2013.6639140', '10.1162/neco.1997.9.8.1735', '10.1613/jair.3994', '10.1093/biomet/28.3-4.321', '10.1109/CVPR.2016.493', '10.1109/T-AFFC.2011.37', '10.1007/s12193-015-0195-2', '10.1145/2388676.2388684', '10.1016/j.neuroimage.2014.06.077', '10.1109/CVPR.2015.7298792', '10.1007/s13735-014-0065-9', '10.1109/CVPR.2016.552', '10.1109/ICASSP.2016.7472669', '10.1145/2512530.2512533', '10.1109/CVPR.2015.7299087', '10.3115/v1/N15-1173', '10.18653/v1/D16-1204', '10.1007/s10994-010-5198-3', '10.1109/CVPR.2016.541', '10.3115/993268.993313', '10.1109/CVPR.2015.7298935', '10.1016/j.neucom.2014.08.003', '10.1109/ICCV.2003.1238406', '10.3115/1073445.1073465', '10.1117/12.333848', '10.1109/JBHI.2013.2285378', '10.1109/CVPR.2016.333', '10.1111/j.1756-8765.2010.01106.x', '10.1023/B:VISI.0000029664.99615.94', '10.1145/2647868.2654969', '10.1016/j.imavis.2012.03.001', '10.1145/2647868.2654931', '10.1007/11608288_66', '10.1109/ICCV.2015.9', '10.1109/CVPR.2016.9', '10.3115/v1/P14-2097', '10.1109/ICASSP.1998.679698', '10.1038/264746a0', '10.1109/ICME.2010.5583006', '10.1109/CVPR.2016.10', '10.1109/JPROC.2010.2050411', '10.1109/ICCV.2015.512', '10.3115/v1/P15-2018', '10.1162/tacl_a_00166', '10.1109/TMM.2012.2188783', '10.1109/CVPR.2016.496', '10.1109/ICASSP.2013.6638346', '10.18653/v1/D15-1293', '10.3115/v1/P15-2038', '10.3115/v1/D14-1005', '10.1109/CVPR.2015.7298932', '10.1023/A:1020346032608', '10.1007/978-3-319-46475-6_5', '10.1109/35.41402', '10.1109/TPAMI.2008.52', '10.1109/TASL.2012.2187195', '10.1016/j.specom.2009.04.004', '10.18653/v1/P16-1169', '10.1109/ICASSP.2013.6639047', '10.1007/s00530-010-0182-0', '10.1111/lnc3.12170', '10.1109/TPAMI.2012.162', '10.1146/annurev.psych.59.103006.093639', '10.1109/TPAMI.2013.50', '10.1109/CVPR.2014.455', '10.1137/1025045', '10.1023/B:MACH.0000035472.73496.0c', '10.1142/S012906570000034X', '10.1007/s11042-013-1391-2', '10.3115/v1/P14-1132', '10.1109/ICCV.2015.11', '10.1109/CVPR.2013.387'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimodal Interaction', 'Machine Learning', 'Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'It’s only a computer: Virtual humans increase willingness to disclose': Paper(DOI='10.1016/j.chb.2014.04.043', crossref_json=None, google_schorlar_metadata=None, title='It’s only a computer: Virtual humans increase willingness to disclose', authors=['Gale M Lucas', 'Jonathan Gratch', 'Aisha King', 'Louis-Philippe Morency'], abstract='Research has begun to explore the use of virtual humans (VHs) in clinical interviews (Bickmore, Gruber, & Picard, 2005). When designed as supportive and “safe” interaction partners, VHs may improve such screenings by increasing willingness to disclose information (Gratch, Wang, Gerten, & Fast, 2007). In health and mental health contexts, patients are often reluctant to respond honestly. In the context of health-screening interviews, we report a study in which participants interacted with a VH interviewer and were led to believe that the VH was controlled by either humans or automation. As predicted, compared to those who believed they were interacting with a human operator, participants who believed they were interacting with a computer reported lower fear of self-disclosure, lower impression management, displayed their sadness more intensely, and were rated by observers as more willing to disclose. These\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1177/089443939201000202', '10.4304/jmm.1.6.22-35', '10.1177/075910639504800111', '10.7326/0003-4819-101-5-692', '10.1016/j.pec.2004.09.008', '10.1016/j.janxdis.2011.04.002', '10.1177/0049124187016002006', '10.1109/34.799905', '10.1007/978-3-540-74997-4_12', '10.1176/ajp.130.12.1327', '10.1001/archfami.3.10.908', '10.1002/ejsp.36', '10.1037/0033-2909.107.1.34', '10.1016/0959-8049(95)00527-7', '10.1080/10417949109372824', '10.1037/0022-3514.44.6.1234', '10.1007/BF02596168', '10.1177/0049124100028004005'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimodal Interaction', 'Machine Learning', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'SimSensei Kiosk: A virtual human interviewer for healthcare decision support': Paper(DOI='10.1609/aaai.v29i1.9777', crossref_json=None, google_schorlar_metadata=None, title='SimSensei Kiosk: A virtual human interviewer for healthcare decision support', authors=['David DeVault', 'Ron Artstein', 'Grace Benn', 'Teresa Dey', 'Ed Fast', 'Alesia Gainer', 'Kallirroi Georgila', 'Jon Gratch', 'Arno Hartholt', 'Margaux Lhommet', 'Gale Lucas', 'Stacy Marsella', 'Fabrizio Morbini', 'Angela Nazarian', 'Stefan Scherer', 'Giota Stratou', 'Apar Suri', 'David Traum', 'Rachel Wood', 'Yuyu Xu', 'Albert Rizzo', 'Louis-Philippe Morency'], abstract='We present SimSensei Kiosk, an implemented virtual human interviewer designed to create an engaging face-to-face interaction where the user feels comfortable talking and sharing information. SimSensei Kiosk is also designed to create interactional situations favorable to the automatic assessment of distress indicators, defined as verbal and nonverbal behaviors correlated with depression, anxiety or post-traumatic stress disorder (PTSD). In this paper, we summarize the design methodology, performed over the past two years, which is based on three main development cycles:(1) analysis of face-to-face human interactions to identify potential distress indicators, dialogue policies and virtual human gestures,(2) development and analysis of a Wizard-of-Oz prototype system where two human operators were deciding the spoken and gestural responses, and (3) development of a fully automatic virtual interviewer able to engage users in 15-25 minute interactions. We show the potential of our fully automatic virtual human interviewer in a user study, and situate its performance in relation to the Wizard-of-Oz prototype.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimodal Interaction', 'Machine Learning', 'Computer Vision'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Memory fusion network for multi-view sequential learning': Paper(DOI='10.1609/aaai.v32i1.12021', crossref_json=None, google_schorlar_metadata=None, title='Memory fusion network for multi-view sequential learning', authors=['Amir Zadeh', 'Paul Pu Liang', 'Navonil Mazumder', 'Soujanya Poria', 'Erik Cambria', 'Louis-Philippe Morency'], abstract='Multi-view sequential learning is a fundamental problem in machine learning dealing with multi-view sequences. In a multi-view sequence, there exists two forms of interactions between different views: view-specific interactions and cross-view interactions. In this paper, we present a new neural architecture for multi-view sequential learning called the Memory Fusion Network (MFN) that explicitly accounts for both interactions in a neural architecture and continuously models them through time. The first component of the MFN is called the System of LSTMs, where view-specific interactions are learned in isolation through assigning an LSTM function to each view. The cross-view interactions are then identified using a special attention mechanism called the Delta-memory Attention Network (DMAN) and summarized through time with a Multi-view Gated Memory. Through extensive experimentation, MFN is compared to various proposed approaches for multi-view sequential learning on multiple publicly available benchmark datasets. MFN outperforms all the multi-view approaches. Furthermore, MFN outperforms all current state-of-the-art models, setting new state-of-the-art results for all three multi-view datasets.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimodal Interaction', 'Machine Learning', 'Computer Vision'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Beyond the imitation game: Quantifying and extrapolating the capabilities of language models': Paper(DOI='10.1016/s0262-4079(14)62389-7', crossref_json=None, google_schorlar_metadata=None, title='Beyond the imitation game: Quantifying and extrapolating the capabilities of language models', authors=['Aarohi Srivastava', 'Abhinav Rastogi', 'Abhishek Rao', 'Abu Awal Md Shoeb', 'Abubakar Abid', 'Adam Fisch', 'Adam R Brown', 'Adam Santoro', 'Aditya Gupta', 'Adrià Garriga-Alonso', 'Agnieszka Kluska', 'Aitor Lewkowycz', 'Akshat Agarwal', 'Alethea Power', 'Alex Ray', 'Alex Warstadt', 'Alexander W Kocurek', 'Ali Safaya', 'Ali Tazarv', 'Alice Xiang', 'Alicia Parrish', 'Allen Nie', 'Aman Hussain', 'Amanda Askell', 'Amanda Dsouza', 'Ambrose Slone', 'Ameet Rahane', 'Anantharaman S Iyer', 'Anders Andreassen', 'Andrea Madotto', 'Andrea Santilli', 'Andreas Stuhlmüller', 'Andrew Dai', 'Andrew La', 'Andrew Lampinen', 'Andy Zou', 'Angela Jiang', 'Angelica Chen', 'Anh Vuong', 'Animesh Gupta', 'Anna Gottardi', 'Antonio Norelli', 'Anu Venkatesh', 'Arash Gholamidavoodi', 'Arfa Tabassum', 'Arul Menezes', 'Arun Kirubarajan', 'Asher Mullokandov', 'Ashish Sabharwal', 'Austin Herrick', 'Avia Efrat', 'Aykut Erdem', 'Ayla Karakaş', 'B Ryan Roberts', 'Bao Sheng Loe', 'Barret Zoph', 'Bartłomiej Bojanowski', 'Batuhan Özyurt', 'Behnam Hedayatnia', 'Behnam Neyshabur', 'Benjamin Inden', 'Benno Stein', 'Berk Ekmekci', 'Bill Yuchen Lin', 'Blake Howald', 'Bryan Orinion', 'Cameron Diao', 'Cameron Dour', 'Catherine Stinson', 'Cedrick Argueta', 'César Ferri Ramírez', 'Chandan Singh', 'Charles Rathkopf', 'Chenlin Meng', 'Chitta Baral', 'Chiyu Wu', 'Chris Callison-Burch', 'Chris Waites', 'Christian Voigt', 'Christopher D Manning', 'Christopher Potts', 'Cindy Ramirez', 'Clara E Rivera', 'Clemencia Siro', 'Colin Raffel', 'Courtney Ashcraft', 'Cristina Garbacea', 'Damien Sileo', 'Dan Garrette', 'Dan Hendrycks', 'Dan Kilman', 'Dan Roth', 'Daniel Freeman', 'Daniel Khashabi', 'Daniel Levy', 'Daniel Moseguí González', 'Danielle Perszyk', 'Danny Hernandez', 'Danqi Chen', 'Daphne Ippolito', 'Dar Gilboa', 'David Dohan', 'David Drakard', 'David Jurgens', 'Debajyoti Datta', 'Deep Ganguli', 'Denis Emelin', 'Denis Kleyko', 'Deniz Yuret', 'Derek Chen', 'Derek Tam', 'Dieuwke Hupkes', 'Diganta Misra', 'Dilyar Buzan', 'Dimitri Coelho Mollo', 'Diyi Yang', 'Dong-Ho Lee', 'Dylan Schrader', 'Ekaterina Shutova', 'Ekin Dogus Cubuk', 'Elad Segal', 'Eleanor Hagerman', 'Elizabeth Barnes', 'Elizabeth Donoway', 'Ellie Pavlick', 'Emanuele Rodola', 'Emma Lam', 'Eric Chu', 'Eric Tang', 'Erkut Erdem', 'Ernie Chang', 'Ethan A Chi', 'Ethan Dyer', 'Ethan Jerzak', 'Ethan Kim', 'Eunice Engefu Manyasi', 'Evgenii Zheltonozhskii', 'Fanyue Xia', 'Fatemeh Siar', 'Fernando Martínez-Plumed', 'Francesca Happé', 'Francois Chollet', 'Frieda Rong', 'Gaurav Mishra', 'Genta Indra Winata', 'Gerard de Melo', 'Germán Kruszewski', 'Giambattista Parascandolo', 'Giorgio Mariani', 'Gloria Wang'], abstract='Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI\\'s GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit \"breakthrough\" behavior at\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Natural Language Processing', 'Computer Vision', 'Machine Learning', 'Multimodal AI'], conference_acronym='New scientist (1971)', publisher=None, query_handler=None),\n",
       " 'Youtube movie reviews: Sentiment analysis in an audio-visual context': Paper(DOI='10.1109/mis.2013.34', crossref_json=None, google_schorlar_metadata=None, title='Youtube movie reviews: Sentiment analysis in an audio-visual context', authors=['Martin Wöllmer', 'Felix Weninger', 'Tobias Knaup', 'Björn Schuller', 'Congkai Sun', 'Kenji Sagae', 'Louis-Philippe Morency'], abstract=\"This work focuses on automatically analyzing a speaker's sentiment in online videos containing movie reviews. In addition to textual information, this approach considers adding audio features as typically used in speech-based emotion recognition as well as video features encoding valuable valence information conveyed by the speaker. Experimental results indicate that training on written movie reviews is a promising alternative to exclusively using (spoken) in-domain data for building a system that analyzes spoken movie review videos, and that language-independent audio-visual analysis can compete with linguistic analysis.\", conference=None, journal=None, year=None, reference_list=['10.1007/s11042-011-0815-0', '10.1016/j.imavis.2012.03.001', '10.3115/1218955.1218990', '10.1145/1873951.1874246', '10.1109/CVPR.2011.5995733', '10.1109/AFGR.2008.4813429'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimodal Interaction', 'Machine Learning', 'Computer Vision'], conference_acronym='IEEE intelligent systems', publisher=None, query_handler=None),\n",
       " 'Multi-attention recurrent network for human communication comprehension': Paper(DOI='10.1609/aaai.v32i1.12024', crossref_json=None, google_schorlar_metadata=None, title='Multi-attention recurrent network for human communication comprehension', authors=['Amir Zadeh', 'Paul Pu Liang', 'Soujanya Poria', 'Prateek Vij', 'Erik Cambria', 'Louis-Philippe Morency'], abstract='Human face-to-face communication is a complex multimodal signal. We use words (language modality), gestures (vision modality) and changes in tone (acoustic modality) to convey our intentions. Humans easily process and understand face-to-face communication, however, comprehending this form of communication remains a significant challenge for Artificial Intelligence (AI). AI must understand each modality and the interactions between them that shape the communication. In this paper, we present a novel neural architecture for understanding human communication called the Multi-attention Recurrent Network (MARN). The main strength of our model comes from discovering interactions between modalities through time using a neural component called the Multi-attention Block (MAB) and storing them in the hybrid memory of a recurrent component called the Long-short Term Hybrid Memory (LSTHM). We perform extensive comparisons on six publicly available datasets for multimodal sentiment analysis, speaker trait recognition and emotion recognition. MARN shows state-of-the-art results performance in all the datasets.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimodal Interaction', 'Machine Learning', 'Computer Vision'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Planck 2015 results. XIII. Cosmological parameters': Paper(DOI='10.1063/1.4953315', crossref_json=None, google_schorlar_metadata=None, title='Planck 2015 results. XIII. Cosmological parameters', authors=['Peter AR Ade', 'Nabila Aghanim', 'M Arnaud', 'Mark Ashdown', 'J Aumont', 'Carlo Baccigalupi', 'AJ Banday', 'RB Barreiro', 'JG Bartlett', 'Nicola Bartolo', 'E Battaner', 'R Battye', 'K Benabed', 'A Benoît', 'A Benoit-Lévy', 'J-P Bernard', 'M Bersanelli', 'P Bielewicz', 'JJ Bock', 'A Bonaldi', 'L Bonavera', 'JR Bond', 'J Borrill', 'FR Bouchet', 'F Boulanger', 'M Bucher', 'C Burigana', 'RC Butler', 'E Calabrese', 'J-F Cardoso', 'A Catalano', 'A Challinor', 'A Chamballu', 'R-R Chary', 'HC Chiang', 'Jens Chluba', 'PR Christensen', 'S Church', 'DL Clements', 'S Colombi', 'LPL Colombo', 'C Combet', 'A Coulais', 'BP Crill', 'A Curto', 'F Cuttaia', 'L Danese', 'RD Davies', 'RJ Davis', 'Paolo de Bernardis', 'A De Rosa', 'Giulia De Zotti', 'J Delabrouille', 'F-X Désert', 'E Di Valentino', 'C Dickinson', 'JM Diego', 'K Dolag', 'H Dole', 'SIMONA Donzelli', 'O Doré', 'M Douspis', 'A Ducout', 'Jo Dunkley', 'X Dupac', 'G Efstathiou', 'F Elsner', 'TA Enßlin', 'HK Eriksen', 'M Farhang', 'J Fergusson', 'F Finelli', 'O Forni', 'M Frailis', 'AA Fraisse', 'E Franceschi', 'A Frejsel', 'S Galeotta', 'S Galli', 'K Ganga', 'C Gauthier', 'Martina Gerbino', 'Tushar Ghosh', 'M Giard', 'Y Giraud-Héraud', 'E Giusarma', 'E Gjerlow', 'J González-Nuevo', 'KM Górski', 'S Gratton', 'Anna Gregorio', 'A Gruppuso', 'JE Gudmundsson', 'J Hamann', 'Frode Kristian Hansen', 'D Hanson', 'DL Harrison', 'G Helou', 'S Henrot-Versillé', 'C Hernández-Monteagudo', 'D Herranz', 'SR Hildebrandt', 'E Hivon', 'M Hobson', 'WA Holmes', 'Allan Hornstrup', 'W Hovest', 'Z Huang', 'KM Huffenberger', 'G Hurier', 'AH Jaffe', 'TR Jaffe', 'WC Jones', 'M Juvela', 'E Keihanen', 'R Keskitalo', 'TS Kisner', 'R Kneissl', 'J Knoche', 'L Knox', 'M Kunz', 'H Kurki-Suonio', 'G Lagache', 'A Lahteenmaki', 'J-M Lamarre', 'A Lasenby', 'M Lattanzi', 'CR Lawrence', 'JP Leahy', 'R Leonardi', 'J Lesgourgues', 'F Levrier', 'A Lewis', 'Michele Liguori', 'Per Barth Lilje', 'M Linden-Vornle', 'M López-Caniego', 'PM Lubin', 'JF Macías-Pérez', 'G Maggio', 'D Maino', 'N Mandolesi', 'A Mangilli', 'A Marchini', 'PG Martin', 'M Martinelli', 'E Martínez-González', 'S Masi', 'S Matarrese', 'P Mazzotta'], abstract='We present results based on full-mission Planck observations of temperature and polarization anisotropies of the CMB. These data are consistent with the six-parameter inflationary LCDM cosmology. From the Planck temperature and lensing data, for this cosmology we find a Hubble constant, H0= (67.8 +/- 0.9) km/s/Mpc, a matter density parameter Omega_m = 0.308 +/- 0.012 and a scalar spectral index with n_s = 0.968 +/- 0.006. (We quote 68% errors on measured parameters and 95% limits on other parameters.) Combined with Planck temperature and lensing data, Planck LFI polarization measurements lead to a reionization optical depth of tau = 0.066 +/- 0.016. Combining Planck with other astrophysical data we find N_ eff = 3.15 +/- 0.23 for the effective number of relativistic degrees of freedom and the sum of neutrino masses is constrained to < 0.23 eV. Spatial curvature is found to be |Omega_K| < 0.005. For LCDM we find a limit on the tensor-to-scalar ratio of r <0.11 consistent with the B-mode constraints from an analysis of BICEP2, Keck Array, and Planck (BKP) data. Adding the BKP data leads to a tighter constraint of r < 0.09. We find no evidence for isocurvature perturbations or cosmic defects. The equation of state of dark energy is constrained to w = -1.006 +/- 0.045. Standard big bang nucleosynthesis predictions for the Planck LCDM cosmology are in excellent agreement with observations. We investigate annihilating dark matter and deviations from standard recombination, finding no evidence for new physics. The Planck results for base LCDM are in agreement with BAO data and with the JLA SNe sample. However the amplitude of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1093/mnras/258.1.37P', '10.1086/186504', '10.1088/0067-0049/208/2/20', '10.1086/466512', '10.1111/j.1365-2966.2011.19250.x', '10.1093/mnras/stu778', '10.1093/mnras/stu523', '10.1093/mnras/stu278', '10.1103/PhysRevLett.114.101301'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym='AIP conference proceedings', publisher=None, query_handler=None),\n",
       " 'Planck 2013 results. I. Overview of products and scientific results': Paper(DOI='10.1063/pt.4.2445', crossref_json=None, google_schorlar_metadata=None, title='Planck 2013 results. I. Overview of products and scientific results', authors=['Peter AR Ade', 'Nabila Aghanim', 'MIR Alves', 'Charmaine Armitage-Caplan', 'M Arnaud', 'M Ashdown', 'F Atrio-Barandela', 'J Aumont', 'H Aussel', 'C Baccigalupi', 'AJ Banday', 'RB Barreiro', 'R Barrena', 'M Bartelmann', 'JG Bartlett', 'Nicola Bartolo', 'S Basak', 'E Battaner', 'R Battye', 'K Benabed', 'A Benoît', 'A Benoit-Lévy', 'J-P Bernard', 'M Bersanelli', 'B Bertincourt', 'M Bethermin', 'P Bielewicz', 'I Bikmaev', 'A Blanchard', 'J Bobin', 'JJ Bock', 'H Böhringer', 'ANNA Bonaldi', 'L Bonavera', 'JR Bond', 'J Borrill', 'FR Bouchet', 'F Boulanger', 'H Bourdin', 'JW Bowyer', 'M Bridges', 'ML Brown', 'M Bucher', 'R Burenin', 'CARLO Burigana', 'RC Butler', 'Erminia Calabrese', 'B Cappellini', 'J-F Cardoso', 'R Carr', 'P Carvalho', 'M Casale', 'G Castex', 'A Catalano', 'A Challinor', 'A Chamballu', 'R-R Chary', 'X Chen', 'HC Chiang', 'L-Y Chiang', 'G Chon', 'PR Christensen', 'E Churazov', 'S Church', 'M Clemens', 'DL Clements', 'S Colombi', 'LPL Colombo', 'C Combet', 'B Comis', 'F Couchot', 'A Coulais', 'BP Crill', 'M Cruz', 'A Curto', 'FRANCESCO Cuttaia', 'A Da Silva', 'Håkon Dahle', 'L Danese', 'RD Davies', 'RJ Davis', 'P De Bernardis', 'ALESSANDRA de Rosa', 'G De Zotti', 'T Déchelette', 'J Delabrouille', 'J-M Delouis', 'J Démoclès', 'F-X Désert', 'J Dick', 'C Dickinson', 'JM Diego', 'K Dolag', 'H Dole', 'S Donzelli', 'O Doré', 'M Douspis', 'A Ducout', 'J Dunkley', 'X Dupac', 'G Efstathiou', 'F Elsner', 'TA Enßlin', 'HK Eriksen', 'O Fabre', 'E Falgarone', 'MC Falvella', 'Y Fantaye', 'J Fergusson', 'C Filliard', 'FABIO Finelli', 'I Flores-Cacho', 'S Foley', 'O Forni', 'P Fosalba', 'Marco Frailis', 'AA Fraisse', 'ENRICO Franceschi', 'M Freschi', 'S Fromenteau', 'M Frommert', 'TC Gaier', 'Samuele Galeotta', 'J Gallegos', 'S Galli', 'B Gandolfo', 'K Ganga', 'C Gauthier', 'RT Génova-Santos', 'T Ghosh', 'M Giard', 'G Giardino', 'M Gilfanov', 'D Girard', 'Y Giraud-Héraud', 'Eirik Gjerløw', 'J González-Nuevo', 'KM Górski', 'S Gratton', 'A Gregorio', 'ALESSANDRO Gruppuso', 'JE Gudmundsson', 'J Haissinski', 'J Hamann', 'Frode Kristian Hansen', 'M Hansen', 'D Hanson', 'DL Harrison', 'A Heavens', 'G Helou'], abstract=\"The ESA's Planck satellite, dedicated to studying the early Universe and its subsequent evolution, was launched 14 May 2009 and has been scanning the microwave and submillimetre sky continuously since 12 August 2009. This paper gives an overview of the mission and its performance, the processing, analysis, and characteristics of the data, the scientific results, and the science data products and papers in the release. The science products include maps of the CMB and diffuse extragalactic foregrounds, a catalogue of compact Galactic and extragalactic sources, and a list of sources detected through the SZ effect. The likelihood code used to assess cosmological models against the Planck data and a lensing likelihood are described. Scientific results include robust support for the standard six-parameter LCDM model of cosmology and improved measurements of its parameters, including a highly significant deviation from scale invariance of the primordial power spectrum. The Planck values for these parameters and others derived from them are significantly different from those previously determined. Several large-scale anomalies in the temperature distribution of the CMB, first detected by WMAP, are confirmed with higher confidence. Planck sets new limits on the number and mass of neutrinos, and has measured gravitational lensing of CMB anisotropies at greater than 25 sigma. Planck finds no evidence for non-Gaussianity in the CMB. Planck's results agree well with results from the measurements of baryon acoustic oscillations. Planck finds a lower Hubble constant than found in some more local measures. Some tension is also present\\xa0…\", conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym='Physics today (Online)', publisher=None, query_handler=None),\n",
       " 'Planck 2015 results. XIV. Dark energy and modified gravity': Paper(DOI='10.1103/physrevd.88.063519', crossref_json=None, google_schorlar_metadata=None, title='Planck 2015 results. XIV. Dark energy and modified gravity', authors=['Planck Collaboration'], abstract=None, conference=None, journal=None, year=None, reference_list=['10.1088/0004-637X/743/1/28', '10.1103/PhysRevLett.107.021301', '10.1103/PhysRevD.74.103510', '10.1103/PhysRevD.86.103507', '10.1111/j.1365-2966.2005.09318.x', '10.1086/466512', '10.1088/0004-637X/730/2/119', '10.1016/0550-3213(88)90193-9', '10.1103/PhysRevD.37.3406', '10.1016/j.physletb.2004.05.008', '10.1088/1475-7516/2006/06/026', '10.1103/PhysRevD.83.023011', '10.1088/2041-8205/749/1/L9', '10.1103/PhysRevD.87.083009', '10.1103/PhysRevD.62.043511', '10.1103/PhysRevD.77.103003', '10.1088/1475-7516/2004/10/005', '10.1103/PhysRevD.72.065024', '10.1103/PhysRevD.78.023015', '10.1016/j.physletb.2007.08.060', '10.1103/PhysRevD.81.063525', '10.1103/PhysRevD.82.123001', '10.1103/PhysRevD.69.044026', '10.1103/PhysRevD.80.104002', '10.1103/PhysRevLett.109.041301', '10.1103/PhysRevD.85.123006', '10.1103/PhysRevD.84.103521', '10.1103/PhysRevD.85.103008', '10.1143/PTPS.78.1', '10.1103/PhysRevD.69.103524', '10.1086/368064', '10.1103/PhysRevD.68.023514', '10.1103/PhysRevD.82.103516', '10.1103/PhysRevD.74.043504', '10.1103/PhysRevD.82.023528', '10.1111/j.1745-3933.2010.00975.x', '10.1111/j.1365-2966.2009.15987.x', '10.1111/j.1365-2966.2010.17758.x', '10.1103/PhysRevD.78.123514', '10.1088/1475-7516/2009/04/007', '10.1016/j.newast.2010.02.003', '10.1088/0004-637X/744/1/3', '10.1103/PhysRevD.84.023504', '10.1103/PhysRevD.80.103514', '10.1103/PhysRevD.81.103534', '10.1088/1475-7516/2013/07/042', '10.1088/0067-0049/192/2/18', '10.1103/PhysRevD.77.103003', '10.1086/309179', '10.1103/PhysRevD.62.043511', '10.1103/PhysRevD.66.103511', '10.1088/0004-637X/755/1/70', '10.1103/PhysRevLett.98.021101', '10.1088/0954-3899/37/7A/075021', '10.1103/PhysRevD.88.023531'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym='Physical review. D, Particles, fields, gravitation, and cosmology', publisher=None, query_handler=None),\n",
       " 'Indications of a Late-Time Interaction in the Dark Sector': Paper(DOI='10.1103/physrevlett.113.181301', crossref_json=None, google_schorlar_metadata=None, title='Indications of a Late-Time Interaction in the Dark Sector', authors=['Valentina Salvatelli', 'Najla Said', 'Marco Bruni', 'Alessandro Melchiorri', 'David Wands'], abstract='We show that a general late-time interaction between cold dark matter and vacuum energy is favored by current cosmological data sets. We characterize the strength of the coupling by a dimensionless parameter q V that is free to take different values in four redshift bins from the primordial epoch up to today. This interacting scenario is in agreement with measurements of cosmic microwave background temperature anisotropies from the Planck satellite, supernovae Ia from Union 2.1 and redshift space distortions from a number of surveys, as well as with combinations of these different data sets. Our analysis of the 4-bin interaction shows that a nonzero interaction is likely at late times. We then focus on the case q V≠ 0 in a single low-redshift bin, obtaining a nested one parameter extension of the standard Λ CDM model. We study the Bayesian evidence, with respect to Λ CDM, of this late-time interaction model\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1088/0067-0049/208/2/20', '10.1088/0004-637X/730/2/119', '10.1088/0004-637X/732/2/129', '10.1088/0004-637X/692/2/1060', '10.1093/mnras/sts443', '10.1103/PhysRevLett.112.051303', '10.1103/PhysRevD.88.063515', '10.1103/PhysRevD.88.023531', '10.1007/BF02728301', '10.1016/0550-3213(87)90129-5', '10.1103/PhysRevD.46.2404', '10.1103/PhysRevD.43.1075', '10.1103/PhysRevD.43.375', '10.1088/0264-9381/29/14/145017', '10.1103/PhysRevD.87.083503', '10.1007/978-3-319-02063-1_13', '10.1016/j.physletb.2013.10.032', '10.1063/1.4891113', '10.1103/PhysRevD.90.023502', '10.1103/PhysRevD.78.063527', '10.1103/PhysRevD.66.103511', '10.1103/PhysRevD.87.103529', '10.1086/309179', '10.1088/0004-637X/746/1/85', '10.1111/j.1365-2966.2012.21136.x', '10.1111/j.1365-2966.2004.08146.x', '10.1111/j.1365-2966.2011.18903.x', '10.1111/j.1365-2966.2011.20169.x', '10.1111/j.1365-2966.2009.15812.x', '10.1093/mnras/stt2206', '10.1088/0004-637X/691/2/1058', '10.1080/00107510802066753', '10.2307/2530899', '10.1103/PhysRevLett.113.041301', '10.1103/PhysRevD.88.023513', '10.1103/PhysRevLett.112.241101', '10.1088/1475-7516/2010/03/027', '10.1103/PhysRevD.88.083520'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym='Physical review letters (Print)', publisher=None, query_handler=None),\n",
       " 'New constraints on Coupled Dark Energy from the Planck satellite experiment': Paper(DOI='10.1103/physrevd.88.023531', crossref_json=None, google_schorlar_metadata=None, title='New constraints on Coupled Dark Energy from the Planck satellite experiment', authors=['Valentina Salvatelli', 'Andrea Marchini', 'Laura Lopez-Honorez', 'Olga Mena'], abstract='We present new constraints on coupled dark energy from the recent measurements of the cosmic microwave background anisotropies from the Planck satellite mission. We found that a coupled dark energy model is fully compatible with the Planck measurements, deriving a weak bound on the dark matter–dark energy coupling parameter ξ=− 0.49− 0.31+ 0.19 at 68% CL Moreover if Planck data are fitted to a coupled dark energy scenario, the constraint on the Hubble constant is relaxed to H 0= 72.1− 2.3+ 3.2 km/s/Mpc, solving the tension with the Hubble Space Telescope (HST) value. We show that a combined PLANCK+ HST analysis provides significant evidence for coupled dark energy finding a nonzero value for the coupling parameter ξ, with− 0.90< ξ<− 0.22 at 95% CL We also consider the combined constraints from the Planck data plus the baryon acoustic oscillation measurements of the 6dF Galaxy Survey\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1088/0004-637X/730/2/119', '10.1088/0004-637X/732/2/129', '10.1103/PhysRevLett.110.241305', '10.1103/PhysRevD.71.063523', '10.1111/j.1365-2966.2009.16115.x', '10.1103/PhysRevD.81.103534', '10.1103/PhysRevD.84.023504', '10.1088/1475-7516/2010/09/029', '10.1103/RevModPhys.75.559', '10.1103/PhysRevD.75.083506', '10.1088/1475-7516/2008/07/020', '10.1088/1475-7516/2009/07/027', '10.1016/j.physletb.2008.11.062', '10.1103/PhysRevD.79.043526', '10.1103/PhysRevD.62.043511', '10.1086/381728', '10.1103/PhysRevD.77.103003', '10.1016/j.physletb.2008.11.062', '10.1088/1475-7516/2009/07/034', '10.1088/1475-7516/2010/05/E01', '10.1088/1475-7516/2010/11/044', '10.1086/309179', '10.1111/j.1365-2966.2011.19250.x', '10.1111/j.1365-2966.2012.21888.x', '10.1111/j.1365-2966.2012.22066.x', '10.1103/PhysRevD.66.103511', '10.1103/PhysRevD.87.103529', '10.1103/PhysRevD.86.103507', '10.1103/PhysRevD.85.043007', '10.1103/PhysRevD.68.063505', '10.1111/j.1365-2966.2009.16140.x', '10.1088/1475-7516/2010/10/014'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym='Physical review. D, Particles, fields, gravitation, and cosmology', publisher=None, query_handler=None),\n",
       " 'Far-infrared fine-structure line diagnostics of ultraluminous infrared galaxies': Paper(DOI='10.1088/0004-637x/776/1/38', crossref_json=None, google_schorlar_metadata=None, title='Far-infrared fine-structure line diagnostics of ultraluminous infrared galaxies', authors=['Duncan Farrah', 'Vianney Lebouteiller', 'Henrik WW Spoon', 'Jeronimo Bernard-Salas', 'Chris Pearson', 'Dimitra Rigopoulou', 'Howard A Smith', 'Eduardo Gonzalez-Alfonso', 'David L Clements', 'Andreas Efstathiou', 'Diane Cormier', 'Jose Afonso', 'Sara M Petty', 'Kathryn Harris', 'Peter Hurley', 'Colin Borys', 'Aprajita Verma', 'Asantha Cooray', 'Valentina Salvatelli'], abstract='We present Herschel observations of 6 fine-structure lines in 25 ultraluminous infrared galaxies at z< 0.27. The lines,[O iii] 52 μm,[N iii] 57 μm,[O i] 63 μm,[N ii] 122 μm,[O i] 145 μm, and [C ii] 158 μm, are mostly single Gaussians with widths< 600 km s− 1 and luminosities of 10 7–10 9 L☉. There are deficits in the [O i] 63/L IR,[N ii]/L IR,[O i] 145/L IR, and [C ii]/L IR ratios compared to lower luminosity systems. The majority of the line deficits are consistent with dustier H ii regions, but part of the [C ii] deficit may arise from an additional mechanism, plausibly charged dust grains. This is consistent with some of the [C ii] originating from photodissociation regions or the interstellar medium (ISM). We derive relations between far-IR line luminosities and both the IR luminosity and star formation rate. We find that [N ii] and both [O i] lines are good tracers of the IR luminosity and star formation rate. In contrast,[C ii] is a poor tracer of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1088/0004-637X/701/2/1147', '10.1086/517987', '10.1086/510107', '10.1111/j.1365-2966.2009.15620.x', '10.1051/0004-6361/201118083', '10.1086/509777', '10.1051/0004-6361:20000435', '10.1088/0067-0049/184/2/230', '10.1051/0004-6361:20066795', '10.1086/497983', '10.1046/j.1365-8711.2003.06818.x', '10.1051/0004-6361:20020156', '10.1086/508849', '10.1086/590249', '10.1086/512029', '10.1088/0004-637X/769/2/91', '10.1086/324019', '10.1086/520082', '10.1086/379120', '10.1093/mnras/279.2.477', '10.1051/0004-6361/201015739', '10.1051/0004-6361/201220392', '10.1111/j.1365-2966.2012.21977.x', '10.1086/321127', '10.1086/504835', '10.1086/528843', '10.1086/507834', '10.1051/0004-6361/201014698', '10.1111/j.1365-2966.2011.19223.x', '10.1086/522104', '10.1088/0004-637X/774/1/68', '10.1051/0004-6361:20010449', '10.1088/2041-8205/753/2/L37', '10.1046/j.1365-8711.2003.06696.x', '10.1086/520834', '10.1088/0004-637X/700/1/395', '10.1086/519492', '10.1086/529485', '10.1111/j.1365-2966.2001.04721.x', '10.1086/428660', '10.1046/j.1365-8711.2002.04991.x', '10.1023/A:1002641429502', '10.1051/0004-6361/201014676', '10.1051/0004-6361:20040963', '10.1046/j.1365-8711.2003.06744.x', '10.1088/2041-8205/767/1/L17', '10.1007/978-94-009-2462-8', '10.1086/305576', '10.1086/323772', '10.1051/0004-6361/201220466', '10.1051/0004-6361/201014664', '10.1086/527292', '10.1086/511975', '10.1111/j.1365-2966.2010.17466.x', '10.1088/2041-8205/728/1/L7', '10.1086/520497', '10.1051/0004-6361/201014519', '10.1007/PL00013287', '10.1088/0004-637X/755/1/57', '10.1093/pasj/64.4.70', '10.1111/j.1365-2966.2009.14660.x', '10.1086/511260', '10.1088/0004-637X/732/2/72', '10.1086/423134', '10.1088/0004-637X/700/1/183', '10.1051/0004-6361/201014807', '10.1086/521867', '10.1086/513715', '10.1093/mnras/stt197', '10.1088/0004-637X/721/1/98', '10.1086/308704', '10.1086/503596', '10.1086/308102', '10.1086/498255', '10.1086/516563', '10.1093/mnras/stt319', '10.1088/0067-0049/196/1/8', '10.1086/650426', '10.1051/0004-6361/201218859', '10.1086/432789', '10.1086/510778', '10.1088/0004-637X/762/2/108', '10.1088/0004-637X/692/1/422', '10.1086/376965', '10.1051/0004-6361:20053890', '10.1111/j.1365-2966.2010.17551.x', '10.1051/0004-6361/201118312', '10.1086/323046', '10.1088/0004-6256/135/4/1207', '10.1088/0004-637X/718/2/928', '10.1051/0004-6361:20011371', '10.1111/j.1365-2966.2005.09460.x', '10.1111/j.1365-2966.2011.18732.x', '10.1111/j.1365-2966.2010.16618.x', '10.1051/0004-6361:20010817', '10.1086/590483', '10.1088/0004-637X/745/2/182', '10.1051/0004-6361:20031415', '10.1051/0004-6361/201118450', '10.1051/0004-6361:20011516', '10.1086/423237', '10.1051/0004-6361/201014759', '10.1088/1538-4357/462/1/L43', '10.1051/0004-6361/201014535', '10.1086/301146', '10.1093/mnras/stt423', '10.1088/0004-637X/767/1/72', '10.1093/mnras/289.2.490', '10.1111/j.1365-2966.2010.17041.x', '10.1088/2041-8205/729/2/L27', '10.1088/0004-637X/757/1/13', '10.1146/annurev.astro.34.1.749', '10.1088/0004-637X/755/2/171', '10.1046/j.1365-8711.2000.03528.x', '10.1088/0004-637X/769/1/75', '10.1086/424896', '10.1086/510549', '10.1086/319968', '10.1086/115691', '10.1086/174994', '10.1086/428495', '10.1088/0004-637X/693/2/1223', '10.1086/511268', '10.1088/0004-637X/724/2/957', '10.1088/0067-0049/206/1/1', '10.1088/2041-8205/733/1/L16', '10.1051/0004-6361:20021043', '10.1086/308247', '10.1111/j.1365-2966.2010.17735.x', '10.1093/mnras/stt330', '10.1086/343075', '10.1086/507985', '10.1086/320543', '10.1088/0004-637X/725/2/1848', '10.1086/309261', '10.1146/annurev.astro.46.060407.145211', '10.1086/341002', '10.1051/0004-6361/201014694', '10.1086/513306', '10.1086/344136', '10.1051/0004-6361:20078883', '10.1086/503188', '10.1086/343844', '10.1088/0067-0049/182/2/628', '10.1051/0004-6361:20030408', '10.1086/340045', '10.1086/500572', '10.1111/j.1365-2966.2010.17811.x', '10.1093/mnras/stt190', '10.1086/466520', '10.1111/j.1365-2966.2012.21214.x', '10.1086/510102', '10.1088/0004-637X/747/2/85', '10.1088/0004-637X/709/2/884', '10.1051/0004-6361:200810141', '10.1088/2041-8205/765/1/L13', '10.1086/340964'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym='The Astrophysical journal', publisher=None, query_handler=None),\n",
       " 'Constraints on modified gravity from Planck 2015: when the health of your theory makes the difference': Paper(DOI='10.1088/1475-7516/2016/09/027', crossref_json=None, google_schorlar_metadata=None, title='Constraints on modified gravity from Planck 2015: when the health of your theory makes the difference', authors=['Valentina Salvatelli', 'Federico Piazza', 'Christian Marinoni'], abstract='We use the effective field theory of dark energy (EFT of DE) formalism to constrain dark energy models belonging to the Horndeski class with the recent Planck 2015 CMB data. The space of theories is spanned by a certain number of parameters determining the linear cosmological perturbations, while the expansion history is set to that of a standard ΛCDM model. We always demand that the theories be free of fatal instabilities. Additionally, we consider two optional conditions, namely that scalar and tensor perturbations propagate with subliminal speed. Such criteria severely restrict the allowed parameter space and are thus very effective in shaping the posteriors. As a result, we confirm that no theory performs better than ΛCDM when CMB data alone are analysed. Indeed, the healthy dark energy models considered here are not able to reproduce those phenomenological behaviours of the effective Newton\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1051/0004-6361/201423413', '10.1103/PhysRevD.91.103503', '10.1088/0004-637X/692/2/1060', '10.1051/0004-6361/201526793', '10.1093/mnras/stt601', '10.1103/PhysRevLett.112.051303', '10.1103/PhysRevD.93.043522', '10.1103/PhysRevLett.111.161301', '10.1051/0004-6361/201321463', '10.1093/mnras/stu1051', '10.1093/mnras/stu197', '10.1093/mnras/stu523', '10.1088/1475-7516/2014/05/042', '10.1103/PhysRevD.93.023513', '10.1103/PhysRevD.79.083513', '10.1103/PhysRevD.81.104023', '10.1111/j.1365-2966.2012.22110.x', '10.1093/mnras/stw707', '10.1088/1475-7516/2009/02/018', '10.1088/1475-7516/2013/02/032', '10.1088/1475-7516/2013/08/010', '10.1088/1475-7516/2013/08/025', '10.1088/1475-7516/2013/12/044', '10.1088/0264-9381/30/21/214007', '10.1007/978-3-319-10070-8_4', '10.1088/1475-7516/2016/02/056', '10.1088/1475-7516/2014/02/026', '10.1103/PhysRevD.89.103530', '10.1103/PhysRevD.90.043513', '10.1103/PhysRevD.89.064059', '10.1088/1475-7516/2015/08/054', '10.1007/BF01807638', '10.1103/PhysRevD.80.064015', '10.1103/PhysRevD.84.064039', '10.1103/PhysRevD.79.064036', '10.1088/1475-7516/2016/02/053', '10.1088/1475-7516/2014/05/043', '10.1088/1475-7516/2015/11/029', '10.1088/1475-7516/2014/07/050', '10.1142/S021827181443010X', '10.1088/1475-7516/2015/02/018', '10.1103/PhysRevD.92.123516', '10.1103/PhysRevD.70.043543', '10.1103/PhysRevD.80.123001', '10.1103/PhysRevD.92.063006', '10.1103/PhysRevLett.116.061101', '10.1088/1126-6708/2006/10/014', '10.1088/1475-7516/2011/08/005', '10.1051/0004-6361/201321942', '10.1088/0067-0049/208/2/20', '10.1103/PhysRevD.66.103511', '10.1103/PhysRevD.87.103529', '10.1103/PhysRevLett.114.211101', '10.1093/mnras/sts493'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Power law cosmology model comparison with CMB scale information': Paper(DOI='10.1103/physrevd.94.103511', crossref_json=None, google_schorlar_metadata=None, title='Power law cosmology model comparison with CMB scale information', authors=['Isaac Tutusaus', 'Brahim Lamine', 'Alain Blanchard', 'Arnaud Dupays', 'Yves Zolnierowski', 'Johann Cohen-Tanugi', 'Anne Ealet', 'Stéphanie Escoffier', 'Olivier Le Fèvre', 'Stéphane Ilić', 'Alice Pisani', 'Stéphane Plaszczynski', 'Ziad Sakr', 'Valentina Salvatelli', 'Thomas Schücker', 'André Tilquin', 'Jean-Marc Virey'], abstract='Despite the ability of the cosmological concordance model (Λ CDM) to describe the cosmological observations exceedingly well, power law expansion of the Universe scale radius, R (t)∝ t n, has been proposed as an alternative framework. We examine here these models, analyzing their ability to fit cosmological data using robust model comparison criteria. Type Ia supernovae (SNIa), baryonic acoustic oscillations (BAO) and acoustic scale information from the cosmic microwave background (CMB) have been used. We find that SNIa data either alone or combined with BAO can be well reproduced by both Λ CDM and power law expansion models with n∼ 1.5, while the constant expansion rate model (n= 1) is clearly disfavored. Allowing for some redshift evolution in the SNIa luminosity essentially removes any clear preference for a specific model. The CMB data are well known to provide the most stringent\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1051/0004-6361/201525830', '10.1086/300499', '10.1086/307221', '10.1103/PhysRevD.55.5881', '10.1103/PhysRevD.59.043514', '10.1103/PhysRevD.61.103507', '10.1088/1475-7516/2014/10/047', '10.1103/PhysRevD.91.103516', '10.1111/j.1365-2966.2011.19906.x', '10.1007/s11467-016-0557-6', '10.1093/mnrasl/slw079', '10.1007/s11467-016-0611-4', '10.1093/mnras/stt592', '10.1111/j.1365-2966.2012.21575.x', '10.1088/0004-6256/149/3/102', '10.1093/mnras/stv2902', '10.1088/0004-637X/764/1/72', '10.1088/0004-6256/149/1/6', '10.1098/rspa.2015.0765', '10.1051/0004-6361/201423413', '10.1093/mnras/stu523', '10.1088/0004-637X/707/2/916', '10.1016/0010-4655(75)90039-9', '10.1214/aos/1176344136', '10.1080/03610927808827599', '10.1093/biomet/76.2.297', '10.1007/BF02294361', '10.1177/0049124104268644', '10.1051/0004-6361/200810693', '10.1051/0004-6361/200912811', '10.1103/PhysRevD.92.123516', '10.1111/j.1365-2966.2011.19250.x', '10.1093/mnras/stv154', '10.1093/mnras/stu371', '10.1051/0004-6361/201423969', '10.1088/1475-7516/2014/05/027', '10.1103/PhysRevD.76.103533', '10.1093/mnras/stu2181', '10.1051/0004-6361/201016103', '10.1086/177989', '10.1103/PhysRevD.93.043013', '10.1111/j.1365-2966.2010.17940.x', '10.1086/312250', '10.1111/j.1365-2966.2009.16136.x', '10.1111/j.1365-2966.2009.15957.x', '10.1111/j.1365-2966.2010.16940.x', '10.1103/PhysRevD.77.083006', '10.1103/PhysRevD.81.083005', '10.1103/PhysRevD.82.063521'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym='Physical review. D.', publisher=None, query_handler=None),\n",
       " 'Utilising artificial intelligence to determine patients at risk of a rare disease: idiopathic pulmonary arterial hypertension': Paper(DOI='10.1177/2045894019890549', crossref_json=None, google_schorlar_metadata=None, title='Utilising artificial intelligence to determine patients at risk of a rare disease: idiopathic pulmonary arterial hypertension', authors=['David G Kiely', 'Orla Doyle', 'Edmund Drage', 'Harvey Jenner', 'Valentina Salvatelli', 'Flora A Daniels', 'John Rigg', 'Claude Schmitt', 'Yevgeniy Samyshkin', 'Allan Lawrie', 'Rito Bergemann'], abstract='Idiopathic pulmonary arterial hypertension is a rare and life-shortening condition often diagnosed at an advanced stage. Despite increased awareness, the delay to diagnosis remains unchanged. This study explores whether a predictive model based on healthcare resource utilisation can be used to screen large populations to identify patients at high risk of idiopathic pulmonary arterial hypertension. Hospital Episode Statistics from the National Health Service in England, providing close to full national coverage, were used as a measure of healthcare resource utilisation. Data for patients with idiopathic pulmonary arterial hypertension from the National Pulmonary Hypertension Service in Sheffield were linked to pre-diagnosis Hospital Episode Statistics records. A non-idiopathic pulmonary arterial hypertension control cohort was selected from the Hospital Episode Statistics population. Patient history was limited to\\xa0…', conference=None, journal=None, year=None, reference_list=['10.7326/0003-4819-115-5-343', '10.1183/09031936.00092306', '10.1164/rccm.200510-1668OC', '10.1136/heartjnl-2012-301992', '10.1016/j.jacc.2013.10.032', '10.4103/2045-8932.109919', '10.1183/09031936.00078411', '10.1093/rheumatology/ken306', '10.1136/bmj.f2028', '10.3899/jrheum.100245', '10.1136/annrheumdis-2013-203301', '10.1371/journal.pmed.1002692', '10.1016/S1470-2045(19)30149-4', '10.1056/NEJMp1702071', '10.1177/2045894018798613', '10.1093/eurheartj/ehv317', '10.1007/BF00058655', '10.1145/2939672.2939785', '10.1161/CIRCRESAHA.115.301146', '10.1016/j.jacc.2013.10.023', '10.2147/CEOR.S119117', '10.1038/nrcardio.2014.191', '10.1016/S0140-6736(08)60919-8', '10.1002/art.30541', '10.1080/13696998.2017.1363049'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Updated constraints from the PLANCK experiment on modified gravity': Paper(DOI='10.1103/physrevd.88.027502', crossref_json=None, google_schorlar_metadata=None, title='Updated constraints from the PLANCK experiment on modified gravity', authors=['Andrea Marchini', 'Valentina Salvatelli'], abstract='A modification of the action of the general relativity produces a different pattern for the growth of the cosmic structures below a certain length scale leaving an imprint on the cosmic microwave background anisotropies. We reexamine the upper limits on the length-scale parameter B 0 of f (R) models using the recent data from the Planck satellite experiment. We also investigate the combined constraints obtained when including the Hubble Space Telescope H 0 measurement and the baryon acoustic oscillations measurements from the SDSS, WiggleZ and BOSS surveys.', conference=None, journal=None, year=None, reference_list=['10.1103/PhysRevD.70.023515', '10.1103/PhysRevD.76.023507', '10.1103/PhysRevD.81.123508', '10.1103/PhysRevD.77.123531', '10.1088/1475-7516/2010/04/030', '10.1103/PhysRevD.80.103516', '10.1103/PhysRevD.87.083527', '10.12942/lrr-2010-3', '10.1088/1475-7516/2011/08/005', '10.1103/PhysRevD.77.023503', '10.1103/PhysRevD.81.049901', '10.1103/PhysRevD.76.063517', '10.1103/PhysRevD.85.124038', '10.1103/PhysRevD.78.024015', '10.1103/PhysRevD.79.083513', '10.1103/PhysRevD.66.103511', '10.1103/PhysRevD.87.103529', '10.1088/0004-637X/730/2/119', '10.1088/0004-637X/732/2/129', '10.1111/j.1365-2966.2011.19250.x', '10.1111/j.1365-2966.2009.15812.x', '10.1111/j.1365-2966.2012.21888.x', '10.1111/j.1365-2966.2011.19592.x', '10.1111/j.1365-2966.2012.22066.x', '10.1103/PhysRevD.77.123531'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym='Physical review. D, Particles, fields, gravitation, and cosmology', publisher=None, query_handler=None),\n",
       " 'Parametrized modified gravity and the CMB bispectrum': Paper(DOI='10.1103/physrevd.86.063517', crossref_json=None, google_schorlar_metadata=None, title='Parametrized modified gravity and the CMB bispectrum', authors=['Eleonora Di Valentino', 'Alessandro Melchiorri', 'Valentina Salvatelli', 'Alessandra Silvestri'], abstract='We forecast the constraints on modified theories of gravity from the cosmic microwave background (CMB) anisotropies bispectrum that arises from correlations between lensing and the Integrated Sachs-Wolfe effect. In models of modified gravity the evolution of the metric potentials is generally altered and the contribution to the CMB bispectrum signal can differ significantly from the one expected in the standard cosmological model. We adopt a parametrized approach and focus on three different classes of models: Linder’s growth index, Chameleon-type models, and f (R) theories. We show that the constraints on the parameters of the models will significantly improve with future CMB bispectrum measurements.', conference=None, journal=None, year=None, reference_list=['10.1016/0370-2693(80)90670-X', '10.1103/PhysRevD.70.043528', '10.1134/S0021364007150027', '10.1103/PhysRevD.77.046009', '10.1103/PhysRevD.63.063504', '10.1103/PhysRevD.61.023518', '10.1103/PhysRevD.70.043539', '10.1016/j.physletb.2003.09.033', '10.1016/S0370-2693(00)00669-9', '10.1016/S0370-2693(01)00160-5', '10.1103/PhysRevD.76.084006', '10.1103/PhysRevD.75.044004', '10.1103/PhysRevD.75.064020', '10.1103/PhysRevD.77.023503', '10.1103/PhysRevD.77.023507', '10.1103/PhysRevD.79.083513', '10.1103/PhysRevD.69.044005', '10.1088/1475-7516/2006/01/016', '10.1103/PhysRevD.75.064003', '10.1103/PhysRevD.77.124031', '10.1103/PhysRevD.77.083512', '10.1103/PhysRevD.78.044017', '10.1103/PhysRevD.76.023507', '10.1103/PhysRevD.77.103513', '10.1088/1475-7516/2010/04/030', '10.1103/PhysRevD.83.023012', '10.1103/PhysRevD.81.123508', '10.1016/j.physrep.2004.08.022', '10.1103/PhysRevLett.107.021301', '10.1086/176363', '10.1103/PhysRevD.56.4494', '10.1103/PhysRevD.59.103002', '10.1103/PhysRevD.65.043007', '10.1103/PhysRevD.71.063522', '10.1103/PhysRevD.71.103009', '10.1111/j.1365-2966.2010.18175.x', '10.1103/PhysRevD.80.123007', '10.1103/PhysRevD.77.107305', '10.1103/PhysRevD.80.083004', '10.1088/1475-7516/2011/03/018', '10.1016/j.astropartphys.2007.09.003', '10.1103/PhysRevD.78.024015', '10.1103/PhysRevD.75.044004', '10.1103/PhysRevD.76.064004', '10.1086/148982', '10.1088/0067-0049/192/2/18', '10.1103/PhysRevD.81.043529', '10.1103/PhysRevD.76.063517', '10.1103/PhysRevD.81.103510', '10.1103/PhysRevD.80.103516'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym='Physical review. D, Particles, fields, gravitation, and cosmology', publisher=None, query_handler=None),\n",
       " 'Future constraints on the Hu-Sawicki modified gravity scenario': Paper(DOI='10.1103/physrevd.85.024006', crossref_json=None, google_schorlar_metadata=None, title='Future constraints on the Hu-Sawicki modified gravity scenario', authors=['Matteo Martinelli', 'Alessandro Melchiorri', 'Olga Mena', 'Valentina Salvatelli', 'Zahara Gironés'], abstract='We present current and future constraints on the Hu and Sawicki modified gravity scenario. This model can reproduce a late time accelerated universe and evade Solar System constraints. While current cosmological data still allows for distinctive deviations from the cosmological constant picture, future measurements of the growth of structure combined with supernova Ia luminosity distance data will greatly improve present constraints.', conference=None, journal=None, year=None, reference_list=['10.1088/0067-0049/192/2/18', '10.1111/j.1365-2966.2009.15812.x', '10.1111/j.1745-3933.2010.00835.x', '10.1088/0004-637X/716/1/712', '10.1103/PhysRevLett.80.1582', '10.1103/PhysRevLett.82.896', '10.1086/308331', '10.1086/185100', '10.1103/PhysRevD.37.3406', '10.1007/978-3-642-10598-2_3', '10.12942/lrr-2010-3', '10.1016/S0370-2693(00)00669-9', '10.1103/PhysRevD.70.043528', '10.1103/PhysRevD.68.063510', '10.1103/PhysRevD.71.063513', '10.1103/PhysRevLett.96.041103', '10.1103/PhysRevD.73.123504', '10.1103/PhysRevD.74.043513', '10.1051/0004-6361:20064994', '10.1103/PhysRevD.75.023519', '10.1103/PhysRevD.74.043502', '10.1103/PhysRevD.77.024017', '10.1103/PhysRevD.78.063503', '10.1038/nature06555', '10.1103/PhysRevD.79.083513', '10.1088/1475-7516/2010/11/004', '10.1088/1475-7516/2010/04/030', '10.1103/PhysRevD.81.103510', '10.1103/PhysRevD.81.063514', '10.1016/j.physletb.2003.09.033', '10.1016/j.physletb.2005.07.008', '10.1103/PhysRevD.72.083505', '10.1103/PhysRevLett.95.261102', '10.1103/PhysRevD.72.044022', '10.1088/1475-7516/2007/02/022', '10.1103/PhysRevD.76.064004', '10.1103/PhysRevD.75.023511', '10.1016/j.physletb.2007.12.041', '10.1103/PhysRevD.76.104019', '10.1103/PhysRevD.77.103009', '10.1103/PhysRevD.75.124014', '10.12942/lrr-2006-3', '10.1103/PhysRevD.69.044026', '10.1103/PhysRevD.73.064029', '10.1103/PhysRevD.76.063505', '10.1103/PhysRevD.77.107501', '10.1103/PhysRevD.78.104021', '10.1103/PhysRevD.79.123516', '10.1103/PhysRevD.77.123515', '10.1103/PhysRevD.75.064020', '10.1086/466512', '10.1111/j.1365-2966.2004.07260.x', '10.1093/mnras/227.1.1', '10.1088/0004-6256/142/3/72', '10.1117/12.672261', '10.1088/1475-7516/2006/01/019', '10.1111/j.1365-2966.2008.14379.x'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym='Physical review. D, Particles, fields, gravitation, and cosmology', publisher=None, query_handler=None),\n",
       " 'RaVÆn: unsupervised change detection of extreme events using ML on-board satellites': Paper(DOI='10.1038/s41598-022-19437-5', crossref_json=None, google_schorlar_metadata=None, title='RaVÆn: unsupervised change detection of extreme events using ML on-board satellites', authors=['Vít Růžička', 'Anna Vaughan', 'Daniele De Martini', 'James Fulton', 'Valentina Salvatelli', 'Chris Bridges', 'Gonzalo Mateo-Garcia', 'Valentina Zantedeschi'], abstract='Applications such as disaster management enormously benefit from rapid availability of satellite observations. Traditionally, data analysis is performed on the ground after being transferred—downlinked—to a ground station. Constraints on the downlink capabilities, both in terms of data volume and timing, therefore heavily affect the response delay of any downstream application. In this paper, we introduce RaVÆn, a lightweight, unsupervised approach for change detection in satellite data based on Variational Auto-Encoders (VAEs), with the specific purpose of on-board deployment. RaVÆn\\xa0pre-processes the sampled data directly on the satellite and flags changed areas to prioritise for downlink, shortening the response time. We verified the efficacy of our system on a dataset—which we release alongside this publication—composed of time series containing a catastrophic event, demonstrating that RaVÆn\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.sbspro.2014.02.114', '10.1080/014311699213659', '10.1080/0143116031000101675', '10.1002/9781119625865', '10.1016/j.agsy.2018.05.010', '10.1145/3376897.3377864', '10.1016/j.actaastro.2011.12.014', '10.1109/49.748789', '10.1109/23.659057', '10.1109/IGARSS.2003.1293687', '10.3390/rs12142205', '10.1038/s41598-021-86650-z', '10.1016/j.rse.2011.11.026', '10.1109/ACCESS.2020.2997327', '10.1109/29.60107', '10.1109/ICIP.2018.8451652', '10.1109/ICASSP.2010.5495301', '10.1109/IGARSS.2013.6723189', '10.1109/TIP.2004.838698', '10.1080/07038992.1993.10855147', '10.1109/IJCNN.2019.8851762', '10.1109/JSTARS.2019.2936771', '10.1109/TGRS.2021.3125567', '10.1145/3292500.3330656', '10.1016/j.rse.2021.112499', '10.1038/s41597-022-01307-4', '10.1117/12.811847', '10.23915/distill.00003', '10.1109/CVPR42600.2020.00872', '10.3390/rs13081518', '10.21105/joss.00861', '10.1109/ICCV48922.2021.00928'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Multichannel autocalibration for the Atmospheric Imaging Assembly using machine learning': Paper(DOI='10.1051/0004-6361/202040051', crossref_json=None, google_schorlar_metadata=None, title='Multichannel autocalibration for the Atmospheric Imaging Assembly using machine learning', authors=['Luiz FG Dos Santos', 'Souvik Bose', 'Valentina Salvatelli', 'Brad Neuberg', 'Mark CM Cheung', 'Miho Janvier', 'Meng Jin', 'Yarin Gal', 'Paul Boerner', 'Atılım Güneş Baydin'], abstract='Context Solar activity plays a quintessential role in affecting the interplanetary medium and space weather around Earth. Remote-sensing instruments on board heliophysics space missions provide a pool of information about solar activity by measuring the solar magnetic field and the emission of light from the multilayered, multithermal, and dynamic solar atmosphere. Extreme-UV (EUV) wavelength observations from space help in understanding the subtleties of the outer layers of the Sun, that is, the chromosphere and the corona. Unfortunately, instruments such as the Atmospheric Imaging Assembly (AIA) on board the NASA Solar Dynamics Observatory (SDO), suffer from time-dependent degradation that reduces their sensitivity. The current best calibration techniques rely on flights of sounding rockets to maintain absolute calibration. These flights are infrequent, complex, and limited to a single vantage point\\xa0…', conference=None, journal=None, year=None, reference_list=['10.3847/1538-3881/aac387', '10.1007/s11207-013-0290-z', '10.1088/0004-637X/798/2/135', '10.1007/s11207-013-0452-z', '10.3847/1538-4357/aaccf1', '10.1016/j.physa.2019.123228', '10.1051/0004-6361/200912904', '10.1007/s11207-014-0485-y', '10.1007/BF00733425', '10.1051/0004-6361:20066854', '10.3847/1538-4365/ab1005', '10.1086/306794', '10.1109/MCSE.2007.55', '10.1088/1757-899X/611/1/012071', '10.1007/s11214-007-9277-0', '10.1038/s41550-019-0711-5', '10.1007/s11207-011-9776-8', '10.1007/s11207-012-9976-x', '10.1080/01621459.1951.10500769', '10.1051/0004-6361/202038467', '10.1007/s11207-011-9841-3', '10.1098/rsta.1976.0029', '10.1051/0004-6361/201936663', '10.3233/IDA-2007-11508', '10.1007/s11207-011-9842-2', '10.1364/AO.37.002646', '10.1051/0004-6361/201424491', '10.1126/sciadv.aaw6548', '10.1109/MCSE.2011.37', '10.7717/peerj.453', '10.1007/lrsp-2015-1', '10.1038/s41592-019-0686-2', '10.1007/s11207-016-0999-6', '10.1007/s11207-009-9487-6'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym='Astronomy & astrophysics (Print)', publisher=None, query_handler=None),\n",
       " 'Constraints on modified gravity from the Atacama Cosmology Telescope and the South Pole Telescope': Paper(DOI='10.1103/physrevd.87.083527', crossref_json=None, google_schorlar_metadata=None, title='Constraints on modified gravity from the Atacama Cosmology Telescope and the South Pole Telescope', authors=['Andrea Marchini', 'Alessandro Melchiorri', 'Valentina Salvatelli', 'Luca Pagano'], abstract='The Atacama Cosmology Telescope (ACT) and the South Pole Telescope (SPT) have recently provided new and precise measurements of the cosmic microwave background anisotropy damping tail. This region of the cosmic microwave background angular spectra, thanks to the angular distortions produced by gravitational lensing, can probe the growth of matter perturbations and provide a new test for general relativity. Here we make use of the ACT and SPT power spectrum measurements (combined with the recent WMAP9 data) to constrain f (R) gravity theories. Adopting a parametrized approach, we obtain an upper limit on the length scale of the theory of B 0< 0.86 at 95% CL from ACT, while we get a much stronger limit from SPT with B 0< 0.14 at 95% CL', conference=None, journal=None, year=None, reference_list=['10.1016/0370-2693(80)90670-X', '10.1103/PhysRevD.70.043528', '10.1134/S0021364007150027', '10.1103/PhysRevD.75.044004', '10.1103/PhysRevD.75.064020', '10.1103/PhysRevD.77.023503', '10.1103/PhysRevD.77.023507', '10.1103/PhysRevD.79.083513', '10.1103/PhysRevD.69.044005', '10.1088/1475-7516/2006/01/016', '10.1103/PhysRevD.75.064003', '10.1103/PhysRevD.77.124031', '10.1103/PhysRevD.77.083512', '10.1103/PhysRevD.78.044017', '10.1103/PhysRevD.76.023507', '10.1103/PhysRevD.77.103513', '10.1088/1475-7516/2010/04/030', '10.1103/PhysRevD.83.023012', '10.1103/PhysRevD.81.123508', '10.1103/PhysRevD.80.103516', '10.1103/PhysRevD.77.123531', '10.1088/1475-7516/2011/08/005', '10.1103/PhysRevD.79.083513', '10.1103/PhysRevD.76.064004', '10.1134/S0021364007150027', '10.1016/j.physletb.2007.08.037', '10.1103/PhysRevD.78.024015', '10.1103/PhysRevD.66.103511', '10.1088/0004-637X/730/2/119', '10.1088/0004-637X/732/2/129', '10.1111/j.1365-2966.2011.19250.x', '10.1093/mnras/sts084', '10.1111/j.1365-2966.2012.21473.x', '10.1103/PhysRevD.81.103007', '10.1088/1475-7516/2010/04/030', '10.1103/PhysRevD.85.124038'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym='Physical review. D, Particles, fields, gravitation, and cosmology', publisher=None, query_handler=None),\n",
       " 'Exploring the Limits of Synthetic Creation of Solar EUV Images via Image-to-image Translation': Paper(DOI='10.3847/1538-4357/ac867b', crossref_json=None, google_schorlar_metadata=None, title='Exploring the Limits of Synthetic Creation of Solar EUV Images via Image-to-image Translation', authors=['Valentina Salvatelli', 'Luiz FG Dos Santos', 'Souvik Bose', 'Brad Neuberg', 'Mark CM Cheung', 'Miho Janvier', 'Meng Jin', 'Yarin Gal', 'Atilim Güneş Baydin'], abstract='The Solar Dynamics Observatory (SDO), a NASA multispectral decade-long mission that has been daily producing terabytes of observational data from the Sun, has been recently used as a use case to demonstrate the potential of machine-learning methodologies and to pave the way for future deep space mission planning. In particular, the idea of using image-to-image translation to virtually produce extreme ultraviolet channels has been proposed in several recent studies, as a way to both enhance missions with fewer available channels and to alleviate the challenges due to the low downlink rate in deep space. This paper investigates the potential and the limitations of such a deep learning approach by focusing on the permutation of four channels and an encoder–decoder based architecture, with particular attention to how morphological traits and brightness of the solar surface affect the neural network\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1088/0004-637X/807/2/143', '10.1051/0004-6361/201731344', '10.3847/1538-4365/ab1005', '10.1109/MCSE.2007.55', '10.1109/CVPR.2017.632', '10.1007/s11207-011-9776-8', '10.3847/2041-8213/ac0d54', '10.25080/Majora-92bf1922-00a', '10.3847/2041-8213/ab46bb', '10.1007/s11207-011-9841-3', '10.1007/978-3-319-24574-4_28', '10.1007/s11207-012-0101-y', '10.1007/s11207-015-0680-5', '10.1126/sciadv.aaw6548', '10.1109/MCSE.2011.37', '10.1038/s41592-019-0686-2', '10.1109/TIP.2003.819861', '10.1145/3097983.3098052'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym='The Astrophysical journal', publisher=None, query_handler=None),\n",
       " 'Unsupervised change detection of extreme events using ML On-board': Paper(DOI='10.1038/s41598-022-19437-5', crossref_json=None, google_schorlar_metadata=None, title='Unsupervised change detection of extreme events using ML On-board', authors=['Vít Růžička', 'Anna Vaughan', 'Daniele De Martini', 'James Fulton', 'Valentina Salvatelli', 'Chris Bridges', 'Gonzalo Mateo-Garcia', 'Valentina Zantedeschi'], abstract='In this paper, we introduce RaVAEn, a lightweight, unsupervised approach for change detection in satellite data based on Variational Auto-Encoders (VAEs) with the specific purpose of on-board deployment. Applications such as disaster management enormously benefit from the rapid availability of satellite observations. Traditionally, data analysis is performed on the ground after all data is transferred - downlinked - to a ground station. Constraint on the downlink capabilities therefore affects any downstream application. In contrast, RaVAEn pre-processes the sampled data directly on the satellite and flags changed areas to prioritise for downlink, shortening the response time. We verified the efficacy of our system on a dataset composed of time series of catastrophic events - which we plan to release alongside this publication - demonstrating that RaVAEn outperforms pixel-wise baselines. Finally we tested our approach on resource-limited hardware for assessing computational and memory limitations.', conference=None, journal=None, year=None, reference_list=['10.1016/j.sbspro.2014.02.114', '10.1080/014311699213659', '10.1080/0143116031000101675', '10.1002/9781119625865', '10.1016/j.agsy.2018.05.010', '10.1145/3376897.3377864', '10.1016/j.actaastro.2011.12.014', '10.1109/49.748789', '10.1109/23.659057', '10.1109/IGARSS.2003.1293687', '10.3390/rs12142205', '10.1038/s41598-021-86650-z', '10.1016/j.rse.2011.11.026', '10.1109/ACCESS.2020.2997327', '10.1109/29.60107', '10.1109/ICIP.2018.8451652', '10.1109/ICASSP.2010.5495301', '10.1109/IGARSS.2013.6723189', '10.1109/TIP.2004.838698', '10.1080/07038992.1993.10855147', '10.1109/IJCNN.2019.8851762', '10.1109/JSTARS.2019.2936771', '10.1109/TGRS.2021.3125567', '10.1145/3292500.3330656', '10.1016/j.rse.2021.112499', '10.1038/s41597-022-01307-4', '10.1117/12.811847', '10.23915/distill.00003', '10.1109/CVPR42600.2020.00872', '10.3390/rs13081518', '10.21105/joss.00861', '10.1109/ICCV48922.2021.00928'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Auto-Calibration of Remote Sensing Solar Telescopes with Deep Learning': Paper(DOI='10.3390/rs14112517', crossref_json=None, google_schorlar_metadata=None, title='Auto-Calibration of Remote Sensing Solar Telescopes with Deep Learning', authors=['Brad Neuberg', 'Souvik Bose', 'Valentina Salvatelli', 'Luiz FG dos Santos', 'Mark Cheung', 'Miho Janvier', 'Atilim Gunes Baydin', 'Yarin Gal', 'Meng Jin'], abstract=\"As a part of NASA's Heliophysics System Observatory (HSO) fleet of satellites,the Solar Dynamics Observatory (SDO) has continuously monitored the Sun since2010. Ultraviolet (UV) and Extreme UV (EUV) instruments in orbit, such asSDO's Atmospheric Imaging Assembly (AIA) instrument, suffer time-dependent degradation which reduces instrument sensitivity. Accurate calibration for (E)UV instruments currently depends on periodic sounding rockets, which are infrequent and not practical for heliophysics missions in deep space. In the present work, we develop a Convolutional Neural Network (CNN) that auto-calibrates SDO/AIA channels and corrects sensitivity degradation by exploiting spatial patterns in multi-wavelength observations to arrive at a self-calibration of (E)UV imaging instruments. Our results remove a major impediment to developing future HSOmissions of the same scientific caliber as SDO but in deep space, able to observe the Sun from more vantage points than just SDO's current geosynchronous orbit.This approach can be adopted to perform autocalibration of other imaging systems exhibiting similar forms of degradation\", conference=None, journal=None, year=None, reference_list=['10.1126/science.243.4887.57', '10.1098/rsta.2011.0246', '10.1175/BAMS-D-12-00149.1', '10.1175/1520-0477(1996)077<0853:CATERE>2.0.CO;2', '10.1126/science.1106484', '10.1175/2009JTECHA1243.1', '10.3390/rs11060663', '10.1175/JAMC-D-16-0406.1', '10.1029/2021GL092994', '10.3847/0004-637X/830/1/25', '10.1109/TGRS.2005.852710', '10.1117/1.JRS.8.083514', '10.3390/rs12193167', '10.1175/1520-0450(2001)040<2249:PRVOTC>2.0.CO;2', '10.1175/JCLI4018.1', '10.1175/2010JTECHA1521.1', '10.1364/AO.57.001594', '10.1364/AO.43.005838', '10.1364/AO.47.004981', '10.1175/2010JTECHA1322.1', '10.1109/TGRS.2018.2799823', '10.1109/TGRS.2015.2417314', '10.1086/430185', '10.1109/TGRS.2012.2226588', '10.3390/rs11161914', '10.1109/TGRS.2012.2228654', '10.1109/TGRS.2015.2400928', '10.1594/PANGAEA.931779', '10.1594/PANGAEA.938078'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'AI', 'Computer Vision', 'Astrophysics', 'Healthcare'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Applications of digital-image-correlation techniques to experimental mechanics': Paper(DOI='10.1007/bf02325092', crossref_json=None, google_schorlar_metadata=None, title='Applications of digital-image-correlation techniques to experimental mechanics', authors=['TC Chu', 'WF Ranson', 'Michael A Sutton'], abstract=\"Optical measurements of macroscopic parameters, such as strain and displacement, have evolved into an accepted branch of experimental stress analysis. Topics such as holographyJ, 2 speckle interferometry, 3 speckle photography/-6 speckle-shearing interferometry/white-light speckle s and moir~ 9'~~ have advanced from the basic research stage into mature methods employed to analyze a variety of engineering problems. However, all of these techniques suffer from two major limitations. First, they have varying stability requirements. All interferometric methods have stringent stability requirements which limit their applicability to research environments for most cases. Speckle photography, moirg and white-light speckle have less stringent requirements than those for interferometric techniques, and are more adaptable to industrial applications. Secondly, the data processing required to reduce the fringe patterns and thereby obtain the desired data is laborious and time consuming for each method noted above. However, many researchers have recognized this difficulty and have developed computerized procedures to simplify the datareduction process. MendenhalP~ developed a completely automated process to digitize both isopachic and isochromatic fringe patterns, compute fringe numbers, determine partial fringe values and finally output profiles of constant principal-stress values. More recently, a completely automated approach for the computation of surface strains and displacements was proposed'and later employed to determine the centerline displacements of a cantilever beam.~ 3 The work presented here outlines (a) the basic\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1063/1.2995245', '10.1088/0022-3735/3/3/312', '10.1080/713818270', '10.1080/713818559', '10.1364/AO.11.001778', '10.1016/0030-4018(74)90200-4', '10.1364/AO.18.000409', '10.1007/BF02326375', '10.1117/12.7972930', '10.1117/12.7972925', '10.1016/0262-8856(83)90064-1', '10.1109/TC.1976.5009235', '10.1117/12.7973231'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Experimental mechanics', publisher=None, query_handler=None),\n",
       " 'Determination of displacements using an improved digital correlation method': Paper(DOI='10.1016/0262-8856(83)90064-1', crossref_json=None, google_schorlar_metadata=None, title='Determination of displacements using an improved digital correlation method', authors=['Michael A Sutton', 'WJ Wolters', 'WH Peters', 'WF Ranson', 'SR McNeill'], abstract='An improved digital correlation method is presented for obtaining the full-field in-plane deformations of an object. The deformations are determined by numerically correlating a selected subset from the digitized intensity pattern of the undeformed object. The improved numerical correlation scheme is discussed in detail. The displacements of a simple object, as computed by the correlation routine, are shown to agree with theoretical calculations.', conference=None, journal=None, year=None, reference_list=['10.1117/12.7972925'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Image and vision computing', publisher=None, query_handler=None),\n",
       " 'Digital image correlation using Newton-Raphson method of partial differential correction': Paper(DOI='10.1007/bf02321405', crossref_json=None, google_schorlar_metadata=None, title='Digital image correlation using Newton-Raphson method of partial differential correction', authors=['HA Bruck', 'SR McNeill', 'Michael A Sutton', 'WH Peters'], abstract=' Digital image correlation is finding wider use in the field of mechanics. One area of weakness in the current technique is the lack of available displacement gradient terms. This technique, based on a coarse-fine search method, is capable of calculating the gradients. However the speed at which it does so has prevented widespread use. Presented in this paper is the development and limited experimental verification of a method which can determine displacements and gradients using the Newton-Raphson method of partial corrections. It will be shown that this method is accurate in determining displacements and certain gradients, while using significantly less CPU time than the current coarse-fine search method.', conference=None, journal=None, year=None, reference_list=['10.1016/0262-8856(86)90057-0', '10.1016/0262-8856(83)90064-1', '10.1117/12.7976778', '10.1007/BF02326046'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Experimental mechanics', publisher=None, query_handler=None),\n",
       " 'Application of an optimized digital correlation method to planar deformation analysis': Paper(DOI='10.1016/0262-8856(86)90057-0', crossref_json=None, google_schorlar_metadata=None, title='Application of an optimized digital correlation method to planar deformation analysis', authors=['MA Sutton', 'Cheng Mingqi', 'WH Peters', 'YJ Chao', 'SR McNeill'], abstract='An optimized digital correlation method (DCM) has been developed to improve previously reported iterative DCMs. The optimized method is shown to be much faster than previous methods while achieving accuracy equivalent to that obtained via simple coarse-fine iterative techniques. The optimized correlation routine employs the Newton-Raphson method with differential corrections to minimize search time. The paper illustrates the optimized DCM and presents a simple flowchart of a practical program. Experiments indicate that the optimized DCM is successful in displacement measurement. In direct strain measurement, however, both DCMs using a coarse-fine search routine and those using the proposed optimized correlation procedure are shown to have large variability in the computed strains.', conference=None, journal=None, year=None, reference_list=['10.1016/0262-8856(83)90064-1', '10.1007/BF02325092'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Image and vision computing', publisher=None, query_handler=None),\n",
       " 'Systematic errors in digital image correlation caused by intensity interpolation': Paper(DOI='10.1117/1.1314593', crossref_json=None, google_schorlar_metadata=None, title='Systematic errors in digital image correlation caused by intensity interpolation', authors=['Hubert W Schreier', 'Joachim R Braasch', 'Michael A Sutton'], abstract='Recently, digital image correlation as a tool for surface deformation measurements has found widespread use and acceptance in the field of experimental mechanics. The method is known to reconstruct displacements with subpixel accuracy that depends on various factors such as image quality, noise, and the correlation algorithm chosen. However, the systematic errors of the method have not been studied in detail. We address the systematic errors of the iterative spatial domain crosscorrelation algorithm caused by gray-value interpolation. We investigate the position-dependent bias in a numerical study and show that it can lead to apparent strains of the order of 40% of the actual strain level. Furthermore, we present methods to reduce this bias to acceptable levels.© 2000 Society of Photo-Optical Instrumentation Engineers.', conference=None, journal=None, year=None, reference_list=['10.1364/AO.32.001839', '10.1016/0262-8856(83)90064-1', '10.1016/0262-8856(86)90057-0', '10.1109/TPAMI.1983.4767373', '10.1109/78.193220', '10.1109/78.193221', '10.1117/12.7976778'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Optical engineering (Bellingham. Print)', publisher=None, query_handler=None),\n",
       " 'The effect of out-of-plane motion on 2D and 3D digital image correlation measurements': Paper(DOI='10.1016/j.optlaseng.2008.05.005', crossref_json=None, google_schorlar_metadata=None, title='The effect of out-of-plane motion on 2D and 3D digital image correlation measurements', authors=['Michael A Sutton', 'JH Yan', 'V Tiwari', 'HW Schreier', 'Jean-José Orteu'], abstract='The effect of out-of-plane motion (including out-of-plane translation and rotation) on two-dimensional (2D) and three-dimensional (3D) digital image correlation measurements is demonstrated using basic theoretical pinhole image equations and experimentally through synchronized, multi-system measurements. Full-field results obtained during rigid body, out-of-plane motion using a single-camera vision system with (a-1) a standard f55mm Nikon lens and (a-2) a single Schneider–Kreuznach Xenoplan telecentric lens are compared with data obtained using a two-camera stereovision system with standard f55mm Nikon lenses. Results confirm that the theoretical equations are in excellent agreement with experimental measurements. Specifically, results show that (a) a single-camera, 2D imaging system is sensitive to out-of-plane motion, with in-plane strain errors (a-1) due to out-of-plane translation being\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1117/12.7972925', '10.1016/0262-8856(83)90064-1', '10.1007/BF02324993', '10.1117/12.7973231', '10.1007/BF02325092', '10.1117/12.7976778', '10.1016/0262-8856(86)90057-0', '10.1007/BF02321405', '10.1520/STP17262S', '10.1117/1.1314593', '10.1007/BF02326485', '10.1007/BF02410987', '10.1016/j.matchar.2004.07.009', '10.1016/S0921-5093(01)01385-5', '10.1016/S0921-5093(02)00116-8', '10.1016/j.optlaseng.2007.05.008', '10.1007/s11340-006-9011-y', '10.1117/12.160877', '10.1007/BF02322488', '10.1117/1.600624', '10.1117/1.1566001', '10.1117/1.1566002', '10.1007/BF02427894', '10.1002/jbm.a.31268', '10.1023/A:1011014917851', '10.1007/s10704-007-9101-6', '10.1117/1.2741279'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Optics and lasers in engineering', publisher=None, query_handler=None),\n",
       " 'Accurate measurement of three-dimensional deformations in deformable and rigid bodies using computer vision': Paper(DOI='10.1007/bf02322488', crossref_json=None, google_schorlar_metadata=None, title='Accurate measurement of three-dimensional deformations in deformable and rigid bodies using computer vision', authors=['PF Luo', 'YJ Chao', 'MA Sutton', 'W-Ha Peters'], abstract=' Recently, digital-image-correlation techniques have been used to accurately determine two-dimensional in-plane displacements and strains. An extension of the two-dimensional method to the acquisition of accurate, three-dimensional surfacedisplacement data from a stereo pair of CCD cameras is presented in this paper. A pin-hole camera model is used to express the transformation relating three-dimensional world coordinates to two-dimensional computer-image coordinates by the use of camera extrinsic and intrinsic parameters. Accurate camera model parameters are obtained for each camera independently by (a) using several points which have three-dimensional world coordinates that are accurate within 0.001 mm and (b) using two-dimensional image-correlation methods that are accurate to within 0.05 pixels to obtain the computer-image coordinates of various object positions. A\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1115/1.3636492', '10.1117/12.7972957', '10.1117/12.7972959', '10.1364/AO.5.000595', '10.1007/BF02322832', '10.1117/12.7971814', '10.1007/BF02327597', '10.1117/12.7972925', '10.1016/0262-8856(83)90064-1', '10.1007/BF02325092', '10.1016/0262-8856(86)90057-0', '10.1117/12.7976778', '10.1007/BF02327782', '10.1016/0004-3702(74)90029-0', '10.1007/BF02322695', '10.1145/356893.356896', '10.1109/JRA.1987.1087109', '10.1109/34.35495', '10.1007/BF02327571'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Experimental mechanics', publisher=None, query_handler=None),\n",
       " 'Systematic errors in digital image correlation due to undermatched subset shape functions': Paper(DOI='10.1007/bf02410987', crossref_json=None, google_schorlar_metadata=None, title='Systematic errors in digital image correlation due to undermatched subset shape functions', authors=['Hubert W Schreier', 'Michael A Sutton'], abstract=' Digital image correlation techniques are commonly used to measure specimen displacements by finding correspondences between an image of the specimen in an undeformed or reference configuration and a second image under load. To establish correspondences between the two images, numerical techniques are used to locate an initially square image subset in a reference image within an image taken under load. During this process, shape functions of varying order can be applied to the initially square subset. Zero order shape functions permit the subset to translate rigidly, while first-order shape functions represent an affine transform of the subset that permits a combination of translation, rotation, shear and normal strains. In this article, the systematic errors that arise from the use of undermatched shape function, i.e., shape functions of lower order than the actual displacement field, are\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF02321649', '10.1016/S0013-7944(00)00011-4', '10.1007/BF02321405', '10.1007/BF02326485', '10.1117/1.1314593', '10.1021/ac60214a047', '10.1007/BF02323101'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Experimental mechanics', publisher=None, query_handler=None),\n",
       " 'Microstructural studies of friction stir welds in 2024-T3 aluminum': Paper(DOI='10.1016/s0921-5093(01)01358-2', crossref_json=None, google_schorlar_metadata=None, title='Microstructural studies of friction stir welds in 2024-T3 aluminum', authors=['Michael A Sutton', 'B Yang', 'Anthony P Reynolds', 'R Taylor'], abstract='Friction stir welds in 7 mm thick, 2024-T351 aluminum rolled sheet material have been completed. Metallurgical, hardness and quantitative energy dispersive X-ray measurements have been performed which demonstrate that a segregated, banded, microstructure consisting of alternating hard particle rich and hard particle poor regions is developed. Mixed-mode I/II monotonic fracture experiments confirm that the observed banded microstructure affects the macroscopic fracture process. Since the band spacing is directly correlated with the welding tool advance per revolution, our results indicated that the opportunity exists to manipulate the friction stir weld process parameters in order to modify the weld microstructure and improve a range of material properties, including fracture resistance.', conference=None, journal=None, year=None, reference_list=['10.4028/www.scientific.net/MSF.331-337.1719', '10.1016/S0013-7944(00)00011-4', '10.1016/S0020-7683(99)00055-4', '10.1023/A:1011014917851', '10.1023/A:1007339625267', '10.1016/S1359-6462(00)00480-2', '10.1016/S1359-6462(96)00344-2', '10.1023/A:1018556332357', '10.1179/136217100101538119', '10.1016/S0001-6160(89)80001-X', '10.1520/STP16323S'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Materials science & engineering. A, Structural materials: properties, microstructure and processing', publisher=None, query_handler=None),\n",
       " 'Estimation of stress intensity factor by digital image correlation': Paper(DOI='10.1016/0013-7944(87)90124-x', crossref_json=None, google_schorlar_metadata=None, title='Estimation of stress intensity factor by digital image correlation', authors=['SR McNeill', 'WH Peters', 'MA Sutton'], abstract='A method of determining stress intensity factors using digital image correlation is presented. The experimental and analytical method is described with results for different specimen geometries given. Effects of using higher order terms in the series expansion are also investigated and presented.', conference=None, journal=None, year=None, reference_list=['10.1007/BF02321339', '10.1117/12.7972925'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Engineering fracture mechanics', publisher=None, query_handler=None),\n",
       " 'Application of digital correlation methods to rigid body mechanics': Paper(DOI='10.1117/12.7973231', crossref_json=None, google_schorlar_metadata=None, title='Application of digital correlation methods to rigid body mechanics', authors=['WH Peters', 'WF Ranson', 'MA Sutton', 'TC Chu', 'J Anderson'], abstract='The random nature of white light speckle is combined with newly developed video digital data acquisition procedures to experimentally determine parameters of interest for a rigid body dynamics problem. Digital correlation of subsequent images during a dynamic event is shown to yield displacements and velocities of discrete points in the rigid body and, therefore, the angular velocity and linear velocity of the rigid body.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Optical engineering (Bellingham. Print)', publisher=None, query_handler=None),\n",
       " 'High-temperature deformation measurements using digital-image correlation': Paper(DOI='10.1007/bf02328699', crossref_json=None, google_schorlar_metadata=None, title='High-temperature deformation measurements using digital-image correlation', authors=['JS Lyons', 'J Liu', 'MA Sutton'], abstract=' The ability of the computer-vision technique of digital-image correlation to measure full-field in-plane surface deformations at elevated temperatures was evaluated by a series of experiments. Samples were subjected to pure translation, free thermal expansion and uniform tensile loads. Results are presented which show that the digital-image-correlation technique remains fully capable of accurate measurement of the displacements and strains on the surface of a planar object at temperatures up to 650°C.', conference=None, journal=None, year=None, reference_list=['10.1007/BF02329029', '10.1111/j.1747-1567.1989.tb00979.x', '10.1117/12.7972925', '10.1016/0262-8856(83)90064-1', '10.1007/BF02325092', '10.1016/0262-8856(86)90057-0', '10.1016/0013-7944(87)90124-X', '10.1117/12.7976778', '10.1007/BF02321405', '10.1007/BF02327571', '10.1007/BF00017337', '10.1002/nme.1620010108', '10.1007/BF02322488'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Experimental mechanics', publisher=None, query_handler=None),\n",
       " 'Improved three-dimensional image correlation for surface displacement measurement': Paper(DOI='10.1117/1.600624', crossref_json=None, google_schorlar_metadata=None, title='Improved three-dimensional image correlation for surface displacement measurement', authors=['Jeffrey D Helm', 'Stephen R McNeill', 'Michael A Sutton'], abstract='A PC‐based, 3‐D surface profile and displacement measurement system capable of micron‐level accuracy using moderately priced off‐the‐shelf equipment has been developed. For use in field applications, a simplified calibration process using precision grids and camera translations is developed. An improved image correlation process is developed which corrects for perspective distortions due to viewpoint differences between the two cameras. The accuracy of the system was assessed experimentally and results expressed using several different error measures, including a new error measure proposed by the authors. The accuracy for both the profile and displacement measurement systems was established through a series of profile and translation tests. The baseline tests confirmed that the measurement system is capable of highly accurate full‐field measurements. The system was also used successfully to\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Optical engineering (Bellingham. Print)', publisher=None, query_handler=None),\n",
       " 'Higher order asymptotic crack tip fields in a power-law hardening material': Paper(DOI='10.1016/0013-7944(93)90002-a', crossref_json=None, google_schorlar_metadata=None, title='Higher order asymptotic crack tip fields in a power-law hardening material', authors=['Shaorui Yang', 'YJ Chao', 'MA Sutton'], abstract='The asymptotic stress and deformation fields for plane problems are developed for a crack tip embedded in a power-law elastic-plastic material. Using an asymptotic expansion and separation of variables for the stress function, a series solution is obtained for the stress and deformation at a crack tip. The most singular term in the series solution is the HRR solution, after Hutchinson and Rice and Rosengren. The stress exponents and the angular distributions for several higher order terms are obtained for different hardening exponents. Both Mode I and Mode II cases are investigated. Good agreement with the finite element results confirms the analytical findings. It is further demonstrated that in the plane strain, Mode I case the first three terms, controlled by two parameters, can be used to characterize the crack tip stress fields for a variety of specimen geometries and materials with various hardening exponents.', conference=None, journal=None, year=None, reference_list=['10.1016/0022-5096(68)90014-8', '10.1016/0022-5096(68)90021-5', '10.1016/0022-5096(68)90013-6', '10.1115/1.3601206', '10.1016/0022-5096(91)90051-O', '10.1016/0013-7944(89)90258-0', '10.1016/0749-6419(90)90036-E', '10.1016/0022-5096(91)90049-T', '10.1115/1.2897135', '10.1016/0022-5096(91)90029-N'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Engineering fracture mechanics', publisher=None, query_handler=None),\n",
       " 'Full-field representation of discretely sampled surface deformation for displacement and strain analysis': Paper(DOI='10.1007/bf02327571', crossref_json=None, google_schorlar_metadata=None, title='Full-field representation of discretely sampled surface deformation for displacement and strain analysis', authors=['MA Sutton', 'JL Turner', 'HA Bruck', 'TA Chae'], abstract=' A detailed evaluation of the feasibility of determining displacements and displacement gradients from measured surface displacement fields is presented. An improved methodology for both the estimation and elimination of noise is proposed. The methodology is used to analyze the gradients for three tests: (1) uniform rotation, (2) uniform strain, and (3) crack-tip displacement fields. Results of the study indicate that the proposed methodology can be used to extract the underlying two-dimensional displacements and their corresponding gradients from the noisy data with reasonable accuracy. Specifically, it is shown that (a) the digital correlation method for acquiring displacement fields has an error in strain of approximately 150 μ strain at each point, (b) the average strain in a region of uniform strain has much less error, typically on the order of 20 μ strain, (c) the displacement ‘nolse’ present in digital\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF02326046', '10.1016/0045-7949(87)90115-5', '10.1243/03093247V133177', '10.1117/12.941714', '10.1007/BF02318859', '10.1007/BF01437407', '10.1007/BF01404567', '10.1016/0262-8856(86)90057-0', '10.1007/BF02321405'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Experimental mechanics', publisher=None, query_handler=None),\n",
       " 'Scanning electron microscopy for quantitative small and large deformation measurements part I: SEM imaging at magnifications from 200 to 10,000': Paper(DOI='10.1007/s11340-007-9042-z', crossref_json=None, google_schorlar_metadata=None, title='Scanning electron microscopy for quantitative small and large deformation measurements part I: SEM imaging at magnifications from 200 to 10,000', authors=['Michael A Sutton', 'N Li', 'DC Joy', 'Anthony P Reynolds', 'Xiaodong Li'], abstract=' A series of baseline displacement measurements have been obtained using 2D Digital Image Correlation (2D-DIC) and images from Scanning Electron Microscopes (SEM). Direct correlation of subsets from a reference image to subsets in a series of uncorrected images is used to identify the presence of non-stationary step-changes in the measured displacements. Using image time integration and recently developed approaches to correct residual drift and spatial distortions in recorded images, results clearly indicate that the corrected SEM images can be used to extract deformations with displacement accuracy of ±0.02 pixels (1\\xa0nm at magnification of 10,000) and mean value strain measurements that are consistent with independent estimates and have point-to-point strain variability of ±1.5\\u2009×\\u200910−4.', conference=None, journal=None, year=None, reference_list=['10.1117/12.7972925', '10.1007/BF02325092', '10.1016/0262-8856(86)90057-0', '10.1007/BF02322695', '10.1117/12.160877', '10.1117/1.600624', '10.1117/12.281287', '10.1016/S0143-8166(99)00040-8', '10.1016/B978-008043020-1/50039-9', '10.1007/BF02427894', '10.1111/0031-868X.00148', '10.1116/1.586646', '10.1007/BF02325739', '10.1007/BF02322141', '10.6028/jres.101.007', '10.1116/1.1622944', '10.1088/0957-0233/17/10/012', '10.5244/C.10.28'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Experimental mechanics', publisher=None, query_handler=None),\n",
       " 'Effects of subpixel image restoration on digital correlation error estimates': Paper(DOI='10.1117/12.7976778', crossref_json=None, google_schorlar_metadata=None, title='Effects of subpixel image restoration on digital correlation error estimates', authors=['Michael A Sutton', 'Stephen R McNeill', 'Jinseng Jang', 'Majid Babai'], abstract='Recently, a method has been developed that uses computer vision to determine the deformations of subsets of an object. Although the method has been used successfully in a variety of applications, to date there has been no critical assessment of the key parameters in the system and their effect on the accuracy of the measured deformations. The present work presents the results of initial studies of this system. The system components are modeled, and a representative intensity pattern is chosen and deformed by known amounts. Then, the effects of varying the various parameters in the model are analyzed numerically. The most significant parameters are found to be (1) the number of quantization levels in the digitization process (i.e., the number of bits in the A/D converter), (2) the ratio of the frequency of the signal to the frequency of the sampling, and (3) the form of the intensity interpolation function.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Optical engineering (Bellingham. Print)', publisher=None, query_handler=None),\n",
       " 'Quantitative error assessment in pattern matching: effects of intensity pattern noise, interpolation, strain and image contrast on motion measurements': Paper(DOI='10.1111/j.1475-1305.2008.00592.x', crossref_json=None, google_schorlar_metadata=None, title='Quantitative error assessment in pattern matching: effects of intensity pattern noise, interpolation, strain and image contrast on motion measurements', authors=['YQ Wang', 'MA Sutton', 'HA Bruck', 'HW Schreier'], abstract=' Basic concepts in probability are employed to develop analytic formulae for both the expectation (bias) and variance for image motions obtained during subset‐based pattern matching. Specifically, the expectation and variance in image motions in the presence of uncorrelated Gaussian intensity noise for each pixel location are obtained by optimising a least squares intensity matching metric. Results for both 1D and 2D image analyses clearly quantify both the bias and the covariance matrix for image motion estimates as a function of: (a) interpolation method, (b) sub‐pixel motion, (c) intensity noise, (d) contrast, (e) level of uniaxial normal strain and (f) subset size. For 1D translations, excellent agreement is demonstrated between simulations, theoretical predictions and experimental measurements. The level of agreement confirms that the analytical formulae can be used to provide a priori estimates for the ‘quality\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICPR.1994.576335', '10.1016/0262-8856(83)90064-1', '10.1007/BF02325092', '10.1117/12.160877', '10.1117/1.1566001', '10.1117/1.1566002', '10.1117/1.2741279', '10.1007/s11340-006-9011-y', '10.1007/s10704-007-9101-6', '10.1007/s003480050005', '10.1016/S0020-7683(02)00176-2', '10.1007/s11340-006-9824-8', '10.1117/1.1314593', '10.1007/BF02323101', '10.1007/s11263-005-6878-5', '10.1117/1.2213609', '10.1007/s11340-006-9005-9'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Digital Image Correlation', 'Stereo Vision', 'Computer Vision', 'Fracture Mechanics', 'Biomechanics'], conference_acronym='Strain', publisher=None, query_handler=None),\n",
       " 'Computing geodesic paths on manifolds': Paper(DOI='10.1073/pnas.95.15.8431', crossref_json=None, google_schorlar_metadata=None, title='Computing geodesic paths on manifolds', authors=['Ron Kimmel', 'James A Sethian'], abstract='The Fast Marching Method is a numerical algorithm for solving the Eikonal equation on a rectangular orthogonal mesh in O(M log M) steps, where M is the total number of grid points. In this paper we extend the Fast Marching Method to triangulated domains with the same computational complexity. As an application, we provide an optimal time algorithm for computing the geodesic distances and thereby extracting shortest paths on triangulated manifolds. ', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image processing', 'computer vision', 'shape analysis', 'medical imaging', 'metric geometry'], conference_acronym='Proceedings of the National Academy of Sciences of the United States of America', publisher=None, query_handler=None),\n",
       " 'Numerical geometry of non-rigid shapes': Paper(DOI='10.1007/978-0-387-73301-2_12', crossref_json=None, google_schorlar_metadata=None, title='Numerical geometry of non-rigid shapes', authors=['Alexander M Bronstein', 'Michael M Bronstein', 'Ron Kimmel'], abstract='Deformable objects are ubiquitous in the world surrounding us, on all levels from micro to macro. The need to study such shapes and model their behavior arises in a wide spectrum of applications, ranging from medicine to security. In recent years, non-rigid shapes have attracted growing interest, which has led to rapid development of the field, where state-of-the-art results from very different sciences-theoretical and numerical geometry, optimization, linear algebra, graph theory, machine learning and computer graphics, to mention several-are applied to find solutions. This book gives an overview of the current state of science in analysis and synthesis of non-rigid shapes. Everyday examples are used to explain concepts and to illustrate different techniques. The presentation unfolds systematically and numerous figures enrich the engaging exposition. Practice problems follow at the end of each chapter, with detailed solutions to selected problems in the appendix. A gallery of colored images enhances the text. This book will be of interest to graduate students, researchers and professionals in different fields of mathematics, computer science and engineering. It may be used for courses in computer vision, numerical geometry and geometric modeling and computer graphics or for self-study.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'machine learning', 'bioinformatics', 'computational chemistry', 'data science'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'A general framework for low level vision': Paper(DOI='10.1109/83.661181', crossref_json=None, google_schorlar_metadata=None, title='A general framework for low level vision', authors=['Nir Sochen', 'Ron Kimmel', 'Ravi Malladi'], abstract='We introduce a new geometrical framework based on which natural flows for image scale space and enhancement are presented. We consider intensity images as surfaces in the (x,I) space. The image is, thereby, a two-dimensional (2-D) surface in three-dimensional (3-D) space for gray-level images, and 2-D surfaces in five dimensions for color images. The new formulation unifies many classical schemes and algorithms via a simple scaling of the intensity contrast, and results in new and efficient schemes. Extensions to multidimensional signals become natural and lead to powerful denoising and scale space algorithms.', conference=None, journal=None, year=None, reference_list=['10.1007/BF01420591', '10.1109/83.541429', '10.1109/34.56205', '10.1002/cpa.3160460106', '10.1006/gmip.1996.0011', '10.1109/CVPR.1996.517146', '10.1016/0167-2789(92)90242-F', '10.1016/0370-2693(81)90743-7', '10.1109/CVPR.1996.517065', '10.1006/jcph.1993.1092', '10.1080/10586458.1993.10504566', '10.1090/S0025-5718-1967-0229394-6', '10.1016/0734-189X(86)90223-9', '10.1007/BF01318902', '10.2307/1971452', '10.1117/12.205467', '10.1016/0022-0396(80)90081-9', '10.1109/ICCV.1995.466871', '10.1007/s002110050294', '10.1016/0022-0396(78)90005-0', '10.1023/A:1007979827043', '10.1109/ICIP.1994.413266', '10.1007/BF00375127', '10.1016/S0734-189X(89)80017-9', '10.7551/mitpress/3132.001.0001', '10.1137/0917016', '10.1007/3-540-63167-4_52', '10.1109/ICCV.1995.466855', '10.1109/ICIP.1996.559533', '10.1007/3-540-63930-6_108', '10.1109/CVPR.1997.609348', '10.1007/3-540-63167-4_54'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image processing', 'computer vision', 'shape analysis', 'medical imaging', 'metric geometry'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Demosaicing: Image reconstruction from color CCD samples': Paper(DOI='10.1109/83.784434', crossref_json=None, google_schorlar_metadata=None, title='Demosaicing: Image reconstruction from color CCD samples', authors=['Ron Kimmel'], abstract=\"A simplified color image formation model is used to construct an algorithm for image reconstruction from CCD sensors samples. The proposed method involves two successive steps. The first is motivated by Cok's (1994) template matching technique, while the second step uses steerable inverse diffusion in color. Classical linear signal processing techniques tend to oversmooth the image and result in noticeable color artifacts along edges and sharp features. The question is how should the different color channels support each other to form the best possible reconstruction. Our answer is to let the edges support the color information, and the color channels support the edges, and thereby achieve better perceptual results than those that are bounded by the sampling theoretical limit.\", conference=None, journal=None, year=None, reference_list=['10.2307/2153246', '10.1109/83.541429', '10.1109/83.661181', '10.1007/3-540-55426-2_15', '10.1364/JOSAA.6.000920', '10.1109/83.661179', '10.1016/0031-3203(94)90013-2'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image processing', 'computer vision', 'shape analysis', 'medical imaging', 'metric geometry'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Generalized multidimensional scaling: a framework for isometry-invariant partial surface matching': Paper(DOI='10.1073/pnas.0508601103', crossref_json=None, google_schorlar_metadata=None, title='Generalized multidimensional scaling: a framework for isometry-invariant partial surface matching', authors=['Alexander M Bronstein', 'Michael M Bronstein', 'Ron Kimmel'], abstract='An efficient algorithm for isometry-invariant matching of surfaces is presented. The key idea is computing the minimum-distortion mapping between two surfaces. For this purpose, we introduce the generalized multidimensional scaling, a computationally efficient continuous optimization algorithm for finding the least distortion embedding of one surface into another. The generalized multidimensional scaling algorithm allows for both full and partial surface matching. As an example, it is applied to the problem of expression-invariant three-dimensional face recognition.', conference=None, journal=None, year=None, reference_list=['10.1007/s11263-005-1085-y', '10.1007/BF01427149', '10.1109/TPAMI.2003.1233902', '10.1109/34.35506', '10.1007/978-1-4757-2711-1', '10.1126/science.290.5500.2323', '10.1073/pnas.1031596100', '10.1007/BF01299740', '10.1007/s10208-004-0145-y', '10.1090/gsm/033', '10.1007/11408031_53', '10.1080/10556780008805795', '10.1073/pnas.95.15.8431', '10.1007/978-0-387-21637-9', '10.1109/83.623193', '10.1109/TPAMI.2005.70', '10.1007/3-540-44887-X_6'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'machine learning', 'bioinformatics', 'computational chemistry', 'data science'], conference_acronym='Proceedings of the National Academy of Sciences of the United States of America', publisher=None, query_handler=None),\n",
       " 'On bending invariant signatures for surfaces': Paper(DOI='10.1109/tpami.2003.1233902', crossref_json=None, google_schorlar_metadata=None, title='On bending invariant signatures for surfaces', authors=['Asi Elad', 'Ron Kimmel'], abstract='Isometric surfaces share the same geometric structure, also known as the \"first fundamental form.\" For example, all possible bendings of a given surface that includes all length preserving deformations without tearing or stretching the surface are considered to be isometric. We present a method to construct a bending invariant signature for such surfaces. This invariant representation is an embedding of the geometric structure of the surface in a small dimensional Euclidean space in which geodesic distances are approximated by Euclidean ones. The bending invariant representation is constructed by first measuring the intergeodesic distances between uniformly distributed points on the surface. Next, a multidimensional scaling technique is applied to extract coordinates in a finite dimensional Euclidean space in which geodesic distances are replaced by Euclidean ones. Applying this transform to various surfaces\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1073/pnas.95.15.8431', '10.1109/PROC.1984.13073', '10.1016/B978-0-12-266722-0.50006-3', '10.4135/9781412985130', '10.5244/C.9.50', '10.1109/5.5966', '10.1145/383259.383282', '10.1109/34.993552', '10.1109/34.615444', '10.1049/cp:19950626', '10.1145/151254.151255', '10.1109/34.385980', '10.1016/S0031-2023(97)00122-2', '10.1109/SMA.2001.923386', '10.1145/4078.4081', '10.1109/9.412624', '10.1145/267734.267771', '10.1017/S0962492900002671', '10.1109/34.35506', '10.1007/978-1-4757-2711-1', '10.1109/SMA.1999.749322', '10.1145/223784.223812', '10.1109/CVPR.2001.990472', '10.1109/83.623193', '10.1109/2945.998671'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image processing', 'computer vision', 'shape analysis', 'medical imaging', 'metric geometry'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Expression-invariant 3D face recognition': Paper(DOI='10.1117/2.1200811.1366', crossref_json=None, google_schorlar_metadata=None, title='Expression-invariant 3D face recognition', authors=['Alexander M Bronstein', 'Michael M Bronstein', 'Ron Kimmel'], abstract=' We present a novel 3D face recognition approach based on geometric invariants introduced by Elad and Kimmel. The key idea of the proposed algorithm is a representation of the facial surface, invariant to isometric deformations, such as those resulting from different expressions and postures of the face. The obtained geometric invariants allow mapping 2D facial texture images into special images that incorporate the 3D geometry of the face. These signature images are then decomposed into their principal components. The result is an efficient and accurate face recognition algorithm that is robust to facial expressions. We demonstrate the results of our method and compare it to existing 2D and 3D face recognition algorithms.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'machine learning', 'bioinformatics', 'computational chemistry', 'data science'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Finding shortest paths on surfaces using level sets propagation': Paper(DOI='10.1109/34.387512', crossref_json=None, google_schorlar_metadata=None, title='Finding shortest paths on surfaces using level sets propagation', authors=['Ron Kimmel', 'Arnon Amir', 'Alfred M.  Bruckstein'], abstract='We present a new algorithm for determining minimal length paths between two regions on a three dimensional surface. The numerical implementation is based on finding equal geodesic distance contours from a given area. These contours are calculated as zero sets of a bivariate function designed to evolve so as to track the equal distance curves on the given surface. The algorithm produces all paths of minimal length between the source and destination areas on the surface given as height values on a rectangular grid.< >', conference=None, journal=None, year=None, reference_list=['10.1016/0031-3203(93)90142-J', '10.1016/0021-9991(88)90002-2', '10.1007/BF01210742', '10.1016/0010-4485(93)90040-U', '10.1016/0898-1221(94)00228-D', '10.1016/0031-3203(93)90018-R', '10.1137/0728049', '10.1137/0727053', '10.1002/int.4550020204', '10.1016/0021-9991(92)90229-R', '10.1080/10586458.1993.10504566', '10.1007/978-1-4613-9583-6_2', '10.1109/34.387512'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Neuromorphic Computing', 'Eye Tracking', 'Video Search', 'Video Archives'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'What energy functions can be minimized via graph cuts?': Paper(DOI='10.1109/tpami.2004.1262177', crossref_json=None, google_schorlar_metadata=None, title='What energy functions can be minimized via graph cuts?', authors=['Vladimir Kolmogorov', 'Ramin Zabih'], abstract='In the last few years, several new algorithms based on graph cuts have been developed to solve energy minimization problems in computer vision. Each of these techniques constructs a graph such that the minimum cut on the graph also minimizes the energy. Yet, because these graph constructions are complex and highly specific to a particular energy function, graph cuts have seen limited application to date. In this paper, we give a characterization of the energy functions that can be minimized by graph cuts. Our results are restricted to functions of binary variables. However, our work generalizes many previous constructions and is easily applicable to vision problems that involve large numbers of labels, such as stereo, motion, image restoration, and scene reconstruction. We give a precise characterization of what energy functions can be minimized using graph cuts, among the energy functions that can be written\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2003.1238389', '10.1109/ICCV.2001.937668', '10.1109/CVPR.1998.698598', '10.1145/502090.502096', '10.1109/TPAMI.2003.1233908', '10.1145/1201775.882264', '10.1109/TSE.1977.233840', '10.1007/3-540-44480-7_1', '10.1038/317314a0', '10.1109/TPAMI.1984.4767596', '10.1109/ICCV.1999.791261', '10.1007/3-540-61576-8_98', '10.1109/34.57681', '10.1109/TIP.2002.999675', '10.1007/BF00054836', '10.1007/978-4-431-66933-3', '10.1145/129712.129736', '10.1006/jctb.2000.1989', '10.1109/ICCV.2003.1238310', '10.1002/net.3230150206', '10.1023/A:1014573219977', '10.1109/CVPR.1998.698673', '10.1109/34.969114', '10.1109/ICCV.1998.710763', '10.1109/ICCV.2001.937505', '10.1007/3-540-44745-8_24', '10.1109/CVPR.2000.855839', '10.1007/978-3-642-97881-4', '10.1145/48014.61051', '10.1109/12.256461'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Medical Imaging'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Non-parametric local transforms for computing visual correspondence': Paper(DOI='10.1007/bfb0028345', crossref_json=None, google_schorlar_metadata=None, title='Non-parametric local transforms for computing visual correspondence', authors=['Ramin Zabih', 'John Woodfill'], abstract=' We propose a new approach to the correspondence problem that makes use of non-parametric local transforms as the basis for correlation. Non-parametric local transforms rely on the relative ordering of local intensity values, and not on the intensity values themselves. Correlation using such transforms can tolerate a significant number of outliers. This can result in improved performance near object boundaries when compared with conventional methods such as normalized correlation. We introduce two non-parametric local transforms: the rank transform, which measures local intensity, and the census transform, which summarizes local image structure. We describe some properties of these transforms, and demonstrate their utility on both synthetic and real data.', conference=None, journal=None, year=None, reference_list=['10.1109/21.44067', '10.1109/TPAMI.1984.4767596', '10.21236/AD0786720', '10.1016/0262-8856(85)90037-X', '10.1098/rspb.1978.0020', '10.1007/BF00127126', '10.1117/12.7973334', '10.1038/317314a0', '10.1117/12.970173'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Medical Imaging'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Comparing images using joint histograms': Paper(DOI='10.1007/s005300050125', crossref_json=None, google_schorlar_metadata=None, title='Comparing images using joint histograms', authors=['Greg Pass', 'Ramin Zabih'], abstract=\"  Color histograms are widely used for content-based image retrieval due to their efficiency and robustness. However, a color histogram only records an image's overall color composition, so images with very different appearances can have similar color histograms. This problem is especially critical in large image databases, where many images have similar color histograms. In this paper, we propose an alternative to color histograms called a joint histogram, which incorporates additional information without sacrificing the robustness of color histograms. We create a joint histogram by selecting a set of local pixel features and constructing a multidimensional histogram. Each entry in a joint histogram contains the number of pixels in the image that are described by a particular combination of feature values. We describe a number of different joint histograms, and evaluate their performance for image retrieval on\\xa0…\", conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Medical Imaging'], conference_acronym='Multimedia systems', publisher=None, query_handler=None),\n",
       " 'A feature-based algorithm for detecting and classifying production effects': Paper(DOI='10.1007/s005300050115', crossref_json=None, google_schorlar_metadata=None, title='A feature-based algorithm for detecting and classifying production effects', authors=['Ramin Zabih', 'Justin Miller', 'Kevin Mai'], abstract='  We describe a new approach to the detection and classification of production effects in video sequences. Our method can detect and classify a variety of effects, including cuts, fades, dissolves, wipes and captions, even in sequences involving significant motion. We detect the appearance of intensity edges that are distant from edges in the previous frame. A global motion computation is used to handle camera or object motion. The algorithm we propose withstands JPEG and MPEG artifacts, even at high compression rates. Experimental evidence demonstrates that our method can detect and classify production effects that are difficult to detect with previous approaches.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Medical Imaging'], conference_acronym='Multimedia systems', publisher=None, query_handler=None),\n",
       " 'Multi-camera scene reconstruction via graph cuts': Paper(DOI='10.1007/3-540-47977-5_6', crossref_json=None, google_schorlar_metadata=None, title='Multi-camera scene reconstruction via graph cuts', authors=['Vladimir Kolmogorov', 'Ramin Zabih'], abstract=' We address the problem of computing the 3-dimensional shape of an arbitrary scene from a set of images taken at known viewpoints. Multi-camera scene reconstruction is a natural generalization of the stereo matching problem. However, it is much more difficult than stereo, primarily due to the difficulty of reasoning about visibility. In this paper, we take an approach that has yielded excellent results for stereo, namely energy minimization via graph cuts. We first give an energy minimization formulation of the multi-camera scene reconstruction problem. The energy that we minimize treats the input images symmetrically, handles visibility properly, and imposes spatial smoothness while preserving discontinuities. As the energy function is NP-hard to minimize exactly, we give a graph cut algorithm that computes a local minimum in a strong sense. We handle all camera configurations where voxel coloring can be\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF00054836', '10.1109/ICCV.1999.791261', '10.1109/34.677269', '10.1007/3-540-44745-8_24', '10.1109/34.969114', '10.1007/BF00129682', '10.1007/BFb0055679', '10.1515/9781400875184', '10.1109/TPAMI.1984.4767596', '10.1007/BFb0055670', '10.1007/3-540-47977-5_5', '10.1023/A:1008191222954', '10.1109/34.273735', '10.1109/TPAMI.1983.4767367', '10.1038/317314a0', '10.1023/A:1008176507526', '10.1006/cviu.1993.1030', '10.1007/3-540-44480-7_1'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Medical Imaging'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Method and system for comparing data objects using joint histograms': Paper(DOI='10.1007/s005300050125', crossref_json=None, google_schorlar_metadata=None, title='Method and system for comparing data objects using joint histograms', authors=['Ramin D Zabih', 'Gregory S Pass'], abstract='A joint histogram is used for content-based data object comparison. For a data object which can be described by a plurality of features and values, a set of features describing the data object is selected. A joint histogram is a k-dimensional vector, such that each entry in the joint histogram contains the number of data points in the data object that are described by a k-tuple of feature values. The data object is analyzed according to the set of features and the results entered into the joint histogram. The joint histogram distinguishes the data object from other data objects. Joint histogram comparisons may be used to identify the similarity between two or more particular data objects.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Medical Imaging'], conference_acronym='Multimedia systems', publisher=None, query_handler=None),\n",
       " 'An experimental comparison of stereo algorithms': Paper(DOI='10.1007/3-540-44480-7_1', crossref_json=None, google_schorlar_metadata=None, title='An experimental comparison of stereo algorithms', authors=['Richard Szeliski', 'Ramin Zabih'], abstract=' While many algorithms for computing stereo correspondence have been proposed, there has been very little work on experimentally evaluating algorithm performance, especially using real (rather than synthetic) imagery. In this paper we propose an experimental comparison of several different stereo algorithms. We use real imagery, and explore two different methodologies, with different strengths and weaknesses. Our first methodology is based upon manual computation of dense ground truth. Here we make use of a two stereo pairs: one of these, from the University of Tsukuba, contains mostly fronto-parallel surfaces; while the other, which we built, is a simple scene with a slanted surface. Our second methodology uses the notion of prediction error, which is the ability of a disparity map to predict an (unseen) third image, taken from a known camera position with respect to the input pair. We present results\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF00054836', '10.1007/BF01420984', '10.1109/34.23109', '10.1109/ICCV.1999.791245', '10.1145/146370.146374', '10.1006/cviu.1996.0040', '10.1109/21.44067', '10.1109/34.134040', '10.1109/TPAMI.1984.4767596', '10.1109/34.121790', '10.1007/BFb0028349', '10.1007/BFb0055670', '10.1002/0471725382', '10.1023/A:1008015117424', '10.1109/ICCV.1999.790301', '10.1016/0167-8655(85)90024-8'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Medical Imaging'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Pointcontrast: Unsupervised pre-training for 3d point cloud understanding': Paper(DOI='10.1007/978-3-030-58580-8_34', crossref_json=None, google_schorlar_metadata=None, title='Pointcontrast: Unsupervised pre-training for 3d point cloud understanding', authors=['Saining Xie', 'Jiatao Gu', 'Demi Guo', 'Charles R Qi', 'Leonidas Guibas', 'Or Litany'], abstract=' Arguably one of the top success stories of deep learning is transfer learning. The finding that pre-training a network on a rich source set (e.g., ImageNet) can help boost performance once fine-tuned on a usually much smaller target set, has been instrumental to many applications in language and vision. Yet, very little is known about its usefulness in 3D point cloud understanding. We see this as an opportunity considering the effort required for annotating data in 3D. In this work, we aim at facilitating research on 3D representation learning. Different from previous works, we focus on high-level scene understanding tasks. To this end, we select a suit of diverse datasets and tasks to measure the effect of unsupervised pre-training on a large source set of 3D scenes. Our findings are extremely encouraging: using a unified triplet of architecture, source dataset, and contrastive loss for pre-training, we achieve\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2016.170', '10.1016/j.cag.2020.02.005', '10.1007/978-3-030-01264-9_9', '10.1109/CVPR.2017.502', '10.1109/CVPR.2019.00319', '10.1109/ICCV.2019.00905', '10.1109/CVPR.2017.261', '10.1007/978-3-030-01228-1_37', '10.1109/ICCV.2015.167', '10.1109/CVPR.2017.265', '10.1007/978-3-030-01234-2_7', '10.1177/0278364913491297', '10.1109/ICCV.2019.00649', '10.1109/CVPR.2018.00961', '10.1109/CVPR.2018.00030', '10.1109/ICCV.2019.00825', '10.1109/CVPR42600.2020.00975', '10.1109/ICCV.2019.00502', '10.1109/CVPR.2016.90', '10.1109/CVPR.2019.00455', '10.1109/CVPR.2018.00109', '10.1007/978-1-4471-4640-7_8', '10.1109/ICCV.2017.99', '10.1109/CVPR.2018.00979', '10.1109/CVPR.2015.7298965', '10.24963/ijcai.2021/653', '10.1109/CVPR42600.2020.00674', '10.1007/978-3-319-46466-4_5', '10.1109/CVPR.2016.278', '10.1109/CVPR42600.2020.00446', '10.1109/ICCV.2019.00937', '10.1109/CVPR.2018.00102', '10.1007/978-3-319-24574-4_28', '10.1109/CVPR.2016.352', '10.1109/CVPR.2018.00478', '10.1007/978-3-642-33715-4_54', '10.1109/CVPR.2015.7298655', '10.1109/CVPR.2016.94', '10.1109/CVPR.2018.00268', '10.1145/3240508.3240621', '10.1109/CVPR.2018.00275', '10.1007/978-3-030-01225-0_4', '10.1109/CVPR.2018.00274', '10.1109/ICCV.2019.00362', '10.1145/3326362', '10.1109/CVPR.2018.00393', '10.1109/ICCV.2013.458', '10.1109/CVPR.2017.634', '10.1109/CVPR.2018.00484', '10.1007/978-3-030-01237-3_6', '10.1109/CVPR.2018.00029', '10.1109/WACV48630.2021.00018', '10.1145/2980179.2980238', '10.1109/CVPR.2017.697', '10.1109/CVPR.2019.00407', '10.1109/CVPR.2017.29', '10.1007/978-3-030-11015-4_24', '10.1007/978-3-319-46487-9_40', '10.1109/CVPR.2017.76', '10.1109/ICCV.2019.00169', '10.1109/CVPR.2019.00110', '10.1109/ICCV.2019.00610'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Computer Vision', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Joint embeddings of shapes and images via cnn image purification': Paper(DOI='10.1145/2816795.2818071', crossref_json=None, google_schorlar_metadata=None, title='Joint embeddings of shapes and images via cnn image purification', authors=['Yangyan Li', 'Hao Su', 'Charles Ruizhongtai Qi', 'Noa Fish', 'Daniel Cohen-Or', 'Leonidas J Guibas'], abstract='Both 3D models and 2D images contain a wealth of information about everyday objects in our environment. However, it is difficult to semantically link together these two media forms, even when they feature identical or very similar objects. We propose a joint embedding space populated by both 3D shapes and 2D images of objects, where the distances between embedded entities reflect similarity between the underlying objects. This joint embedding space facilitates comparison between entities of either form, and allows for cross-modality retrieval. We construct the embedding space using 3D shape similarity measure, as 3D shapes are more pure and complete than their appearance in images, leading to more robust distance metrics. We then employ a Convolutional Neural Network (CNN) to \"purify\" images by muting distracting factors. The CNN is trained to map an image to a point in the embedding space, so\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2014.487', '10.1111/cgf.12547', '10.1145/2766959', '10.1145/1057432.1057449', '10.1111/1467-8659.00669', '10.1109/CVPR.2013.12', '10.1109/CVPR.2005.177', '10.1145/588272.588279', '10.1109/TVCG.2007.45', '10.1109/CVPR.2006.100', '10.1109/ICCV.2005.167', '10.1111/cgf.12703', '10.1145/383259.383282', '10.1145/2508363.2508364', '10.1145/2766890', '10.1145/2601097.2601209', '10.1007/BF02289565', '10.1038/nature14539', '10.1007/BF01589116', '10.1016/j.patcog.2006.04.045', '10.5555/518910.850486', '10.1504/IJCAT.2005.006466', '10.1145/571647.571648', '10.1126/science.290.5500.2323', '10.1109/T-C.1969.222678', '10.1007/s11263-013-0636-x', '10.1109/34.895972', '10.1145/2601097.2601159', '10.1109/ICCV.2015.308', '10.1109/CVPR.2015.7298594', '10.1007/s11042-007-0181-0', '10.1109/CVPR.2010.5540018', '10.1007/s10994-010-5198-3', '10.5591/978-1-57735-516-8/IJCAI11-460', '10.1145/2010324.1964975', '10.1145/2185520.2185595'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Computer Vision', 'Machine Learning'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " 'Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks.': Paper(DOI='10.1016/j.neunet.2023.10.012', crossref_json=None, google_schorlar_metadata=None, title='Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks.', authors=['Zhihao Jia', 'Sina Lin', 'Charles R Qi', 'Alex Aiken'], abstract='The past few years have witnessed growth in the computational requirements for training deep convolutional neural networks. Current approaches parallelize training onto multiple devices by applying a single parallelization strategy (eg, data or model parallelism) to all layers in a network. Although easy to reason about, these approaches result in suboptimal runtime performance in largescale distributed training, since different layers in a network may prefer different parallelization strategies. In this paper, we propose layer-wise parallelism that allows each layer in a network to use an individual parallelization strategy. We jointly optimize how each layer is parallelized by solving a graph search problem. Our evaluation shows that layer-wise parallelism outperforms state-of-the-art approaches by increasing training throughput, reducing communication costs, achieving better scalability to multiple GPUs, while maintaining original network accuracy.', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2012.230', '10.1162/neco.1989.1.4.541', '10.1080/00029890.2007.11920484', '10.1109/CVPR.2017.758'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Computer Vision', 'Machine Learning'], conference_acronym='Neural networks (Print)', publisher=None, query_handler=None),\n",
       " 'Transformational leadership and performance across criteria and levels: A meta-analytic review of 25 years of research': Paper(DOI='10.1177/1059601111401017', crossref_json=None, google_schorlar_metadata=None, title='Transformational leadership and performance across criteria and levels: A meta-analytic review of 25 years of research', authors=['Gang Wang', 'In-Sue Oh', 'Stephen H Courtright', 'Amy E Colbert'], abstract='Although transformational leadership has been studied extensively, the magnitude of the relationship between transformational leadership and follower performance across criterion types and levels of analysis remains unclear. Based on 117 independent samples over 113 primary studies, the current meta-analytic study showed that transformational leadership was positively related to individual-level follower performance across criterion types, with a stronger relationship for contextual performance than for task performance across most study settings. In addition, transformational leadership was positively related to performance at the team and organization levels. Moreover, both meta-analytic regression and relative importance analyses consistently showed that transformational leadership had an augmentation effect over transactional leadership (contingent reward) in predicting individual-level contextual\\xa0…', conference=None, journal=None, year=None, reference_list=['10.5465/AMJ.2006.20785800', '10.1037/0021-9010.77.6.836', '10.2307/257094', '10.1521/jscp.1986.4.3.359', '10.1037/0021-9010.81.6.827', '10.1111/j.1744-6570.1991.tb00688.x', '10.2307/3094912', '10.1037/0003-066X.52.2.130', '10.1037/0021-9010.88.2.207', '10.1111/j.1559-1816.1997.tb00643.x', '10.1037/0021-9010.83.1.43', '10.1177/10717919070130030201', '10.2307/30040649', '10.1037/0021-9010.89.5.901', '10.1108/01437739910292607', '10.1037/0021-9010.91.4.954', '10.1037/0033-2909.114.3.542', '10.1037/0021-9010.80.4.468', '10.1177/107179190601200305', '10.1037/0021-9010.83.2.234', '10.1016/j.leaqua.2009.06.003', '10.1016/j.leaqua.2010.03.006', '10.5465/AMJ.2008.30717744', '10.1111/j.1744-6570.2006.00045.x', '10.1002/job.4030150508', '10.4135/9781452204932', '10.1037/0033-2909.129.1.3', '10.5465/amr.2009.0486', '10.1177/1059601106291131', '10.1037/h0025471', '10.1080/13594320444000164', '10.1111/j.1936-4490.2000.tb00234.x', '10.1111/j.1559-1816.1995.tb02638.x', '10.1037/a0012716', '10.1016/j.leaqua.2006.02.002', '10.1016/j.jbusvent.2005.04.006', '10.2466/pr0.1996.78.1.271', '10.1108/09534810810856435', '10.1037/0021-9010.86.3.513', '10.1111/j.1464-0597.1998.tb00035.x', '10.5465/AMJ.2009.43670890', '10.1002/job.111', '10.1108/09578230410534667', '10.1037/0021-9010.87.5.819', '10.1016/j.jbusres.2007.07.032', '10.5465/AMJ.2006.20786077', '10.1037/0021-9010.87.2.268', '10.1037/0021-9010.73.4.695', '10.1037/0021-9010.90.3.509', '10.1177/014920639702300306', '10.5465/AME.1992.4274395', '10.1037/0021-9010.78.6.891', '10.1037/0021-9010.84.5.680', '10.1016/j.indmarman.2005.12.002', '10.4135/9781412985031', '10.1037/0021-9010.89.5.755', '10.1207/S15326934CRJ1302_6', '10.1016/S1048-9843(03)00050-X', '10.1016/j.leaqua.2008.07.007', '10.1037/0021-9010.88.2.246', '10.1037/a0013077', '10.1037/0021-9010.91.1.202', '10.5465/AMJ.2009.43669971', '10.5465/amr.1994.9410210745', '10.1016/1048-9843(95)90034-9', '10.1016/S1048-9843(02)00103-0', '10.1002/job.4030160404', '10.1037/0021-9010.87.1.131', '10.2307/20159559', '10.1177/1094428107302900', '10.1037/0021-9010.92.4.1006', '10.1016/j.leaqua.2008.01.006', '10.1037/0021-9010.89.4.610', '10.1037/0021-9010.93.4.923', '10.1016/S1048-9843(96)90027-2', '10.1177/03079459994506', '10.1177/0149206308316061', '10.1016/S1048-9843(02)00143-1', '10.1177/1742715007082966', '10.2307/256657', '10.1037/1089-2699.6.2.172', '10.1177/0149206307312512', '10.5465/AMJ.2006.20786079', '10.1177/014920639902500606', '10.1037/0021-9010.82.2.262', '10.1016/j.obhdp.2005.09.002', '10.1037/0021-9010.88.5.879', '10.1016/1048-9843(90)90009-7', '10.1037/a0013079', '10.1207/s15327043hup1901_1', '10.1002/job.329', '10.1177/01461672972310008', '10.1037/0021-9010.87.1.66', '10.1016/j.leaqua.2007.01.003', '10.1037/0021-9010.92.4.1020', '10.1177/1094428107303161', '10.1016/j.leaqua.2005.10.008', '10.1287/orsc.4.4.577', '10.2307/30040662', '10.1037/0021-9010.92.6.1709', '10.1016/j.leaqua.2005.01.002', '10.1037/0033-2909.87.2.245', '10.1111/j.1744-6570.1999.tb00173.x', '10.1016/j.leaqua.2004.02.010', '10.1037/0021-9010.90.1.25', '10.1348/096317907X202482', '10.1108/00483480710773981', '10.1111/j.1744-6570.1995.tb01784.x', '10.1177/105960119001500404', '10.1016/j.leaqua.2004.02.013', '10.2307/3069341', '10.1111/j.1467-6486.2006.00642.x', '10.5465/amr.1999.1893936', '10.1111/j.1744-6570.2008.00131.x', '10.5465/AMJ.2005.17407908', '10.2307/256941', '10.1111/j.1744-6570.2009.01162.x', '10.1016/j.leaqua.2004.07.001', '10.1177/014920639101700305', '10.1016/S1048-9843(98)90042-X', '10.1108/02683940610684409', '10.1016/j.leaqua.2005.09.002', '10.1111/j.1744-6570.1994.tb01576.x', '10.2307/257027', '10.1016/S1048-9843(98)90041-8', '10.1177/1059601108331242', '10.1016/j.leaqua.2004.06.001', '10.1002/job.130'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Structural Materials'], conference_acronym='Group & organization management', publisher=None, query_handler=None),\n",
       " 'Antecedents and consequences of psychological and team empowerment in organizations: a meta-analytic review.': Paper(DOI='10.1037/a0022676.supp', crossref_json=None, google_schorlar_metadata=None, title='Antecedents and consequences of psychological and team empowerment in organizations: a meta-analytic review.', authors=['Scott E Seibert', 'Gang Wang', 'Stephen H Courtright'], abstract='This paper provides meta-analytic support for an integrated model specifying the antecedents and consequences of psychological and team empowerment. Results indicate that contextual antecedent constructs representing perceived high-performance managerial practices, socio-political support, leadership, and work characteristics are each strongly related to psychological empowerment. Positive self-evaluation traits are related to psychological empowerment and are as strongly related as the contextual factors. Psychological empowerment is in turn positively associated with a broad range of employee outcomes, including job satisfaction, organizational commitment, and task and contextual performance, and is negatively associated with employee strain and turnover intentions. Team empowerment is positively related to team performance. Further, the magnitude of parallel antecedent and outcome\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Structural Materials'], conference_acronym='Journal of applied psychology', publisher=None, query_handler=None),\n",
       " 'Super plastic bulk metallic glasses at room temperature': Paper(DOI='10.1126/science.1136726', crossref_json=None, google_schorlar_metadata=None, title='Super plastic bulk metallic glasses at room temperature', authors=['Yan Hui Liu', 'Gang Wang', 'Ru Ju Wang', 'De Qian Zhao', 'Ming Xiang Pan', 'Wei Hua Wang'], abstract='In contrast to the poor plasticity that is usually observed in bulk metallic glasses, super plasticity is achieved at room temperature in ZrCuNiAl synthesized through the appropriate choice of its composition by controlling elastic moduli. Microstructures analysis indicates that the super plastic bulk metallic glasses are composed of hard regions surrounded by soft regions, which enable the glasses to undergo true strain of more than 160%. This finding is suggestive of a solution to the problem of brittleness in, and has implications for understanding the deformation mechanism of, metallic glasses.', conference=None, journal=None, year=None, reference_list=['10.1126/science.1086636', '10.1016/j.actamat.2003.08.032', '10.1126/science.267.5206.1947', '10.1103/PhysRevB.64.180201', '10.1126/science.1123889', '10.1103/PhysRevLett.94.205501', '10.1080/09500830500395237', '10.1103/PhysRevLett.96.105503', '10.1103/PhysRevLett.93.255506', '10.1038/nmat797', '10.1080/09500830500080474', '10.1016/j.scriptamat.2006.05.029', '10.1063/1.2206149', '10.1063/1.2193060', '10.1016/S1359-6454(03)00164-2', '10.1063/1.119643', '10.1016/0001-6160(77)90232-2', '10.1063/1.1949467', '10.1007/s10853-005-2846-2', '10.1016/S0966-9795(02)00137-1', '10.1557/JMR.2002.0214', '10.2320/matertrans.42.642', '10.1016/j.intermet.2006.01.003', '10.1103/PhysRevB.60.9212', '10.1557/JMR.2003.0283', '10.1103/PhysRevLett.95.245501', '10.1016/0001-6160(79)90055-5', '10.1016/j.intermet.2004.04.033', '10.1103/PhysRevLett.96.245502', '10.1063/1.1289801', '10.1016/j.actamat.2004.11.040', '10.1038/nmat1758', '10.1016/j.actamat.2005.03.012'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Structural Materials'], conference_acronym='Science (New York, N.Y.)', publisher=None, query_handler=None),\n",
       " 'Evading the strength–ductility trade-off dilemma in steel through gradient hierarchical nanotwins': Paper(DOI='10.1038/ncomms4580', crossref_json=None, google_schorlar_metadata=None, title='Evading the strength–ductility trade-off dilemma in steel through gradient hierarchical nanotwins', authors=['Yujie Wei', 'Yongqiang Li', 'Lianchun Zhu', 'Yao Liu', 'Xianqi Lei', 'Gang Wang', 'Yanxin Wu', 'Zhenli Mi', 'Jiabin Liu', 'Hongtao Wang', 'Huajian Gao'], abstract='The strength–ductility trade-off has been a long-standing dilemma in materials science. This has limited the potential of many structural materials, steels in particular. Here we report a way of enhancing the strength of twinning-induced plasticity steel at no ductility trade-off. After applying torsion to cylindrical twinning-induced plasticity steel samples to generate a gradient nanotwinned structure along the radial direction, we find that the yielding strength of the material can be doubled at no reduction in ductility. It is shown that this evasion of strength–ductility trade-off is due to the formation of a gradient hierarchical nanotwinned structure during pre-torsion and subsequent tensile deformation. A series of finite element simulations based on crystal plasticity are performed to understand why the gradient twin structure can cause strengthening and ductility retention, and how sequential torsion and tension lead to the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.2355/isijinternational.43.438', '10.5772/14086', '10.1002/srin.200200212', '10.1016/S0749-6419(00)00015-2', '10.1051/metal:2005151', '10.1016/0956-7151(94)90493-6', '10.1038/nmat3115', '10.1038/nmat1141', '10.1038/nmat1035', '10.1016/j.pmatsci.2005.08.003', '10.1016/j.cossms.2011.04.002', '10.1126/science.1092905', '10.1126/science.1167641', '10.1126/science.1159610', '10.1016/S1359-6454(02)00594-3', '10.1126/science.1200177', '10.1038/ncomms1062', '10.1016/j.actamat.2012.03.035', '10.1016/j.scriptamat.2012.05.041', '10.1016/j.actamat.2009.06.047', '10.1038/nature08929', '10.1038/nature08692', '10.1063/1.4747333', '10.1016/j.actamat.2007.11.020', '10.1016/j.actamat.2012.02.026', '10.1016/0022-5096(67)90018-X', '10.1016/j.jmps.2013.08.007'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Structural Materials'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Inactivation of porcine endogenous retrovirus in pigs using CRISPR-Cas9': Paper(DOI='10.1111/xen.12363', crossref_json=None, google_schorlar_metadata=None, title='Inactivation of porcine endogenous retrovirus in pigs using CRISPR-Cas9', authors=['Dong Niu', 'Hong-Jiang Wei', 'Lin Lin', 'Haydy George', 'Tao Wang', 'I-Hsiu Lee', 'Hong-Ye Zhao', 'Yong Wang', 'Yinan Kan', 'Ellen Shrock', 'Emal Lesha', 'Gang Wang', 'Yonglun Luo', 'Yubo Qing', 'Deling Jiao', 'Heng Zhao', 'Xiaoyang Zhou', 'Shouqi Wang', 'Hong Wei', 'Marc Güell', 'George M Church', 'Luhan Yang'], abstract='Xenotransplantation is a promising strategy to alleviate the shortage of organs for human transplantation. In addition to the concerns about pig-to-human immunological compatibility, the risk of cross-species transmission of porcine endogenous retroviruses (PERVs) has impeded the clinical application of this approach. We previously demonstrated the feasibility of inactivating PERV activity in an immortalized pig cell line. We now confirm that PERVs infect human cells, and we observe the horizontal transfer of PERVs among human cells. Using CRISPR-Cas9, we inactivated all of the PERVs in a porcine primary cell line and generated PERV-inactivated pigs via somatic cell nuclear transfer. Our study highlights the value of PERV inactivation to prevent cross-species viral transmission and demonstrates the successful production of PERV-inactivated animals to address the safety concern in clinical xenotransplantation.', conference=None, journal=None, year=None, reference_list=['10.1126/science.aan4187', '10.1126/science.aad1191', '10.1111/xen.12210', '10.1128/JVI.74.1.49-56.2000', '10.1016/S0042-6822(03)00428-8', '10.1038/nri1489', '10.1111/j.1600-6143.2010.03146.x', '10.1007/s00705-012-1490-9', '10.1128/JVI.75.12.5465-5472.2001', '10.1128/JVI.78.5.2494-2501.2004', '10.1186/1743-422X-3-91', '10.1128/JVI.78.24.13880-13890.2004', '10.1007/s00705-008-0141-7', '10.1128/JVI.78.16.8868-8877.2004', '10.1016/j.virol.2005.10.021', '10.3727/096368908787648056', '10.1111/j.1399-3089.2009.00515.x', '10.3390/v8080215'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Structural Materials'], conference_acronym='Xenotransplantation (København)', publisher=None, query_handler=None),\n",
       " 'Blautia—a new functional genus with potential probiotic properties?': Paper(DOI='10.1080/19490976.2021.1875796', crossref_json=None, google_schorlar_metadata=None, title='Blautia—a new functional genus with potential probiotic properties?', authors=['Xuemei Liu', 'Bingyong Mao', 'Jiayu Gu', 'Jiaying Wu', 'Shumao Cui', 'Gang Wang', 'Jianxin Zhao', 'Hao Zhang', 'Wei Chen'], abstract='Blautia is a genus of anaerobic bacteria with probiotic characteristics that occur widely in the feces and intestines of mammals. Based on phenotypic and phylogenetic analyses, some species in the genera Clostridium and Ruminococcus have been reclassified as Blautia, so to date, there are 20 new species with valid published names in this genus. An extensive body of research has recently focused on the probiotic effects of this genus, such as biological transformation and its ability to regulate host health and alleviate metabolic syndrome. This article reviews the origin and biological characteristics of Blautia and the factors that affect its abundance and discusses its role in host health, thus laying a theoretical foundation for the development of new functional microorganisms with probiotic properties.', conference=None, journal=None, year=None, reference_list=['10.1016/j.vetmic.2013.01.030', '10.3390/d5030627', '10.1007/s12088-018-0746-9', '10.3923/ijds.2017.47.51', '10.1186/s12866-016-0708-5', '10.1038/srep08397', '10.3390/nu10081105', '10.1021/jf504074n', '10.1111/1462-2920.14454', '10.1126/science.1110591', '10.1099/ijs.0.65208-0', '10.1099/00207713-26-4-482', '10.1099/00207713-24-2-260', '10.1007/s002030050373', '10.1078/0723-2020-00112', '10.1099/00207713-44-1-130', '10.1016/0378-1097(96)00195-4', '10.1007/s00203-010-0566-8', '10.1099/ijs.0.031625-0', '10.1099/ijs.0.036541-0', '10.1099/00207713-26-2-238', '10.1038/nmicrobiol.2016.131', '10.1016/j.anaerobe.2016.12.001', '10.1016/j.nmni.2017.05.017', '10.1016/j.nmni.2017.02.004', '10.1016/j.nmni.2017.05.005', '10.1099/ijsem.0.002623', '10.1099/ijsem.0.002981', '10.1016/j.nmni.2019.100648', '10.1099/ijsem.0.004015', '10.1038/nature09944', '10.3920/BM2014.0133', '10.1007/BF00456709', '10.1128/AEM.47.5.961-964.1984', '10.1371/journal.pone.0109999', '10.3389/fmicb.2020.00253', '10.1016/j.gde.2005.09.006', '10.1073/pnas.1523199113', '10.1016/j.gene.2017.04.019', '10.3168/jds.2019-18049', '10.1016/j.gpb.2017.04.005', '10.1099/mic.0.27008-0', '10.1016/j.procbio.2019.04.009', '10.1002/jsfa.3942', '10.1016/j.ygeno.2019.06.016', '10.1016/j.micpath.2020.104423', '10.1016/j.gdata.2015.08.007', '10.1016/j.phrs.2012.10.020', '10.1186/s40064-016-2950-6', '10.1371/journal.pone.0146144', '10.1017/S0007114514003274', '10.1039/C4FO00731J', '10.1111/1750-3841.13391', '10.1263/jbb.99.548', '10.1155/2016/3089303', '10.1002/mnfr.201500815', '10.1016/j.anaerobe.2014.06.006', '10.1016/j.vetmic.2011.05.021', '10.1186/s12866-016-0895-0', '10.1016/j.micpath.2019.05.014', '10.1016/j.arr.2017.01.001', '10.1038/ncomms4654', '10.1038/s41598-017-17194-4', '10.3389/fmicb.2017.01162', '10.1038/ismej.2015.11', '10.1038/ng.3663', '10.1038/ismej.2014.97', '10.1038/nature11552', '10.1248/bpb.35.686', '10.1007/s13765-012-2175-5', '10.1016/j.jep.2009.04.059', '10.1021/jf060234n', '10.3390/molecules21091158', '10.1021/acs.jafc.7b00943', '10.1158/1078-0432.CCR-06-1839', '10.1007/s12640-015-9558-4', '10.1016/j.neuint.2014.10.008', '10.1021/jf403924c', '10.2147/ceg.S186097', '10.1080/19490976.2016.1158395', '10.1111/1574-6968.12348', '10.1111/1462-2920.12864', '10.1016/j.tifs.2017.07.010', '10.1111/j.1574-6941.2010.00863.x', '10.1016/j.biotechadv.2012.12.009', '10.1038/nrmicro1273', '10.1093/nar/gkt449', '10.1186/s12864-018-4809-4', '10.1016/j.biotechadv.2013.01.010', '10.1016/j.apcbee.2012.06.010', '10.1016/j.synbio.2020.06.002', '10.1016/j.apsoil.2018.08.022', '10.1016/j.soilbio.2020.107838', '10.1128/aem.01223-15', '10.1099/mic.0.000515', '10.1016/j.chom.2017.04.002', '10.1038/s41586-019-1501-z', '10.1172/jci58109', '10.1038/s41522-019-0101-x', '10.1007/s10620-016-4179-1', '10.1002/hep.26093', '10.1016/j.jgr.2013.12.004', '10.1038/srep14405', '10.1186/1741-7015-11-46', '10.3164/jcbn.17-44', '10.1128/mBio.02392-17', '10.1038/ncomms2852', '10.1128/mSystems.00857-19', '10.1016/j.cell.2020.05.036', '10.1016/j.bbmt.2019.11.008', '10.1016/j.bbmt.2015.04.016', '10.1371/journal.pone.0066934', '10.1016/j.jhep.2013.01.003', '10.4110/in.2014.14.6.277', '10.1053/j.gastro.2011.10.001', '10.1097/MD.0000000000000051', '10.1371/journal.pone.0039743', '10.1080/19490976.2017.1379637', '10.1073/pnas.0804812105', '10.1007/s00134-010-1826-4', '10.1053/j.gastro.2011.07.043', '10.1007/s00535-017-1384-4', '10.1080/01635581.2017.1263750', '10.1128/aem.72.5.3593-3599.2006', '10.1146/annurev-micro-090110-102844', '10.1038/nmicrobiol.2016.152'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Structural Materials'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Validity of observer ratings of the five-factor model of personality traits: a meta-analysis.': Paper(DOI='10.1037/a0021832', crossref_json=None, google_schorlar_metadata=None, title='Validity of observer ratings of the five-factor model of personality traits: a meta-analysis.', authors=['In-Sue Oh', 'Gang Wang', 'Michael K Mount'], abstract='Conclusions reached in previous research about the magnitude and nature of personality–performance linkages have been based almost exclusively on self-report measures of personality. The purpose of this study is to address this void in the literature by conducting a meta-analysis of the relationship between observer ratings of the five-factor model (FFM) personality traits and overall job performance. Our results show that the operational validities of FFM traits based on observer ratings are higher than those based on self-report ratings. In addition, the results show that when based on observer ratings, all FFM traits are significant predictors of overall performance. Further, observer ratings of FFM traits show meaningful incremental validity over self-reports of corresponding FFM traits in predicting overall performance, but the reverse is not true. We conclude that the validity of FFM traits in predicting overall\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Structural Materials'], conference_acronym='Journal of applied psychology (Online)', publisher=None, query_handler=None),\n",
       " 'Transformation-mediated ductility in CuZr-based bulk metallic glasses': Paper(DOI='10.1038/nmat2767', crossref_json=None, google_schorlar_metadata=None, title='Transformation-mediated ductility in CuZr-based bulk metallic glasses', authors=['S Pauly', 'S Gorantla', 'G Wang', 'U Kühn', 'J Eckert'], abstract='Bulk metallic glasses (BMGs) generally fail in a brittle manner under uniaxial, quasistatic loading at room temperature. The lack of plastic strain is a consequence of shear softening, a phenomenon that originates from shear-induced dilation that causes plastic strain to be highly localized in shear bands. So far, significant tensile ductility has been reported only for microscopic samples of around 100\\u2009nm (ref.\\xa0) as well as for high strain rates, and so far no mechanisms are known, which could lead to work hardening and ductility in quasistatic tension in macroscopic BMG samples. In the present work we developed CuZr-based BMGs, which polymorphically precipitate nanocrystals during tensile deformation and subsequently these nanocrystals undergo twinning. The formation of such structural heterogeneities hampers shear band generation and results in macroscopically detectable plastic strain and work hardening\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.actamat.2007.01.052', '10.1016/0001-6160(77)90232-2', '10.1016/j.scriptamat.2005.09.046', '10.1038/nmat1984', '10.1080/09500830902873575', '10.1557/jmr.2007.0050', '10.1016/j.actamat.2009.07.042', '10.1016/0956-716X(92)90153-6', '10.1016/j.scriptamat.2008.11.015', '10.1063/1.3222973', '10.1007/BF00551811', '10.1038/nmat1536', '10.1063/1.2956666', '10.1103/PhysRevLett.96.245502', '10.1038/367541a0', '10.1126/science.1067453', '10.1063/1.1672587', '10.1126/science.267.5206.1935', '10.1103/PhysRevLett.95.195501', '10.1557/mrs2007.127', '10.1063/1.478340', '10.1103/PhysRevLett.97.195501', '10.1103/PhysRevLett.95.245501', '10.1007/BF02647618', '10.1016/j.actamat.2009.05.011', '10.1557/JMR.2003.0382', '10.1557/JMR.1990.0286', '10.1016/S1359-6462(98)00078-5', '10.1126/science.1083727', '10.1126/science.1167641', '10.1016/j.actamat.2004.08.003', '10.1063/1.3183584', '10.1103/PhysRevLett.70.1120', '10.1016/0001-6160(79)90055-5', '10.1126/science.1149308', '10.1016/j.jallcom.2006.08.109', '10.1111/j.1151-2916.1977.tb15509.x'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Structural Materials'], conference_acronym='Nature materials (Print)', publisher=None, query_handler=None),\n",
       " 'Do CEOs matter to firm strategic actions and firm performance? A meta‐analytic investigation based on upper echelons theory': Paper(DOI='10.1111/peps.12140', crossref_json=None, google_schorlar_metadata=None, title='Do CEOs matter to firm strategic actions and firm performance? A meta‐analytic investigation based on upper echelons theory', authors=['Gang Wang', 'R Michael Holmes Jr', 'In‐Sue Oh', 'Weichun Zhu'], abstract=\"What roles do CEOs play in firm performance? To address this question, the management field has accumulated a substantial amount of research over the past 3 decades built on upper echelons theory (UET), which posits that CEO characteristics manifest in firm strategic actions and, in this way, future firm performance. Hence, there is a need to systematically amass and take stock of prior empirical findings for UET testing and development. We use meta‐analytic techniques to synthesize prior UET research on the relationships among commonly studied CEO characteristics, firm strategic actions, and future firm performance. Based on 308 studies, meta‐analytic results generally support UET's predictions with a few exceptions: CEO characteristics (i.e., tenure, formal education, prior career experience, and positive self‐concept) are significantly associated with firm strategic actions, which in turn are significantly\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.5465/amj.2011.0639', '10.5465/AMJ.2006.20785800', '10.1177/1094428110375720', '10.1146/annurev-orgpsych-031413-091231', '10.1177/1094428109333339', '10.1111/j.1467-6494.2005.00351.x', '10.1177/1088868306294907', '10.1177/1094428111403495', '10.2307/1556340', '10.1287/mnsc.48.6.782.187', '10.2307/1556340', '10.1111/j.1744-6570.1991.tb00688.x', '10.1037/0021-9010.78.1.111', '10.1111/j.1744-6570.2003.tb00143.x', '10.5465/amr.2010.0479', '10.1037/0021-9010.89.4.587', '10.2307/256364', '10.1002/smj.2338', '10.1162/003355303322552775', '10.2307/257060', '10.2307/3094914', '10.1111/j.1744-6570.1995.tb01772.x', '10.1037/0021-9010.89.5.901', '10.1002/smj.4250140805', '10.1111/j.1467-6486.1996.tb00814.x', '10.1037/a0038047', '10.1037/0003-066X.36.2.129', '10.1016/j.jm.2004.06.001', '10.1037/0022-006X.66.1.7', '10.1177/0149206311419661', '10.1037/0021-9010.90.5.928', '10.2189/asqu.52.3.351', '10.1177/0001839211427534', '10.1016/j.jcorpfin.2011.06.007', '10.1177/003803857200600101', '10.1287/orsc.1060.0192', '10.1111/peps.12036', '10.1016/j.leaqua.2012.03.004', '10.5465/AMJ.2010.48036305', '10.1111/j.1744-6570.2006.00045.x', '10.5465/AMR.1988.4306983', '10.5465/AMJ.2010.52814589', '10.1037/a0022147', '10.5465/amj.2012.0469', '10.1002/smj.897', '10.1002/(SICI)1097-0266(200004)21:4<515:AID-SMJ92>3.0.CO;2-1', '10.1177/1094428112470846', '10.1002/(SICI)1097-0266(199809)19:9<833::AID-SMJ971>3.0.CO;2-V', '10.2307/3069342', '10.2307/2785898', '10.1016/S0883-9026(98)00013-5', '10.1002/smj.817', '10.1002/smj.355', '10.1287/orsc.1070.0317', '10.1146/annurev.ps.41.020190.002221', '10.1111/j.1744-6570.2011.01229.x', '10.1037/0022-3514.71.2.390', '10.2307/2393314', '10.2307/20159597', '10.1037/a0013116', '10.1509/jmkg.65.2.67.18259', '10.1002/smj.4250120708', '10.5465/AMR.2007.24345254', '10.1002/smj.407', '10.5465/AMR.1991.4279621', '10.5465/AMR.1984.4277628', '10.1002/smj.2108', '10.1002/hrm.21617', '10.2307/2095567', '10.2307/2393810', '10.2307/3069339', '10.1111/j.1467-6486.2006.00610.x', '10.1037/0003-066X.52.12.1280', '10.1037/0003-066X.55.11.1217', '10.1002/smj.455', '10.2307/256948', '10.1002/smj.4250120502', '10.5465/AMJ.2009.41330755', '10.1016/S0742-3322(06)23017-1', '10.1177/0149206310394863', '10.1002/smj.394', '10.1177/014920639001600210', '10.1016/0001-8791(86)90013-8', '10.2307/3033676', '10.1002/smj.393', '10.1037/0021-9010.86.1.80', '10.1037/0021-9010.87.4.765', '10.1080/08959285.1998.9668030', '10.1037/0022-3514.83.3.693', '10.1111/j.1744-6570.2003.tb00152.x', '10.1037/0021-9010.87.4.797', '10.1037/a0013115', '10.1002/hrm.20367', '10.2307/256170', '10.2307/2391211', '10.1037/0021-9010.88.2.256', '10.1016/j.paid.2004.09.016', '10.5465/AMJ.2010.48036912', '10.1007/s10843-006-8000-7', '10.3386/w10807', '10.1002/smj.680', '10.1037/14359-000', '10.1177/0149206309345019', '10.1016/0883-9026(95)00081-X', '10.1086/226106', '10.1287/mnsc.37.1.34', '10.2307/255988', '10.2307/256459', '10.2307/256289', '10.1037/0021-9010.77.6.926', '10.5465/AMJ.2010.54533196', '10.1002/smj.417', '10.1111/j.1744-6570.2008.01130.x', '10.1002/(SICI)1097-0266(199707)18:1+<187::AID-SMJ936>3.0.CO;2-K', '10.1287/orsc.1100.0602', '10.2307/2666961', '10.1111/peps.12026', '10.1080/08959285.2014.913594', '10.1037/0021-9010.88.5.795', '10.1111/j.1744-6570.2012.01253.x', '10.1177/0149206307312512', '10.1287/orsc.1090.0471', '10.1177/0149206313512152', '10.1086/262055', '10.1111/j.1744-6570.1995.tb01785.x', '10.5465/AMR.1997.9707180259', '10.1037/0022-0663.74.2.166', '10.1002/smj.4250121003', '10.1037/0033-2909.124.2.165', '10.5465/AMR.2004.13670972', '10.1016/j.jcorpfin.2013.12.013', '10.5465/AMR.2003.10196776', '10.2307/3069434', '10.5465/AMJ.2007.28226158', '10.2307/2550441', '10.2307/30040610', '10.1002/smj.599', '10.1002/smj.800', '10.1037/0021-9010.81.6.619', '10.1111/j.1744-6570.1998.tb00728.x', '10.1002/smj.4250120704', '10.1177/014920630002600207', '10.1177/014920638401000108', '10.5465/AMR.1986.4283976', '10.2307/256605', '10.2307/256343', '10.1016/B978-012134645-4/50030-5', '10.1037/0022-3514.54.6.1063', '10.1037/0022-3514.76.5.820', '10.2307/2393674', '10.2307/256871', '10.1016/j.jfineco.2012.11.003', '10.2307/2393940', '10.1002/smj.689', '10.1002/smj.812', '10.1037/0021-9010.91.2.259'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Structural Materials'], conference_acronym='Personnel psychology', publisher=None, query_handler=None),\n",
       " 'Exceptionally high glass-forming ability of an FeCoCrMoCBY alloy': Paper(DOI='10.1063/1.1897426', crossref_json=None, google_schorlar_metadata=None, title='Exceptionally high glass-forming ability of an FeCoCrMoCBY alloy', authors=['Jun Shen', 'Qingjun Chen', 'Jianfei Sun', 'Hongbo Fan', 'Gang Wang'], abstract='It has been well documented that the maximum thickness of as-cast glassy samples attainable through conventional metallurgical routes is the decisive criteria for measuring the glass-forming ability (GFA) of bulk metallic glasses (BMGs). Here we report the exceptionally high GFA of an FeCoCrMoCBY alloy which can be fabricated in the form of glassy rods with a maximum sample thickness of at least 16 mm\\u2060. It is demonstrated that, by substituting Fe with a proper amount of Co in a previously reported Fe-based BMG alloy, the glass formation of the resultant new alloy can be extensively favored both thermodynamically and kinetically. The new ferrous BMG alloy also exhibits a high fracture strength of 3500 MPa and Vickers hardness of 1253 kg mm− 2\\u2060.', conference=None, journal=None, year=None, reference_list=['10.1016/S1359-6454(99)00300-6', '10.1063/1.110520', '10.1063/1.1616192', '10.1063/1.1614833', '10.1016/j.jnoncrysol.2004.07.056', '10.1063/1.360537', '10.1063/1.1643776', '10.1557/JMR.2000.0348', '10.1063/1.1544434', '10.1063/1.124273', '10.1063/1.1614833', '10.1103/PhysRevLett.92.245503', '10.1557/JMR.2004.0176', '10.1016/j.actamat.2004.05.022', '10.2320/matertrans.43.3222', '10.1103/PhysRevLett.92.245504', '10.1063/1.1614420', '10.1016/0022-3093(81)90006-5', '10.1063/1.1350624'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Structural Materials'], conference_acronym='Applied physics letters', publisher=None, query_handler=None),\n",
       " 'Antibacterial activity of large-area monolayer graphene film manipulated by charge transfer': Paper(DOI='10.1038/srep04359', crossref_json=None, google_schorlar_metadata=None, title='Antibacterial activity of large-area monolayer graphene film manipulated by charge transfer', authors=['Jinhua Li', 'Gang Wang', 'Hongqin Zhu', 'Miao Zhang', 'Xiaohu Zheng', 'Zengfeng Di', 'Xuanyong Liu', 'Xi Wang'], abstract='Graphene has attracted increasing attention for potential applications in biotechnology due to its excellent electronic property and biocompatibility. Here we use both Gram-positive Staphylococcus aureus (S. aureus) and Gram-negative Escherichia coli (E. coli) to investigate the antibacterial actions of large-area monolayer graphene film on conductor Cu, semiconductor Ge and insulator SiO2. The results show that the graphene films on Cu and Ge can surprisingly inhibit the growth of both bacteria, especially the former. However, the proliferation of both bacteria cannot be significantly restricted by the graphene film on SiO2. The morphology of S. aureus and E. coli on graphene films further confirms that the direct contact of both bacteria with graphene on Cu and Ge can cause membrane damage and destroy membrane integrity, while no evident membrane destruction is induced by graphene on SiO2. From the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1126/science.1102896', '10.1038/nmat1849', '10.1126/science.1158877', '10.1021/nn4009406', '10.1038/ncomms1767', '10.1021/nn200500h', '10.1038/srep02465', '10.1021/nn101097v', '10.1021/nn101390x', '10.1016/j.carbon.2011.12.035', '10.1021/nn101081t', '10.1002/adfm.201303040', '10.1038/nnano.2013.125', '10.1063/1.2982585', '10.1126/science.1171245', '10.1021/nn103028d', '10.1016/j.carbon.2012.04.048', '10.1016/j.cell.2011.01.015', '10.1073/pnas.0907468107', '10.1038/nature03661', '10.1038/382445a0', '10.1073/pnas.0900086106', '10.1039/TF9605601432', '10.1021/nn102767d', '10.1099/00221287-146-3-551', '10.1063/1.323539', '10.1063/1.2410241', '10.1021/nn800354m', '10.1063/1.363895', '10.1103/PhysRev.140.A569', '10.1073/pnas.1017200108', '10.1242/jeb.203.1.51', '10.1021/nn202451x', '10.1021/nl0726398', '10.1021/nl063020t', '10.1002/adma.201104110', '10.1103/PhysRevLett.97.187401', '10.1038/ncomms1067'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Structural Materials'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'C3N—A 2D Crystalline, Hole‐Free, Tunable‐Narrow‐Bandgap Semiconductor with Ferromagnetic Properties': Paper(DOI='10.1002/adma.201770110', crossref_json=None, google_schorlar_metadata=None, title='C3N—A 2D Crystalline, Hole‐Free, Tunable‐Narrow‐Bandgap Semiconductor with Ferromagnetic Properties', authors=['Siwei Yang', 'Wei Li', 'Caichao Ye', 'Gang Wang', 'He Tian', 'Chong Zhu', 'Peng He', 'Guqiao Ding', 'Xiaoming Xie', 'Yang Liu', 'Yeshayahu Lifshitz', 'Shuit‐Tong Lee', 'Zhenhui Kang', 'Mianheng Jiang'], abstract='Graphene has initiated intensive research efforts on 2D crystalline materials due to its extraordinary set of properties and the resulting host of possible applications. Here the authors report on the controllable large‐scale synthesis of C3N, a 2D crystalline, hole‐free extension of graphene, its structural characterization, and some of its unique properties. C3N is fabricated by polymerization of 2,3‐diaminophenazine. It consists of a 2D honeycomb lattice with a homogeneous distribution of nitrogen atoms, where both N and C atoms show a D6h‐symmetry. C3N is a semiconductor with an indirect bandgap of 0.39 eV that can be tuned to cover the entire visible range by fabrication of quantum dots with different diameters. Back‐gated field‐effect transistors made of single‐layer C3N display an on–off current ratio reaching 5.5 × 1010. Surprisingly, C3N exhibits a ferromagnetic order at low temperatures (<96 K) when\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Structural Materials'], conference_acronym='Advanced materials (Weinheim. Print)', publisher=None, query_handler=None),\n",
       " 'Two-dimensional locally resonant phononic crystals with binary structures': Paper(DOI='10.1103/physrevlett.93.154302', crossref_json=None, google_schorlar_metadata=None, title='Two-dimensional locally resonant phononic crystals with binary structures', authors=['Gang Wang', 'Xisen Wen', 'Jihong Wen', 'Lihui Shao', 'Yaozong Liu'], abstract='The lumped-mass method is applied to study the propagation of elastic waves in two-dimensional binary periodic systems, ie, periodic soft rubber/epoxy and vacuum/epoxy composites, for which the conventional methods fail or converge very slowly. A comprehensive study is performed for the two-dimensional binary locally resonant phononic crystals, which are composed of periodic soft rubber cylinders immersed in epoxy host. Numerical simulations predict that subfrequency gaps also appear because of the high contrast of mass density and elastic constant of the soft rubber. The locally resonant mechanism in forming the subfrequency gaps is thoroughly analyzed by studying the two-dimensional model and its quasi-one-dimensional mechanical analog. The rule used to judge whether a resonant mode in the phononic crystals can result in a corresponding subfrequency gap or not is found.', conference=None, journal=None, year=None, reference_list=['10.1126/science.289.5485.1734', '10.1038/378241a0', '10.1103/PhysRevB.65.165116', '10.1103/PhysRevB.67.144301', '10.1103/PhysRevLett.88.225502', '10.1103/PhysRevB.62.7387', '10.1103/PhysRevB.69.184302', '10.1016/j.physleta.2004.05.047', '10.7498/aps.52.1943', '10.1103/PhysRevLett.58.2059', '10.1103/PhysRevLett.58.2486', '10.1103/PhysRevB.28.1711'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Structural Materials'], conference_acronym='Physical review letters (Print)', publisher=None, query_handler=None),\n",
       " 'Emergence of plasmid-mediated high-level tigecycline resistance genes in animals and humans': Paper(DOI='10.1038/s41564-019-0445-2', crossref_json=None, google_schorlar_metadata=None, title='Emergence of plasmid-mediated high-level tigecycline resistance genes in animals and humans', authors=['Tao He', 'Ran Wang', 'Dejun Liu', 'Timothy R Walsh', 'Rong Zhang', 'Yuan Lv', 'Yuebin Ke', 'Quanjiang Ji', 'Ruicheng Wei', 'Zhihai Liu', 'Yingbo Shen', 'Gang Wang', 'Lichang Sun', 'Lei Lei', 'Ziquan Lv', 'Yun Li', 'Maoda Pang', 'Liyuan Wang', 'Qiaoling Sun', 'Yulin Fu', 'Huangwei Song', 'Yuxin Hao', 'Zhangqi Shen', 'Shaolin Wang', 'Gongxiang Chen', 'Congming Wu', 'Jianzhong Shen', 'Yang Wang'], abstract='Tigecycline is a last-resort antibiotic that is used to treat severe infections caused by extensively drug-resistant bacteria. tet(X) has been shown to encode a flavin-dependent monooxygenase that modifies tigecycline,. Here, we report two unique mobile tigecycline-resistance genes, tet(X3) and tet(X4), in numerous Enterobacteriaceae and Acinetobacter that were isolated from animals, meat for consumption and humans. Tet(X3) and Tet(X4) inactivate all tetracyclines, including tigecycline and the newly FDA-approved eravacycline and omadacycline. Both tet(X3) and tet(X4) increase (by 64–128-fold) the tigecycline minimal inhibitory concentration values for Escherichia coli, Klebsiella pneumoniae and Acinetobacter baumannii. In addition, both Tet(X3) (A. baumannii) and Tet(X4) (E. coli) significantly compromise tigecycline in in vivo infection models. Both tet(X3) and tet(X4) are adjacent to insertion sequence IS\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.chembiol.2015.05.017', '10.1021/bi0506066', '10.1126/science.aaf9286', '10.1016/S1473-3099(08)70279-2', '10.1128/CMR.00079-17', '10.1016/S1473-3099(15)00424-7', '10.1093/jac/dky262', '10.1016/S1473-3099(11)70177-3', '10.1093/jac/dku185', '10.1128/JCM.00001-14', '10.1086/431676', '10.1086/431675', '10.1016/j.ijantimicag.2012.09.005', '10.3389/fmicb.2018.00648', '10.1016/S1473-3099(17)30628-X', '10.1074/jbc.M409573200', '10.1128/AAC.01727-13', '10.1016/j.ijantimicag.2013.04.014', '10.1016/j.anaerobe.2013.03.001', '10.1038/nature17672', '10.1016/0147-619X(84)90031-3', '10.1016/j.bmc.2016.07.029', '10.1128/AAC.01288-13', '10.1016/j.febslet.2011.03.012', '10.1107/S0907444913013802', '10.1038/nchembio.2396', '10.1038/nchembio.2376', '10.1073/pnas.1503141112', '10.1128/AAC.02465-15', '10.1128/AAC.00993-09', '10.1093/jac/dkw357', '10.1016/j.vetmic.2017.06.010', '10.1038/nmicrobiol.2016.260', '10.1128/mBio.00943-18'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Structural Materials'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Assessing the trait theory of leadership using self and observer ratings of personality: The mediating role of contributions to group success': Paper(DOI='10.1016/j.leaqua.2012.03.004', crossref_json=None, google_schorlar_metadata=None, title='Assessing the trait theory of leadership using self and observer ratings of personality: The mediating role of contributions to group success', authors=['Amy E Colbert', 'Timothy A Judge', 'Daejeong Choi', 'Gang Wang'], abstract='The trait theory of leadership suggests that personality traits influence leader emergence and effectiveness. While initial empirical evidence supports this perspective, the majority of studies have examined the relationship between personality and leadership using self ratings of personality. We believe that this research may underestimate the relationship between personality and leadership. We propose that personality assessed using both self and observer ratings explains more variance in leadership than self ratings of personality alone. Results from 155 participants in leaderless group discussions supported this hypothesis. Further, relative weight analysis revealed that observer ratings of extraversion explained the largest percentage of variance in leadership, followed by self ratings of openness to experience and observer ratings of openness to experience. Results of two-stage least squares regression\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.leaqua.2010.10.010', '10.1111/j.1744-6570.1991.tb00688.x', '10.1111/1468-2389.00160', '10.1037/0021-9010.87.1.43', '10.1037/0022-3514.65.3.546', '10.1037/a0025559', '10.1037/a0021212', '10.1016/j.leaqua.2010.07.005', '10.1111/j.1468-2389.2007.00371.x', '10.1037/0022-3514.55.2.258', '10.5465/AMR.2010.53503267', '10.1037/0022-3514.53.3.497', '10.1146/annurev.ps.41.020190.002221', '10.1207/s15327957pspr0204_5', '10.1037/0033-295X.102.4.652', '10.2307/256377', '10.1111/j.1467-6494.2009.00574.x', '10.1037/0003-066X.49.6.493', '10.1177/014920639702300306', '10.1111/j.1467-6494.1996.tb00522.x', '10.1207/S15327906MBR3501_1', '10.1037/0021-9010.87.4.765', '10.1037/0021-9010.87.3.530', '10.1016/j.leaqua.2009.09.004', '10.1037/0021-9010.64.5.526', '10.1037/0021-9010.68.4.678', '10.1037/0003-066X.43.1.23', '10.1177/1046496491224005', '10.1111/j.1467-6494.1996.tb00513.x', '10.1037/0021-9010.71.3.402', '10.1016/0030-5073(84)90043-6', '10.1037/h0044587', '10.5465/amr.2001.4845785', '10.1037/0022-3514.52.6.1258', '10.1037/0022-3514.49.3.710', '10.1111/j.1744-6570.2007.00089.x', '10.1016/1048-9843(94)90005-1', '10.1037/a0021832', '10.1111/j.1744-6570.2007.00099.x', '10.2307/256814', '10.1037/0022-3514.60.2.307', '10.1037/0022-3514.46.3.598', '10.1016/j.brat.2008.10.002', '10.1146/annurev.psych.59.103006.093716', '10.1037/0022-3514.73.6.1380', '10.1080/00223980.1948.9917362', '10.1037/h0053857', '10.1037/0021-9010.88.3.500', '10.1006/jrpe.2000.2292', '10.1016/j.leaqua.2006.10.007', '10.1177/104649648701800102', '10.1177/001872678603901106', '10.1037/0021-9010.84.4.632', '10.1080/03637757709390125', '10.1111/j.1467-6494.2006.00392.x', '10.1037/0021-9010.76.2.308'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Structural Materials'], conference_acronym='The Leadership quarterly', publisher=None, query_handler=None),\n",
       " 'Object tracking by an unmanned aerial vehicle using visual sensors': Paper(DOI='10.3390/s18092751', crossref_json=None, google_schorlar_metadata=None, title='Object tracking by an unmanned aerial vehicle using visual sensors', authors=['Saumitro Dasgupta', 'Hayk Martirosyan', 'Hema Koppula', 'Alex Kendall', 'Austin Stone', 'Matthew Donahoe', 'Abraham Galton Bachrach', 'Adam Parker Bry'], abstract='Systems and methods are disclosed for tracking objects in a physical environment using visual sensors onboard an autonomous unmanned aerial vehicle (UAV). In certain embodiments, images of the physical environment captured by the onboard visual sensors are processed to extract semantic information about detected objects. Processing of the captured images may involve applying machine learning techniques such as a deep convolutional neural network to extract semantic cues regarding objects detected in the images. The object tracking can be utilized, for example, to facilitate autonomous navigation by the UAV or to generate and display augmentative information regarding tracked objects to users.', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2011.239', '10.1109/TPAMI.2015.2465908', '10.3390/s18072046', '10.3390/s17112626', '10.1016/j.eswa.2015.05.055', '10.3390/s16091406', '10.1109/TIP.2017.2775060', '10.1109/TIP.2016.2520358', '10.1109/TPAMI.2014.2345390', '10.1109/TPAMI.2016.2609928', '10.1109/TSMC.2016.2629509', '10.1109/TIP.2016.2531283'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision', 'Robotics', 'Control'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Probabilistic future prediction for video scene understanding': Paper(DOI='10.1007/978-3-030-58517-4_45', crossref_json=None, google_schorlar_metadata=None, title='Probabilistic future prediction for video scene understanding', authors=['Anthony Hu', 'Fergal Cotter', 'Nikhil Mohan', 'Corina Gurau', 'Alex Kendall'], abstract=' We present a novel deep learning architecture for probabilistic future prediction from video. We predict the future semantics, geometry and motion of complex real-world urban scenes and use this representation to control an autonomous vehicle. This work is the first to jointly predict ego-motion, static scene, and the motion of dynamic agents in a probabilistic manner, which allows sampling consistent, highly probable futures from a compact latent space. Our model learns a representation from RGB video with a spatio-temporal convolutional module. The learned representation can be explicitly decoded to future semantic segmentation, depth, and optical flow, in addition to being an input to a learnt driving policy. To model the stochasticity of the future, we introduce a conditional variational approach which minimises the divergence between the present distribution (what could happen given what we have\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICRA.2019.8793579', '10.1109/CVPR.2018.00885', '10.1109/CVPR.2016.331', '10.1109/CVPR.2017.502', '10.1109/ICRA.2018.8460487', '10.1109/CVPR.2016.350', '10.1109/CVPR.2017.787', '10.1109/ICRA.2017.7989324', '10.1109/ICCV.2017.477', '10.1109/ICCVW.2017.373', '10.1109/CVPR.2016.90', '10.1609/aaai.v32i1.11796', '10.1109/CVPRW.2018.00141', '10.1109/ICRA.2019.8793742', '10.1109/CVPR.2017.233', '10.1109/CVPR.2018.00218', '10.1109/ICCV.2017.77', '10.1109/ICCV.2017.534', '10.1109/ICCV.2019.00291', '10.1109/ICPR.2004.1334462', '10.1109/ICIP.2017.8296851', '10.1109/CVPR.2018.00931', '10.1109/ICCV.2015.522', '10.1109/CVPR.2015.7298594', '10.1109/CVPR.2018.00675', '10.1109/CVPR.2017.634', '10.1109/CVPR.2017.441'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision', 'Robotics', 'Control'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Deformable DETR: Deformable Transformers for End-to-End Object Detection': Paper(DOI='10.1016/j.ecoinf.2022.101902', crossref_json=None, google_schorlar_metadata=None, title='Deformable DETR: Deformable Transformers for End-to-End Object Detection', authors=['Xizhou Zhu', 'Weijie Su', 'Lewei Lu', 'Bin Li', 'Xiaogang Wang', 'Jifeng Dai'], abstract='DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2019.2956516', '10.1016/j.compag.2020.105522', '10.1016/j.ecoinf.2021.101460', '10.1016/j.ecoinf.2020.101089', '10.1111/j.1439-0418.2009.01400.x', '10.1016/j.compag.2020.105585', '10.1016/j.ecoinf.2022.101556', '10.1016/j.compag.2018.04.004'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'deep learning'], conference_acronym='Ecological informatics (Print)', publisher=None, query_handler=None),\n",
       " 'Mmdetection: Open mmlab detection toolbox and benchmark': Paper(DOI='10.1016/j.simpa.2021.100081', crossref_json=None, google_schorlar_metadata=None, title='Mmdetection: Open mmlab detection toolbox and benchmark', authors=['Kai Chen', 'Jiaqi Wang', 'Jiangmiao Pang', 'Yuhang Cao', 'Yu Xiong', 'Xiaoxiao Li', 'Shuyang Sun', 'Wansen Feng', 'Ziwei Liu', 'Jiarui Xu', 'Zheng Zhang', 'Dazhi Cheng', 'Chenchen Zhu', 'Tianheng Cheng', 'Qijie Zhao', 'Buyu Li', 'Xin Lu', 'Rui Zhu', 'Yue Wu', 'Jifeng Dai', 'Jingdong Wang', 'Jianping Shi', 'Wanli Ouyang', 'Chen Change Loy', 'Dahua Lin'], abstract='We present MMDetection, an object detection toolbox that contains a rich set of object detection and instance segmentation methods as well as related components and modules. The toolbox started from a codebase of MMDet team who won the detection track of COCO Challenge 2018. It gradually evolves into a unified platform that covers many popular detection methods and contemporary modules. It not only includes training and inference codes, but also provides weights for more than 200 network models. We believe this toolbox is by far the most complete detection toolbox. In this paper, we introduce the various features of this toolbox. In addition, we also conduct a benchmarking study on different methods, components, and their hyper-parameters. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new detectors. Code and models are available at https://github.com/open-mmlab/mmdetection. The project is under active development and we will keep this document updated.', conference=None, journal=None, year=None, reference_list=['10.1109/TIP.2014.2329449', '10.1007/s10589-013-9576-1', '10.1109/TIT.2016.2556683', '10.1109/CVPR.2016.55', '10.1109/CVPR.2018.00196', '10.1007/978-3-030-01249-6_30', '10.1109/5.726791', '10.1109/ICCV.2015.425'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'deep learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Relation networks for object detection': Paper(DOI='10.3390/electronics10161918', crossref_json=None, google_schorlar_metadata=None, title='Relation networks for object detection', authors=['Han Hu', 'Jiayuan Gu', 'Zheng Zhang', 'Jifeng Dai', 'Yichen Wei'], abstract='Although it is well believed for years that modeling relations between objects would help object recognition, there has not been evidence that the idea is working in the deep learning era. All state-of-the-art object detection systems still rely on recognizing object instances extbf {individually}, without exploiting their relations during learning. This work proposes an object relation module. It processes a set of objects extbf {simultaneously} through interaction between their appearance feature and geometry, thus allowing modeling of their relations. It is lightweight and in-place. It does not require additional supervision and is easy to embed in existing networks. It is shown effective on improving object recognition and duplicate removal steps in the modern object detection pipeline. It verifies the efficacy of modeling object relations in CNN based detection. It gives rise to the extbf {first fully end-to-end object detector}.', conference=None, journal=None, year=None, reference_list=['10.1038/nature14539', '10.1109/TPAMI.2016.2577031', '10.1162/neco.1997.9.8.1735', '10.1109/TPAMI.2015.2437384', '10.1109/TCSVT.2017.2736553', '10.1109/ACCESS.2018.2861223', '10.1109/TIP.2017.2651367', '10.1109/TPAMI.2019.2910529', '10.1016/S0734-189X(87)80014-2', '10.1109/78.650093', '10.1109/TPAMI.2008.137'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['deep learning', 'computer vision', 'medical image analysis'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Bevformer: Learning bird’s-eye-view representation from multi-camera images via spatiotemporal transformers': Paper(DOI='10.1007/978-3-031-20077-9_1', crossref_json=None, google_schorlar_metadata=None, title='Bevformer: Learning bird’s-eye-view representation from multi-camera images via spatiotemporal transformers', authors=['Zhiqi Li', 'Wenhai Wang', 'Hongyang Li', 'Enze Xie', 'Chonghao Sima', 'Tong Lu', 'Yu Qiao', 'Jifeng Dai'], abstract='3D visual perception tasks, including 3D detection and map segmentation based on multi-camera images, are essential for autonomous driving systems. In this work, we present a new framework termed BEVFormer, which learns unified BEV representations with spatiotemporal transformers to support multiple autonomous driving perception tasks. In a nutshell, BEVFormer exploits both spatial and temporal information by interacting with spatial and temporal space through predefined grid-shaped BEV queries. To aggregate spatial information, we design spatial cross-attention that each BEV query extracts the spatial features from the regions of interest across camera views. For temporal information, we propose temporal self-attention to recurrently fuse the history BEV information. Our approach achieves the new state-of-the-art 56.9% in terms of NDS metric on the nuScenes test set, which is 9.0 points higher than\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2019.00938', '10.1007/978-3-030-58592-1_9', '10.1109/IVS.2019.8814056', '10.1109/CVPR42600.2020.01164', '10.1109/ICCV48922.2021.01537', '10.1007/978-3-030-58452-8_13', '10.1109/CVPR.2017.691', '10.1109/ICCV48922.2021.01550', '10.3115/v1/W14-4012', '10.1109/ICCV.2017.89', '10.1109/CVPR.2016.90', '10.1162/neco.1997.9.8.1735', '10.1109/ICCV48922.2021.01499', '10.1109/CVPR.2016.95', '10.1109/CVPR.2019.01298', '10.1109/CVPRW.2019.00103', '10.1109/CVPR52688.2022.00134', '10.1109/CVPR.2017.106', '10.1109/CVPR.2018.00376', '10.1109/CVPR.2017.597', '10.1109/LRA.2020.3004325', '10.1109/ICCV48922.2021.00313', '10.1007/978-3-030-58568-6_12', '10.1109/CVPR46437.2021.00607', '10.1109/CVPR46437.2021.00845', '10.1109/ITSC45102.2020.9294462', '10.1109/WACV51458.2022.00133', '10.1109/ICRA46639.2022.9811901', '10.1109/ICCV.2019.00208', '10.1109/CVPR42600.2020.00252', '10.1109/ICCV.2019.00972', '10.1109/CVPR42600.2020.00466', '10.1109/ICCVW54120.2021.00107', '10.1109/CVPR.2019.00864', '10.1109/CVPR.2018.00249', '10.3390/s18103337', '10.1109/CVPR46437.2021.01528', '10.1109/CVPR46437.2021.01161', '10.1109/CVPR.2018.00472', '10.1007/978-3-030-58595-2_35', '10.1109/CVPR.2017.441'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'deep learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Mining cross-image semantics for weakly supervised semantic segmentation': Paper(DOI='10.3390/rs15040986', crossref_json=None, google_schorlar_metadata=None, title='Mining cross-image semantics for weakly supervised semantic segmentation', authors=['Guolei Sun', 'Wenguan Wang', 'Jifeng Dai', 'Luc Van Gool'], abstract=' This paper studies the problem of learning semantic segmentation from image-level supervision only. Current popular solutions leverage object localization maps from classifiers as supervision signals, and struggle to make the localization maps capture more complete object content. Rather than previous efforts that primarily focus on intra-image information, we address the value of cross-image semantic relations for comprehensive object pattern mining. To achieve this, two neural co-attentions are incorporated into the classifier to complimentarily capture cross-image semantic similarities and differences. In particular, given a pair of training images, one co-attention enforces the classifier to recognize the common semantics from co-attentive objects, while the other one, called contrastive co-attention, drives the classifier to identify the unshared semantics from the rest, uncommon objects. This helps the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2017.660', '10.1007/978-3-030-01234-2_49', '10.1145/3240508.3240542', '10.1109/CVPR.2017.315', '10.1109/CVPR.2019.00325', '10.1109/ICCV.2015.203', '10.1109/CVPR.2017.181', '10.1007/978-3-319-46493-0_42', '10.1016/j.patcog.2016.01.015', '10.1109/CVPR.2017.687', '10.1109/CVPR.2016.319', '10.1109/TPAMI.2016.2636150', '10.1109/CVPR.2019.00231', '10.1109/CVPR.2018.00523', '10.1109/CVPR42600.2020.01229', '10.1109/IJCNN52387.2021.9533846', '10.1109/ICCV.2017.606', '10.1109/JSTARS.2020.3021098', '10.3390/rs14040879', '10.1109/MVA.2015.7153250', '10.1109/CVPRW56347.2022.00148', '10.1007/978-3-319-46478-7_34', '10.1109/CVPR.2016.344', '10.1109/CVPR.2018.00759', '10.1109/CVPR42600.2020.00901', '10.1109/CVPR.2019.00541', '10.1109/CVPR.2019.00404', '10.1109/CVPR.2017.563', '10.1109/CVPR.2017.239', '10.1109/CVPR.2018.00148', '10.1007/978-3-319-46493-0_24', '10.1609/aaai.v34i07.6705', '10.1007/978-3-030-58536-5_21', '10.1109/TMM.2021.3139459', '10.3390/rs10121970', '10.3390/rs12061049', '10.1109/TCYB.2020.2992433', '10.1109/CVPR52688.2022.00104', '10.1109/ICCV.2011.6126343', '10.1109/TGRS.2022.3207171', '10.1109/CVPR.2016.90', '10.1109/TPAMI.2017.2699184', '10.1109/CVPR.2017.181', '10.3390/rs12193169', '10.1109/TPAMI.2021.3083269', '10.1109/CVPR46437.2021.01649', '10.1109/CVPR46437.2021.00545', '10.1109/CVPR.2018.00147', '10.5244/C.31.20', '10.1109/CVPR.2018.00733', '10.1007/978-3-030-01240-3_23'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neural-Symbolic AI', 'Embodied AI', 'Autonomous Cars', 'Computer Vision', 'Artificial Intelligence'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'The next 700 accelerated layers: From mathematical expressions of network computation graphs to accelerated gpu kernels, automatically': Paper(DOI='10.1145/3355606', crossref_json=None, google_schorlar_metadata=None, title='The next 700 accelerated layers: From mathematical expressions of network computation graphs to accelerated gpu kernels, automatically', authors=['Nicolas Vasilache', 'Oleksandr Zinenko', 'Theodoros Theodoridis', 'Priya Goyal', 'Zachary Devito', 'William S Moses', 'Sven Verdoolaege', 'Andrew Adams', 'Albert Cohen'], abstract='Deep learning frameworks automate the deployment, distribution, synchronization, memory allocation, and hardware acceleration of models represented as graphs of computational operators. These operators wrap high-performance libraries such as cuDNN or NNPACK. When the computation does not match any predefined library call, custom operators must be implemented, often at high engineering cost and performance penalty, limiting the pace of innovation. To address this productivity gap, we propose and evaluate: (1) a domain-specific language with a tensor notation close to the mathematics of deep learning; (2) a Just-In-Time optimizing compiler based on the polyhedral framework; (3) carefully coordinated linear optimization and evolutionary algorithms to synthesize high-performance CUDA kernels; (4) the transparent integration of our flow into PyTorch and Caffe2, providing the fully automatic synthesis\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CGO.2006.37', '10.1145/109625.109631', '10.1145/2854038.2854048', '10.5555/1025127.1025992', '10.1145/2597809.2597818', '10.1145/2896389', '10.1145/1854273.1854317', '10.1145/1375581.1375595', '10.1007/978-3-642-35289-8_28', '10.1145/3211346.3211354', '10.1145/2000064.2000108', '10.1007/BF01379404', '10.1016/j.parco.2012.05.002', '10.1007/s10766-006-0012-3', '10.1142/S0129626412500107', '10.1145/73560.73588', '10.1145/3133901', '10.1145/2866569', '10.1145/2897824.2925952', '10.1145/2694344.2694364', '10.1145/1926385.1926449', '10.1145/2435264.2435273', '10.1145/183432.183525', '10.1177/1094342004041291', '10.1145/2491956.2462176', '10.1109/ICDM.2010.127', '10.1145/351397.351408', '10.1145/2908080.2908105', '10.1007/978-3-642-15582-6_49', '10.1145/2400682.2400713', '10.1145/3178372.3179507'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimodal learning', 'Computer Vision', 'Machine Learning'], conference_acronym='ACM transactions on architecture and code optimization', publisher=None, query_handler=None),\n",
       " 'Beyond part models: person retrieval with refined part pooling (and a strong convolutional baseline)': Paper(DOI='10.1007/978-3-030-01225-0_30', crossref_json=None, google_schorlar_metadata=None, title='Beyond part models: person retrieval with refined part pooling (and a strong convolutional baseline)', authors=['Yifan Sun', 'Liang Zheng', 'Yi Yang', 'Qi Tian', 'Shengjin Wang'], abstract='Employing part-level features offers fine-grained information for pedestrian image description. A prerequisite of part discovery is that each part should be well located. Instead of using external resources like pose estimator, we consider content consistency within each part for precise part location. Specifically, we target at learning discriminative part-informed features for person retrieval and make two contributions.(i) A network named Part-based Convolutional Baseline (PCB). Given an image input, it outputs a convolutional descriptor consisting of several part-level features. With a uniform partition strategy, PCB achieves competitive results with the state-of-the-art methods, proving itself as a strong convolutional baseline for person retrieval.(ii) A refined part pooling (RPP) method. Uniform partition inevitably incurs outliers in each part, which are in fact more similar to other parts. RPP re-assigns these outliers to the parts they are closest to, resulting in refined parts with enhanced within-part consistency. Experiment confirms that RPP allows PCB to gain another round of performance boost. For instance, on the Market-1501 dataset, we achieve (77.4+ 4.2)% mAP and (92.3+ 1.5)% rank-1 accuracy, surpassing the state of the art by a large margin.', conference=None, journal=None, year=None, reference_list=['10.1016/j.cviu.2017.12.002', '10.1109/CVPR.2017.143', '10.1109/ICCVW.2017.304', '10.5244/C.25.68', '10.1007/978-3-319-10605-2_22', '10.1109/CVPR.2009.5206848', '10.1109/CVPR.2016.387', '10.1109/CVPR.2008.4587597', '10.1007/978-3-540-88682-2_21', '10.1109/CVPR.2016.90', '10.1007/978-3-319-46466-4_3', '10.1007/978-3-319-46454-1_53', '10.1109/CVPR.2014.27', '10.24963/ijcai.2017/305', '10.1109/CVPR.2018.00243', '10.1007/s11263-016-0945-y', '10.1109/CVPR.2015.7298832', '10.1007/978-3-319-46448-0_2', '10.1109/ICCV.2017.46', '10.1109/CVPR.2015.7298965', '10.1109/ICCV.2013.443', '10.1007/978-3-319-46484-8_29', '10.1007/978-3-319-48881-3_2', '10.1109/ICCV.2017.427', '10.1109/ICCV.2017.410', '10.1609/aaai.v31i1.11231', '10.1145/3123266.3123279', '10.1109/CVPR.2016.511', '10.1109/CVPR.2016.140', '10.1109/CVPR.2018.00454', '10.1109/ICCV.2017.349', '10.1109/ICCV.2015.133', '10.1109/ICCV.2017.405', '10.1109/CVPR.2017.389', '10.1109/CVPR.2018.00541'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Object Re-identification', 'Dataset-centered Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'MARS: a video benchmark for large-scale person re-identification': Paper(DOI='10.1007/978-3-319-46466-4_52', crossref_json=None, google_schorlar_metadata=None, title='MARS: a video benchmark for large-scale person re-identification', authors=['Liang Zheng', 'Zhi Bie', 'Yifan Sun', 'Jingdong Wang', 'Chi Su', 'Shengjin Wang', 'Qi Tian'], abstract=' This paper considers person re-identification (re-id) in videos. We introduce a new video re-id dataset, named Motion Analysis and Re-identification Set (MARS), a video extension of the Market-1501 dataset. To our knowledge, MARS is the largest video re-id dataset to date. Containing 1,261 IDs and around 20,000 tracklets, it provides rich visual information compared to image-based datasets. Meanwhile, MARS reaches a step closer to practice. The tracklets are automatically generated by the Deformable Part Model (DPM) as pedestrian detector and the GMMCP tracker. A number of false detection/tracking results are also included as distractors which would exist predominantly in practical video databases. Extensive evaluation of the state-of-the-art methods including the space-time descriptors and CNN is presented. We show that CNN in classification mode can be trained from scratch using the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2015.7299016', '10.5244/C.26.92', '10.1145/2072572.2072590', '10.1109/DICTA.2012.6411689', '10.1109/CVPR.2015.7298764', '10.1007/978-3-319-10605-2_22', '10.1109/CVPR.2015.7299036', '10.1016/j.patcog.2015.04.005', '10.1109/CVPR.2010.5539926', '10.1109/TPAMI.2009.167', '10.1109/TPAMI.2006.38', '10.1109/CVPR.2016.90', '10.1007/978-3-642-21227-7_9', '10.1145/2647868.2654889', '10.1007/s11263-009-0308-z', '10.5244/C.22.99', '10.1109/CVPR.2012.6247939', '10.1007/s11263-005-1838-7', '10.1109/CVPR.2013.461', '10.1109/CVPR.2014.27', '10.1109/CVPR.2015.7298832', '10.1109/ICCV.2015.434', '10.1109/ICCV.2013.329', '10.1016/j.imavis.2014.04.002', '10.1049/el.2012.1607', '10.1109/CVPR.2016.148', '10.1016/j.imavis.2009.11.014', '10.1007/s11263-013-0636-x', '10.1109/SIBGRAPI.2009.42', '10.1145/1291233.1291311', '10.1109/ICCV.2015.426', '10.1007/978-3-319-10593-2_45', '10.1007/978-3-540-88688-4_48', '10.1007/978-3-319-10584-0_1', '10.1109/ICPR.2014.16', '10.1109/CVPR.2016.150', '10.1109/CVPR.2013.460', '10.1109/ICCV.2015.133', '10.1109/CVPR.2013.213', '10.1109/CVPR.2015.7298783', '10.5244/C.23.23'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Object Re-identification', 'Dataset-centered Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'A discriminatively learned CNN embedding for person re-identification': Paper(DOI='10.1145/3159171', crossref_json=None, google_schorlar_metadata=None, title='A discriminatively learned CNN embedding for person re-identification', authors=['Zhedong Zheng', 'Liang Zheng', 'Yi Yang'], abstract='In this article, we revisit two popular convolutional neural networks in person re-identification (re-ID): verification and identification models. The two models have their respective advantages and limitations due to different loss functions. Here, we shed light on how to combine the two models to learn more discriminative pedestrian descriptors. Specifically, we propose a Siamese network that simultaneously computes the identification loss and verification loss. Given a pair of training images, the network predicts the identities of the two input images and whether they belong to the same identity. Our network learns a discriminative embedding and a similarity measurement at the same time, thus taking full usage of the re-ID annotations. Our method can be easily applied on different pretrained networks. Albeit simple, the learned embedding improves the state-of-the-art performance on two public person re-ID\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-319-10590-1_38', '10.1142/S0218001493000339', '10.1109/TNNLS.2016.2582746', '10.1109/TPAMI.2016.2608901', '10.1109/CVPR.2016.142', '10.1109/CVPR.2016.149', '10.1109/TMM.2015.2508146', '10.1016/j.patcog.2015.04.005', '10.1109/CVPR.2006.100', '10.1109/CVPR.2016.90', '10.1145/3063595', '10.1007/978-3-319-46604-0_48', '10.5555/2354409.2354748', '10.5555/2354409.2354748', '10.1109/CVPR.2014.27', '10.1145/3038916', '10.1109/CVPR.2016.152', '10.5555/2354409.2354975', '10.1145/2710128', '10.1109/CVPR.2016.434', '10.1109/CVPR.2007.383172', '10.1145/2671188.2749366', '10.1007/978-3-319-46448-0_1', '10.1007/s11263-015-0816-y', '10.5555/2627435.2670313', '10.1109/CVPR.2015.7298907', '10.1007/978-3-319-46484-8_48', '10.1007/978-3-319-46478-7_9', '10.1145/2733373.2807412', '10.1109/TMM.2015.2505083', '10.1016/j.patcog.2016.12.022', '10.1109/TMM.2016.2602938', '10.1145/3089249', '10.1145/1631272.1631298', '10.1109/ICPR.2014.16', '10.1109/CVPR.2016.139', '10.1109/CVPR.2016.143', '10.1109/ICCV.2013.314', '10.1007/978-3-319-46466-4_52', '10.5555/2919332.2919877', '10.1109/TMM.2015.2408563', '10.1109/TIP.2014.2330763', '10.1007/s11263-016-0889-2', '10.1109/TPAMI.2012.138', '10.1109/ICCV.2017.405'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Object Re-identification', 'Dataset-centered Vision'], conference_acronym='ACM transactions on multimedia computing communications and applications', publisher=None, query_handler=None),\n",
       " 'CamStyle: a novel data augmentation method for person re-identification': Paper(DOI='10.1109/tip.2018.2874313', crossref_json=None, google_schorlar_metadata=None, title='CamStyle: a novel data augmentation method for person re-identification', authors=['Zhun Zhong', 'Liang Zheng', 'Zhedong Zhenga', 'Shaozi Li', 'Yi Yang'], abstract='Person re-identification (re-ID) is a cross-camera retrieval task that suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle). CamStyle can serve as a data augmentation approach that reduces the risk of deep network overfitting and that smooths the CamStyle disparities. Specifically, with a style transfer model, labeled training images can be style transferred to each camera, and along with the original training samples, form the augmented training set. This method, while increasing data diversity against overfitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-1-4471-6296-4_11', '10.1109/CVPR.2017.389', '10.1109/ICCV.2017.349', '10.1109/TPAMI.2009.167', '10.1109/WACV.2016.7477681', '10.1109/CVPR.2017.358', '10.1109/ICCV.2017.427', '10.1109/ICPR.2014.16', '10.1109/TPAMI.2017.2666805', '10.1109/TIP.2017.2700762', '10.1109/ICCV.2017.577', '10.1109/CVPR.2017.18', '10.1109/CVPR.2016.146', '10.1109/CVPR.2015.7298629', '10.1109/TIP.2018.2867747', '10.1109/ICIP.2003.1246634', '10.1109/CVPR.2005.71', '10.1109/TMM.2016.2602938', '10.1016/j.cviu.2007.01.003', '10.1109/TCSVT.2012.2190471', '10.5244/C.22.64', '10.1109/TIP.2017.2695101', '10.1109/TIP.2014.2330763', '10.1007/s11263-016-0889-2', '10.1109/TPAMI.2017.2709749', '10.1109/TIP.2016.2514498', '10.1109/CVPR.2016.90', '10.1109/CVPR.2018.00431', '10.1109/CVPR.2016.265', '10.1109/CVPR.2018.00916', '10.1109/CVPR.2017.632', '10.1109/ICCV.2017.244', '10.1109/CVPR.2016.308', '10.1109/CVPR.2018.00543', '10.1109/CVPR.2017.316', '10.1109/TMM.2012.2237023', '10.1109/CVPR.2018.00243', '10.1109/CVPR.2018.00242', '10.1109/CVPR.2018.00051', '10.1109/CVPR.2018.00110', '10.1109/CVPR.2018.00016', '10.1109/CVPR.2017.360', '10.1109/CVPR.2018.00541', '10.24963/ijcai.2017/305', '10.1109/CVPR.2012.6247939', '10.1109/TCSVT.2016.2515309', '10.1109/CVPR.2016.139', '10.1109/CVPR.2015.7298832', '10.1109/ICCV.2017.113', '10.1109/ICCV.2017.410', '10.1109/ICCV.2017.405', '10.1145/3243316', '10.1109/ICCV.2015.133', '10.1109/AVSS.2015.7301739', '10.1109/CVPR.2018.00535', '10.1109/CVPR.2017.357', '10.1109/CVPR.2009.5206848'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Object Re-identification', 'Dataset-centered Vision'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'SIFT meets CNN: a decade survey of instance retrieval': Paper(DOI='10.1109/tpami.2017.2709749', crossref_json=None, google_schorlar_metadata=None, title='SIFT meets CNN: a decade survey of instance retrieval', authors=['Liang Zheng', 'Yi Yang', 'Qi Tian'], abstract='In the early days, content-based image retrieval (CBIR) was studied with global features. Since 2003, image retrieval based on local descriptors (de facto SIFT) has been extensively studied for over a decade due to the advantage of SIFT in dealing with image transformations. Recently, image representations based on the convolutional neural network (CNN) have attracted increasing interest in the community and demonstrated impressive performance. Given this time of rapid evolution, this article provides a comprehensive survey of instance retrieval over the last decade. Two broad categories, SIFT-based and CNN-based methods, are presented. For the former, according to the codebook size, we organize the literature into using large/medium-sized/small codebooks. For the latter, we discuss three lines of methods, i.e., using pre-trained or fine-tuned CNN models, and hybrid methods. The first two perform a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2015.133', '10.1109/ICME.2016.7553002', '10.1109/TIP.2015.2423557', '10.5244/C.27.99', '10.1109/CVPR.2012.6248018', '10.1109/CVPR.2009.5206529', '10.1109/CVPR.2014.250', '10.1007/s11263-005-3848-x', '10.1109/ICIP.2014.7025618', '10.5244/C.27.132', '10.1109/TPAMI.2014.2301163', '10.1016/j.imavis.2004.02.006', '10.1109/ICCV.2013.177', '10.1109/CVPR.2011.5995528', '10.1109/CVPR.2009.5206848', '10.1109/CVPR.2011.5995601', '10.1561/0600000017', '10.1109/ICCV.2007.4408891', '10.1109/CVPR.2008.4587635', '10.1109/TPAMI.2015.2484342', '10.5244/C.25.76', '10.1007/978-3-319-46466-4_28', '10.1109/ICCV.2015.19', '10.1145/2964284.2967197', '10.1145/2671188.2749300', '10.1109/ICCVW.2015.134', '10.1145/2671188.2749289', '10.1109/CVPRW.2015.7301273', '10.1007/s11263-013-0620-5', '10.1109/CVPR.2014.317', '10.1007/978-3-319-10599-4_25', '10.1109/CVPR.2010.5540018', '10.1145/2502081.2502171', '10.1109/CVPR.2013.207', '10.1109/TPAMI.2011.235', '10.1109/TPAMI.2005.188', '10.1109/CVPR.2015.7298790', '10.1038/nature14539', '10.1109/CVPR.2014.180', '10.1109/TMM.2015.2478055', '10.1109/CVPR.2011.5995432', '10.1109/ICCV.2003.1238663', '10.1023/B:VISI.0000029664.99615.94', '10.1109/CVPRW.2015.7301269', '10.1016/j.patrec.2014.06.011', '10.1109/CVPRW.2014.131', '10.1109/CVPR.2015.7298998', '10.1109/TPAMI.2011.103', '10.1109/CVPRW.2015.7301272', '10.1145/2671188.2749366', '10.1016/j.cviu.2012.10.010', '10.1109/CVPR.2010.5540009', '10.1109/ICCV.2011.6126544', '10.1109/CVPR.2016.90', '10.1109/CVPR.2015.7298594', '10.1109/ICCV.2013.210', '10.5244/C.28.6', '10.1109/CVPR.2009.5206609', '10.1109/TPAMI.2015.2500224', '10.1145/2393347.2396358', '10.1145/2324796.2324816', '10.1145/2393347.2393377', '10.1016/j.sigpro.2016.05.021', '10.1109/CVPR.2013.213', '10.1109/TMM.2014.2323945', '10.1109/CVPR.2015.7298642', '10.1007/s11263-016-0889-2', '10.1109/ICASSP.2015.7178196', '10.1016/j.neucom.2015.08.076', '10.1145/2911996.2912061', '10.1109/TPAMI.2014.2321376', '10.1109/TPAMI.2010.57', '10.1145/276698.276876', '10.1109/TMM.2014.2329648', '10.1109/CVPR.2016.36', '10.1007/978-3-319-46475-6_48', '10.1109/ICRA.2011.5980382', '10.1145/2766959', '10.1145/2647868.2654895', '10.1109/CVPRW.2016.56', '10.1109/34.895972', '10.1109/CVPR.2016.228', '10.1109/CVPR.2015.7299064', '10.1109/CVPR.2007.383222', '10.1109/TPAMI.2006.3', '10.1109/TPAMI.2016.2545667', '10.1109/TPAMI.2007.70716', '10.1109/CVPR.2009.5206582', '10.1109/TPAMI.2009.154', '10.1109/TPAMI.2008.285', '10.1109/CVPR.2011.5995711', '10.1109/CVPR.2013.211', '10.1145/1631272.1631285', '10.1145/2072298.2072035', '10.1145/1180639.1180664', '10.1007/s11263-010-0363-5', '10.1007/s11263-013-0659-3', '10.1109/ICCV.2013.323', '10.1109/CVPR.2015.7298613', '10.1145/2072298.2072034', '10.1007/978-3-642-33709-3_47', '10.1109/CVPR.2006.264', '10.1109/CVPR.2007.383172', '10.1109/CVPR.2010.5540039', '10.1109/CVPR.2009.5206531', '10.1007/978-3-319-46466-4_15', '10.1109/ICCVW.2009.5457541', '10.1109/CVPR.2014.417', '10.1109/CVPR.2012.6248038', '10.1145/1991996.1992016', '10.1109/ICCV.2015.218', '10.1109/CVPR.2011.5995373', '10.1109/CVPR.2013.119', '10.1145/2461466.2461486', '10.1016/j.patcog.2014.04.007', '10.1109/ICCV.2013.214', '10.1109/TBDATA.2016.2566664', '10.1109/CVPR.2017.105', '10.1007/s11263-009-0285-2', '10.1109/CVPR.2015.7298783', '10.1109/TMM.2010.2046265', '10.1145/2324796.2324820'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Object Re-identification', 'Dataset-centered Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Unsupervised person re-identification: clustering and fine-tuning': Paper(DOI='10.1145/3243316', crossref_json=None, google_schorlar_metadata=None, title='Unsupervised person re-identification: clustering and fine-tuning', authors=['Hehe Fan', 'Liang Zheng', 'Yi Yang'], abstract='The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this article, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between (1) pedestrian clustering and (2) fine-tuning of the convolutional neural network (CNN) to improve the initialization model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning, when the model is weak, CNN is fine-tuned on a small amount of reliable examples that locate near to\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2017.358', '10.1109/TPAMI.2018.2828815', '10.1109/ICCV.2017.90', '10.1145/1553374.1553380', '10.1109/CVPR.2016.142', '10.1109/CVPR.2017.145', '10.1109/CVPR.2016.149', '10.1109/TIP.2018.2821921', '10.1109/CVPR.2010.5539926', '10.1109/TPAMI.2009.167', '10.1007/978-3-540-88682-2_21', '10.1109/CVPR.2016.90', '10.1109/CVPR.2014.27', '10.1109/ICCV.2013.62', '10.1109/TIP.2017.2700762', '10.1145/2964284.2967209', '10.1109/TMM.2017.2751966', '10.5555/3305890.3305916', '10.1016/j.patcog.2016.11.018', '10.1109/TMM.2017.2659221', '10.1109/CVPR.2016.146', '10.1007/978-3-319-46448-0_1', '10.1007/978-3-319-48881-3_2', '10.1007/s11263-015-0816-y', '10.1109/CVPR.2015.7298682', '10.1109/ICCV.2017.410', '10.1007/978-3-319-46484-8_48', '10.1007/978-3-319-46493-0_25', '10.1007/978-3-319-10593-2_45', '10.1109/CVPR.2018.00543', '10.1109/CVPR.2016.140', '10.1109/CVPR.2017.360', '10.1109/TITS.2017.2749977', '10.1109/TITS.2017.2749965', '10.1145/3089249', '10.1109/TMM.2012.2237023', '10.1109/TMM.2016.2605058', '10.1109/ICPR.2014.16', '10.1109/CVPR.2016.139', '10.1109/CVPR.2016.143', '10.1109/ICCV.2013.314', '10.1109/CVPR.2013.460', '10.1109/CVPR.2014.26', '10.1007/978-3-319-46466-4_52', '10.5555/2919332.2919877', '10.1109/CVPR.2015.7298783', '10.1109/TPAMI.2017.2709749', '10.1109/ICCV.2017.405', '10.1109/CVPR.2017.389'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Object Re-identification', 'Dataset-centered Vision'], conference_acronym='ACM transactions on multimedia computing communications and applications', publisher=None, query_handler=None),\n",
       " 'Pose invariant embedding for deep person re-identification': Paper(DOI='10.1109/tip.2019.2910414', crossref_json=None, google_schorlar_metadata=None, title='Pose invariant embedding for deep person re-identification', authors=['Liang Zheng', 'Yujia Huang', 'Huchuan Lu', 'Yi Yang'], abstract='Pedestrian misalignment, which mainly arises from detector errors and pose variations, is a critical problem for a robust person re-identification (re-ID) system. With poor alignment, the feature learning and matching process might be largely compromised. To address this problem, this paper introduces pose-invariant embedding (PIE) as a pedestrian descriptor. First, in order to align pedestrians to a standard pose, the PoseBox structure is introduced, which is generated through pose estimation followed by affine transformations. Second, to reduce the impact of pose estimation errors and information loss during the PoseBox construction, we design a PoseBox fusion (PBF) CNN architecture that takes the original image, the PoseBox, and the pose estimation confidence as input. The proposed PIE descriptor is thus defined as the fully connected layer of the PBF network for the retrieval task. Experiments are conducted\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2009.5206848', '10.1109/CVPR.2018.00431', '10.1109/TPAMI.2014.2360373', '10.1109/CVPR.2018.00242', '10.1109/CVPR.2018.00110', '10.1109/CVPR.2018.00117', '10.1109/TIP.2018.2874313', '10.1109/CVPR.2017.360', '10.1109/CVPR.2018.00632', '10.1109/CVPRW.2017.186', '10.1109/CVPR.2016.151', '10.1109/CVPR.2018.00051', '10.1109/ICCV.2015.133', '10.1109/TIP.2019.2891895', '10.1145/3243316', '10.1109/WACV.2016.7477681', '10.1007/978-3-319-46478-7_9', '10.1109/SMC.2013.748', '10.1109/CVPR.2010.5539926', '10.1109/ICCV.2017.349', '10.1109/CVPR.2017.358', '10.1109/CVPR.2018.00243', '10.1109/ICCVW.2017.304', '10.1109/ICCV.2017.410', '10.5244/C.31.135', '10.1145/3159171', '10.24963/ijcai.2017/305', '10.1109/ICCV.2017.427', '10.1007/978-1-4471-6296-4_7', '10.1109/CVPR.2016.511', '10.1109/ICCV.2017.405', '10.1109/CVPR.2014.214', '10.1007/978-3-319-46484-8_29', '10.1109/CVPR.2016.533', '10.1109/ICPR.2014.16', '10.1109/CVPR.2016.149', '10.1007/978-3-319-46466-4_52', '10.1109/CVPR.2014.27', '10.1109/CVPR.2016.140', '10.1109/CVPR.2015.7299016', '10.1109/CVPR.2016.90', '10.1109/CVPR.2017.782', '10.5244/C.25.68', '10.1109/ICCV.2017.256', '10.1109/CVPR.2012.6247939', '10.1109/CVPR.2016.139', '10.1109/CVPR.2016.142', '10.1109/CVPR.2014.471', '10.1109/CVPRW.2014.131', '10.1145/2647868.2654889'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Object Re-identification', 'Dataset-centered Vision'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Generating Natural-Language Video Descriptions Using Text-Mined Knowledge': Paper(DOI='10.1609/aaai.v27i1.8679', crossref_json=None, google_schorlar_metadata=None, title='Generating Natural-Language Video Descriptions Using Text-Mined Knowledge', authors=['Niveda Krishnamoorthy', 'Girish Malkarnenkar', 'Raymond Mooney', 'Kate Saenko', 'Sergio Guadarrama'], abstract='We present a holistic data-driven technique that generates natural-language descriptions for videos. We combine the output of state-of-the-art object and activity detectors with\" real-world\\'knowledge to select the most probable subject-verb-object triplet for describing a video. We show that this knowledge, automatically mined from web-scale text corpora, enhances the triplet selection algorithm by providing it contextual information and leads to a four-fold increase in activity identification. Unlike previous methods, our approach can annotate arbitrary videos without requiring the expensive collection and annotation of a similar training video corpus. We evaluate our technique against a baseline that does not use text-mined knowledge and show that humans prefer our descriptions 61% of the time.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Reinforcement Learning', 'Computer Vision', 'Machine Learning', 'NLP', 'Computing with Words'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'What computing with words means to me [discussion forum]': Paper(DOI='10.1109/mci.2009.934561', crossref_json=None, google_schorlar_metadata=None, title='What computing with words means to me [discussion forum]', authors=['Jerry M Mendel', 'Lotfi A Zadeh', 'Enric Trillas', 'Ronald Yager', 'Jonathan Lawry', 'Hani Hagras', 'Sergio Guadarrama'], abstract='Computing with words (CWW) means different things to different people. This article is the start of a position paper, written by some of the members of the CIS Fuzzy Systems Technical Committee Task Force on CWW, that answers the question \"What does CWW mean to me?\"', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Reinforcement Learning', 'Computer Vision', 'Machine Learning', 'NLP', 'Computing with Words'], conference_acronym='IEEE computational intelligence magazine', publisher=None, query_handler=None),\n",
       " 'Fuzzy Prolog: A new approach using soft constraints propagation': Paper(DOI='10.1016/j.fss.2003.10.017', crossref_json=None, google_schorlar_metadata=None, title='Fuzzy Prolog: A new approach using soft constraints propagation', authors=['Sergio Guadarrama', 'S Munoz', 'Claudio Vaucheret'], abstract='We present a definition of a Fuzzy Prolog Language that models B ([0,1]) -valued Fuzzy Logic, and subsumes former approaches because it uses a truth value representation based on a union of sub-intervals on [0,1] and is defined using general operators that can model different logics. This extension to Prolog is implemented by interpreting fuzzy reasoning as a set of constraints that are propagated through the rules by means of aggregation operators. Declarative and procedural semantics for Fuzzy Logic programs are given and their equivalence is proven. In addition, we present the implementation of an interpreter for this conceived language using constraint logic programming over real numbers.', conference=None, journal=None, year=None, reference_list=['10.1023/A:1026441215081', '10.1145/383721.383725', '10.1007/978-1-4684-3384-5_11', '10.1007/978-1-4615-5209-3_24', '10.1007/BF00132735', '10.1016/0020-0255(85)90027-1', '10.1007/BF03037082', '10.1145/41625.41635', '10.1145/129393.129398', '10.1145/321679.321688', '10.1109/FUZZ.2001.1009138', '10.1007/3-540-45329-6_29', '10.1016/S0888-613X(02)00064-6', '10.2140/pjm.1955.5.285', '10.1016/0165-0114(94)00348-B', '10.1016/S0165-0114(01)00106-3', '10.1016/0165-0114(78)90029-5'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Reinforcement Learning', 'Computer Vision', 'Machine Learning', 'NLP', 'Computing with Words'], conference_acronym='Fuzzy sets and systems', publisher=None, query_handler=None),\n",
       " 'Image tagging based upon cross domain context': Paper(DOI='10.1007/s00530-014-0396-7', crossref_json=None, google_schorlar_metadata=None, title='Image tagging based upon cross domain context', authors=['Simon John Baker', 'Ashish Kapoor', 'Gang Hua', 'Dahua Lin'], abstract='A method described herein includes receiving a digital image, wherein the digital image includes a first element that corresponds to a first domain and a second element that corresponds to a second domain. The method also includes automatically assigning a label to the first element in the digital image based at least in part upon a computed probability that the label corresponds to the first element, wherein the probability is computed through utilization of a first model that is configured to infer labels for elements in the first domain and a second model that is configured to infer labels for elements in the second domain. The first model receives data that identifies learned relationships between elements in the first domain and elements in the second domain, and the probability is computed by the first model based at least in part upon the learned relationships.', conference=None, journal=None, year=None, reference_list=['10.1145/1873951.1873970', '10.1007/s11263-011-0472-9', '10.1145/2578726.2578748', '10.1109/CVPR.2011.5995379', '10.1109/TMM.2012.2188782', '10.1145/2072298.2072054', '10.1145/1291233.1291276', '10.1145/1015330.1015436', '10.1109/ICIP.2008.4711716', '10.1109/TMM.2013.2265079', '10.7551/mitpress/7503.003.0080', '10.7551/mitpress/7503.003.0172', '10.1145/2505515.2505532', '10.1007/978-3-642-37331-2_21', '10.1109/TPAMI.2009.57', '10.1109/IGARSS.2005.1526130', '10.1016/j.jvcir.2012.02.007', '10.1007/s11042-010-0567-2', '10.1109/CLEI.2012.6427187', '10.1109/ICDMW.2009.47', '10.1609/icwsm.v4i1.14045', '10.1007/978-0-85729-436-4_5', '10.1007/978-0-85729-436-4_11', '10.1109/TIP.2012.2202676', '10.1109/TPAMI.2009.167', '10.1023/B:VISI.0000013087.49260.fb', '10.1145/2393347.2393358', '10.1109/TIP.2011.2176950', '10.1016/j.cviu.2011.10.009', '10.1145/1273496.1273607', '10.1109/TPAMI.2011.114', '10.1016/j.neucom.2012.01.035', '10.1109/CVPR.2005.177', '10.1145/2072298.2071970', '10.1145/1386352.1386375'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Robotics', 'Quantum Computing', 'Computer Vision'], conference_acronym='Multimedia systems', publisher=None, query_handler=None),\n",
       " 'An invitation to 3-d vision: from images to geometric models': Paper(DOI='10.1111/j.0031-868x.2004.295_2.x', crossref_json=None, google_schorlar_metadata=None, title='An invitation to 3-d vision: from images to geometric models', authors=['Yi Ma', 'Stefano Soatto', 'Jana Košecká', 'Shankar Sastry'], abstract=\"This book is intended to give students at the advanced undergraduate or introduc tory graduate level, and researchers in computer vision, robotics and computer graphics, a self-contained introduction to the geometry of three-dimensional (3-D) vision. This is the study of the reconstruction of 3-D models of objects from a collection of 2-D images. An essential prerequisite for this book is a course in linear algebra at the advanced undergraduate level. Background knowledge in rigid-body motion, estimation and optimization will certainly improve the reader's appreciation of the material but is not critical since the first few chapters and the appendices provide a review and summary of basic notions and results on these topics. Our motivation Research monographs and books on geometric approaches to computer vision have been published recently in two batches: The first was in the mid 1990s with books on the\\xa0…\", conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['robotics', 'control', 'hybrid systems', 'cyber security', 'embedded systems'], conference_acronym='Photogrammetric record', publisher=None, query_handler=None),\n",
       " 'Quick shift and kernel methods for mode seeking': Paper(DOI='10.1007/978-3-540-88693-8_52', crossref_json=None, google_schorlar_metadata=None, title='Quick shift and kernel methods for mode seeking', authors=['Andrea Vedaldi', 'Stefano Soatto'], abstract=' We show that the complexity of the recently introduced medoid-shift algorithm in clustering N points is O(N 2), with a small constant, if the underlying distance is Euclidean. This makes medoid shift considerably faster than mean shift, contrarily to what previously believed. We then exploit kernel methods to extend both mean shift and the improved medoid shift to a large family of distances, with complexity bounded by the effective rank of the resulting kernel matrix, and with explicit regularization constraints. Finally, we show that, under certain conditions, medoid shift fails to cluster data points belonging to the same mode, resulting in over-fragmentation. We propose remedies for this problem, by introducing a novel, simple and extremely efficient clustering algorithm, called quick shift, that explicitly trades off under- and over-fragmentation. Like medoid shift, quick shift operates in non-Euclidean spaces\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1143844.1143864', '10.1109/34.400568', '10.1016/j.patcog.2006.04.025', '10.1109/34.1000236', '10.1109/TIT.1975.1055330', '10.1109/TC.1976.1674719', '10.1109/CVPR.2007.383228', '10.1016/S0167-9473(01)00053-6', '10.1109/ICCV.2007.4408978', '10.1109/CVPR.2007.382997', '10.1007/s11263-006-0635-2', '10.1109/ICCV.2003.1238383', '10.1109/ICCV.2007.4408979', '10.1007/s11263-006-9794-4'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence', 'Robotics', 'Augmented Reality'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Entropy-sgd: Biasing gradient descent into wide valleys': Paper(DOI='10.1088/1742-5468/ab39d9', crossref_json=None, google_schorlar_metadata=None, title='Entropy-sgd: Biasing gradient descent into wide valleys', authors=['Pratik Chaudhari', 'Anna Choromanska', 'Stefano Soatto', 'Yann LeCun', 'Carlo Baldassi', 'Christian Borgs', 'Jennifer Chayes', 'Levent Sagun', 'Riccardo Zecchina'], abstract='This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1073/pnas.1608103113', '10.1103/PhysRevE.93.052313', '10.1088/1742-5468/2016/02/023301', '10.1103/PhysRevLett.115.128101', '10.1016/0893-6080(89)90014-2', '10.1080/01621459.2017.1285773', '10.1007/978-3-642-35289-8_25', '10.4171/022-3/26', '10.1103/PhysRevLett.98.150201', '10.1103/PhysRevE.54.717', '10.1007/s10955-007-9386-x', '10.1214/aos/1030741081', '10.1162/neco.1997.9.8.1735', '10.1162/neco.1997.9.1.1', '10.1109/5.726791', '10.1103/PhysRevLett.75.2432', '10.1162/neco.1994.6.1.147', '10.1137/0330046', '10.1023/A:1023562417138'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence', 'Robotics', 'Augmented Reality'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Visual-inertial navigation, mapping and localization: A scalable real-time causal approach': Paper(DOI='10.1177/0278364910388963', crossref_json=None, google_schorlar_metadata=None, title='Visual-inertial navigation, mapping and localization: A scalable real-time causal approach', authors=['Eagle S Jones', 'Stefano Soatto'], abstract='We describe a model to estimate motion from monocular visual and inertial measurements. We analyze the model and characterize the conditions under which its state is observable, and its parameters are identifiable. These include the unknown gravity vector, and the unknown transformation between the camera coordinate frame and the inertial unit. We show that it is possible to estimate both state and parameters as part of an on-line procedure, but only provided that the motion sequence is ‘rich enough’, a condition that we characterize explicitly. We then describe an efficient implementation of a filter to estimate the state and parameters of this model, including gravity and camera-to-inertial calibration. It runs in real-time on an embedded platform. We report experiments of continuous operation, without failures, re-initialization, or re-calibration, on paths of length up to 30 km. We also describe an integrated\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TAC.1972.1100034', '10.1109/34.387503', '10.1177/0278364904049393', '10.1109/34.993559', '10.1109/34.121789', '10.1007/s00371-002-0160-4', '10.1007/s11263-006-0025-9', '10.1109/70.938382', '10.1007/s11263-006-0020-1', '10.1007/978-3-662-02581-9', '10.1007/978-3-642-00196-3_59', '10.1109/TRO.2008.2004832', '10.1016/j.robot.2005.03.008', '10.1002/(SICI)1097-4563(199902)16:2<81::AID-ROB2>3.0.CO;2-9', '10.1002/rob.20103', '10.1364/JOSAA.18.002982', '10.1016/S0005-1098(97)00048-4', '10.1109/JRA.1987.1087109', '10.1109/TAES.2006.4439212', '10.1007/s11263-006-9794-4'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence', 'Robotics', 'Augmented Reality'], conference_acronym='The international journal of robotics research', publisher=None, query_handler=None),\n",
       " 'Kernel density estimation and intrinsic alignment for shape priors in level set segmentation': Paper(DOI='10.1007/s11263-006-7533-5', crossref_json=None, google_schorlar_metadata=None, title='Kernel density estimation and intrinsic alignment for shape priors in level set segmentation', authors=['Daniel Cremers', 'Stanley J Osher', 'Stefano Soatto'], abstract=' In this paper, we make two contributions to the field of level set based image segmentation. Firstly, we propose shape dissimilarity measures on the space of level set functions which are analytically invariant under the action of certain transformation groups. The invariance is obtained by an intrinsic registration of the evolving level set function. In contrast to existing approaches to invariance in the level set framework, this closed-form solution removes the need to iteratively optimize explicit pose parameters. The resulting shape gradient is more accurate in that it takes into account the effect of boundary variation on the object’s pose. Secondly, based on these invariant shape dissimilarity measures, we propose a statistical shape prior which allows to accurately encode multiple fairly distinct training shapes. This prior constitutes an extension of kernel density estimators to the level set domain. In\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/S0042-6989(98)00043-1', '10.1109/ICIP.2003.1247272', '10.1109/ICCV.1995.466871', '10.1109/83.902291', '10.1007/s10208-003-0094-x', '10.1023/A:1020878408985', '10.1214/aos/1176346053', '10.1006/cviu.1995.1004', '10.1016/S0262-8856(98)00175-9', '10.1109/TPAMI.2006.161', '10.1016/S0031-3203(03)00056-6', '10.1007/978-3-540-28649-3_5', '10.1007/s11263-005-4882-4', '10.1007/3-540-44935-3_27', '10.1007/s11263-005-3676-z', '10.1023/A:1020826424915', '10.1007/BFb0086904', '10.1109/TC.1976.1674577', '10.1109/34.817410', '10.1007/978-1-4612-3046-5', '10.2307/1426091', '10.1109/ICCV.1995.466855', '10.1109/TPAMI.2004.1262333', '10.1109/34.368173', '10.1002/cpa.3160420503', '10.1007/b97541', '10.1016/0021-9991(88)90002-2', '10.1023/A:1014080923068', '10.1214/aoms/1177704472', '10.1109/ICCV.2003.1238443', '10.7551/mitpress/3206.001.0001', '10.1214/aoms/1177728190', '10.1109/CVPR.2003.1211535', '10.1007/3-540-47967-8_6', '10.1007/978-3-540-30135-6_26', '10.1093/biomet/65.1.1', '10.1006/jcph.1994.1155', '10.1023/A:1008001603737', '10.1109/83.935033', '10.1109/TIT.1975.1055408', '10.1023/A:1023048024042', '10.1137/S0036139995287685', '10.1006/jcph.1996.0167'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence', 'Robotics', 'Augmented Reality'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Structure from motion causally integrated over time': Paper(DOI='10.1109/34.993559', crossref_json=None, google_schorlar_metadata=None, title='Structure from motion causally integrated over time', authors=['Alessandro Chiuso', 'Paolo Favaro', 'Hailin Jin', 'Stefano Soatto'], abstract='We describe an algorithm for reconstructing three-dimensional structure and motion causally, in real time from monocular sequences of images. We prove that the algorithm is minimal and stable, in the sense that the estimation error remains bounded with probability one throughout a sequence of arbitrary length. We discuss a scheme for handling occlusions (point features appearing and disappearing) and drift in the scale factor. These issues are crucial for the algorithm to operate in real time on real scenes. We describe in detail the implementation of the algorithm, which runs on a personal computer and has been made available to the community. We report the performance of our implementation on a few representative long sequences of real and synthetic images. The algorithm, which has been tested extensively over the course of the past few years, exhibits honest performance when the scene contains at\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF00204399', '10.1016/0921-8890(90)90019-W', '10.1007/BF01212362', '10.1364/JOSAA.8.000377', '10.1016/0262-8856(93)90021-8', '10.1109/CVPR.2000.854954', '10.1007/BF00126394', '10.1109/34.232074', '10.1109/MVIEW.1999.781081', '10.1109/34.865186', '10.1007/BF00133032', '10.1109/ICPR.1994.576402', '10.1109/34.67631', '10.1109/34.584098', '10.1007/BF00115698', '10.1109/TPAMI.1986.4767755', '10.1109/34.713360', '10.1016/S0005-1098(97)00048-4', '10.1109/CDC.1994.411638', '10.1023/A:1026563712076', '10.1007/BFb0028336', '10.1016/0167-6911(94)00064-3', '10.1109/ICPR.1994.576308', '10.1006/ciun.1994.1010', '10.1109/9.754809', '10.1109/TPAMI.1985.4767678', '10.1007/BF00129684', '10.1109/34.149584', '10.1109/34.387503', '10.1006/jvci.1994.1002'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'computational photography', 'inverse problems', 'optimization methods'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Information dropout: Learning optimal representations through noisy computation': Paper(DOI='10.1109/tpami.2017.2784440', crossref_json=None, google_schorlar_metadata=None, title='Information dropout: Learning optimal representations through noisy computation', authors=['Alessandro Achille', 'Stefano Soatto'], abstract='The cross-entropy loss commonly used in deep learning is closely related to the defining properties of optimal representations, but does not enforce some of the key properties. We show that this can be solved by adding a regularization term, which is in turn related to injecting multiplicative noise in the activations of a Deep Neural Network, a special case of which is the common practice of dropout. We show that our regularized loss function can be efficiently minimized using Information Dropout, a generalization of dropout rooted in information theoretic principles that automatically adapts to the data and can better exploit architectures of limited capacity. When the task is the reconstruction of the input, we show that our loss function yields a Variational Autoencoder as a special case, thus providing a link between representation learning, information theory and variational inference. Finally, we prove that we can\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1098/rsta.2009.0110', '10.1109/CVPR.2009.5206704', '10.1093/imaiai/iaw009', '10.1109/CVPR.2011.5995635', '10.1109/TPAMI.2013.50', '10.1147/rd.41.0066', '10.1109/ITW.2015.7133169', '10.1007/978-94-011-5014-9_10', '10.1016/0165-1684(94)90029-9', '10.1162/neco.1995.7.6.1129'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence', 'Robotics', 'Augmented Reality'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Motion competition: A variational approach to piecewise parametric motion segmentation': Paper(DOI='10.1007/s11263-005-4882-4', crossref_json=None, google_schorlar_metadata=None, title='Motion competition: A variational approach to piecewise parametric motion segmentation', authors=['Daniel Cremers', 'Stefano Soatto'], abstract=' We present a novel variational approach for segmenting the image plane into a set of regions of parametric motion on the basis of two consecutive frames from an image sequence. Our model is based on a conditional probability for the spatio-temporal image gradient, given a particular velocity model, and on a geometric prior on the estimated motion field favoring motion boundaries of minimal length. Exploiting the Bayesian framework, we derive a cost functional which depends on parametric motion models for each of a set of regions and on the boundary separating these regions. The resulting functional can be interpreted as an extension of the Mumford-Shah functional from intensity segmentation to motion segmentation. In contrast to most alternative approaches, the problems of segmentation and motion estimation are jointly solved by continuous minimization of a single functional. Minimizing\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.1995.466859', '10.1109/34.85668', '10.1007/3-540-57956-7_15', '10.1006/cviu.1996.0006', '10.1007/BF01420735', '10.1007/978-3-540-24673-2_3', '10.1137/S0036142994275044', '10.1109/ICCV.1995.466871', '10.1109/83.902291', '10.1007/3-540-44935-3_42', '10.1109/CVPR.2003.1211337', '10.1016/S0262-8856(02)00128-2', '10.1109/ICCV.2003.1238442', '10.1023/A:1020826424915', '10.1007/978-3-540-45243-0_41', '10.1007/3-540-45053-X_25', '10.1007/3-540-45054-8_38', '10.1109/34.250841', '10.1023/A:1023031708305', '10.1007/BF00133570', '10.1109/ICCV.1995.466855', '10.1109/34.161350', '10.1023/A:1008318126505', '10.1109/34.368173', '10.1109/ICCV.1995.466850', '10.1109/83.668027', '10.1023/A:1013539930159', '10.1007/978-1-4684-0567-5', '10.1002/cpa.3160420503', '10.1109/TPAMI.1986.4767833', '10.1109/CVPR.2000.855864', '10.1006/jvci.1995.1029', '10.1016/S0165-1684(98)00003-6', '10.1016/0021-9991(88)90002-2', '10.1109/34.841758', '10.1023/A:1008183109594', '10.1007/BF00127172', '10.1137/S1064827596298245', '10.1006/jcph.1994.1155', '10.1109/83.334981', '10.1023/A:1013614317973', '10.1023/A:1023079624234', '10.1006/jcph.1996.0167', '10.1109/83.413167', '10.1109/34.537343'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence', 'Robotics', 'Augmented Reality'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Neural module networks': Paper(DOI='10.1016/s0893608002002356', crossref_json=None, google_schorlar_metadata=None, title='Neural module networks', authors=['Jacob Andreas', 'Marcus Rohrbach', 'Trevor Darrell', 'Dan Klein'], abstract='Visual question answering is fundamentally compositional in nature---a question like\" where is the dog?\" shares substructure with questions like\" what color is the dog?\" and\" where is the cat?\" This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning _neural module networks_, which compose collections of jointly-trained neural\" modules\" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'AI'], conference_acronym='Neural networks (Print)', publisher=None, query_handler=None),\n",
       " 'Memory Aware Synapses: Learning what (not) to forget': Paper(DOI='10.1007/978-3-030-01219-9_9', crossref_json=None, google_schorlar_metadata=None, title='Memory Aware Synapses: Learning what (not) to forget', authors=['Rahaf Aljundi', 'Francesca Babiloni', 'Mohamed Elhoseiny', 'Marcus Rohrbach', 'Tinne Tuytelaars'], abstract='Humans can learn in a continuous manner. Old rarely utilized knowledge can be overwritten by new incoming information while important, frequently used knowledge is prevented from being erased. In artificial learning systems, lifelong learning so far has focused mainly on accumulating knowledge over tasks and overcoming catastrophic forgetting. In this paper, we argue that, given the limited model capacity and the unlimited new information to be learned, knowledge has to be preserved or erased selectively. Inspired by neuroplasticity, we propose a novel approach for lifelong learning, coined Memory Aware Synapses (MAS). It computes the importance of the parameters of a neural network in an unsupervised and online manner. Given a new sample which is fed to the network, MAS accumulates an importance measure for each parameter of the network, based on how sensitive the predicted output function is to a change in this parameter. When learning a new task, changes to important parameters can then be penalized, effectively preventing important knowledge related to previous tasks from being overwritten. Further, we show an interesting connection between a local version of our method and Hebb’s rule, which is a model for the learning process in the brain. We test our method on a sequence of object recognition tasks and on the challenging problem of learning an embedding for predicting< subject, predicate, object> triplets. We show state-of-the-art performance and, for the first time, the ability to adapt the importance of the parameters based on unlabeled data towards what the network needs (not) to forget, which may vary\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2017.753', '10.1609/aaai.v31i1.11214', '10.1016/S1364-6613(99)01294-2', '10.1073/pnas.1717042115', '10.1109/ICCVW.2013.77', '10.1007/978-3-319-46493-0_37', '10.1016/S0079-7421(08)60536-8', '10.1109/ICVGIP.2008.47', '10.1109/CVPR.2009.5206537', '10.1109/ICCV.2017.148', '10.1109/CVPR.2017.587', '10.1023/A:1007331723572', '10.1109/CVPR.2015.7298746', '10.1007/s11263-015-0816-y', '10.1109/ICCV.2017.368', '10.1016/0921-8890(95)00004-Y'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'AI'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'On tiny episodic memories in continual learning': Paper(DOI='10.1007/s00138-023-01420-3', crossref_json=None, google_schorlar_metadata=None, title='On tiny episodic memories in continual learning', authors=['Arslan Chaudhry', 'Marcus Rohrbach', 'Mohamed Elhoseiny', 'Thalaiyasingam Ajanthan', 'Puneet K Dokania', 'Philip HS Torr', \"Marc'Aurelio Ranzato\"], abstract='In continual learning (CL), an agent learns from a stream of tasks leveraging prior experience to transfer knowledge to future tasks. It is an ideal framework to decrease the amount of supervision in the existing learning algorithms. But for a successful knowledge transfer, the learner needs to remember how to perform previous tasks. One way to endow the learner the ability to perform tasks seen in the past is to store a small memory, dubbed episodic memory, that stores few examples from previous tasks and then to replay these examples when training for future tasks. In this work, we empirically analyze the effectiveness of a very small episodic memory in a CL setup where each training example is only seen once. Surprisingly, across four rather different supervised learning benchmarks adapted to CL, a very simple baseline, that jointly trains on both examples from the current task as well as examples stored in the episodic memory, significantly outperforms specifically designed CL approaches with and without episodic memory. Interestingly, we find that repetitive training on even tiny memories of past tasks does not harm generalization, on the contrary, it improves it, with gains between 7\\\\% and 17\\\\% when the memory is populated with a single example per class.', conference=None, journal=None, year=None, reference_list=['10.1037/0033-295X.97.2.285', '10.1007/978-1-4615-5529-2_11', '10.1016/0921-8890(95)00004-Y', '10.1609/aaai.v35i8.16861', '10.1016/j.neucom.2021.10.021', '10.1007/978-3-030-58536-5_31', '10.1109/ICCV48922.2021.00925', '10.1109/CVPR.2016.319', '10.1109/ACCESS.2018.2870052', '10.1109/ICCV.2017.74', '10.1007/s11263-017-1059-x', '10.1109/ICCV48922.2021.00814', '10.1609/aaai.v35i11.17159', '10.1109/WACV56688.2023.00524', '10.1109/TPAMI.2021.3057446', '10.1073/pnas.1611835114', '10.1007/978-3-030-01219-9_9', '10.1109/TPAMI.2017.2773081', '10.1109/CVPR.2019.00528', '10.1109/CVPR.2018.00810', '10.1109/ACCESS.2021.3126027', '10.1080/09540099550039318', '10.1109/CVPR.2017.587', '10.1609/aaai.v37i8.26157', '10.1109/CVPRW53098.2021.00398', '10.22541/au.162464884.44336363/v1', '10.1109/CVPR.2015.7298731', '10.1109/CVPR.2015.7298938', '10.1109/WACV.2018.00097', '10.1109/CVPR52688.2022.00998', '10.1109/ICCV48922.2021.00108', '10.1109/ICRA.2019.8793982', '10.1109/ICCV.2019.00612', '10.1145/3489517.3530642', '10.1007/s11263-016-0911-8'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'AI'], conference_acronym='Machine vision and applications', publisher=None, query_handler=None),\n",
       " 'Unsupervised Learning of Visual Features by Contrasting Cluster Assignments': Paper(DOI='10.3390/vision3030047', crossref_json=None, google_schorlar_metadata=None, title='Unsupervised Learning of Visual Features by Contrasting Cluster Assignments', authors=['Mathilde Caron', 'Ishan Misra', 'Julien Mairal', 'Priya Goyal', 'Piotr Bojanowski', 'Armand Joulin'], abstract='Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the code of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.', conference=None, journal=None, year=None, reference_list=['10.1113/jphysiol.1968.sp008455', '10.1002/9783527680863.ch14', '10.1088/0954-898X_7_2_014', '10.1109/TSP.2018.2846226', '10.1038/srep11400', '10.1016/S0042-6989(97)00169-7', '10.1561/0600000058', '10.1038/nrn1949', '10.1523/JNEUROSCI.6284-11.2012', '10.1038/90526', '10.1038/nrn3136', '10.1152/jn.2002.88.1.455', '10.1007/s10827-006-0003-9', '10.1162/neco_a_00997', '10.1371/journal.pcbi.1005070', '10.1016/j.neucom.2004.01.133', '10.1038/4580', '10.1162/neco.2010.05-08-795', '10.1038/nature04485', '10.1007/BF00275687', '10.1214/009053604000000067', '10.1137/080716542', '10.1523/JNEUROSCI.23-21-07940.2003', '10.1103/PhysRevLett.90.088104', '10.1162/neco_a_01191', '10.1109/TAC.1974.1100705', '10.1109/TNN.2004.833303', '10.1155/2007/90727', '10.1137/1118101', '10.1515/znc-1981-9-1040', '10.1146/annurev.neuro.24.1.1193'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'artificial intelligence', 'optimization', 'computer vision', 'image processing'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Learning Multiscale Sparse Representations for Image and Video Restoration': Paper(DOI='10.1137/070697653', crossref_json=None, google_schorlar_metadata=None, title='Learning Multiscale Sparse Representations for Image and Video Restoration', authors=['Julien Mairal', 'Guillermo Sapiro', 'Michael Elad'], abstract='This paper presents a framework for learning multiscale sparse representations of color images and video with overcomplete dictionaries. A single-scale K-SVD algorithm was introduced in [M. Aharon, M. Elad, and A. M. Bruckstein, IEEE Trans. Signal Process., 54 (2006), pp. 4311–4322], formulating sparse dictionary learning for grayscale image representation as an optimization problem, efficiently solved via orthogonal matching pursuit (OMP) and singular value decomposition (SVD). Following this work, we propose a multiscale learned representation, obtained by using an efficient quadtree decomposition of the learned dictionary and overlapping image patches. The proposed framework provides an alternative to predefined dictionaries such as wavelets and is shown to lead to state-of-the-art results in a number of image and video enhancement and restoration applications. This paper describes the proposed\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TSP.2006.881199', '10.1137/040616024', '10.1214/aos/1028674842', '10.1002/cpa.10116', '10.1137/S1064827596304010', '10.1109/TIP.2006.877521', '10.1109/TIP.2004.833105', '10.1109/TIP.2007.901238', '10.1007/BF02678430', '10.1117/12.173207', '10.1109/TSP.2003.815389', '10.1214/aos/1018031261', '10.1109/TIT.2005.860430', '10.1109/TIP.2006.881969', '10.1109/34.93808', '10.1109/TIP.2006.877529', '10.1109/83.784434', '10.1137/040619454', '10.1109/TIP.2005.847295', '10.1109/TIP.2007.911828', '10.1109/TIP.2005.843753', '10.1109/78.258082', '10.1109/TIP.2004.838697', '10.1109/TIP.2006.888343', '10.1109/TIP.2004.823819', '10.1109/18.119725', '10.1109/TIT.2004.834793', '10.1109/TIT.2005.864420', '10.1109/TPAMI.2007.60'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'artificial intelligence', 'optimization', 'computer vision', 'image processing'], conference_acronym='Multiscale modeling & simulation (Print)', publisher=None, query_handler=None),\n",
       " 'A Universal Catalyst for First-Order Optimization': Paper(DOI='10.1007/s11590-023-02060-2', crossref_json=None, google_schorlar_metadata=None, title='A Universal Catalyst for First-Order Optimization', authors=['Hongzhou Lin', 'Julien Mairal', 'Zaid Harchaoui'], abstract='We introduce a generic scheme for accelerating first-order optimization methods in the sense of Nesterov, which builds upon a new analysis of the accelerated proximal point algorithm. Our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. This strategy applies to a large class of algorithms, including gradient descent, block coordinate descent, SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants. For all of these methods, we provide acceleration and explicit support for non-strongly convex objectives. In addition to theoretical speed-up, we also show that acceleration is useful in practice, especially for ill-conditioned problems where we measure significant improvements.', conference=None, journal=None, year=None, reference_list=['10.1137/050644641', '10.1007/s10107-016-1091-6', '10.1145/2049662.2049663', '10.1137/21M1428601', '10.1137/19m1244603', '10.1080/01621459.1986.10478327', '10.1137/110848864', '10.1145/227683.227684', '10.1137/18M117306X', '10.1007/s10957-023-02178-4', '10.5802/aif.1638', '10.1007/s10107-013-0737-x', '10.1109/WSC.2013.6721494', '10.5802/aif.1384', '10.1111/1467-9868.00082', '10.1080/03610918208812247', '10.1007/s10107-014-0790-0', '10.1007/s10208-021-09502-2', '10.1137/18M1224568'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'artificial intelligence', 'optimization', 'computer vision', 'image processing'], conference_acronym='Optimization letters (Print)', publisher=None, query_handler=None),\n",
       " 'Convolutional Kernel Networks': Paper(DOI='10.31525/ct1-nct03857373', crossref_json=None, google_schorlar_metadata=None, title='Convolutional Kernel Networks', authors=['Julien Mairal', 'Piotr Koniusz', 'Zaid Harchaoui', 'Cordelia Schmid'], abstract='An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data. Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, eg, digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'artificial intelligence', 'optimization', 'computer vision', 'image processing'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Structured sparsity through convex optimization': Paper(DOI='10.1214/12-sts394', crossref_json=None, google_schorlar_metadata=None, title='Structured sparsity through convex optimization', authors=['Francis Bach', 'Rodolphe Jenatton', 'Julien Mairal', 'Guillaume Obozinski'], abstract='  Sparse estimation methods are aimed at using or obtaining parsimonious representations of data or models. While naturally cast as a combinatorial optimization problem, variable or feature selection admits a convex relaxation through the regularization by the -norm. In this paper, we consider situations where we are not only interested in sparsity, but where some structural prior knowledge is available as well. We show that the -norm can then be extended to structured norms built on either disjoint or overlapping groups of variables, leading to a flexible framework that can deal with various structures. We present applications to unsupervised learning, for structured sparse principal component analysis and hierarchical dictionary learning, and to supervised learning in the context of nonlinear variable selection. ', conference=None, journal=None, year=None, reference_list=['10.1214/009053604000000067', '10.1111/j.2517-6161.1996.tb02080.x', '10.1214/009053606000000281', '10.1137/S1064827596304010', '10.1198/016214506000000735', '10.1111/j.1467-9868.2005.00532.x', '10.1214/07-AOS584', '10.1198/1061860032148', '10.1198/106186006X113430', '10.1016/0167-2789(92)90242-F', '10.1214/11-AOS896', '10.1093/biostatistics/kxp008', '10.1214/07-AOAS131', '10.1038/44565', '10.1073/pnas.0307752101', '10.1137/080716542', '10.1080/01621459.1995.10476626', '10.1214/009053606000000722', '10.1038/381607a0', '10.1093/bioinformatics/btp218', '10.1214/09-AOS778', '10.1111/j.1467-9868.2005.00490.x', '10.1007/s11222-008-9111-x', '10.1214/07-AOAS147', '10.1109/TIT.2009.2016018', '10.1016/0022-247X(71)90184-3', '10.1214/08-AOS620', '10.1111/j.1467-9868.2009.00718.x', '10.1109/TIT.2004.834793', '10.1109/TIT.2005.864420', '10.1109/TIT.2005.858979', '10.1137/090759574', '10.1137/090756855', '10.1109/TSP.2009.2016892', '10.1214/08-AOS659', '10.1111/j.1541-0420.2007.00843.x', '10.1198/jasa.2010.tm09380', '10.1109/TIT.2011.2144150', '10.1214/09-AOS776', '10.1017/CBO9780511809682', '10.1007/978-0-387-21606-5', '10.1007/3-540-36755-1_3', '10.1214/11-AOAS514', '10.1007/978-1-4419-9569-8_10', '10.1109/ISBI.2009.5193003', '10.1145/1553374.1553431', '10.1109/CVPR.2009.5206545', '10.1145/1553374.1553458', '10.1016/B978-012466606-1/50008-8', '10.1109/ICIP.2011.6115845', '10.1145/1390156.1390263', '10.1007/978-3-540-87481-2_24', '10.1109/CISS.2010.5464845', '10.1109/TSP.2006.881199', '10.1561/2200000015', '10.1109/TIT.2010.2040894', '10.1145/1667053.1667056', '10.1007/978-0-387-31256-9', '10.21236/ADA520187', '10.1109/PRNI.2011.15', '10.1109/34.908974', '10.1016/j.acha.2008.07.002', '10.1007/978-1-4419-8853-9', '10.1214/12-EJS672', '10.1093/bioinformatics/btn188', '10.1109/TSP.2009.2020754', '10.1198/004017005000000139', '10.7551/mitpress/7503.003.0200'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'artificial intelligence', 'optimization', 'computer vision', 'image processing'], conference_acronym='Statistical science', publisher=None, query_handler=None),\n",
       " 'Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning': Paper(DOI='10.1137/140957639', crossref_json=None, google_schorlar_metadata=None, title='Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning', authors=['Julien Mairal'], abstract='Majorization-minimization algorithms consist of successively minimizing a sequence of upper bounds of the objective function. These upper bounds are tight at the current estimate, and each iteration monotonically drives the objective function downhill. Such a simple principle is widely applicable and has been very popular in various scientific fields, especially in signal processing and statistics. We propose an incremental majorization-minimization scheme for minimizing a large sum of continuous functions, a problem of utmost importance in machine learning. We present convergence guarantees for nonconvex and convex optimization when the upper bounds approximate the objective up to a smooth error; we call such upper bounds “first-order surrogate functions.” More precisely, we study asymptotic stationary point guarantees for nonconvex problems, and for convex ones, we provide convergence rates for the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TMI.2005.862740', '10.1561/2200000015', '10.1137/080716542', '10.1137/120887679', '10.1137/040615961', '10.1007/BF00049423', '10.1007/s00041-008-9045-x', '10.1023/A:1013912006537', '10.1137/050626090', '10.1002/cpa.20042', '10.1088/0031-9155/44/11/311', '10.1109/TPAMI.2005.59', '10.1109/TSP.2009.2026004', '10.1137/110848864', '10.1137/070698920', '10.1023/A:1021765131316', '10.1007/s10107-010-0434-y', '10.1080/10618600.2000.10474858', '10.1137/070704277', '10.1007/s10107-012-0629-5', '10.1137/120891009', '10.1198/004017005000000139', '10.1561/2200000001', '10.1109/TSP.2009.2016892', '10.1111/j.1467-9868.2005.00532.x'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'artificial intelligence', 'optimization', 'computer vision', 'image processing'], conference_acronym='SIAM journal on optimization (Print)', publisher=None, query_handler=None),\n",
       " 'Pointer Sentinel Mixture Models': Paper(DOI='10.1080/05544246.1989.9945371', crossref_json=None, google_schorlar_metadata=None, title='Pointer Sentinel Mixture Models', authors=['Stephen Merity', 'Caiming Xiong', 'James Bradbury', 'Richard Socher'], abstract='Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'NLP', 'Computer Vision', 'Multimedia', 'Data Mining'], conference_acronym='The Pointer (Washington, D.C.)', publisher=None, query_handler=None),\n",
       " 'Dynamic Memory Networks for Visual and Textual Question Answering': Paper(DOI='10.1016/j.eswa.2017.03.006', crossref_json=None, google_schorlar_metadata=None, title='Dynamic Memory Networks for Visual and Textual Question Answering', authors=['Caiming Xiong', 'Stephen Merity', 'Richard Socher'], abstract='Neural network architectures with memory and attention mechanisms exhibit certain reason-ing capabilities required for question answering. One such architecture, the dynamic memory net-work (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the bAbI-10k text question-answering dataset without supporting fact supervision.', conference=None, journal=None, year=None, reference_list=['10.1147/JRD.2012.2186519', '10.1162/neco.1997.9.8.1735', '10.1109/ACCESS.2016.2609279', '10.1109/ACCESS.2016.2600622', '10.1109/78.650093'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'NLP', 'Computer Vision', 'Multimedia', 'Data Mining'], conference_acronym='Expert systems with applications', publisher=None, query_handler=None),\n",
       " 'Prototypical contrastive learning of unsupervised representations': Paper(DOI='10.1016/j.neucom.2021.07.015', crossref_json=None, google_schorlar_metadata=None, title='Prototypical contrastive learning of unsupervised representations', authors=['Junnan Li', 'Pan Zhou', 'Caiming Xiong', 'Steven CH Hoi'], abstract='This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that addresses the fundamental limitations of instance-wise contrastive learning. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.', conference=None, journal=None, year=None, reference_list=['10.1109/TGRS.2018.2867679', '10.1007/978-3-7908-2604-3_16', '10.1109/TGRS.2016.2584107', '10.1109/LGRS.2007.900751', '10.1016/S1352-2310(97)00447-0', '10.1155/2015/258619', '10.1109/TBDATA.2019.2921572', '10.1109/TGRS.2016.2536685', '10.1109/TGRS.2011.2165957', '10.1109/TGRS.2017.2769673', '10.1109/TGRS.2013.2277251', '10.1109/TGRS.2019.2908756', '10.1109/TGRS.2016.2636241', '10.1038/nbt1206-1565', '10.1109/LGRS.2017.2686878', '10.1109/TGRS.2015.2478379', '10.1109/21.97458', '10.1109/TGRS.2020.3045273', '10.1109/TGRS.2018.2794326', '10.1109/LGRS.2015.2482520', '10.1109/TGRS.2011.2153861', '10.1109/TGRS.2019.2912468', '10.1016/0169-7439(87)80084-9', '10.1109/LGRS.2017.2780890', '10.1109/TGRS.2016.2616649', '10.1109/JSTARS.2019.2894802', '10.1109/TGRS.2017.2755542', '10.1109/TGRS.2018.2805286'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Multimodal AI', 'NLP', 'Multimedia', 'Computer Vision'], conference_acronym='Neurocomputing (Amsterdam)', publisher=None, query_handler=None),\n",
       " 'Quasi-Recurrent Neural Networks': Paper(DOI='10.1016/j.neunet.2018.04.007', crossref_json=None, google_schorlar_metadata=None, title='Quasi-Recurrent Neural Networks', authors=['James Bradbury', 'Stephen Merity', 'Caiming Xiong', 'Richard Socher'], abstract=\"Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.\", conference=None, journal=None, year=None, reference_list=['10.1007/978-3-319-48317-7_10', '10.1109/3.622622', '10.1007/s11071-015-2242-7', '10.1109/TNNLS.2015.2475737', '10.1016/j.neunet.2016.05.003', '10.1109/TNNLS.2014.2322499', '10.1016/S0960-0779(04)00542-9', '10.1007/s11043-016-9291-2', '10.1016/0250-6874(89)87036-2', '10.1109/TNNLS.2013.2294638', '10.1016/j.physa.2017.04.124', '10.1016/j.physa.2017.01.009', '10.1016/j.amc.2016.07.029', '10.1049/iet-cta.2017.0196', '10.1016/j.neunet.2017.02.011', '10.1080/10652460310001600717', '10.1016/0893-6080(94)00083-X', '10.1002/cta.4490200508', '10.1016/j.chaos.2015.08.003', '10.1016/j.ijleo.2016.10.067', '10.1007/s12215-016-0248-8', '10.1016/j.camwa.2011.04.057', '10.1142/S0218127417502091', '10.1016/j.physa.2015.03.089', '10.1109/TNNLS.2015.2506738', '10.1016/j.neucom.2013.01.041', '10.1016/j.neucom.2017.01.014', '10.1016/j.neunet.2015.02.007', '10.1016/j.neunet.2013.10.002', '10.1016/j.amc.2016.11.027', '10.1016/j.chaos.2017.03.009', '10.1109/TSMC.2016.2523935', '10.1016/j.physa.2016.12.030', '10.1007/s11071-017-3613-z'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'NLP', 'Computer Vision', 'Multimedia', 'Data Mining'], conference_acronym='Neural networks (Print)', publisher=None, query_handler=None),\n",
       " 'Summeval: Re-evaluating summarization evaluation': Paper(DOI='10.1162/tacl_a_00373', crossref_json=None, google_schorlar_metadata=None, title='Summeval: Re-evaluating summarization evaluation', authors=['Alexander R Fabbri', 'Wojciech Kryściński', 'Bryan McCann', 'Caiming Xiong', 'Richard Socher', 'Dragomir Radev'], abstract=' The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics; and 5) we\\xa0…', conference=None, journal=None, year=None, reference_list=['10.18653/v1/D19-1307', '10.18653/v1/P19-2034', '10.18653/v1/P18-1060', '10.18653/v1/P18-1063', '10.18653/v1/P19-1264', '10.18653/v1/2020.nlposs-1.17', '10.18653/v1/D18-1409', '10.18653/v1/2020.acl-main.454', '10.18653/v1/D18-1443', '10.18653/v1/D15-1013', '10.18653/v1/P18-1064', '10.18653/v1/P19-1330', '10.18653/v1/P18-1013', '10.18653/v1/D18-1440', '10.18653/v1/D18-1208', '10.18653/v1/D19-1051', '10.18653/v1/2020.emnlp-main.750', '10.18653/v1/D18-1207', '10.3115/1626355.1626389', '10.1162/COLI_a_00123', '10.18653/v1/D15-1222', '10.18653/v1/P19-1502', '10.18653/v1/W17-4510', '10.18653/v1/W15-3049', '10.18653/v1/D19-1320', '10.18653/v1/P17-1099', '10.18653/v1/D18-1085', '10.18653/v1/D19-1323', '10.18653/v1/2020.acl-main.450', '10.1007/BF00992696', '10.18653/v1/D18-1089', '10.18653/v1/D18-1088', '10.18653/v1/P19-1499'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'NLP', 'Computer Vision', 'Multimedia', 'Data Mining'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Sparse subspace clustering: Algorithm, theory, and applications': Paper(DOI='10.1109/tpami.2013.57', crossref_json=None, google_schorlar_metadata=None, title='Sparse subspace clustering: Algorithm, theory, and applications', authors=['Ehsan Elhamifar', 'Rene Vidal'], abstract='Many real-world problems deal with collections of high-dimensional data, such as images, videos, text, and web documents, DNA microarray data, and more. Often, such high-dimensional data lie close to low-dimensional structures corresponding to several classes or categories to which the data belong. In this paper, we propose and study an algorithm, called sparse subspace clustering, to cluster data points that lie in a union of low-dimensional subspaces. The key idea is that, among the infinitely many possible representations of a data point in terms of other points, a sparse representation corresponds to selecting a few points from the same subspace. This motivates solving a sparse optimization program whose solution is used in a spectral clustering framework to infer the clustering of the data into subspaces. Since solving the sparse optimization program is in general NP-hard, we consider a convex relaxation\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2011.6126422', '10.1109/ICASSP.2010.5495317', '10.1214/12-AOS1034', '10.1137/070697835', '10.1109/CVPR.2009.5206547', '10.1145/1970392.1970395', '10.1007/s10208-009-9045-5', '10.1109/TIT.2005.858979', '10.1002/cpa.20132', '10.1109/ICCV.2001.937679', '10.1109/ICCV.2009.5459173', '10.1109/CVPR.2003.1211482', '10.1109/CVPR.2007.382974', '10.1007/s11222-007-9033-z', '10.1109/WVM.1991.212809', '10.1007/s11263-008-0178-9', '10.1109/TPAMI.2005.92', '10.1109/TPAMI.2009.191', '10.1109/CVPR.2007.383235', '10.1109/CVPR.2011.5995679', '10.1145/358669.358692', '10.1109/CVPR.2012.6247852', '10.1016/j.sigpro.2005.05.031', '10.1016/0898-1221(76)90003-1', '10.1109/CVPR.2004.1315101', '10.1007/978-3-540-30212-4_2', '10.1073/pnas.0437847100', '10.1109/TIT.2003.820031', '10.1109/TPAMI.2008.79', '10.1109/TIT.2010.2043876', '10.1109/CVPR.2003.1211332', '10.1023/A:1004678431677', '10.1109/TPAMI.2005.244', '10.1162/089976699300016728', '10.1137/060655523', '10.1561/2200000016', '10.1023/A:1008026310903', '10.1016/S0304-3975(97)00115-1', '10.1109/TSP.2012.2196694', '10.1109/JSTSP.2008.924384', '10.1007/BF00129684', '10.1109/TSP.2009.2020754', '10.1109/TPAMI.2003.1177153', '10.1109/TIT.2009.2030471', '10.1214/ss/1028905973', '10.1109/TSP.2010.2044837', '10.1109/MSP.2010.939739', '10.1017/CBO9780511804441', '10.1109/TIP.2006.882016', '10.1109/JSTSP.2007.910971', '10.1016/j.cviu.2007.07.005', '10.1023/A:1008000628999', '10.1109/TPAMI.2012.88', '10.1109/CVPR.2011.5995365', '10.1109/CVPR.2011.5995664'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Biomedical Data Science', 'Artificial Intelligence', 'Control Theory'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Probabilistic pursuit-evasion games: theory, implementation, and experimental evaluation': Paper(DOI='10.1109/tra.2002.804040', crossref_json=None, google_schorlar_metadata=None, title='Probabilistic pursuit-evasion games: theory, implementation, and experimental evaluation', authors=['Rene Vidal', 'Omid Shakernia', 'H Jin Kim', 'David Hyunchul Shim', 'Shankar Sastry'], abstract='We consider the problem of having a team of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) pursue a second team of evaders while concurrently building a map in an unknown environment. We cast the problem in a probabilistic game theoretical framework, and consider two computationally feasible greedy pursuit policies: local-mar and global-max. To implement this scenario on real UAVs and UGVs, we propose a distributed hierarchical hybrid system architecture which emphasizes the autonomy of each agent, yet allows for coordinated team efforts. We describe the implementation of the architecture on a fleet of UAVs and UGVs, detailing components such as high-level pursuit policy computation, map building and interagent communication, and low-level navigation, sensing, and control. We present both simulation and experimental results of real pursuit-evasion games involving our\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CDC.2001.980175', '10.1109/CDC.2000.914136', '10.1023/A:1008942012299', '10.1109/IJCNN.1999.830822', '10.1109/ROBOT.2000.846477', '10.1109/ROBOT.1999.772581', '10.1109/ICSMC.1999.816641', '10.1109/ROBOT.2001.932859', '10.1137/0221051', '10.1109/ROBOT.1999.770350', '10.1109/CDC.1999.831290', '10.1145/274787.274788', '10.2514/6.2000-4057', '10.1007/3-540-64473-3_46', '10.1016/S0004-3702(99)00026-0', '10.1109/9.664155', '10.1109/CDC.2001.981057', '10.1109/CDC.1997.657516'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Biomedical Data Science', 'Artificial Intelligence', 'Control Theory'], conference_acronym='IEEE transactions on robotics and automation', publisher=None, query_handler=None),\n",
       " 'Low rank subspace clustering (LRSC)': Paper(DOI='10.1016/j.patrec.2013.08.006', crossref_json=None, google_schorlar_metadata=None, title='Low rank subspace clustering (LRSC)', authors=['René Vidal', 'Paolo Favaro'], abstract='We consider the problem of fitting a union of subspaces to a collection of data points drawn from one or more subspaces and corrupted by noise and/or gross errors. We pose this problem as a non-convex optimization problem, where the goal is to decompose the corrupted data matrix as the sum of a clean and self-expressive dictionary plus a matrix of noise and/or gross errors. By self-expressive we mean a dictionary whose atoms can be expressed as linear combinations of themselves with low-rank coefficients. In the case of noisy data, our key contribution is to show that this non-convex matrix decomposition problem can be solved in closed form from the SVD of the noisy data matrix. The solution involves a novel polynomial thresholding operator on the singular values of the data matrix, which requires minimal shrinkage. For one subspace, a particular case of our framework leads to classical PCA, which\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1055558.1055581', '10.1109/TPAMI.2003.1177153', '10.1023/A:1008324625522', '10.1137/080738970', '10.1145/1970392.1970395', '10.1007/s11263-008-0178-9', '10.1023/A:1008000628999', '10.1109/CVPR.2009.5206547', '10.1109/ICASSP.2010.5495317', '10.1109/CVPR.2011.5995365', '10.1023/A:1008026310903', '10.1109/CVPR.2007.383235', '10.1109/CVPR.2003.1211332', '10.1109/TIP.2006.882016', '10.1109/ICCV.2009.5459173', '10.1109/TPAMI.2005.92', '10.1145/1143844.1143919', '10.1109/TPAMI.2007.1085', '10.1007/BF01647331', '10.1561/9781601987174', '10.1109/CVPR.2008.4587437', '10.1109/TPAMI.2009.191', '10.1137/070697835', '10.1007/978-3-540-30212-4_2', '10.1162/089976699300016728', '10.1007/BF00129684', '10.1109/CVPR.2007.382974', '10.1023/A:1004678431677', '10.1023/A:1017501703105', '10.1109/MSP.2010.939739', '10.1109/TPAMI.2005.244', '10.1007/s11263-007-0099-z', '10.1007/s11222-007-9033-z', '10.1007/11744085_8', '10.1016/j.cviu.2007.07.005', '10.1109/ICCVW.2009.5457695', '10.1109/CVPR.2010.5539866'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'computational photography', 'inverse problems', 'optimization methods'], conference_acronym='Pattern recognition letters', publisher=None, query_handler=None),\n",
       " 'Identification of hybrid systems: A tutorial': Paper(DOI='10.3166/ejc.13.242-260', crossref_json=None, google_schorlar_metadata=None, title='Identification of hybrid systems: A tutorial', authors=['Simone Paoletti', 'Aleksandar Lj Juloski', 'Giancarlo Ferrari-Trecate', 'René Vidal'], abstract='This tutorial paper is concerned with the identification of hybrid models, i.e. dynamical models whose behavior is determined by interacting continuous and discrete dynamics. Methods specifically aimed at the identification of models with a hybrid structure are of very recent date. After discussing the main issues and difficulties connected with hybrid system identification, and giving an overview of the related literature, this paper focuses on four different approaches for the identification of switched affine and piecewise affine models, namely an algebraic procedure, a Bayesian procedure, a clusteringbased procedure, and a bounded-error procedure. The main features of the selected procedures are presented, and possible interactions to still enhance their effectiveness are suggested.', conference=None, journal=None, year=None, reference_list=['10.1016/S0166-218X(01)00260-8', '10.1109/78.978374', '10.1109/72.97915', '10.1109/TAC.2000.880987', '10.1109/TAC.2005.856667', '10.1016/S0005-1098(98)00178-2', '10.1080/10556789208805504', '10.1080/10556789408805554', '10.1080/00207178708933894', '10.1109/TCST.2005.860527', '10.1023/A:1008324625522', '10.1023/A:1008663629662', '10.1109/18.256506', '10.1111/j.1467-9892.1986.tb00501.x', '10.1109/72.329691', '10.1109/TCS.1982.1085192', '10.1109/TCS.1983.1085342', '10.1007/BF00994018', '10.1137/S0363012999354648', '10.1080/0020717021000031484', '10.1016/S0005-1098(02)00224-8', '10.1016/S0893-6080(00)00024-1', '10.1109/18.265499', '10.1137/S0036139997325199', '10.1016/S0005-1098(01)00059-0', '10.1109/78.506610', '10.1109/72.728357', '10.1023/A:1008382309369', '10.1016/0005-1098(94)00096-2', '10.1109/81.754847', '10.1109/81.668868', '10.1016/j.conengprac.2004.04.004', '10.1109/TAC.2005.856649', '10.1109/81.168933', '10.1198/106186002317375712', '10.1016/0005-1098(91)90134-N', '10.1016/j.automatica.2004.12.005', '10.1109/18.669422', '10.1016/j.automatica.2003.08.006', '10.1016/j.automatica.2004.11.011', '10.1016/0005-1098(95)00120-8', '10.1080/00207179208934230', '10.1109/TAC.1981.1102596', '10.1007/978-1-4684-7888-4', '10.1109/9.664151', '10.1007/s10851-006-8286-z'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Biomedical Data Science', 'Artificial Intelligence', 'Control Theory'], conference_acronym='European journal of control', publisher=None, query_handler=None),\n",
       " 'Sparse manifold clustering and embedding': Paper(DOI='10.3390/app8112175', crossref_json=None, google_schorlar_metadata=None, title='Sparse manifold clustering and embedding', authors=['Ehsan Elhamifar', 'René Vidal'], abstract='We propose an algorithm called Sparse Manifold Clustering and Embedding (SMCE) for simultaneous clustering and dimensionality reduction of data lying in multiple nonlinear manifolds. Similar to most dimensionality reduction methods, SMCE finds a small neighborhood around each data point and connects each point to its neighbors with appropriate weights. The key difference is that SMCE finds both the neighbors and the weights automatically. This is done by solving a sparse optimization problem, which encourages selecting nearby points that lie in the same manifold and approximately span a low-dimensional affine subspace. The optimal solution encodes information that can be used for clustering and dimensionality reduction using spectral clustering and embedding. Moreover, the size of the optimal neighborhood of a data point, which can be different for different points, provides an estimate of the dimension of the manifold to which the point belongs. Experiments demonstrate that our method can effectively handle multiple manifolds that are very close to each other, manifolds with non-uniform sampling and holes, as well as estimate the intrinsic dimensions of the manifolds.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Biomedical Data Science', 'Artificial Intelligence', 'Control Theory'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Online multiperson tracking-by-detection from a single, uncalibrated camera': Paper(DOI='10.1109/tpami.2010.232', crossref_json=None, google_schorlar_metadata=None, title='Online multiperson tracking-by-detection from a single, uncalibrated camera', authors=['Michael D Breitenstein', 'Fabian Reichlin', 'Bastian Leibe', 'Esther Koller-Meier', 'Luc Van Gool'], abstract='In this paper, we address the problem of automatically detecting and tracking a variable number of persons in complex scenes using a monocular, potentially moving, uncalibrated camera. We propose a novel approach for multiperson tracking-by-detection in a particle filtering framework. In addition to final high-confidence detections, our algorithm uses the continuous confidence of pedestrian detectors and online-trained, instance-specific classifiers as a graded observation model. Thus, generic object category knowledge is complemented by instance-specific information. The main contribution of this paper is to explore how these unreliable information sources can be used for robust multiperson tracking. The algorithm detects and tracks a large number of dynamically moving people in complex scenes with occlusions, does not rely on background modeling, requires no camera or ground plane calibration, and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ROBOT.2001.932850', '10.1109/34.927458', '10.1109/TAC.1979.1102177', '10.1109/CVPR.2006.195', '10.1109/ICCV.2009.5459260', '10.1016/j.imavis.2006.06.003', '10.1109/TPAMI.2004.73', '10.1109/ICCV.2003.1238473', '10.1007/s11263-006-0027-7', '10.1109/WMVC.2008.4544052', '10.1109/CVPR.2005.177', '10.1109/CVPR.2006.258', '10.1155/2008/246309', '10.1109/CVPR.2008.4587583', '10.1109/TPAMI.2007.35', '10.1109/TPAMI.2003.1233896', '10.1109/ICCV.2009.5459278', '10.1023/A:1008078328650', '10.1109/JOE.1983.1145560', '10.1109/TPAMI.2009.109', '10.1007/978-1-4757-3437-9', '10.1109/CVPR.2006.215', '10.1049/ip-f-2.1993.0015', '10.1002/nav.3800020109', '10.1109/TPAMI.2005.223', '10.1109/TPAMI.2006.177', '10.1109/CVPR.2010.5540148', '10.1109/TPAMI.2008.170', '10.1007/s11263-007-0095-3', '10.1109/TPAMI.2008.73'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Object Recognition', 'Tracking', 'Scene Understanding'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Superpixels: An evaluation of the state-of-the-art': Paper(DOI='10.1016/j.cviu.2017.03.007', crossref_json=None, google_schorlar_metadata=None, title='Superpixels: An evaluation of the state-of-the-art', authors=['David Stutz', 'Alexander Hermans', 'Bastian Leibe'], abstract='Superpixels group perceptually similar pixels to create visually meaningful entities while heavily reducing the number of primitives for subsequent processing steps. As of these properties, superpixel algorithms have received much attention since their naming in\\xa02003 (Ren and Malik, 2003). By today, publicly available superpixel algorithms have turned into standard tools in low-level vision. As such, and due to their quick adoption in a wide range of applications, appropriate benchmarks are crucial for algorithm selection and comparison. Until now, the rapidly growing number of algorithms as well as varying experimental setups hindered the development of a unifying benchmark. We present a comprehensive evaluation of 28 state-of-the-art superpixel algorithms utilizing a benchmark focussing on fair comparison and designed to provide new insights relevant for applications. To this end, we explicitly discuss\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2012.120', '10.1109/TPAMI.2010.161', '10.1023/B:VISI.0000022288.19776.77', '10.1007/s11263-008-0140-x', '10.1007/s11263-014-0777-6', '10.1007/s11263-015-0822-0', '10.1145/1073204.1073232', '10.1007/s11263-006-0031-y', '10.1109/TPAMI.2009.96', '10.1007/978-1-4471-6515-6', '10.1109/TMM.2013.2285526', '10.1109/TMI.2011.2171705', '10.1007/BF02998459', '10.1109/TPAMI.2004.1273918', '10.1007/s11263-007-0090-8', '10.1109/TIP.2014.2302892', '10.1109/34.868688', '10.1109/TIP.2014.2300823'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Object Recognition', 'Tracking', 'Scene Understanding'], conference_acronym='Computer vision and image understanding (Print)', publisher=None, query_handler=None),\n",
       " 'Coupled object detection and tracking from static cameras and moving vehicles': Paper(DOI='10.1109/tpami.2008.170', crossref_json=None, google_schorlar_metadata=None, title='Coupled object detection and tracking from static cameras and moving vehicles', authors=['Bastian Leibe', 'Konrad Schindler', 'Nico Cornelis', 'Luc Van Gool'], abstract='We present a novel approach for multi-object tracking which considers object detection and spacetime trajectory estimation as a coupled optimization problem. Our approach is formulated in an MDL hypothesis selection framework, which allows it to recover from mismatches and temporarily lost tracks. Building upon a multi-view/multi-category object detector, it localizes cars and pedestrians in the input images. The 2D object detections are converted to 3D observations, which are accumulated in a world coordinate frame. Trajectory analysis in a spacetime window yields physically plausible trajectory candidates. Tracking is achieved by performing model selection after every frame. At each time instant, our approach searches for the globally optimal set of spacetime trajectories which provides the best explanation for the current image and all evidence collected so far, while satisfying the constraints that no two\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2006.258', '10.1007/s001380050126', '10.1016/S0166-218X(01)00341-9', '10.1109/34.1000236', '10.1109/TPAMI.2003.1195991', '10.1109/CVPR.2006.118', '10.1007/s11263-007-0081-9', '10.1007/BF01440847', '10.1007/s11263-007-0095-3', '10.5244/C.20.119', '10.1109/CVPR.2005.144', '10.1109/CVPR.2006.202', '10.1023/B:VISI.0000029664.99615.94', '10.1109/ICCV.1999.791202', '10.1007/BF01679685', '10.1109/JOE.1983.1145560', '10.1109/CVPR.2005.272', '10.1109/IVS.2000.898368', '10.1109/ICCV.2007.4409092', '10.1109/CVPR.2005.177', '10.1016/S0262-8856(02)00129-4', '10.1109/TPAMI.2005.188', '10.1109/ICCV.2007.4408936', '10.1109/TAC.1979.1102177', '10.1109/CVPR.2006.215', '10.1109/ICCV.2005.107', '10.1023/A:1008078328650', '10.1109/CVPR.2007.383203', '10.1109/CVPR.2005.53', '10.1109/CVPR.2006.283', '10.1007/BF01539538', '10.1109/CVPR.2007.383146', '10.1109/TPAMI.2006.177', '10.1109/CVPR.1999.784637', '10.1023/B:VISI.0000013087.49260.fb', '10.1109/ICCV.2003.1238422', '10.1109/ICCV.2005.74'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Photogrammetry', 'Remote Sensing', 'Image Analysis', 'Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Lisocabtagene maraleucel for patients with relapsed or refractory large B-cell lymphomas (TRANSCEND NHL 001): a multicentre seamless design study': Paper(DOI='10.1016/s0140-6736(20)31366-0', crossref_json=None, google_schorlar_metadata=None, title='Lisocabtagene maraleucel for patients with relapsed or refractory large B-cell lymphomas (TRANSCEND NHL 001): a multicentre seamless design study', authors=['Jeremy S Abramson', 'M Lia Palomba', 'Leo I Gordon', 'Matthew A Lunning', 'Michael Wang', 'Jon Arnason', 'Amitkumar Mehta', 'Enkhtsetseg Purev', 'David G Maloney', 'Charalambos Andreadis', 'Alison Sehgal', 'Scott R Solomon', 'Nilanjan Ghosh', 'Tina M Albertson', 'Jacob Garcia', 'Ana Kostic', 'Mary Mallaney', 'Ken Ogasawara', 'Kathryn Newhall', 'Yeonhee Kim', 'Daniel Li', 'Tanya Siddiqi'], abstract='BackgroundLisocabtagene maraleucel (liso-cel) is an autologous, CD19-directed, chimeric antigen receptor (CAR) T-cell product. We aimed to assess the activity and safety of liso-cel in patients with relapsed or refractory large B-cell lymphomas.MethodsWe did a seamless design study at 14 cancer centres in the USA. We enrolled adult patients (aged ≥18 years) with relapsed or refractory large B-cell lymphomas. Eligible histological subgroups included diffuse large B-cell lymphoma, high-grade B-cell lymphoma with rearrangements of MYC and either BCL2, BCL6, or both (double-hit or triple-hit lymphoma), diffuse large B-cell lymphoma transformed from any indolent lymphoma, primary mediastinal B-cell lymphoma, and follicular lymphoma grade 3B. Patients were assigned to one of three target dose levels of liso-cel as they were sequentially tested in the trial (50\\u2008×\\u2008106 CAR+ T cells [one or two doses], 100\\u2008×\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1200/JCO.2010.28.1618', '10.1038/s41409-019-0650-x', '10.1038/bmt.2015.213', '10.1182/blood-2017-03-769620', '10.1111/bjh.14922', '10.1182/blood-2010-07-259333', '10.1200/JCO.2016.71.3024', '10.1056/NEJMoa1707447', '10.1056/NEJMoa1804980', '10.1126/scitranslmed.aaf8621', '10.1016/j.ebiom.2020.102931', '10.1038/leu.2015.247', '10.1182/blood-2019-127150', '10.1182/blood-2016-01-643569', '10.1093/jnci/djy196', '10.1182/blood-2014-05-552729', '10.1200/JCO.2013.54.8800', '10.1016/j.cct.2016.04.004', '10.1016/S1470-2045(18)30864-7', '10.1186/s13045-018-0571-y', '10.1177/2040620719841581', '10.1182/blood-2019-124750', '10.1182/blood.2019001694', '10.1186/s13045-019-0838-y', '10.1016/j.mayocp.2019.09.006', '10.1182/blood-2019-129097', '10.1182/blood-2019-129624', '10.1182/blood-2017-06-793141'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neurosurgery'], conference_acronym='Lancet (British edition)', publisher=None, query_handler=None),\n",
       " 'KTE-X19 CAR T-cell therapy in relapsed or refractory mantle-cell lymphoma': Paper(DOI='10.1097/01.cot.0000668140.38010.d4', crossref_json=None, google_schorlar_metadata=None, title='KTE-X19 CAR T-cell therapy in relapsed or refractory mantle-cell lymphoma', authors=['Michael Wang', 'Javier Munoz', 'Andre Goy', 'Frederick L Locke', 'Caron A Jacobson', 'Brian T Hill', 'John M Timmerman', 'Houston Holmes', 'Samantha Jaglowski', 'Ian W Flinn', 'Peter A McSweeney', 'David B Miklos', 'John M Pagel', 'Marie-Jose Kersten', 'Noel Milpied', 'Henry Fung', 'Max S Topp', 'Roch Houot', 'Amer Beitinjaneh', 'Weimin Peng', 'Lianqing Zheng', 'John M Rossi', 'Rajul K Jain', 'Arati V Rao', 'Patrick M Reagan'], abstract=' Background Patients with relapsed or refractory mantle-cell lymphoma who have disease progression during or after the receipt of Bruton’s tyrosine kinase (BTK) inhibitor therapy have a poor prognosis. KTE-X19, an anti-CD19 chimeric antigen receptor (CAR) T-cell therapy, may have benefit in patients with relapsed or refractory mantle-cell lymphoma. Methods In a multicenter, phase 2 trial, we evaluated KTE-X19 in patients with relapsed or refractory mantle-cell lymphoma. Patients had disease that had relapsed or was refractory after the receipt of up to five previous therapies; all patients had to have received BTK inhibitor therapy previously. Patients underwent leukapheresis and optional bridging therapy, followed by conditioning chemotherapy and a single infusion of KTE-X19 at a dose of 2×106 CAR T cells per kilogram of body weight. The primary end point was the percentage of patients with an objective\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neurosurgery'], conference_acronym='Oncology times', publisher=None, query_handler=None),\n",
       " 'A phase 2 study of single-agent carfilzomib (PX-171-003-A1) in patients with relapsed and refractory multiple myeloma': Paper(DOI='10.1016/s1557-9190(11)70562-5', crossref_json=None, google_schorlar_metadata=None, title='A phase 2 study of single-agent carfilzomib (PX-171-003-A1) in patients with relapsed and refractory multiple myeloma', authors=['David S Siegel', 'Thomas Martin', 'Michael Wang', 'Ravi Vij', 'Andrzej J Jakubowiak', 'Sagar Lonial', 'Suzanne Trudel', 'Vishal Kukreti', 'Nizar Bahlis', 'Melissa Alsina', 'Asher Chanan-Khan', 'Francis Buadi', 'Frederic J Reu', 'George Somlo', 'Jeffrey Zonder', 'Kevin Song', 'A Keith Stewart', 'Edward Stadtmauer', 'Lori Kunkel', 'Sandra Wear', 'Alvin F Wong', 'Robert Z Orlowski', 'Sundar Jagannath'], abstract=' Carfilzomib is a next-generation, selective proteasome inhibitor being evaluated for the treatment of relapsed and refractory multiple myeloma. In this open-label, single-arm phase 2 study (PX-171-003-A1), patients received single-agent carfilzomib 20 mg/m2 intravenously twice weekly for 3 of 4 weeks in cycle 1, then 27 mg/m2 for ≤ 12 cycles. The primary endpoint was overall response rate (≥ partial response). Secondary endpoints included clinical benefit response rate (≥ minimal response), duration of response, progression-free survival, overall survival, and safety. A total of 266 patients were evaluable for safety, 257 for efficacy; 95% were refractory to their last therapy; 80% were refractory or intolerant to both bortezomib and lenalidomide. Patients had median of 5 prior lines of therapy, including bortezomib, lenalidomide, and thalidomide. Overall response rate was 23.7% with median duration of\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neurosurgery'], conference_acronym='Clinical lymphoma & myeloma', publisher=None, query_handler=None),\n",
       " 'T cells expressing an anti–B-cell maturation antigen chimeric antigen receptor cause remissions of multiple myeloma': Paper(DOI='10.1182/blood-2016-04-711903', crossref_json=None, google_schorlar_metadata=None, title='T cells expressing an anti–B-cell maturation antigen chimeric antigen receptor cause remissions of multiple myeloma', authors=['Syed Abbas Ali', 'Victoria Shi', 'Irina Maric', 'Michael Wang', 'David F Stroncek', 'Jeremy J Rose', 'Jennifer N Brudno', 'Maryalice Stetler-Stevenson', 'Steven A Feldman', 'Brenna G Hansen', 'Vicki S Fellowes', 'Frances T Hakim', 'Ronald E Gress', 'James N Kochenderfer'], abstract=' Therapies with novel mechanisms of action are needed for multiple myeloma (MM). B-cell maturation antigen (BCMA) is expressed in most cases of MM. We conducted the first-in-humans clinical trial of chimeric antigen receptor (CAR) T cells targeting BCMA. T cells expressing the CAR used in this work (CAR-BCMA) specifically recognized BCMA-expressing cells. Twelve patients received CAR-BCMA T cells in this dose-escalation trial. Among the 6 patients treated on the lowest 2 dose levels, limited antimyeloma activity and mild toxicity occurred. On the third dose level, 1 patient obtained a very good partial remission. Two patients were treated on the fourth dose level of 9 × 106 CAR+ T cells/kg body weight. Before treatment, the first patient on the fourth dose level had chemotherapy-resistant MM, making up 90% of bone marrow cells. After treatment, bone marrow plasma cells became undetectable by flow\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s11899-015-0282-1', '10.1038/leu.2013.350', '10.1182/blood-2014-11-568923', '10.1038/nrclinonc.2013.46', '10.1158/2159-8290.CD-12-0548', '10.1016/j.coi.2012.06.004', '10.1111/imr.12131', '10.1007/s11899-013-0197-7', '10.1038/nm827', '10.1182/blood-2002-07-1989', '10.1182/blood-2010-04-281931', '10.1158/1078-0432.CCR-12-2422', '10.1097/CJI.0b013e3181ac6138', '10.1182/blood-2011-10-384388', '10.1056/NEJMoa1103849', '10.1056/NEJMoa1407222', '10.1016/S0140-6736(14)61403-3', '10.1126/scitranslmed.3005930', '10.1200/JCO.2014.56.2025', '10.1182/blood-2013-06-506741', '10.1182/blood-2013-08-519413', '10.1111/bjh.13340', '10.1200/JCO.2015.64.5929', '10.1182/blood-2014-05-552729', '10.1126/scitranslmed.3008226', '10.1038/nbt.2725', '10.1002/j.1460-2075.1992.tb05482.x', '10.1093/nar/22.7.1147', '10.1016/j.smim.2006.04.006', '10.4049/jimmunol.173.2.807', '10.1182/blood-2003-06-2043', '10.1182/blood-2004-11-4463', '10.1182/blood-2010-01-265041', '10.1084/jem.20050732', '10.1084/jem.155.4.1063', '10.1038/sj.leu.2404284', '10.1111/j.1365-2141.2012.09241.x', '10.1038/44385', '10.1038/nri3862', '10.1182/blood-2002-07-2103', '10.1189/jlb.0809566', '10.1007/BF00177259', '10.1038/nm.3910', '10.1056/NEJMoa1504542', '10.1182/blood-2006-11-059139', '10.1038/mt.2011.256'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neurosurgery'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'High rate of durable remissions after treatment of newly diagnosed aggressive mantle-cell lymphoma with rituximab plus hyper-CVAD alternating with rituximab plus high-dose\\xa0…': Paper(DOI='10.1200/jco.2005.01.1825', crossref_json=None, google_schorlar_metadata=None, title='High rate of durable remissions after treatment of newly diagnosed aggressive mantle-cell lymphoma with rituximab plus hyper-CVAD alternating with rituximab plus high-dose\\xa0…', authors=['Jorge E Romaguera', 'Luis Fayad', 'Maria A Rodriguez', 'Kristine R Broglio', 'Frederick B Hagemeister', 'Barbara Pro', 'Peter McLaughlin', 'Anas Younes', 'Felipe Samaniego', 'Andre Goy', 'Andreas H Sarris', 'Nam H Dang', 'Michael Wang', 'Virginia Beasley', 'L Jeffrey Medeiros', 'Ruth L Katz', 'Harish Gagneja', 'Barry I Samuels', 'Terry L Smith', 'Fernando F Cabanillas'], abstract='PurposeTo determine the response, failure-free survival (FFS), and overall survival rates and toxicity of rituximab plus an intense chemotherapy regimen in patients with previously untreated aggressive mantle-cell lymphoma (MCL).', conference=None, journal=None, year=None, reference_list=['10.1046/j.1365-2559.2000.00895.x', '10.1200/JCO.1997.15.4.1664', '10.1002/hon.2900070505', '10.1093/oxfordjournals.annonc.a059154', '10.1182/blood.V89.6.2067', '10.1200/JCO.1995.13.11.2819', '10.1093/oxfordjournals.annonc.a059155', '10.1182/blood.V85.4.1075.bloodjournal8541075', '10.1182/blood.V87.10.4302.bloodjournal87104302', '10.1200/JCO.2000.18.2.317', '10.1200/JCO.2002.20.5.1288', '10.1200/JCO.1998.16.12.3803', '10.1093/ajcp/112.4.524', '10.1309/69EJ-RFM5-E976-BUTP', '10.1200/JCO.1999.17.4.1244', '10.1080/01621459.1958.10501452', '10.2307/2344317', '10.1056/NEJM199309303291402', '10.1016/S1470-2045(00)00255-2', '10.1182/blood-2003-09-3363', '10.1182/blood-2002-08-2476', '10.1002/cncr.11838', '10.1046/j.1365-2141.2003.04140.x', '10.1034/j.1600-0609.2003.00093.x', '10.1182/blood-2004-10-3883', '10.1038/sj.leu.2402406', '10.1200/JCO.1989.7.10.1518', '10.7326/0003-4819-114-10-855', '10.1200/JCO.2003.07.113', '10.1093/annonc/mdf042'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neurosurgery'], conference_acronym='Journal of clinical oncology', publisher=None, query_handler=None),\n",
       " \"Phase II study of proteasome inhibitor bortezomib in relapsed or refractory B-cell non-Hodgkin's lymphoma\": Paper(DOI='10.1182/blood.v108.11.2462.2462', crossref_json=None, google_schorlar_metadata=None, title=\"Phase II study of proteasome inhibitor bortezomib in relapsed or refractory B-cell non-Hodgkin's lymphoma\", authors=['Andre Goy', 'Anas Younes', 'Peter McLaughlin', 'Barbara Pro', 'Jorge E Romaguera', 'Frederick Hagemeister', 'Luis Fayad', 'Nam H Dang', 'Felipe Samaniego', 'Michael Wang', 'Kristine Broglio', 'Barry Samuels', 'Frederic Gilles', 'Andreas H Sarris', 'Susan Hart', 'Elizabeth Trehu', 'David Schenkein', 'Fernando Cabanillas', 'Alma M Rodriguez'], abstract='PurposeEvaluate efficacy and toxicity of bortezomib in patients with relapsed or refractory B-cell non-Hodgkin’s lymphoma.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neurosurgery'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'T cells genetically modified to express an anti–B-cell maturation antigen chimeric antigen receptor cause remissions of poor-prognosis relapsed multiple myeloma': Paper(DOI='10.1200/jco.2018.77.8084', crossref_json=None, google_schorlar_metadata=None, title='T cells genetically modified to express an anti–B-cell maturation antigen chimeric antigen receptor cause remissions of poor-prognosis relapsed multiple myeloma', authors=['Jennifer N Brudno', 'Irina Maric', 'Steven D Hartman', 'Jeremy J Rose', 'Michael Wang', 'Norris Lam', 'Maryalice Stetler-Stevenson', 'Dalia Salem', 'Constance Yuan', 'Steven Pavletic', 'Jennifer A Kanakry', 'Syed Abbas Ali', 'Lekha Mikkilineni', 'Steven A Feldman', 'David F Stroncek', 'Brenna G Hansen', 'Judith Lawrence', 'Rashmika Patel', 'Frances Hakim', 'Ronald E Gress', 'James N Kochenderfer'], abstract='PurposeTherapies with novel mechanisms of action are needed for multiple myeloma (MM). T cells can be genetically modified to express chimeric antigen receptors (CARs), which are artificial proteins that target T cells to antigens. B-cell maturation antigen (BCMA) is expressed by normal and malignant plasma cells but not normal essential cells. We conducted the first-in-humans clinical trial, to our knowledge, of T cells expressing a CAR targeting BCMA (CAR-BCMA).', conference=None, journal=None, year=None, reference_list=['10.1056/NEJMra1011442', '10.1182/asheducation-2017.1.508', '10.1038/leu.2015.356', '10.1182/blood-2017-06-793869', '10.1038/nrclinonc.2017.128', '10.1158/2159-8290.CD-12-0548', '10.1016/j.coi.2012.06.004', '10.1158/1078-0432.CCR-15-1433', '10.1172/JCI85309', '10.1056/NEJMoa1407222', '10.1016/S0140-6736(14)61403-3', '10.1126/scitranslmed.3008226', '10.1126/scitranslmed.3005930', '10.1126/scitranslmed.aaf8621', '10.1200/JCO.2014.56.2025', '10.1200/JCO.2016.71.3024', '10.1016/j.ymthe.2017.07.004', '10.1182/blood-2010-04-281931', '10.1158/1078-0432.CCR-12-2422', '10.1056/NEJMoa1504542', '10.1182/blood-2016-04-711903', '10.1172/JCI86000', '10.1093/nar/22.7.1147', '10.1084/jem.20031330', '10.4049/jimmunol.173.2.807', '10.1182/blood-2003-06-2043', '10.1084/jem.192.1.129', '10.1182/blood-2010-01-265041', '10.1084/jem.155.4.1063', '10.1084/jem.20050732', '10.1007/s11523-017-0538-x', '10.1016/j.ncl.2017.06.008', '10.1200/JCO.2015.64.5929', '10.1038/nri2959', '10.1002/cyto.a.22351', '10.1182/blood-2002-07-2103', '10.1038/44385', '10.1016/j.ymthe.2017.07.013', '10.1182/blood-2013-08-519413', '10.1038/leu.2014.13', '10.1038/ncomms3997', '10.1038/leu.2014.321', '10.1200/JCO.2006.05.9964', '10.1038/mt.2010.24'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neurosurgery'], conference_acronym='Journal of clinical oncology', publisher=None, query_handler=None),\n",
       " 'Jet momentum dependence of jet quenching in PbPb collisions at sNN= 2.76 TeV': Paper(DOI='10.1063/1.3700694', crossref_json=None, google_schorlar_metadata=None, title='Jet momentum dependence of jet quenching in PbPb collisions at sNN= 2.76 TeV', authors=['Serguei Chatrchyan', 'V Khachatryan', 'Albert M Sirunyan', 'A Tumasyan', 'W Adam', 'T Bergauer', 'M Dragicevic', 'J Erö', 'C Fabjan', 'M Friedl', 'R Fruehwirth', 'VM Ghete', 'J Hammer', 'N Hoermann', 'J Hrubec', 'M Jeitler', 'W Kiesenhofer', 'M Krammer', 'D Liko', 'I Mikulec', 'M Pernicka', 'B Rahbaran', 'C Rohringer', 'H Rohringer', 'R Schoefbeck', 'J Strauss', 'A Taurok', 'F Teischinger', 'P Wagner', 'W Waltenberger', 'G Walzel', 'E Widl', 'C-E Wulz', 'N Shumeiko', 'J Suarez Gonzalez', 'S Bansal', 'T Cornelis', 'EA De Wolf', 'X Janssen', 'S Luyckx', 'T Maes', 'L Mucibello', 'S Ochesanu', 'B Roland', 'R Rougny', 'M Selvaggi', 'H Van Haevermaet', 'P Van Mechelen', 'A Van Spilbeeck', 'F Blekman', 'S Blyweert', 'J DʼHondt', 'R Gonzalez Suarez', 'A Kalogeropoulos', 'M Maes', 'A Olbrechts', 'W Van Doninck', 'P Van Mulders', 'GP Van Onsem', 'I Villella', 'O Charaf', 'B Clerbaux', 'G De Lentdecker', 'V Dero', 'APR Gay', 'T Hreus', 'A Léonard', 'PE Marage', 'L Thomas', 'C Vander Velde', 'P Vanlaer', 'Volker Adler', 'Kelly Beernaert', 'Anna Cimmino', 'Silvia Costantini', 'Guillaume Garcia', 'M Grunewald', 'Benjamin Klein', 'Jeremie Lellouch', 'Andrey Marinov', 'Joseph McCartin', 'AA Ocampo Rios', 'Dirk Ryckbosch', 'Nadja Strobbe', 'Filip Thyssen', 'Michael Tytgat', 'Lukas Vanelderen', 'Piet Verwilligen', 'S Walsh', 'Efe Yazgan', 'Nikolaos Zaganidis', 'S Basegmez', 'G Bruno', 'L Ceard', 'C Delaere', 'T du Pree', 'D Favart', 'L Forthomme', 'A Giammanco', 'J Hollar', 'V Lemaitre', 'J Liao', 'O Militaru', 'C Nuttens', 'D Pagano', 'A Pin', 'K Piotrzkowski', 'N Schul', 'N Beliy', 'T Caebergs', 'E Daubie', 'GH Hammad', 'GA Alves', 'M Correa Martins Junior', 'D De Jesus Damiao', 'T Martins', 'ME Pol', 'MHG Souza', 'WL Aldá Júnior', 'W Carvalho', 'A Custódio', 'EM Da Costa', 'C De Oliveira Martins', 'S Fonseca De Souza', 'D Matos Figueiredo', 'L Mundim', 'H Nogima', 'V Oguri', 'WL Prado Da Silva', 'A Santoro', 'SM Silva Do Amaral', 'L Soares Jorge', 'A Sznajder', 'TS Anjos', 'CA Bernardes', 'FA Dias', 'TR Fernandez Perez Tomei', 'EM Gregores', 'C Lagana', 'F Marinho', 'PG Mercadante', 'SF Novaes', 'Sandra S Padula', 'V Genchev', 'P Iaydjiev', 'S Piperov', 'M Rodozov', 'S Stoykova', 'G Sultanov', 'V Tcholakov'], abstract='Dijet production in PbPb collisions at a nucleon–nucleon center-of-mass energy of 2.76 TeV is studied with the CMS detector at the LHC. A data sample corresponding to an integrated luminosity of 150 μb−1 is analyzed. Jets are reconstructed using combined information from tracking and calorimetry, using the anti-kT algorithm with R=0.3. The dijet momentum balance and angular correlations are studied as a function of collision centrality and leading jet transverse momentum. For the most peripheral PbPb collisions, good agreement of the dijet momentum balance distributions with pp data and reference calculations at the same collision energy is found, while more central collisions show a strong imbalance of leading and subleading jet transverse momenta attributed to the jet-quenching effect. The dijets in central collisions are found to be more unbalanced than the reference, for leading jet transverse momenta\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neurosurgery'], conference_acronym='AIP conference proceedings', publisher=None, query_handler=None),\n",
       " 'Unopposed production of granulocyte-macrophage colony-stimulating factor by tumors inhibits CD8+ T cell responses by dysregulating antigen-presenting cell maturation': Paper(DOI='10.4049/jimmunol.162.10.5728', crossref_json=None, google_schorlar_metadata=None, title='Unopposed production of granulocyte-macrophage colony-stimulating factor by tumors inhibits CD8+ T cell responses by dysregulating antigen-presenting cell maturation', authors=['Vincenzo Bronte', 'Dale B Chappell', 'Elisa Apolloni', 'Anna Cabrelle', 'Michael Wang', 'Patrick Hwu', 'Nicholas P Restifo'], abstract='Tumor cells gene-modified to produce GM-CSF potently stimulate antitumor immune responses, in part, by causing the growth and differentiation of dendritic cells (DC). However, GM-CSF-modified tumor cells must be γ-irradiated or they will grow progressively, killing the host. We observed that 23 of 75 (31%) human tumor lines and two commonly used mouse tumor lines spontaneously produced GM-CSF. In mice, chronic GM-CSF production by tumors suppressed Ag-specific CD8+ T cell responses. Interestingly, an inhibitory population of adherent CD11b (Mac-1)/Gr-1 double-positive cells caused the observed impairment of CD8+ T cell function upon direct cell-to-cell contact. The inhibitory cells were positive for some markers associated with Ag presenting cells, like F4/80, but were negative for markers associated with fully mature DC like DEC205, B7. 2, and MHC class II. We have previously reported that a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1084/jem.188.2.277', '10.1073/pnas.96.6.2982', '10.1038/nm0398-321', '10.1038/nm0595-471', '10.1084/jem.188.9.1553', '10.1038/nm0598supp-525', '10.1006/viro.1998.9330', '10.1016/S1357-4310(98)01304-5', '10.1038/32588', '10.1073/pnas.94.7.3183', '10.1084/jem.188.6.1075', '10.1084/jem.186.7.1177', '10.1084/jem.187.7.1019', '10.1073/pnas.90.7.3038', '10.1038/42030', '10.4049/jimmunol.156.10.3858', '10.1089/hum.1998.9.6-835', '10.1073/pnas.90.8.3539', '10.1038/32183', '10.1084/jem.188.7.1359', '10.4049/jimmunol.141.2.699', '10.1002/ijc.2910470318', '10.4049/jimmunol.161.10.5313', '10.4049/jimmunol.154.10.5282', '10.4049/jimmunol.154.9.4685', '10.4049/jimmunol.151.8.3971', '10.1006/meth.1997.0461', '10.1038/ki.1996.407', '10.1002/(SICI)1097-0215(19980729)77:3<378::AID-IJC12>3.0.CO;2-4', '10.4049/jimmunol.159.6.2580', '10.1007/BF03401653', '10.1093/jnci/89.21.1595', '10.1093/jnci/89.5.390', '10.1038/375151a0', '10.4049/jimmunol.157.9.3819', '10.1084/jem.183.4.1789', '10.1073/pnas.75.10.5132', '10.1084/jem.166.5.1484', '10.1016/S0092-8674(00)81477-4', '10.1084/jem.184.6.2445', '10.1073/pnas.94.11.5750', '10.1126/science.275.5303.1122', '10.1016/S0171-2985(98)80078-8', '10.1097/00001622-199901000-00012', '10.1084/jem.184.5.1953', '10.1002/eji.1830270107', '10.1097/00007890-199560120-00028', '10.1097/00007890-199609150-00021', '10.4049/jimmunol.159.2.770', '10.1097/00008390-199502000-00008', '10.1002/ijc.2910560524', '10.1084/jem.186.8.1183', '10.1038/nm1096-1096', '10.1038/bjc.1985.180', '10.4049/jimmunol.155.6.3112', '10.1084/jem.177.2.265', '10.1093/jnci/88.2.100', '10.1016/S0167-5699(97)01115-8', '10.1007/s002620050505', '10.1056/NEJM198804073181401'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neurosurgery'], conference_acronym='The Journal of immunology (1950)', publisher=None, query_handler=None),\n",
       " 'Apoptotic death of CD8+ T lymphocytes after immunization: induction of a suppressive population of Mac-1+/Gr-1+ cells': Paper(DOI='10.4049/jimmunol.161.10.5313', crossref_json=None, google_schorlar_metadata=None, title='Apoptotic death of CD8+ T lymphocytes after immunization: induction of a suppressive population of Mac-1+/Gr-1+ cells', authors=['Vincenzo Bronte', 'Michael Wang', 'Willem W Overwijk', 'Deborah R Surman', 'Federica Pericle', 'Steven A Rosenberg', 'Nicholas P Restifo'], abstract='Following an infection or immunization, a primary CD8+ T cell response generally rises then falls rapidly before giving rise to a “memory” response. When we immunized mice with recombinant viral immunogens optimized to enhance the lytic capability of CD8+ T cells, we measured a profound depression in Ag-specific effector function after early restimulation. Indeed, a “mirror image” cytolytic capability was observed: the most powerful immunogens, as measured by cytolytic capacity 6 days after immunization, elicited the weakest secondary immune response when evaluated following an additional 6 days after restimulation. To understand the mechanism of this suppression, we examined the fate of splenocytes immunized with a vaccinia virus encoding Ag and IL-2 then restimulated ex vivo. We found that these splenocytes underwent an apoptotic cell death, upon early restimulation, that was not dependent on the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.4049/jimmunol.156.7.2352', '10.1038/353858a0', '10.1126/science.7509084', '10.1002/jcb.240590202', '10.1016/S0092-8674(00)81042-9', '10.1016/S0952-7915(96)80125-7', '10.1016/S0952-7915(97)80082-9', '10.1038/362758a0', '10.1128/jvi.67.10.5754-5765.1993', '10.1002/(SICI)1097-0215(19960729)67:3<333::AID-IJC5>3.0.CO;2-S', '10.1016/S0065-230X(08)60827-1', '10.4049/jimmunol.141.2.699', '10.1038/bjc.1982.116', '10.1002/jlb.57.6.919', '10.4049/jimmunol.151.10.5492', '10.4049/jimmunol.146.8.2737', '10.4049/jimmunol.154.10.5282', '10.4049/jimmunol.154.9.4685', '10.1128/MCB.5.12.3403', '10.1016/0042-6822(87)90004-3', '10.1073/pnas.94.7.3183', '10.4049/jimmunol.151.8.3971', '10.1006/meth.1997.0461', '10.1016/S0091-679X(08)61925-1', '10.1097/00002371-199510000-00001', '10.4049/jimmunol.147.1.22', '10.1128/jvi.64.9.4076-4083.1990', '10.1084/jem.179.6.1933', '10.1084/jem.185.9.1629', '10.1084/jem.185.7.1241', '10.1016/S1074-7613(00)80469-0', '10.1084/jem.187.9.1367', '10.1016/0167-5699(95)80079-4', '10.1006/smim.1996.0054', '10.1016/0008-8749(92)90187-T', '10.1038/373444a0', '10.1182/blood.V89.4.1341', '10.1016/0092-8674(95)90013-6', '10.1128/jvi.70.11.8199-8203.1996', '10.1146/annurev.iy.02.040184.001251', '10.4049/jimmunol.132.1.101', '10.4049/jimmunol.135.3.1644', '10.1007/BF03401653', '10.1038/nm0398-321'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neurosurgery'], conference_acronym='The Journal of immunology (1950)', publisher=None, query_handler=None),\n",
       " 'Clinical and radiographic comparison of mini–open transforaminal lumbar interbody fusion with open transforaminal lumbar interbody fusion in 42 patients with long-term follow-up': Paper(DOI='10.3171/spi.2008.9.08142', crossref_json=None, google_schorlar_metadata=None, title='Clinical and radiographic comparison of mini–open transforaminal lumbar interbody fusion with open transforaminal lumbar interbody fusion in 42 patients with long-term follow-up', authors=['Sanjay S Dhall', 'Michael Y Wang', 'Praveen V Mummaneni'], abstract=' Object As minimally invasive approaches gain popularity in spine surgery, clinical outcomes and effectiveness of mini–open transforaminal lumbar interbody fusion (TLIF) compared with traditional open TLIF have yet to be established. The authors retrospectively compared the outcomes of patients who underwent mini–open TLIF with those who underwent open TLIF. Methods Between 2003 and 2006, 42 patients underwent TLIF for degenerative disc disease or spondylolisthesis; 21 patients underwent mini–open TLIF and 21 patients underwent open TLIF. The mean age in each group was 53 years, and there was no statistically significant difference in age between the groups (p = 0.98). Data were collected perioperatively. In addition, complications, length of stay (LOS), fusion rate, and modified Prolo Scale (mPS) scores were recorded at routine intervals\\xa0…', conference=None, journal=None, year=None, reference_list=['10.3171/foc.2006.20.3.11', '10.1227/01.NEU.0000068354.22859.05', '10.3171/spi.2002.97.1.0007', '10.3171/foc.2001.10.4.11', '10.1097/00007632-200308011-00006', '10.1097/00007632-199905150-00017', '10.1055/s-2008-1051624', '10.3171/foc.2006.20.3.7', '10.3171/spi.2005.3.2.0098', '10.1007/s007760200084', '10.1097/00007632-199604150-00007', '10.1097/00007632-199411001-00017', '10.1097/00007632-199411001-00018', '10.1097/00007632-198901000-00006', '10.1227/01.NEU.0000176408.95304.F3', '10.1097/01.brs.0000256473.49791.f4', '10.1097/00006123-200103000-00022', '10.1227/01.NEU.0000103493.25162.18', '10.1227/01.NEU.0000255388.03088.B7', '10.1097/01.bsd.0000132291.50455.d0', '10.1097/00007632-199304000-00009', '10.1097/00007632-198709000-00006', '10.1097/00007632-199712150-00004', '10.1053/j.otns.2004.08.002', '10.1097/00029679-200511010-00001', '10.2106/00004623-196850050-00004'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neurosurgery'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Robust Monte Carlo Localization for Mobile Robots': Paper(DOI='10.1016/s0004-3702(01)00069-8', crossref_json=None, google_schorlar_metadata=None, title='Robust Monte Carlo Localization for Mobile Robots', authors=['Sebastian Thrun', 'Dieter Fox', 'Wolfram Burgard', 'Frank Dellaert'], abstract=\"Mobile robot localization is the problem of determining a robot's pose from sensor data. This article presents a family of probabilistic localization algorithms known as Monte Carlo Localization (MCL). MCL algorithms represent a robot's belief by a set of weighted hypotheses (samples), which approximate the posterior under a common Bayesian formulation of the localization problem. Building on the basic MCL algorithm, this article develops a more robust algorithm called Mixture-MCL, which integrates two complimentary ways of generating samples in the estimation. To apply this algorithm to mobile robots equipped with range finders, a kernel density tree is learned that permits fast sampling. Systematic empirical results illustrate the robustness and computational efficiency of the approach.\", conference=None, journal=None, year=None, reference_list=['10.1145/358841.358850', '10.1016/S0004-3702(99)00070-3', '10.1109/70.75902', '10.1016/0004-3702(94)90029-9', '10.1023/A:1008937911390', '10.1016/S0921-8890(98)00049-9', '10.1613/jair.616', '10.1023/A:1008078328650', '10.1115/1.3662552', '10.1177/027836499201100402', '10.1080/01621459.1998.10473765', '10.1023/A:1008854305733', '10.1162/neco.1997.9.3.683', '10.1080/01621459.1999.10474153', '10.1023/A:1007554531242', '10.1177/02783640022067922', '10.1109/3477.499799'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision'], conference_acronym='Artificial intelligence (General ed.)', publisher=None, query_handler=None),\n",
       " 'iSAM: Incremental smoothing and mapping': Paper(DOI='10.1109/tro.2008.2006706', crossref_json=None, google_schorlar_metadata=None, title='iSAM: Incremental smoothing and mapping', authors=['Michael Kaess', 'Ananth Ranganathan', 'Frank Dellaert'], abstract='In this paper, we present incremental smoothing and mapping (iSAM), which is a novel approach to the simultaneous localization and mapping problem that is based on fast incremental matrix factorization. iSAM provides an efficient and exact solution by updating a QR factorization of the naturally sparse smoothing information matrix, thereby recalculating only those matrix entries that actually change. iSAM is efficient even for robot trajectories with many loops as it avoids unnecessary fill-in in the factor matrix by periodic variable reordering. Also, to enable data association in real time, we provide efficient algorithms to access the estimation uncertainties of interest based on the factored information matrix. We systematically evaluate the different components of iSAM as well as the overall algorithm using various simulated and real-world datasets for both landmark and pose-only settings.', conference=None, journal=None, year=None, reference_list=['10.1109/TRO.2007.900608', '10.1023/A:1008854305733', '10.1109/IROS.2001.973391', '10.1109/70.938382', '10.1109/ROBOT.2004.1307180', '10.1177/0278364904049393', '10.1023/A:1015269615729', '10.1109/56.768', '10.1109/ROBOT.2001.933280', '10.1177/027836499201100402', '10.1177/027836498600500404', '10.1109/TRO.2006.889486', '10.1109/ROBOT.2006.1642040', '10.1109/MRA.2006.1678144', '10.1109/MRA.2006.1638022', '10.1109/ROBOT.2007.363563', '10.1109/TPAMI.2006.247', '10.1137/S0895479897321076', '10.1093/imamat/12.3.329', '10.1090/S0025-5718-1974-0343558-6', '10.1109/78.134393', '10.1007/978-1-4613-8997-2_14', '10.1145/1024074.1024079', '10.1007/BF02278710', '10.1016/0024-3795(80)90156-1', '10.1109/70.976019', '10.1177/0278364906072768', '10.1109/ROBOT.2005.1570475', '10.1109/IROS.2006.282531', '10.1177/0278364906072512', '10.1109/TRO.2004.839220', '10.1007/s10514-006-9043-2'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision'], conference_acronym='IEEE transactions on robotics', publisher=None, query_handler=None),\n",
       " 'Square Root SAM: Simultaneous localization and mapping via square root information smoothing': Paper(DOI='10.1177/0278364906072768', crossref_json=None, google_schorlar_metadata=None, title='Square Root SAM: Simultaneous localization and mapping via square root information smoothing', authors=['Frank Dellaert', 'Michael Kaess'], abstract='Solving the SLAM (simultaneous localization and mapping) problem is one way to                     enable a robot to explore, map, and navigate in a previously unknown                     environment. Smoothing approaches have been investigated as a viable alternative                     to extended Kalman filter (EKF)-based solutions to the problem. In particular,                     approaches have been looked at that factorize either the associated information                     matrix or the measurement Jacobian into square root form. Such techniques have                     several significant advantages over the EKF: they are faster yet exact; they can                     be used in either batch or incremental mode; are better equipped to deal with                     non-linear process and measurement models; and yield the entire robot                     trajectory, at lower cost for a large class of SLAM problems. In addition, in an                     indirect but dramatic way\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1137/S0895479894278952', '10.1109/70.88101', '10.1007/978-1-4613-8369-7_1', '10.1109/70.795798', '10.1137/S0895479897321076', '10.1145/1024074.1024079', '10.1109/ROBOT.2000.844737', '10.15607/RSS.2005.I.024', '10.1109/70.938381', '10.1109/ROBOT.2000.845330', '10.1023/A:1015269615729', '10.1007/978-3-540-32255-9_25', '10.1109/TRO.2004.839220', '10.1109/70.917081', '10.1016/0024-3795(80)90156-1', '10.1111/j.1477-9730.1980.tb00020.x', '10.1109/70.938382', '10.1177/0278364904042203', '10.15607/RSS.2006.II.010', '10.1109/18.910572', '10.1109/48.972094', '10.1177/027836499201100402', '10.1007/978-1-4615-3652-9', '10.1137/0136016', '10.1023/A:1008854305733', '10.1023/A:1007957421070', '10.1145/174603.174408', '10.1177/027836498600500404', '10.1007/978-1-4613-8997-2_14', '10.1006/jvci.1994.1002', '10.1109/21.141316', '10.1177/027836402320556340', '10.1177/0278364904045479', '10.1007/3-540-44480-7_21', '10.1007/978-3-642-97522-6'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision'], conference_acronym='The international journal of robotics research', publisher=None, query_handler=None),\n",
       " 'On-manifold preintegration for real-time visual--inertial odometry': Paper(DOI='10.1109/tro.2016.2597321', crossref_json=None, google_schorlar_metadata=None, title='On-manifold preintegration for real-time visual--inertial odometry', authors=['Christian Forster', 'Luca Carlone', 'Frank Dellaert', 'Davide Scaramuzza'], abstract='Current approaches for visual-inertial odometry (VIO) are able to attain highly accurate state estimation via nonlinear optimization. However, real-time optimization quickly becomes infeasible as the trajectory grows over time; this problem is further emphasized by the fact that inertial measurements come at high rate, hence, leading to the fast growth of the number of variables in the optimization. In this paper, we address this issue by preintegrating inertial measurements between selected keyframes into single relative motion constraints. Our first contribution is a preintegration theory that properly addresses the manifold structure of the rotation group. We formally discuss the generative measurement model as well as the nature of the rotation noise and derive the expression for the maximum a posteriori state estimator. Our theoretical development enables the computation of all necessary Jacobians for the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1137/1.9780898719512', '10.1109/IROS.2013.6696615', '10.1109/ISMAR.2009.5336495', '10.1109/ROBOT.2010.5509636', '10.1007/s11263-015-0811-3', '10.1016/j.robot.2013.05.001', '10.1109/TRO.2008.2006706', '10.1109/ISMAR.2007.4538852', '10.1109/ICRA.2014.6907385', '10.1177/0278364913515286', '10.1017/CBO9780511811685', '10.1002/0471221279', '10.1177/0278364904045593', '10.1109/ICASSP.1997.595466', '10.1109/ICRA.2014.6906584', '10.1109/ICRA.2014.6906892', '10.1109/ROBOT.2009.5152678', '10.1109/70.326576', '10.1109/CVPR.2012.6248074', '10.1109/TRO.2011.2170332', '10.1109/TRO.2011.2160468', '10.1109/CVPRW.2008.4563131', '10.1109/ICRA.2011.5980267', '10.1002/rob.20360', '10.1177/0278364914554813', '10.15607/RSS.2013.IX.037', '10.1109/IROS.2011.6048760', '10.1109/78.984753', '10.1109/18.910572', '10.1109/IROS.2013.6696514', '10.1109/TAES.2006.1642588', '10.2514/1.58558', '10.1109/TRO.2005.852253', '10.1016/S0168-9274(98)00030-0', '10.1007/BF02429858', '10.15607/RSS.2005.I.024', '10.1109/TPAMI.2007.1049', '10.1109/IROS.2015.7353389', '10.1007/978-3-319-23778-7_21', '10.1177/0278364910388963', '10.1109/ROBOT.2002.1014801', '10.1109/ICRA.2015.7139924', '10.1561/2300000030', '10.1007/978-3-642-00196-3_43', '10.1177/0278364913509675', '10.1109/ICRA.2015.7139507', '10.1109/ICRA.2014.6907483', '10.1177/0278364911430419', '10.15607/RSS.2015.XI.006', '10.15607/RSS.2015.XI.008', '10.1109/9.250476', '10.1109/TRO.2014.2298059', '10.1007/s10208-005-0179-9', '10.1109/TRO.2006.878978', '10.1137/S0895479801383877', '10.1109/ICCV.1999.791285', '10.1177/0278364908097583'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Robot Perception', 'Computer Vision', 'Estimation and Inference', 'Optimization and Learning'], conference_acronym='IEEE transactions on robotics', publisher=None, query_handler=None),\n",
       " 'MCMC-based particle filtering for tracking a variable number of interacting targets': Paper(DOI='10.1109/tpami.2005.223', crossref_json=None, google_schorlar_metadata=None, title='MCMC-based particle filtering for tracking a variable number of interacting targets', authors=['Zia Khan', 'Tucker Balch', 'Frank Dellaert'], abstract='We describe a particle filter that effectively deals with interacting targets, targets that are influenced by the proximity and/or behavior of other targets. The particle filter includes a Markov random field (MRF) motion prior that helps maintain the identity of targets throughout an interaction, significantly reducing tracker failures. We show that this MRF prior can be easily implemented by including an additional interaction factor in the importance weights of the particle filter. However, the computational requirements of the resulting multitarget filter render it unusable for large numbers of targets. Consequently, we replace the traditional importance sampling step in the particle filter with a novel Markov chain Monte Carlo (MCMC) sampling step to obtain a more efficient MCMC-based multitarget filter. We also show how to extend this MCMC-based filter to address a variable number of interacting targets. Finally, we present both\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/375735.376434', '10.1016/0004-3702(94)90029-9', '10.1109/34.927458', '10.1109/ROBOT.2001.932850', '10.1049/ip-f-2.1993.0015', '10.1109/TAC.1979.1102177', '10.1109/JOE.1983.1145560', '10.1016/0262-8856(90)80002-B', '10.2307/2290063', '10.1049/ip-rsn:19990255', '10.1109/ROBOT.1999.772544', '10.1109/IROS.2003.1250637', '10.1093/biomet/82.4.711', '10.1007/978-4-431-66933-3', '10.1007/978-1-4757-3437-9_6', '10.1109/78.978374', '10.1109/ICCV.2001.937594', '10.1109/HOST.1999.778709', '10.2307/2965410', '10.1109/34.1000239', '10.1093/biomet/57.1.97', '10.2307/1390815', '10.1109/ICCV.1999.791275', '10.5244/C.16.26', '10.1109/CVPR.2003.1211339', '10.1109/ICCV.2003.1238473', '10.1111/1467-9868.00280', '10.1109/SSP.2003.1289519', '10.1109/CVPRW.2003.10097'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Probabilistic algorithms and the interactive museum tour-guide robot minerva': Paper(DOI='10.1177/02783640022067922', crossref_json=None, google_schorlar_metadata=None, title='Probabilistic algorithms and the interactive museum tour-guide robot minerva', authors=['Sebastian Thrun', 'Michael Beetz', 'Maren Bennewitz', 'Wolfram Burgard', 'Armin B Cremers', 'Frank Dellaert', 'Dieter Fox', 'Dirk Haehnel', 'Chuck Rosenberg', 'Nicholas Roy', 'Jamieson Schulte', 'Dirk Schulz'], abstract='This paper describes Minerva, an interactive tour-guide robot that was successfully                 deployed in a Smithsonian museum. Minerva’s software is pervasively                 probabilistic, relying on explicit representations of uncertainty in perception and                 control. During 2 weeks of operation, the robot interacted with thousands of people,                 both in the museum and through the Web, traversing more than 44 km at speeds of up                 to 163 cm/sec in the unmodified museum.', conference=None, journal=None, year=None, reference_list=['10.1016/S0921-8890(98)00009-8', '10.1109/70.88137', '10.1162/neco.1989.1.2.253', '10.1016/S0921-8890(05)80025-9', '10.1007/978-1-4471-3087-1_26', '10.1016/S0004-3702(99)00070-3', '10.1109/70.795798', '10.1207/s15516709cog1901_1', '10.1109/70.75902', '10.1111/j.1467-8640.1989.tb00324.x', '10.1109/JRA.1987.1087096', '10.1613/jair.616', '10.1016/S0004-3702(98)00023-X', '10.1613/jair.301', '10.1115/1.3662552', '10.1016/0921-8890(91)90014-C', '10.1016/S0921-8890(98)00011-6', '10.1177/027836499201100402', '10.1080/01621459.1998.10473765', '10.1023/A:1008854305733', '10.1287/mnsc.28.1.1', '10.1007/s004220050296', '10.1016/S0004-3702(99)00027-2', '10.1080/01621459.1999.10474153', '10.1109/MASSP.1986.1165342', '10.1109/100.833575', '10.1109/MRA.2000.833571', '10.1109/37.120453', '10.1287/opre.21.5.1071', '10.1287/opre.26.2.282', '10.1023/A:1007554531242', '10.1016/S0004-3702(97)00078-7', '10.1023/A:1007436523611', '10.1002/(SICI)1097-4563(199702)14:2<107::AID-ROB5>3.0.CO;2-W', '10.1016/0925-2312(95)00097-6'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Machine Learning', 'Human-Robot Interaction', 'Micro Air Vehicles'], conference_acronym='The international journal of robotics research', publisher=None, query_handler=None),\n",
       " 'An MCMC-based particle filter for tracking multiple interacting targets': Paper(DOI='10.1007/978-3-540-24673-2_23', crossref_json=None, google_schorlar_metadata=None, title='An MCMC-based particle filter for tracking multiple interacting targets', authors=['Zia Khan', 'Tucker Balch', 'Frank Dellaert'], abstract=' We describe a Markov chain Monte Carlo based particle filter that effectively deals with interacting targets, i.e., targets that are influenced by the proximity and/or behavior of other targets. Such interactions cause problems for traditional approaches to the data association problem. In response, we developed a joint tracker that includes a more sophisticated motion model to maintain the identity of targets throughout an interaction, drastically reducing tracker failures. The paper presents two main contributions: (1) we show how a Markov random field (MRF) motion prior, built on the fly at each time step, can substantially improve tracking when targets interact, and (2) we show how this can be done efficiently using Markov chain Monte Carlo (MCMC) sampling. We prove that incorporating an MRF to model interactions is equivalent to adding an additional interaction factor to the importance weights in a joint\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/0262-8856(90)80002-B', '10.1016/0004-3702(94)90029-9', '10.1109/34.927458', '10.1145/375735.376434', '10.1007/BFb0015549', '10.2307/2965410', '10.1111/1467-9868.00280', '10.1109/78.905890', '10.5244/C.16.26', '10.1109/ICCV.1999.791275', '10.1109/78.978374', '10.1007/978-4-431-66933-3', '10.1093/biomet/57.1.97'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'A unified multi-scale deep convolutional neural network for fast object detection': Paper(DOI='10.1007/978-3-319-46493-0_22', crossref_json=None, google_schorlar_metadata=None, title='A unified multi-scale deep convolutional neural network for fast object detection', authors=['Zhaowei Cai', 'Quanfu Fan', 'Rogerio S Feris', 'Nuno Vasconcelos'], abstract=' A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is proposed for fast multi-scale object detection. The MS-CNN consists of a proposal sub-network and a detection sub-network. In the proposal sub-network, detection is performed at multiple output layers, so that receptive fields match objects of different scales. These complementary scale-specific detectors are combined to produce a strong multi-scale object detector. The unified network is learned end-to-end, by optimizing a multi-task loss. Feature upsampling by deconvolution is also explored, as an alternative to input upsampling, to reduce the memory and computation costs. State-of-the-art object detection performance, at up to 15 fps, is reported on datasets, such as KITTI and Caltech, containing a substantial number of small objects.', conference=None, journal=None, year=None, reference_list=['10.1023/B:VISI.0000013087.49260.fb', '10.1109/TPAMI.2014.2300479', '10.1109/CVPR.2014.81', '10.1109/ICCV.2015.169', '10.1007/978-3-319-10578-9_23', '10.1109/ICCV.2015.135', '10.1109/ICCV.2011.6126456', '10.1109/CVPR.2012.6248074', '10.1109/CVPR.2015.7298965', '10.1109/ICCV.2015.164', '10.1109/TPAMI.2011.155', '10.1109/ICCV.2015.384', '10.1109/TITS.2015.2409889', '10.1109/TPAMI.2009.167', '10.1109/ICCV.2013.10', '10.1109/CVPR.2016.91', '10.1109/CVPR.2015.7298594', '10.1109/CVPR.2012.6248017', '10.1109/CVPR.2016.314', '10.1007/s11263-009-0275-4', '10.1007/s11263-015-0816-y', '10.1145/2647868.2654889', '10.1109/TPAMI.2015.2465908', '10.1109/CVPR.2014.414', '10.1007/978-3-319-10602-1_26', '10.1109/CVPR.2014.49', '10.1109/TPAMI.2015.2408347', '10.1109/CVPR.2015.7298800', '10.1007/978-3-319-10599-4_42', '10.1109/ICCV.2015.221', '10.1109/CVPR.2015.7298784', '10.1007/978-3-319-10593-2_36', '10.1109/CVPR.2016.234'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Supervised learning of semantic classes for image annotation and retrieval': Paper(DOI='10.1109/tpami.2007.61', crossref_json=None, google_schorlar_metadata=None, title='Supervised learning of semantic classes for image annotation and retrieval', authors=['Gustavo Carneiro', 'Antoni B Chan', 'Pedro J Moreno', 'Nuno Vasconcelos'], abstract='A probabilistic formulation for semantic image annotation and retrieval is proposed. Annotation and retrieval are posed as classification problems where each class is defined as the group of database images labeled with a common semantic label. It is shown that, by establishing this one-to-one correspondence between semantic labels and semantic classes, a minimum probability of error annotation and retrieval are feasible with algorithms that are 1) conceptually simple, 2) computationally efficient, and 3) do not require prior semantic segmentation of training images. In particular, images are represented as bags of localized feature vectors, a mixture density estimated for each image, and the mixtures associated with all images annotated with a common semantic label pooled into a density estimate for the corresponding semantic class. This pooling is justified by a multiple instance learning argument and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1076034.1076129', '10.1109/CVPR.2005.164', '10.1145/860458.860460', '10.1109/ICCV.2001.937654', '10.1145/322017.322021', '10.1117/12.274546', '10.1109/ICPR.2002.1048195', '10.1109/TPAMI.2003.1227984', '10.1109/34.506794', '10.1109/34.531803', '10.1016/S0031-3203(98)00079-X', '10.1109/CAIVD.1998.646032', '10.1109/CVPR.2001.990449', '10.1109/TSP.2004.831125', '10.1109/DCC.1997.581989', '10.1109/CVPR.2004.1315242', '10.1117/12.205308', '10.1145/244130.244151', '10.1109/93.621578', '10.1016/0031-3203(95)00160-3', '10.1023/A:1008108327226', '10.1109/34.391417', '10.1109/IVL.1997.629716', '10.1109/CVPR.2004.1315274', '10.1109/CVPR.1997.609399', '10.1016/S0004-3702(96)00034-3', '10.1007/BF00123143', '10.1117/12.143648', '10.1109/34.895972', '10.1109/ICCV.1998.710701', '10.1109/ICIP.1995.529708'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Surveillance', 'Eye Gaze Analysis', 'Computer Audition'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Modeling, clustering, and segmenting video with mixtures of dynamic textures': Paper(DOI='10.1109/tpami.2007.70738', crossref_json=None, google_schorlar_metadata=None, title='Modeling, clustering, and segmenting video with mixtures of dynamic textures', authors=['Antoni B Chan', 'Nuno Vasconcelos'], abstract='A dynamic texture is a spatio-temporal generative model for video, which represents video sequences as observations from a linear dynamical system. This work studies the mixture of dynamic textures, a statistical model for an ensemble of video sequences that is sampled from a finite collection of visual processes, each of which is a dynamic texture. An expectation-maximization (EM) algorithm is derived for learning the parameters of the model, and the model is related to previous works in linear systems, machine learning, time- series clustering, control theory, and computer vision. Through experimentation, it is shown that the mixture of dynamic textures is a suitable representation for both the appearance and dynamics of a variety of visual processes that have traditionally been challenging for computer vision (for example, fire, steam, water, vehicle and pedestrian traffic, and so forth). When compared with state-of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1111/j.1467-9892.2005.00441.x', '10.1007/978-1-4757-3502-4', '10.1016/0005-1098(94)90230-5', '10.1111/j.1467-9892.1982.tb00349.x', '10.1162/089976699300016674', '10.1109/ICCV.2005.151', '10.1109/34.1000236', '10.1109/34.868688', '10.1109/TSP.2004.831125', '10.1007/BF01908075', '10.1109/CVPR.1997.609375', '10.1109/CVPR.2005.263', '10.1109/34.908972', '10.1109/CVPR.2005.279', '10.1109/CVPR.1999.786972', '10.1109/TPAMI.2002.1023800', '10.1109/ICCV.2001.937584', '10.1023/A:1021669406132', '10.1109/CVPR.2001.990925', '10.1109/ICCV.2003.1238632', '10.1109/ICCV.2001.937658', '10.1109/TPAMI.2003.1195991', '10.1109/CVPR.1993.341161', '10.1109/34.868677', '10.1016/j.patcog.2005.01.025', '10.1162/089976600300015619', '10.1007/s11263-007-0062-z', '10.1016/0304-4076(94)90036-1', '10.1016/j.patcog.2003.12.018', '10.1109/ACC.2002.1024543', '10.2307/2669629', '10.1109/CVPR.1999.784983', '10.1109/ICCV.1998.710707', '10.1109/9.554398', '10.1109/PROC.1976.10284', '10.1109/89.242489', '10.1109/TCS.1983.1085288', '10.1080/01621459.1991.10475107', '10.1109/TAC.1965.1098191', '10.1007/BF01420984', '10.1007/978-1-4615-3236-1_1', '10.1016/0004-3702(81)90024-2', '10.1109/ACV.1994.341288', '10.1023/A:1008078328650', '10.1109/83.334981', '10.1109/34.531801'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Surveillance', 'Eye Gaze Analysis', 'Computer Audition'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Spatiotemporal saliency in dynamic scenes': Paper(DOI='10.1109/tpami.2009.112', crossref_json=None, google_schorlar_metadata=None, title='Spatiotemporal saliency in dynamic scenes', authors=['Vijay Mahadevan', 'Nuno Vasconcelos'], abstract='A spatiotemporal saliency algorithm based on a center-surround framework is proposed. The algorithm is inspired by biological mechanisms of motion-based perceptual grouping and extends a discriminant formulation of center-surround saliency previously proposed for static imagery. Under this formulation, the saliency of a location is equated to the power of a predefined set of features to discriminate between the visual stimuli in a center and a surround window, centered at that location. The features are spatiotemporal video patches and are modeled as dynamic textures, to achieve a principled joint characterization of the spatial and temporal components of saliency. The combination of discriminant center-surround saliency with the modeling power of dynamic textures yields a robust, versatile, and fully unsupervised spatiotemporal saliency algorithm, applicable to scenes with highly dynamic backgrounds and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICPR.2004.1333992', '10.1109/ICCV.2003.1238312', '10.1145/1177352.1177355', '10.1109/TPAMI.2003.1233909', '10.1023/A:1021669406132', '10.1109/JPROC.2002.801448', '10.1162/neco.2009.11-06-391', '10.1109/ICCV.2003.1238315', '10.1152/jn.1965.28.2.229', '10.1007/BF01420982', '10.1109/CVPR.2005.40', '10.1016/S0042-6989(99)00163-7', '10.1109/34.868680', '10.1016/S0896-6273(00)81208-8', '10.1007/3-540-45479-9_28', '10.1038/nature01652', '10.1109/CVPR.2007.383244', '10.1109/34.598236', '10.1109/CVPR.2005.279', '10.1002/0471200611', '10.1016/0042-6989(93)90020-W', '10.1109/TPAMI.2005.213', '10.1007/s00138-002-0091-0', '10.1109/CVPR.2003.1211430', '10.1109/CVPR.1999.784637'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'deep learning', 'image processing', 'machine learning', 'multimedia'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Counting people with low-level features and Bayesian regression': Paper(DOI='10.1109/tip.2011.2172800', crossref_json=None, google_schorlar_metadata=None, title='Counting people with low-level features and Bayesian regression', authors=['Antoni B Chan', 'Nuno Vasconcelos'], abstract='An approach to the problem of estimating the size of inhomogeneous crowds, which are composed of pedestrians that travel in different directions, without using explicit object segmentation or tracking is proposed. Instead, the crowd is segmented into components of homogeneous motion, using the mixture of dynamic-texture motion model. A set of holistic low-level features is extracted from each segmented region, and a function that maps features into estimates of the number of people per segment is learned with Bayesian regression. Two Bayesian regression models are examined. The first is a combination of Gaussian process regression with a compound kernel, which accounts for both the global and local trends of the count mapping but is limited by the real-valued outputs that do not match the discrete counts. We address this limitation with a second model, which is based on a Bayesian treatment of Poisson\\xa0…', conference=None, journal=None, year=None, reference_list=['10.2307/2344614', '10.1093/biomet/61.3.539', '10.1109/TCSVT.2008.927109', '10.1109/CVPR.2009.5206621', '10.2307/2983618', '10.1145/1553374.1553376', '10.1109/CVPR.2001.990644', '10.1109/IJCNN.2007.4371217', '10.1109/3477.775269', '10.1016/0165-1684(96)00075-8', '10.1049/ecej:19950106', '10.5244/C.19.63', '10.1109/SIBGRA.1998.722773', '10.1109/ICCV.2007.4409075', '10.1109/CVPR.2008.4587569', '10.1002/9781118625590', '10.1109/CVPR.2005.272', '10.1109/TPAMI.2007.70770', '10.1109/3468.983420', '10.1109/ICCV.2005.74', '10.1109/CVPR.2006.320', '10.1109/CVPR.2006.92', '10.1109/CVPR.2003.1211503', '10.1109/ICCV.2007.4408936', '10.1007/s11263-005-6644-8', '10.1017/CBO9780511814365', '10.1109/ICPR.2004.1333992', '10.1111/1467-9876.00113', '10.1109/ICCV.2009.5459191', '10.1109/TPAMI.2007.70738', '10.1109/ICASSP.1999.757602', '10.1109/CVPR.2008.4587597', '10.1109/TPAMI.1986.4767851', '10.1109/CVPR.2005.177'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Surveillance', 'Eye Gaze Analysis', 'Computer Audition'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Discriminant saliency, the detection of suspicious coincidences, and applications to visual recognition': Paper(DOI='10.1109/tpami.2009.27', crossref_json=None, google_schorlar_metadata=None, title='Discriminant saliency, the detection of suspicious coincidences, and applications to visual recognition', authors=['Dashan Gao', 'Sunhyoung Han', 'Nuno Vasconcelos'], abstract=\"A discriminant formulation of top-down visual saliency, intrinsically connected to the recognition problem, is proposed. The new formulation is shown to be closely related to a number of classical principles for the organization of perceptual systems, including infomax, inference by detection of suspicious coincidences, classification with minimal uncertainty, and classification with minimum probability of error. The implementation of these principles with computational parsimony, by exploitation of the statistics of natural images, is investigated. It is shown that Barlow's principle of inference by the detection of suspicious coincidences enables computationally efficient saliency measures which are nearly optimal for classification. This principle is adopted for the solution of the two fundamental problems in discriminant saliency, feature selection and saliency detection. The resulting saliency detector is shown to have a\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1109/83.982822', '10.5244/C.2.23', '10.1109/TPAMI.2006.54', '10.1109/34.192463', '10.1109/ICCV.2003.1238407', '10.1109/76.350779', '10.1109/TPAMI.2007.1155', '10.1109/CVPR.2007.383050', '10.1007/s11263-006-9794-4', '10.1109/ICCV.2005.142', '10.1109/ICCV.2005.77', '10.1109/CVPR.2003.1211479', '10.1007/11949619_41', '10.1016/j.tics.2007.09.009', '10.1109/CVPR.2006.333', '10.1007/978-3-540-77343-6_12', '10.1023/A:1023052124951', '10.1007/BFb0028370', '10.1109/TIP.2004.834657', '10.1109/72.298224', '10.1016/0004-3702(95)00025-9', '10.1109/34.730558', '10.1109/ICCV.2003.1238356', '10.1109/TPAMI.2008.77', '10.3758/BF03200774', '10.1007/978-1-4899-5379-7', '10.1016/0010-0285(80)90005-5', '10.1080/00335558008248231', '10.1016/0004-3702(81)90025-4', '10.1016/0004-3702(87)90070-1', '10.1109/83.806616', '10.1016/j.neunet.2006.10.001', '10.1007/s11263-006-9784-6', '10.1109/CVPR.2006.68', '10.1023/A:1012460413855', '10.1080/net.12.3.241.253', '10.1109/2.36', '10.1037/h0054663', '10.1162/neco.2009.11-06-391', '10.1167/8.7.13', '10.1109/CVPR.2005.189', '10.1109/TPAMI.2007.1144', '10.1023/A:1007617005950', '10.1145/1008992.1009034', '10.1109/ICCV.2005.66', '10.1109/TPAMI.1986.4767747', '10.1007/BF01418978', '10.1109/TPAMI.2004.29', '10.1109/ICCV.1999.790410', '10.1080/757582976', '10.1023/B:VISI.0000027790.02288.f2', '10.1016/S0167-8655(02)00192-7'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'deep learning', 'image processing', 'machine learning', 'multimedia'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'On the plausibility of the discriminant center-surround hypothesis for visual saliency': Paper(DOI='10.1167/8.7.13', crossref_json=None, google_schorlar_metadata=None, title='On the plausibility of the discriminant center-surround hypothesis for visual saliency', authors=['Dashan Gao', 'Vijay Mahadevan', 'Nuno Vasconcelos'], abstract='It has been suggested that saliency mechanisms play a role in perceptual organization. This work evaluates the plausibility of a recently proposed generic principle for visual saliency: that all saliency decisions are optimal in a decision-theoretic sense. The discriminant saliency hypothesis is combined with the classical assumption that bottom-up saliency is a center-surround process to derive a (decision-theoretic) optimal saliency architecture. Under this architecture, the saliency of each image location is equated to the discriminant power of a set of features with respect to the classification problem that opposes stimuli at center and surround. The optimal saliency detector is derived for various stimulus modalities, including intensity, color, orientation, and motion, and shown to make accurate quantitative predictions of various psychophysics of human saliency for both static and motion stimuli. These include some classical nonlinearities of orientation and motion saliency and a Weber law that governs various types of saliency asymmetries. The discriminant saliency detectors are also applied to various saliency problems of interest in computer vision, including the prediction of human eye fixations on natural scenes, motion-based saliency in the presence of ego-motion, and background subtraction in highly dynamic scenes. In all cases, the discriminant saliency detectors outperform previously proposed methods from both the saliency and the general computer vision literatures.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'deep learning', 'image processing', 'machine learning', 'multimedia'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Bridging the gap: Query by semantic example': Paper(DOI='10.1109/tmm.2007.900138', crossref_json=None, google_schorlar_metadata=None, title='Bridging the gap: Query by semantic example', authors=['Nikhil Rasiwasia', 'Pedro J Moreno', 'Nuno Vasconcelos'], abstract='A combination of query-by-visual-example (QBVE) and semantic retrieval (SR), denoted as query-by-semantic-example (QBSE), is proposed. Images are labeled with respect to a vocabulary of visual concepts, as is usual in SR. Each image is then represented by a vector, referred to as a semantic multinomial, of posterior concept probabilities. Retrieval is based on the query-by-example paradigm: the user provides a query image, for which 1) a semantic multinomial is computed and 2) matched to those in the database. QBSE is shown to have two main properties of interest, one mostly practical and the other philosophical. From a practical standpoint, because it inherits the generalization ability of SR inside the space of known visual concepts (referred to as the semantic space) but performs much better outside of it, QBSE produces retrieval systems that are more accurate than what was previously possible\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/IVL.1997.629723', '10.1117/12.360434', '10.1109/TCSVT.2002.808087', '10.1109/TMM.2003.813280', '10.1117/12.536579', '10.1109/ICME.2003.1221649', '10.1145/190627.190639', '10.1117/12.143648', '10.1109/34.895972', '10.1109/TPAMI.2007.61', '10.1109/TSP.2004.831125', '10.1145/361219.361220', '10.1007/3-540-45479-9_5', '10.1038/381520a0', '10.1109/ADC.2001.904476', '10.1016/S0031-3203(98)00079-X', '10.1109/IVL.1997.629716', '10.7551/mitpress/7287.001.0001', '10.1109/CVPR.1997.609399', '10.1109/ICCV.2001.937654', '10.1145/860458.860460', '10.1109/CVPR.2004.1315274', '10.1109/CVPR.2005.164', '10.1109/ICIP.2001.958037', '10.1007/BF00123143', '10.1016/0031-3203(95)00160-3', '10.1117/12.274546', '10.1109/CVPR.2000.855825', '10.1109/CAIVD.1998.646032', '10.1007/BF00130487', '10.1109/IVL.1998.694520', '10.1201/9780429258411', '10.1109/CVPR.2001.990449', '10.1002/0471200611', '10.1109/ICPR.2000.905271'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'deep learning', 'image processing', 'machine learning', 'multimedia'], conference_acronym='IEEE transactions on multimedia', publisher=None, query_handler=None),\n",
       " 'Peak-piloted deep network for facial expression recognition': Paper(DOI='10.1007/978-3-319-46475-6_27', crossref_json=None, google_schorlar_metadata=None, title='Peak-piloted deep network for facial expression recognition', authors=['Xiangyun Zhao', 'Xiaodan Liang', 'Luoqi Liu', 'Teng Li', 'Yugang Han', 'Nuno Vasconcelos', 'Shuicheng Yan'], abstract=' Objective functions for training of deep networks for face-related recognition tasks, such as facial expression recognition (FER), usually consider each sample independently. In this work, we present a novel peak-piloted deep network (PPDN) that uses a sample with peak expression (easy sample) to supervise the intermediate feature responses for a sample of non-peak expression (hard sample) of the same type and from the same subject. The expression evolving process from non-peak expression to peak expression can thus be implicitly embedded in the network to achieve the invariance to expression intensities. A special-purpose back-propagation procedure, peak gradient suppression (PGS), is proposed for network training. It drives the intermediate-layer feature responses of non-peak expression samples towards those of the corresponding peak expression samples, while avoiding the inverse\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2014.226', '10.1109/ICCV.2015.431', '10.1109/CVPR.2014.233', '10.1145/2818346.2830595', '10.1109/ICCV.2015.341', '10.1007/978-3-319-10599-4_7', '10.1016/j.imavis.2008.08.005', '10.1109/CVPR.2015.7299170', '10.1109/CVPRW.2010.5543262', '10.1016/j.imavis.2011.07.002', '10.1016/j.imavis.2009.08.002', '10.1007/978-3-642-33709-3_45', '10.5244/C.22.99', '10.1007/978-3-642-33868-7_25', '10.1109/CVPR.2015.7298594', '10.1109/ICCV.2013.21', '10.1109/CVPR.2015.7298682', '10.1007/978-3-7908-2604-3_16', '10.1007/978-3-319-16817-3_10', '10.1109/TIP.2011.2160957'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'deep learning', 'image processing', 'machine learning', 'multimedia'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Centrist: A visual descriptor for scene categorization': Paper(DOI='10.1109/tpami.2010.224', crossref_json=None, google_schorlar_metadata=None, title='Centrist: A visual descriptor for scene categorization', authors=['Jianxin Wu', 'Jim M Rehg'], abstract='CENsus TRansform hISTogram (CENTRIST), a new visual descriptor for recognizing topological places or scene categories, is introduced in this paper. We show that place and scene recognition, especially for indoor environments, require its visual descriptor to possess properties that are different from other vision domains (e.g., object recognition). CENTRIST satisfies these properties and suits the place and scene recognition task. It is a holistic representation and has strong generalizability for category recognition. CENTRIST mainly encodes the structural properties within an image and suppresses detailed textural information. Our experiments demonstrate that CENTRIST outperforms the current state of the art in several place and scene recognition data sets, compared with other descriptors such as SIFT and Gist. Besides, it is easy to implement and evaluates extremely fast.', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2007.41', '10.1109/CVPR.2007.383018', '10.1016/j.cviu.2005.09.012', '10.1109/MRA.2006.1638022', '10.1109/ROBOT.2001.932909', '10.1109/70.928558', '10.1016/S0004-3702(01)00069-8', '10.1109/TPAMI.2002.1017623', '10.1007/BFb0028345', '10.1109/CVPR.2003.1211479', '10.1109/TPAMI.2007.70716', '10.1023/B:VISI.0000042934.15159.49', '10.1109/CVPR.2005.16', '10.1109/TPAMI.2005.188', '10.1109/CVPR.2006.68', '10.1007/s11263-006-8614-1', '10.1109/IROS.2006.281789', '10.1109/CAIVD.1998.646032', '10.1023/A:1011139631724', '10.1109/TPAMI.2007.1155', '10.1109/CVPR.2009.5206537', '10.1109/ICCV.2007.4408866', '10.1023/B:VISI.0000029664.99615.94', '10.1109/ROBOT.2002.1013387', '10.1109/CVPR.2005.177', '10.1007/BF00130487', '10.1109/CVPR.2008.4587784', '10.1109/ICCV.2007.4408870', '10.1109/ICCV.2007.4408872'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'robotics', 'machine learning', 'human-computer interaction', 'parallel and distributed'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Visual tracking of high dof articulated structures: an application to human hand tracking': Paper(DOI='10.1007/bfb0028333', crossref_json=None, google_schorlar_metadata=None, title='Visual tracking of high dof articulated structures: an application to human hand tracking', authors=['James M Rehg', 'Takeo Kanade'], abstract=' Passive sensing of human hand and limb motion is important for a wide range of applications from human-computer interaction to athletic performance measurement. High degree of freedom articulated mechanisms like the human hand are difficult to track because of their large state space and complex image appearance. This article describes a model-based hand tracking system, called DigitEyes, that can recover the state of a 27 DOF hand model from ordinary gray scale images at speeds of up to 10 Hz.', conference=None, journal=None, year=None, reference_list=['10.1007/BF01469225', '10.1007/BF00126395', '10.1007/BF00127170', '10.1109/TPAMI.1980.6447699', '10.1109/34.85661', '10.1145/127719.122754', '10.1145/29933.275628'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'robotics', 'machine learning', 'human-computer interaction', 'parallel and distributed'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Learning to recognize daily actions using gaze': Paper(DOI='10.1007/978-3-642-33718-5_23', crossref_json=None, google_schorlar_metadata=None, title='Learning to recognize daily actions using gaze', authors=['Alireza Fathi', 'Yin Li', 'James M Rehg'], abstract=' We present a probabilistic generative model for simultaneously recognizing daily actions and predicting gaze locations in videos recorded from an egocentric camera. We focus on activities requiring eye-hand coordination and model the spatio-temporal relationship between the gaze point, the scene objects, and the action label. Our model captures the fact that the distribution of both visual features and object occurrences in the vicinity of the gaze point is correlated with the verb-object pair describing the action. It explicitly incorporates known properties of gaze behavior from the psychology literature, such as the temporal delay between fixation and manipulation events. We present an inference method that can predict the best sequence of gaze locations and the associated action label from an input sequence of images. We demonstrate improvements in action recognition rates and gaze prediction\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2009.5206707', '10.1109/CVPR.2012.6247710', '10.1167/8.14.18', '10.1109/ICCV.2011.6126269', '10.1109/CVPR.2012.6247805', '10.1109/CVPR.2011.5995444', '10.1109/TPAMI.2009.83', '10.1016/j.tics.2005.02.009', '10.1109/34.730558', '10.1109/ICCV.2009.5459462', '10.1109/CVPR.2011.5995406', '10.1016/S0042-6989(01)00102-X', '10.1007/3-540-61123-1_167', '10.1016/S0042-6989(01)00245-0', '10.1109/CVPR.2012.6248010', '10.1109/CVPR.2010.5540074', '10.1007/3-540-49256-9_4', '10.1109/CVPRW.2009.5204354', '10.1037/0033-295X.113.4.766', '10.1109/ICCV.2007.4408865', '10.1007/978-1-4899-5379-7', '10.1142/S0219843609001863'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'robotics', 'machine learning', 'human-computer interaction', 'parallel and distributed'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Fast asymmetric learning for cascade face detection': Paper(DOI='10.1109/tpami.2007.1181', crossref_json=None, google_schorlar_metadata=None, title='Fast asymmetric learning for cascade face detection', authors=['Jianxin Wu', 'S Charles Brubaker', 'Matthew D Mullin', 'James M Rehg'], abstract='A cascade face detector uses a sequence of node classifiers to distinguish faces from nonfaces. This paper presents a new approach to design node classifiers in the cascade detector. Previous methods used machine learning algorithms that simultaneously select features and form ensemble classifiers. We argue that if these two parts are decoupled, we have the freedom to design a classifier that explicitly addresses the difficulties caused by the asymmetric learning goal. There are three contributions in this paper: The first is a categorization of asymmetries in the learning goal and why they make face detection hard. The second is the forward feature selection (FFS) algorithm and a fast precomputing strategy for AdaBoost. FFS and the fast AdaBoost can reduce the training time by approximately 100 and 50 times, in comparison to a naive implementation of the AdaBoost feature selection method. The last\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.655648', '10.1109/34.655647', '10.1145/1143844.1143904', '10.1109/ICCV.2003.1238422', '10.1109/CVPR.1997.609310', '10.1109/CVPR.2003.1211496', '10.1023/B:VISI.0000013087.49260.fb', '10.1023/A:1011113216584', '10.1162/089976699300016197', '10.1145/1095878.1095882', '10.1007/s11263-007-0060-1', '10.1109/CVPR.2004.1315144', '10.1007/978-3-540-45243-0_39', '10.1109/ICCV.2003.1238417', '10.1109/CVPR.2004.1315241', '10.1145/290941.290996', '10.1109/CVPR.1996.517125', '10.1109/ICCV.1998.710772', '10.1214/009053605000000174', '10.1007/BF00058680', '10.1145/1102351.1102476', '10.1145/1007730.1007734', '10.1214/aos/1024691352', '10.1109/CVPR.2001.990919', '10.1109/34.982883', '10.1016/S0167-8655(02)00106-X', '10.1109/34.935848', '10.1109/ICCV.2001.937694'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Development and evaluation of an artificial intelligence system for COVID-19 diagnosis': Paper(DOI='10.1038/s41467-020-18685-1', crossref_json=None, google_schorlar_metadata=None, title='Development and evaluation of an artificial intelligence system for COVID-19 diagnosis', authors=['Cheng Jin', 'Weixiang Chen', 'Yukun Cao', 'Zhanwei Xu', 'Zimeng Tan', 'Xin Zhang', 'Lei Deng', 'Chuansheng Zheng', 'Jie Zhou', 'Heshui Shi', 'Jianjiang Feng'], abstract='Early detection of COVID-19 based on chest CT enables timely treatment of patients and helps control the spread of the disease. We proposed an artificial intelligence (AI) system for rapid COVID-19 detection and performed extensive statistical analysis of CTs of COVID-19 based on the AI system. We developed and evaluated our system on a large dataset with more than 10 thousand CT volumes from COVID-19, influenza-A/B, non-viral community acquired pneumonia (CAP) and non-pneumonia subjects. In such a difficult multi-class diagnosis task, our deep convolutional neural network-based system is able to achieve an area under the receiver operating characteristic curve (AUC) of 97.81% for multi-way classification on test cohort of 3,199 scans, AUC of 92.99% and 93.25% on two publicly available datasets, CC-CCII and MosMedData respectively. In a reader study involving five radiologists, the AI system\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1148/radiol.2020200823', '10.1148/radiol.2020200642', '10.1016/j.chest.2020.04.003', '10.1148/radiol.2020201160', '10.1016/S1473-3099(20)30086-4', '10.1109/CVPR.2009.5206848', '10.1038/nature14539', '10.1038/nature21056', '10.1016/j.media.2017.07.005', '10.1038/s41591-018-0316-z', '10.1038/s41591-018-0300-7', '10.1038/s41591-019-0447-x', '10.1038/s41598-018-37186-2', '10.1038/s41591-020-0931-3', '10.1016/j.patcog.2018.07.031', '10.1109/TNNLS.2019.2892409', '10.1109/WACV.2018.00079', '10.1038/s41598-019-56589-3', '10.1109/RBME.2020.2987975', '10.1109/RBME.2020.2990959', '10.1016/j.cell.2020.04.045', '10.1148/radiol.2020200905', '10.1148/radiol.2020201491', '10.1109/TMI.2020.2995965', '10.1109/TMI.2020.2996256', '10.1109/TMI.2020.2995508', '10.1148/radiol.2020201874', '10.1118/1.3528204', '10.1101/2020.05.20.20100362', '10.1109/ICCV.2017.74', '10.1158/0008-5472.CAN-17-0339', '10.1148/radiol.2020200463', '10.1148/radiol.2020200241', '10.1007/978-3-319-24574-4_28', '10.1109/CVPR.2016.90', '10.1183/16000617.0053-2016'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Learning compact binary face descriptor for face recognition': Paper(DOI='10.1016/j.patrec.2018.05.027', crossref_json=None, google_schorlar_metadata=None, title='Learning compact binary face descriptor for face recognition', authors=['Jiwen Lu', 'Venice Erin Liong', 'Xiuzhuang Zhou', 'Jie Zhou'], abstract='Binary feature descriptors such as local binary patterns (LBP) and its variations have been widely used in many face recognition systems due to their excellent robustness and strong discriminative power. However, most existing binary face descriptors are hand-crafted, which require strong prior knowledge to engineer them by hand. In this paper, we propose a compact binary face descriptor (CBFD) feature learning method for face representation and recognition. Given each face image, we first extract pixel difference vectors (PDVs) in local patches by computing the difference between each pixel and its neighboring pixels. Then, we learn a feature mapping to project these pixel difference vectors into low-dimensional binary vectors in an unsupervised manner, where 1) the variance of all binary codes in the training set is maximized, 2) the loss between the original real-valued codes and the learned binary codes is\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1167/9.6.23', '10.1109/TPAMI.2017.2679193', '10.1167/6.12.2', '10.1016/j.visres.2008.09.025', '10.1109/TPAMI.2012.30', '10.1109/TPAMI.2017.2710183', '10.1109/TIM.2012.2187468', '10.1098/rspb.2009.0677', '10.1016/j.patcog.2016.10.026', '10.1016/j.patcog.2012.10.004', '10.1109/TIP.2015.2390975', '10.1109/TIP.2017.2717505', '10.1109/MSP.2017.2732900', '10.1109/TPAMI.2017.2737538', '10.1109/TPAMI.2015.2408359', '10.1109/TIP.2015.2405340', '10.1109/TMM.2012.2187436', '10.1016/j.imavis.2016.08.009', '10.1016/j.patcog.2017.03.001', '10.1109/TIFS.2014.2327757', '10.1109/TCYB.2014.2376934'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Computer Vision'], conference_acronym='Pattern recognition letters', publisher=None, query_handler=None),\n",
       " 'Moving vehicle detection for automatic traffic monitoring': Paper(DOI='10.1109/tvt.2006.883735', crossref_json=None, google_schorlar_metadata=None, title='Moving vehicle detection for automatic traffic monitoring', authors=['Jie Zhou', 'Dashan Gao', 'David Zhang'], abstract=' A video-based traffic monitoring system must be capable of working in various weather and illumination conditions. In this paper, we will propose an example-based algorithm for moving vehicle detection. Different from previous works, this algorithm learns from examples and does not rely on any a priori model for vehicles. First, a novel scheme for adaptive background estimation is introduced. Then, the image is divided into many small nonoverlapped blocks. The candidates of the vehicle part can be found from the blocks if there is some change in gray level between the current image and the background. A low-dimensional feature is produced by applying principal component analysis to two histograms of each candidate, and a classifier based on a support vector machine is designed to classify it as a part of a real vehicle or not. Finally, all classified results are combined, and a parallelogram is built to represent\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/WVM.1991.212768', '10.1145/235815.235821', '10.1109/VS.2000.856858', '10.1142/S0218001402001800', '10.1142/S0218001402001575', '10.1006/cviu.2000.0870', '10.1109/ITSC.2001.948679', '10.1016/S0262-8856(02)00008-2', '10.1109/6979.880969', '10.1109/25.728525', '10.1109/6046.748172', '10.1109/ISCAS.1998.698804', '10.1109/ITSC.2000.881084', '10.1016/S0923-5965(99)00063-6', '10.1016/S0031-3203(02)00264-9', '10.1016/S0031-3203(00)00054-6', '10.1109/CVPR.1999.784637', '10.1109/34.868677', '10.1109/ICCV.1998.710772', '10.1109/34.683777', '10.1016/S0262-8856(99)00046-3', '10.1109/ICPR.2000.905341', '10.1007/BF01539538', '10.1109/25.69968', '10.1109/ACV.1992.240332', '10.1109/VNIS.1993.585604'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Computer Vision'], conference_acronym='IEEE transactions on vehicular technology', publisher=None, query_handler=None),\n",
       " 'Smooth representation clustering': Paper(DOI='10.1007/s11063-020-10306-8', crossref_json=None, google_schorlar_metadata=None, title='Smooth representation clustering', authors=['Han Hu', 'Zhouchen Lin', 'Jianjiang Feng', 'Jie Zhou'], abstract='Subspace clustering is a powerful technology for clustering data according to the underlying subspaces. Representation based methods are the most popular subspace clustering approach in recent years. In this paper, we analyze the grouping effect of representation based methods in depth. In particular, we introduce the enforced grouping effect conditions, which greatly facilitate the analysis of grouping effect. We further find that grouping effect is important for subspace clustering, which should be explicitly enforced in the data self-representation model, rather than implicitly implied by the model as in some prior work. Based on our analysis, we propose the SMooth Representation (SMR) model. We also propose a new affinity measure based on the grouping effect, which proves to be much more effective than the commonly used one. As a result, our SMR significantly outperforms the state-of-the-art ones on benchmark datasets.', conference=None, journal=None, year=None, reference_list=['10.1145/1007730.1007731', '10.1109/MSP.2010.939739', '10.1109/CVPRW.2009.5206547', '10.1109/TPAMI.2013.57', '10.1109/TPAMI.2012.88', '10.1109/TPAMI.2009.191', '10.1109/TPAMI.2007.1085', '10.1016/j.patrec.2013.08.006', '10.1016/j.eswa.2015.04.041', '10.1109/34.868688', '10.1109/ICCV.2011.6126422', '10.1109/TIP.2017.2691557', '10.1016/j.patcog.2018.05.020', '10.1109/TNNLS.2014.2306063', '10.1016/j.jvcir.2016.03.017', '10.1109/TGRS.2012.2226730', '10.1109/TPAMI.2015.2462360', '10.1007/978-3-642-33786-4_26', '10.1109/CVPR.2014.484', '10.1109/TCYB.2014.2336697', '10.1109/TIE.2014.2378735', '10.1109/TII.2018.2884211', '10.1109/TIP.2018.2804218', '10.1109/tpami.2019.2932058', '10.1109/TNNLS.2019.2908982', '10.1109/tcyb.2020.2969046', '10.1016/j.jsb.2012.10.010', '10.1109/TPAMI.2008.79', '10.1145/361573.361582', '10.1007/s11063-018-9783-y', '10.1109/CVPR.2007.382974', '10.1109/TPAMI.2005.92'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Deep Learning', 'Machine Learning'], conference_acronym='Neural Processing Letters', publisher=None, query_handler=None),\n",
       " \"A model-based method for the computation of fingerprints' orientation field\": Paper(DOI='10.1109/tip.2003.822608', crossref_json=None, google_schorlar_metadata=None, title=\"A model-based method for the computation of fingerprints' orientation field\", authors=['Jie Zhou', 'Jinwei Gu'], abstract='As a global feature of fingerprints, the orientation field is very important for automatic fingerprint recognition. Many algorithms have been proposed for orientation field estimation, but their results are unsatisfactory, especially for poor quality fingerprint images. In this paper, a model-based method for the computation of orientation field is proposed. First a combination model is established for the representation of the orientation field by considering its smoothness except for several singular points, in which a polynomial model is used to describe the orientation field globally and a point-charge model is taken to improve the accuracy locally at each singular point. When the coarse field is computed by using the gradient-based algorithm, a further result can be gained by using the model for a weighted approximation. Due to the global approximation, this model-based orientation field estimation algorithm has a robust\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.761265', '10.1016/0031-3203(95)00106-9', '10.1016/0031-3203(89)90035-6', '10.1016/0031-3203(84)90079-7', '10.1109/83.661195', '10.1016/0031-3203(93)90006-I', '10.1016/0031-3203(95)00154-9', '10.1017/CBO9780511564345', '10.1109/99.641608', '10.1109/34.990140', '10.1016/0031-3203(90)90134-7', '10.1109/34.587996', '10.1109/TPAMI.1982.4767240', '10.1109/TPAMI.2002.1023799', '10.1109/TPAMI.2002.1017618', '10.1109/5.628674', '10.1007/978-1-4615-4519-4', '10.1109/83.841531', '10.1007/978-1-4757-2377-9', '10.1007/978-1-4613-9777-9', '10.1016/0734-189X(87)90043-0', '10.1016/S0031-3203(02)00264-9', '10.1016/S0262-8856(02)00018-5'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Computer Vision'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Laplacian pyramid reconstruction and refinement for semantic segmentation': Paper(DOI='10.1007/978-3-319-46487-9_32', crossref_json=None, google_schorlar_metadata=None, title='Laplacian pyramid reconstruction and refinement for semantic segmentation', authors=['Golnaz Ghiasi', 'Charless C Fowlkes'], abstract=' CNN architectures have terrific recognition performance but rely on spatial pooling which makes it difficult to adapt them to tasks that require dense, pixel-accurate labeling. This paper makes two contributions: (1) We demonstrate that while the apparent spatial resolution of convolutional feature maps is low, the high-dimensional feature representation contains significant sub-pixel localization information. (2) We describe a multi-resolution reconstruction architecture based on a Laplacian pyramid that uses skip connections from higher resolution feature maps and multiplicative gating to successively refine segment boundaries reconstructed from lower-resolution maps. This approach yields state-of-the-art semantic segmentation results on the PASCAL VOC and Cityscapes segmentation benchmarks without resorting to more complex random-field inference or instance detection driven architectures.', conference=None, journal=None, year=None, reference_list=['10.1109/TCOM.1983.1095851', '10.1007/s11263-011-0507-2', '10.1109/TPAMI.2011.231', '10.1109/CVPR.2016.492', '10.1109/CVPR.2016.396', '10.1109/CVPR.2016.350', '10.1109/ICCV.2015.191', '10.1007/s11263-014-0733-5', '10.1109/ICCV.2015.135', '10.1109/ICCV.2011.6126343', '10.1109/CVPR.2015.7298642', '10.1007/978-3-319-46493-0_38', '10.1007/s11263-008-0202-0', '10.1109/CVPR.2016.348', '10.1007/978-3-319-10602-1_48', '10.1109/ICCV.2015.162', '10.1109/CVPR.2015.7298965', '10.1109/CVPR.2015.7298959', '10.1109/ICCV.2015.178', '10.1007/978-3-319-46448-0_5', '10.1007/s11263-007-0109-1', '10.1145/2733373.2807412', '10.1109/ICCV.2015.164', '10.1109/ICCV.2015.144', '10.1007/978-3-319-10590-1_53', '10.1109/ICCV.2011.6126474', '10.1109/ICCV.2015.179'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'biological image analysis'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Photo aesthetics ranking network with attributes and content adaptation': Paper(DOI='10.1007/978-3-319-46448-0_40', crossref_json=None, google_schorlar_metadata=None, title='Photo aesthetics ranking network with attributes and content adaptation', authors=['Shu Kong', 'Xiaohui Shen', 'Zhe Lin', 'Radomir Mech', 'Charless Fowlkes'], abstract=' Real-world applications could benefit from the ability to automatically generate a fine-grained ranking of photo aesthetics. However, previous methods for image aesthetics analysis have primarily focused on the coarse, binary categorization of images into high- or low-aesthetic categories. In this work, we propose to learn a deep convolutional neural network to rank photo aesthetics in which the relative ranking of photo aesthetics are directly modeled in the loss function. Our model incorporates joint learning of meaningful photographic attributes and image content information which can help regularize the complicated photo aesthetics rating problem. To train and analyze this model, we have assembled a new aesthetics and attributes database (AADB) which contains aesthetic scores and meaningful attributes assigned to each image by multiple human raters. Anonymized rater identities are\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2015.119', '10.1145/2647868.2654927', '10.1007/11744078_23', '10.1109/CVPR.2011.5995467', '10.1109/CVPR.2011.5995539', '10.1023/B:VISI.0000029664.99615.94', '10.1007/978-3-642-15561-1_11', '10.1109/CVPR.2007.383266', '10.1109/ICCV.2011.6126444', '10.1109/CVPR.2014.224', '10.1109/CVPR.2012.6247954', '10.1145/2072298.2072308', '10.1145/2187836.2187896', '10.1109/MMUL.2015.50', '10.5244/C.27.7', '10.5244/C.26.110', '10.1007/978-3-540-88690-7_29', '10.1214/aos/1013699998', '10.1145/1240624.1240684', '10.1109/CVPR.2014.180', '10.1109/CVPR.2015.7298682', '10.1007/978-3-319-10578-9_23', '10.1109/CVPR.2011.5995439'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'biological image analysis'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Multiresolution models for object detection': Paper(DOI='10.1117/12.977986', crossref_json=None, google_schorlar_metadata=None, title='Multiresolution models for object detection', authors=['Dennis Park', 'Deva Ramanan', 'Charless Fowlkes'], abstract=' Most current approaches to recognition aim to be scale-invariant. However, the cues available for recognizing a 300 pixel tall object are qualitatively different from those for recognizing a 3 pixel tall object. We argue that for sensors with finite resolution, one should instead use scale-variant, or multiresolution representations that adapt in complexity to the size of a putative detection window. We describe a multiresolution model that acts as a deformable part-based model when scoring large instances and a rigid template with scoring small instances. We also examine the interplay of resolution and context, and demonstrate that context is most helpful for detecting low-resolution instances when local models are limited in discriminative power. We demonstrate impressive results on the Caltech Pedestrian benchmark, which contains object instances at a wide range of scales. Whereas recent state-of-the-art\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'biological image analysis'], conference_acronym='Proceedings of SPIE, the International Society for Optical Engineering', publisher=None, query_handler=None),\n",
       " 'A Quantitative Spatiotemporal Atlas of Gene Expression in the Drosophila Blastoderm': Paper(DOI='10.1016/j.cell.2008.01.053', crossref_json=None, google_schorlar_metadata=None, title='A Quantitative Spatiotemporal Atlas of Gene Expression in the Drosophila Blastoderm', authors=['Charless C Fowlkes', 'Cris L Luengo Hendriks', 'Soile VE Keränen', 'Gunther H Weber', 'Oliver Rübel', 'Min-Yu Huang', 'Sohail Chatoor', 'Angela H DePace', 'Lisa Simirenko', 'Clara Henriquez', 'Amy Beaton', 'Richard Weiszmann', 'Susan Celniker', 'Bernd Hamann', 'David W Knowles', 'Mark D Biggin', 'Michael B Eisen', 'Jitendra Malik'], abstract=\"To fully understand animal transcription networks, it is essential to accurately measure the spatial and temporal expression patterns of transcription factors and their targets. We describe a registration technique that takes image-based data from hundreds of Drosophila blastoderm embryos, each costained for a reference gene and one of a set of genes of interest, and builds a model VirtualEmbryo. This model captures in a common framework the average expression patterns for many genes in spite of significant variation in morphology and expression between individual embryos. We establish the method's accuracy by showing that relationships between a pair of genes' expression inferred from the model are nearly identical to those measured in embryos costained for the pair. We present a VirtualEmbryo containing data for 95 genes at six time cohorts. We show that known gene-regulatory interactions can be\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1242/dev.122.1.205', '10.2307/2410702', '10.1016/j.gde.2005.02.007', '10.1038/nature02189', '10.1016/0092-8674(86)90771-3', '10.1242/dev.111.2.367', '10.1016/j.cell.2007.05.025', '10.1038/ng1886', '10.1038/nature02678', '10.1186/gb-2006-7-12-r124', '10.1101/gr.209601', '10.1126/science.1099247', '10.1242/dev.122.7.2303', '10.1038/nature05453', '10.1371/journal.pbio.0030093', '10.1186/gb-2006-7-12-r123', '10.1142/S0219720007002874', '10.1093/bioinformatics/17.1.3', '10.1016/j.gde.2004.06.004', '10.1002/j.1460-2075.1992.tb05498.x', '10.1006/rtim.2002.0292', '10.1242/dev.01075', '10.1016/j.cub.2005.12.044', '10.1186/gb-2007-8-7-r145', '10.1016/0012-1606(76)90070-1', '10.1093/nar/gkh029', '10.1093/genetics/9.1.41', '10.1242/dev.128.5.617'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'biological image analysis'], conference_acronym='Cell (Cambridge)', publisher=None, query_handler=None),\n",
       " 'Local figure–ground cues are valid for natural images': Paper(DOI='10.1167/7.8.2', crossref_json=None, google_schorlar_metadata=None, title='Local figure–ground cues are valid for natural images', authors=['Charless C Fowlkes', 'David R Martin', 'Jitendra Malik'], abstract='Figure–ground organization refers to the visual perception that a contour separating two regions belongs to one of the regions. Recent studies have found neural correlates of figure–ground assignment in V2 as early as 10–25 ms after response onset, providing strong support for the role of local bottom–up processing. How much information about figure–ground assignment is available from locally computed cues? Using a large collection of natural images, in which neighboring regions were assigned a figure–ground relation by human observers, we quantified the extent to which figural regions locally tend to be smaller, more convex, and lie below ground regions. Our results suggest that these Gestalt cues are ecologically valid, and we quantify their relative power. We have also developed a simple bottom–up computational model of figure–ground assignment that takes image contours as input. Using parameters fit to natural image statistics, the model is capable of matching human-level performance when scene context limited.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'biological image analysis'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Bidirectional LSTM-CRF models for sequence tagging': Paper(DOI='10.5626/jok.2018.45.8.792', crossref_json=None, google_schorlar_metadata=None, title='Bidirectional LSTM-CRF models for sequence tagging', authors=['Zhiheng Huang', 'Wei Xu', 'Kai Yu'], abstract='In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['artificial intelligence', 'deep learning', 'NLP', 'computer vision', 'reinforcement learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Image classification using super-vector coding of local image descriptors': Paper(DOI='10.1007/978-3-642-15555-0_11', crossref_json=None, google_schorlar_metadata=None, title='Image classification using super-vector coding of local image descriptors', authors=['Xi Zhou', 'Kai Yu', 'Tong Zhang', 'Thomas S Huang'], abstract=' This paper introduces a new framework for image classification using local visual descriptors. The pipeline first performs a nonlinear feature transformation on descriptors, then aggregates the results together to form image-level representations, and finally applies a classification model. For all the three steps we suggest novel solutions which make our approach appealing in theory, more scalable in computation, and transparent in classification. Our experiments demonstrate that the proposed classification method achieves state-of-the-art accuracy on the well-known PASCAL benchmarks.', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2007.383157', '10.21236/ADA454604', '10.1007/978-3-540-88690-7_24', '10.1109/CVPR.2008.4587633', '10.1016/j.cviu.2005.09.012', '10.1007/s11263-009-0275-4', '10.1109/ICCV.2007.4408875', '10.1109/CVPR.2007.383272', '10.1007/978-3-540-45167-9_6', '10.1109/5.726791', '10.1109/CVPR.2010.5540018', '10.1109/CVPR.2007.383266'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision', 'Speech Recognition', 'Language Understanding', 'Robotics'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Geometric separation of partially overlapping nonrigid objects applied to automatic chromosome classification': Paper(DOI='10.1109/34.632981', crossref_json=None, google_schorlar_metadata=None, title='Geometric separation of partially overlapping nonrigid objects applied to automatic chromosome classification', authors=['Gady Agam', \"Its'hak Dinstein\"], abstract='A common task in cytogenetic tests is the classification of human chromosomes. Successful separation between touching and overlapping chromosomes in a metaphase image is vital for correct classification. Current systems for automatic chromosome classification are mostly interactive and require human intervention for correct separation between touching and overlapping chromosomes. Since chromosomes are nonrigid objects, special separation methods are required to segregate them. Common methods for overlapping chromosomes separation between touching chromosomes tend to fail where ambiguity or incomplete information are involved, and so are unable to segregate overlapping chromosomes. The proposed approach treats the separation problem as an identification problem, and, in this way, manages to segregate overlapping chromosomes. This approach encompasses low-level knowledge\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/0167-8655(87)90002-X', '10.1109/TPAMI.1987.4767936', '10.1016/0734-189X(86)90028-9', '10.1016/0031-3203(81)90012-1', '10.1016/0165-1684(80)90019-5', '10.1109/TPAMI.1984.4767603', '10.1016/0262-8856(89)90020-6', '10.1109/34.308476', '10.1109/TC.1977.1674825', '10.1016/0031-3203(94)00108-X', '10.1016/0031-3203(81)90066-2', '10.1109/TPAMI.1980.4767029', '10.1117/12.165005', '10.1016/0031-3203(89)90021-6', '10.1007/BF00142568', '10.1016/0010-4825(78)90030-6'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Processing', 'Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'A comparative study of neural network based feature extraction paradigms': Paper(DOI='10.1016/s0167-8655(98)00120-2', crossref_json=None, google_schorlar_metadata=None, title='A comparative study of neural network based feature extraction paradigms', authors=['Boaz Lerner', 'Hugo Guterman', 'Mayer Aladjem', \"Its' hak Dinstein\"], abstract=\"The projection maps and derived classification accuracies of a neural network (NN) implementation of Sammon's mapping, an auto-associative NN (AANN) and a multilayer perceptron (MLP) feature extractor are compared with those of the conventional principal component analysis (PCA). Tested on five real-world databases, the MLP provides the highest classification accuracy at the cost of deforming the data structure, whereas the linear models preserve the structure but usually with inferior accuracy.\", conference=None, journal=None, year=None, reference_list=['10.1201/9781420050646.ptb6', '10.1007/BF00332918', '10.1016/B978-0-08-047865-4.50007-7', '10.1002/aic.690370209', '10.1109/3477.704293', '10.1016/0031-3203(95)00042-X', '10.1016/S0031-3203(97)00064-2', '10.1007/BF01413744', '10.1109/72.363467', '10.1109/T-C.1969.222678'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Processing', 'Computer Vision'], conference_acronym='Pattern recognition letters', publisher=None, query_handler=None),\n",
       " 'Binarization, character extraction, and writer identification of historical Hebrew calligraphy documents': Paper(DOI='10.1007/s10032-007-0041-5', crossref_json=None, google_schorlar_metadata=None, title='Binarization, character extraction, and writer identification of historical Hebrew calligraphy documents', authors=['Itay Bar-Yosef', 'Isaac Beckman', 'Klara Kedem', 'Itshak Dinstein'], abstract=' We present our work on the paleographic analysis and recognition system intended for processing of historical Hebrew calligraphy documents. The main goal is to analyze documents of different writing styles in order to identify the locations, dates, and writers of test documents. Using interactive software tools, a data base of extracted characters has been established. It now contains about 20,000 characters of 34 different writers, and will be distinctly expanded in the near future. Preliminary results of automatic extraction of pre-specified letters using the erosion operator are presented. We further propose and test topological features for handwriting style classification based on a selected subset of the Hebrew alphabet. A writer identification experiment using 34 writers yielded 100% correct classification.', conference=None, journal=None, year=None, reference_list=['10.1109/TSMC.1982.4308832', '10.1016/0031-3203(91)90082-G', '10.1016/j.patrec.2004.07.014', '10.1109/34.391389', '10.1007/978-3-540-30541-5_3', '10.1109/TIP.2003.821114', '10.1109/TPAMI.1987.4767941', '10.1007/s100320050014', '10.1109/ICPR.1998.711116', '10.1016/S0031-3203(99)00006-0', '10.1109/ICDAR.2003.1227797', '10.1109/ICDM.2002.1183917', '10.1007/978-94-009-9941-1_3', '10.1109/34.574797', '10.1016/0167-8655(94)90127-9'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Processing', 'Computer Vision'], conference_acronym='International journal on document analysis and recognition (Print)', publisher=None, query_handler=None),\n",
       " 'A spatial clustering procedure for multi-image data': Paper(DOI='10.1109/tcs.1975.1084059', crossref_json=None, google_schorlar_metadata=None, title='A spatial clustering procedure for multi-image data', authors=['R Haralick', \"Its' hak Dinstein\"], abstract=\"A spatial clustering procedure applicable to multispectral image data is discussed. The procedure takes into account the spatial distribution of the measurements as well as their distribution in measurement space. The procedure calls for the generation and then thresholding of the gradient image, cleaning the thresholded image, labeling the connected regions in the cleaned image, and clustering the labeled regions. An experiment was carried out on ERTS data in order to study the effect of the selection of the gradient image, the threshold, and the cleaning process. Three gradients, three gradient thresholds, and two cleaning parameters yielded 18 gradient-thresholds combinations. The combination that yielded connected homogeneous regions with the smallest variance was Robert's gradient with distance 2, thresholded by its running mean, and a cleaning process that considered a resolution cell to be\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1145/321623.321635', '10.1145/1461518.1461549', '10.1130/0016-7606(1968)79[1663:TDANTF]2.0.CO;2', '10.1109/PROC.1968.6414', '10.1147/rd.162.0138', '10.1126/science.132.3434.1115', '10.1016/B978-1-4832-3093-1.50033-X', '10.1080/01621459.1963.10500845', '10.1016/0004-3702(70)90008-1', '10.1109/TSMC.1971.4308295', '10.1109/PROC.1969.7020', '10.1147/rd.81.0022', '10.1016/0031-3203(71)90017-3', '10.1145/321356.321357', '10.1109/T-C.1971.223290', '10.1099/00221287-17-1-184', '10.1093/comjnl/10.1.29'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Processing', 'Computer Vision'], conference_acronym='IEEE transactions on circuits and systems (Print)', publisher=None, query_handler=None),\n",
       " 'An algorithm for polygonal approximation based on iterative point elimination': Paper(DOI='10.1016/0167-8655(95)80001-a', crossref_json=None, google_schorlar_metadata=None, title='An algorithm for polygonal approximation based on iterative point elimination', authors=['Arie Pikaz', \"Its' hak Dinstein\"], abstract='A simple and fast algorithm for polygonal approximation of digital curves is proposed. The algorithm is based on a greedy iterative elimination of a point with the currently minimal error value. The error criterion is defined such that the elimination of a curve point requires the update of the error associated only with its two neighbors. The use of a heap data-structure yields a worst case complexity of O(n log n). The algorithm is independent of the starting point.', conference=None, journal=None, year=None, reference_list=['10.1037/h0054663', '10.1016/0167-8655(93)90084-Q', '10.1109/TPAMI.1986.4767753', '10.1016/0146-664X(82)90011-9', '10.1016/0167-8655(88)90107-9', '10.1145/361953.361967', '10.1109/T-C.1974.224041', '10.1016/S0146-664X(72)80017-0', '10.1109/TC.1972.5008948', '10.1016/0031-3203(80)90031-X', '10.1109/34.31447', '10.1109/T-C.1974.223961', '10.1016/0146-664X(78)90055-2', '10.1016/0031-3203(93)90103-4'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Processing', 'Computer Vision'], conference_acronym='Pattern recognition letters', publisher=None, query_handler=None),\n",
       " 'DCT/DST alternate-transform image coding': Paper(DOI='10.1109/26.46533', crossref_json=None, google_schorlar_metadata=None, title='DCT/DST alternate-transform image coding', authors=['Kenneth Rose', 'Arie Heiman', \"Its' hak Dinstein\"], abstract='An image coding method for low bit rates is proposed. It is based on alternate use of the discrete cosine transform (DCT) and the discrete sine transform (DST) on image blocks. This procedure achieves the removal of redundancies in the correlation between neighboring blocks as well as the preservation of continuity across the block boundaries. An outline of the mathematical justification of the method, assuming a certain first-order Gauss-Markov model, is given. The resulting coding method is then adapted to nonstationary real images by locally adapting the model parameters and improving the block classification technique. Simulation results are shown and compared with the performance of related previous methods, namely adaptive DCT and fast Karhunen-Loeve transform (FKLT).< >', conference=None, journal=None, year=None, reference_list=['10.1109/ICASSP.1983.1172006', '10.1109/TIT.1960.1057548', '10.1109/TCOM.1980.1094577', '10.1109/PROC.1985.13183', '10.1109/PROC.1981.11971', '10.1109/TCOM.1981.1094935', '10.1109/TCOM.1986.1096509', '10.1109/TCOM.1977.1093941', '10.1109/TCOM.1976.1093409', '10.1109/TCOM.1977.1093763', '10.1109/TCOM.1980.1094656'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Processing', 'Computer Vision'], conference_acronym='IEEE transactions on communications (Print)', publisher=None, query_handler=None),\n",
       " 'Medial axis transform-based features and a neural network for human chromosome classification': Paper(DOI='10.1016/0031-3203(95)00042-x', crossref_json=None, google_schorlar_metadata=None, title='Medial axis transform-based features and a neural network for human chromosome classification', authors=['Boaz Lerner', 'Hugo Guterman', 'I Dinstein', 'Yitzhak Romem'], abstract='Medial axis transform (MAT) based features and a multilayer perceptron (MLP) neural network (NN) were used for human chromosome classification. Two approaches to the MAT, one based on skeletonization and the other based on a piecewise linear (PWL) approximation, were examined. The former yielded a finer medial axis, as well as better chromosome classification performances. Geometrical along with intensity-based features were extracted and tested. The probability of correct training set classification of five chromosome types was 99.3–99.6%. The probability of correct test set classification was greater than 98% and greater than 97% using features extracted by the first and second approaches, respectively. It was found that only 5–10, out of all the considered features, were required to correctly classify the chromosomes with almost no performance degradation.', conference=None, journal=None, year=None, reference_list=['10.1111/j.1601-5223.1956.tb03010.x', '10.1002/ajmg.1320350205', '10.1111/j.1601-5223.1971.tb02364.x', '10.1109/TBME.1976.324629', '10.1016/0167-8655(89)90056-1', '10.1016/0165-1684(80)90019-5', '10.1109/34.42838', '10.1109/TASSP.1975.1162664', '10.1016/1049-9652(92)90079-D'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Processing', 'Computer Vision'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'Pitting corrosion evaluation by computer image processing': Paper(DOI='10.1016/0010-938x(81)90059-7', crossref_json=None, google_schorlar_metadata=None, title='Pitting corrosion evaluation by computer image processing', authors=['D Itzhak', 'I Dinstein', 'T Zilberberg'], abstract='Pitting corrosion of AISI 304L stainless steel in a solution of FeCl3 is evaluated by computer image processing methods. Pitting probability and histograms of pit-areas are computed. Pitting probability is defined as the ratio of pitted area over the total area. The pitting probability in this work was found to be 9.73%. Computer image processing is shown to be a promising tool for statistical evaluation of pitting corrosion.', conference=None, journal=None, year=None, reference_list=['10.1149/1.2429915'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Processing', 'Computer Vision'], conference_acronym='Corrosion science', publisher=None, query_handler=None),\n",
       " 'A classification-driven partially occluded object segmentation (CPOOS) method with application to chromosome analysis': Paper(DOI='10.1109/78.720391', crossref_json=None, google_schorlar_metadata=None, title='A classification-driven partially occluded object segmentation (CPOOS) method with application to chromosome analysis', authors=['Boaz Lerner', 'Hugo Guterman', 'I Dinstein'], abstract='Classification of segment images created by connecting points of high concavity along curvatures is used to resolve partial occlusion in images. Modeling of shape or curvature is not necessary nor is the traditional excessive use of heuristics. Applied to human cell images, 82.6% of the analyzed clusters of chromosomes are correctly separated, rising to 90.5% following rejection of 8.7% of the images.', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.1987.4767901', '10.1002/cyto.990170303', '10.1109/3477.704293', '10.1109/34.55107', '10.1109/34.632981'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Processing', 'Computer Vision'], conference_acronym='IEEE transactions on signal processing', publisher=None, query_handler=None),\n",
       " 'Using simple decomposition for smoothing and feature point detection of noisy digital curves': Paper(DOI='10.1109/34.308476', crossref_json=None, google_schorlar_metadata=None, title='Using simple decomposition for smoothing and feature point detection of noisy digital curves', authors=['Arie Pikaz', \"Its'hak Dinstein\"], abstract='This correspondence presents an algorithm for smoothed polygonal approximation of noisy digital planar curves, and feature point detection. The resulting smoothed polygonal representation preserves the signs of the curvature function of the curve. The algorithm is based on a simple decomposition of noisy digital curves into a minimal number of convex and concave sections. The location of each separation point is optimized, yielding the minimal possible distance between the smoothed approximation and the original curve. Curve points within a convex (concave) section are discarded if their angle signs do not agree with the section sign, and if the resulted deviations from the curve are less than a threshold /spl epsi/, which is derived automatically. Inflection points are curve points between pairs of convex-concave sections, and cusps are curve points between pairs of convex-convex or concave-concave sections\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/0146-664X(82)90011-9', '10.1145/361953.361967', '10.1109/TPAMI.1980.4767029', '10.1007/978-1-4612-1098-6', '10.1109/34.126805', '10.1109/TC.1973.5009188', '10.1109/T-C.1975.224342', '10.1177/027836498700600203', '10.1007/BF00126396', '10.1109/34.67634', '10.1109/TPAMI.1986.4767753', '10.1109/TC.1977.1674812', '10.1109/TC.1977.1674825', '10.1109/TPAMI.1986.4767756', '10.1037/h0054663', '10.1109/TPAMI.1986.4767747', '10.1109/ICCV.1990.139496', '10.1109/TC.1972.5008948', '10.1109/T-C.1974.223961', '10.1109/34.31447'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Processing', 'Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Adapting multimedia internet content for universal access': Paper(DOI='10.1109/6046.748175', crossref_json=None, google_schorlar_metadata=None, title='Adapting multimedia internet content for universal access', authors=['Rakesh Mohan', 'John R.  Smith', 'Chung-Sheng Li'], abstract='Content delivery over the Internet needs to address both the multimedia nature of the content and the capabilities of the diverse client platforms the content is being delivered to. We present a system that adapts multimedia Web documents to optimally match the capabilities of the client device requesting it. This system has two key components. 1) A representation scheme called the InfoPyramid that provides a multimodal, multiresolution representation hierarchy for multimedia. 2) A customizer that selects the best content representation to meet the client capabilities while delivering the most value. We model the selection process as a resource allocation problem in a generalized rate distortion framework. In this framework, we address the issue of both multiple media types in a Web document and multiple resource types at the client. We extend this framework to allow prioritization on the content items in a Web\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/29.90373', '10.1109/5.241504', '10.1109/AVSS.2011.6027403', '10.1109/76.488825', '10.1109/83.334987', '10.1016/S0169-7552(97)00026-3', '10.1109/ISCAS.1998.704083', '10.1109/RELDIS.1998.740482', '10.1117/12.325833', '10.1002/j.1538-7305.1948.tb01338.x', '10.1007/978-0-585-28766-9_6', '10.1145/217279.215277', '10.1145/237090.237177', '10.1016/0169-7552(96)00027-X', '10.1109/98.709365', '10.1109/ICIP.1998.998987', '10.1109/MSP.1998.733494'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Multimedia', 'Computer Vision', 'Machine Learning', 'Information Retrieval'], conference_acronym='IEEE transactions on multimedia', publisher=None, query_handler=None),\n",
       " 'Visual information retrieval from large distributed online repositories': Paper(DOI='10.1016/s0920-5489(99)90763-8', crossref_json=None, google_schorlar_metadata=None, title='Visual information retrieval from large distributed online repositories', authors=['Shih-Fu Chang', 'John R Smith', 'Mandis Beigi', 'Ana Benitez'], abstract='Can solutions be developed that are as effective as existing text and nonvisual information search engines? With the increasing numbers of distributed repositories and users, how can we design scalable visual information retrieval systems? Digital imagery is a rich and subjective source of information. For example, different people extract different meanings from the same picture. Their response also varies over time and in different viewing contexts. A picture also has meaning at multiple levels—description, analysis, and interpretation—as described in [10]. Visual information is also represented in multiple forms—still images, video sequences, computer graphics, animations, stereoscopic images—and expected in such future applications as multiview and 3D video. Furthermore, visual information systems demand large resources for transmission, storage, and processing. These factors make the indexing, retrieval\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Multimedia', 'Computer Vision', 'Machine Learning', 'Information Retrieval'], conference_acronym='Computer standards & interfaces', publisher=None, query_handler=None),\n",
       " 'Deep learning, sparse coding, and SVM for melanoma recognition in dermoscopy images': Paper(DOI='10.1007/978-3-319-24888-2_15', crossref_json=None, google_schorlar_metadata=None, title='Deep learning, sparse coding, and SVM for melanoma recognition in dermoscopy images', authors=['Noel Codella', 'Junjie Cai', 'Mani Abedini', 'Rahil Garnavi', 'Alan Halpern', 'John R Smith'], abstract=' This work presents an approach for melanoma recognition in dermoscopy images that combines deep learning, sparse coding, and support vector machine (SVM) learning algorithms. One of the beneficial aspects of the proposed approach is that unsupervised learning within the domain, and feature transfer from the domain of natural photographs, eliminates the need of annotated data in the target task to learn good features. The applied feature transfer also allows the system to draw analogies between observations in dermoscopic images and observations in the natural world, mimicking the process clinical experts themselves employ to describe patterns in skin lesions. To evaluate the methodology, performance is measured on a dataset obtained from the International Skin Imaging Collaboration, containing 2624 clinical cases of melanoma (334), atypical nevi (144), and benign lesions (2146). The\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/S0140-6736(15)61087-X', '10.1001/jamadermatol.2014.4576', '10.1016/S1470-2045(02)00679-4', '10.1016/j.jaad.2001.11.001', '10.1046/j.1365-2133.2003.05023.x', '10.1590/S0365-05962006000300009', '10.1007/1-4020-3830-5', '10.1001/archderm.141.4.434', '10.1109/TITB.2012.2212282', '10.1109/42.918473', '10.1007/BFb0056241', '10.1007/978-3-642-40760-4_57', '10.1016/j.compmedimag.2008.08.003', '10.1109/EMBC.2013.6610779', '10.1561/0600000058', '10.1147/JRD.2015.2390017', '10.1109/ICME.2008.4607524', '10.1007/978-3-319-10470-6_61', '10.1109/ICPR.2010.751'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Machine Learning', 'Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Dynamic adaptation in an image transcoding proxy for mobile web browsing': Paper(DOI='10.1109/98.736473', crossref_json=None, google_schorlar_metadata=None, title='Dynamic adaptation in an image transcoding proxy for mobile web browsing', authors=['Richard Han', 'Pravin Bhagwat', 'Richard LaMaire', 'Todd Mummert', 'Veronique Perret', 'Jim Rubas'], abstract='Transcoding proxies are used as intermediaries between generic World Wide Web servers and a variety of client devices in order to adapt to the greatly varying bandwidths of different client communication links and to handle the heterogeneity of possibly small-screened client devices. Such transcoding proxies can adaptively adjust the amount by which a data stream is reduced, using an aggressive lossy compression method (e.g., an image becomes less clear, text is summarized). We present an analytical framework for determining whether to transcode and how much to transcode an image for the two cases of store-and-forward transcoding as well as streamed transcoding. These methods require prediction of transcoding delay, prediction of transcoded image size (in bytes), and estimation of network bandwidth. We discuss methods of adaptation based on fixed quality as well as fixed delay (automated/dynamic\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICIP.1998.998987', '10.1109/98.709365', '10.1145/288235.288270', '10.1109/RIDE.1997.583714', '10.1016/0169-7552(96)00027-X'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Multimedia', 'Computer Vision', 'Machine Learning', 'Information Retrieval'], conference_acronym='IEEE personal communications', publisher=None, query_handler=None),\n",
       " 'IBM Research TRECVID-2003 Video Retrieval System.': Paper(DOI='10.1007/3-540-45113-7_3', crossref_json=None, google_schorlar_metadata=None, title='IBM Research TRECVID-2003 Video Retrieval System.', authors=['Arnon Amir', 'Janne Argillander', 'Murray Campbell', 'Alexander Haubold', 'Giridharan Iyengar', 'Shahram Ebadollahi', 'Feng Kang', 'Milind R Naphade', 'Apostol Natsev', 'John R Smith', 'Jelena Tesic', 'Timo Volkmer'], abstract='In this paper we describe our participation in the NIST TRECVID-2003 evaluation. We participated in four tasks of the benchmark including shot boundary detection, high-level feature detection, story segmentation, and search. We describe the different runs we submitted for each track and discuss our performance.', conference=None, journal=None, year=None, reference_list=['10.1117/12.238675', '10.1117/12.403810'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Neuromorphic Computing', 'Eye Tracking', 'Video Search', 'Video Archives'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'The onion technique: Indexing for linear optimization queries': Paper(DOI='10.1145/335191.335433', crossref_json=None, google_schorlar_metadata=None, title='The onion technique: Indexing for linear optimization queries', authors=['Yuan-Chi Chang', 'Lawrence Bergman', 'Vittorio Castelli', 'Chung-Sheng Li', 'Ming-Ling Lo', 'John R Smith'], abstract='This paper describes the Onion technique, a special indexing structure for linear optimization queries. Linear optimization queries ask for top-N records subject to the maximization or minimization of linearly weighted sum of record attribute values. Such query appears in many applications employing linear models and is an effective way to summarize representative cases, such as the top-50 ranked colleges. The Onion indexing is based on a geometric property of convex hull, which guarantees that the optimal value can always be found at one or more of its vertices. The Onion indexing makes use of this property to construct convex hulls in layers with outer layers enclosing inner layers geometrically. A data record is indexed by its layer number or equivalently its depth in the layered convex hull. Queries with linear weightings issued at run time are evaluated from the outmost layer inwards. We show experimentally\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/275487.275506', '10.1007/3-540-49257-7_15', '10.1145/276304.276318', '10.1145/237218.237397', '10.1145/275487.275488', '10.1145/263661.263689', '10.1145/142675.142683', '10.1145/98524.98570'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Multimedia', 'Computer Vision', 'Machine Learning', 'Information Retrieval'], conference_acronym='SIGMOD record', publisher=None, query_handler=None),\n",
       " 'Image and video search engine for the world wide web': Paper(DOI='10.1117/12.263446', crossref_json=None, google_schorlar_metadata=None, title='Image and video search engine for the world wide web', authors=['John R Smith', 'Shih-Fu Chang'], abstract='We describe a visual information system prototype for searching for images and videos on the World-Wide Web. New visual information in the form of images, graphics, animations and videos is being published on the Web at an incredible rate. However, cataloging this visual data is beyond the capabilities of current text-based Web search engines. In this paper, we describe a complete system by which visual information on the Web is (1) collected by automated agents, (2) processed in both text and visual feature domains, (3) catalogued and (4) indexed for fast search and retrieval. We introduce an image and video search engine which utilizes both text-based navigation and content-based technology for searching visually through the catalogued images and videos. Finally, we provide an initial evaluation based upon the cataloging of over one half million images and videos collected from the Web.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial Intelligence', 'Multimedia', 'Computer Vision', 'Machine Learning', 'Information Retrieval'], conference_acronym='Proceedings of SPIE, the International Society for Optical Engineering', publisher=None, query_handler=None),\n",
       " 'Face detection using deep learning: An improved faster RCNN approach': Paper(DOI='10.1016/j.neucom.2018.03.030', crossref_json=None, google_schorlar_metadata=None, title='Face detection using deep learning: An improved faster RCNN approach', authors=['Xudong Sun', 'Pengcheng Wu', 'Steven CH Hoi'], abstract='In this paper, we present a new face detection scheme using deep learning and achieve the state-of-the-art detection performance on the well-known FDDB face detection benchmark evaluation. In particular, we improve the state-of-the-art Faster RCNN framework by combining a number of strategies, including feature concatenation, hard negative mining, multi-scale training, model pre-training, and proper calibration of key parameters. As a consequence, the proposed scheme obtained the state-of-the-art face detection performance and was ranked as one of the best models in terms of ROC curves of the published methods on the FDDB benchmark.1', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2001.990517', '10.1023/B:VISI.0000013087.49260.fb', '10.1109/BTAS.2014.6996284', '10.1109/TPAMI.2015.2448075', '10.1109/ICCV.2007.4409038', '10.1007/s11263-007-0060-1', '10.1109/TPAMI.2007.1011', '10.1016/j.neucom.2015.07.130', '10.1007/978-3-319-10593-2_47'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Multimodal AI', 'NLP', 'Multimedia', 'Computer Vision'], conference_acronym='Neurocomputing (Amsterdam)', publisher=None, query_handler=None),\n",
       " 'Online learning: A comprehensive survey': Paper(DOI='10.1016/j.neucom.2021.04.112', crossref_json=None, google_schorlar_metadata=None, title='Online learning: A comprehensive survey', authors=['Steven CH Hoi', 'Doyen Sahoo', 'Jing Lu', 'Peilin Zhao'], abstract='Online learning represents a family of machine learning methods, where a learner attempts to tackle some predictive (or any type of decision-making) task by learning from a sequence of data instances one by one at each time. The goal of online learning is to maximize the accuracy/correctness for the sequence of predictions/decisions made by the online learner given the knowledge of correct answers to previous prediction/learning tasks and possibly additional information. This is in contrast to traditional batch or offline machine learning methods that are often designed to learn a model from the entire training data set at once. Online learning has become a promising technique for learning from continuous streams of data in many real-world applications. This survey aims to provide a comprehensive survey of the online machine learning literature through a systematic review of basic ideas and key principles and a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/2133803.2184450', '10.1145/1143844.1143846', '10.1201/b15410', '10.1016/B978-012088469-8.50075-9', '10.4153/CJM-1954-037-2', '10.1137/S0097539702405619', '10.1007/s10107-003-0436-0', '10.1007/s11390-014-1416-y', '10.1109/Allerton.2012.6483308', '10.4086/toc.2012.v008a006', '10.1016/j.neucom.2019.07.106', '10.1016/j.tcs.2009.01.016', '10.1023/A:1013689704352', '10.1137/S0097539701398375', '10.1023/A:1007465907571', '10.1007/3-540-28349-8_2', '10.1007/s10115-013-0659-1', '10.1109/TIP.2014.2362658', '10.1007/978-3-540-28650-9_7', '10.1007/978-3-7908-2604-3_16', '10.1561/2200000016', '10.1561/2200000024', '10.1137/1.9781611972764.29', '10.1109/TNNLS.2012.2201167', '10.1109/72.159059', '10.1016/0893-6080(91)90012-T', '10.1007/978-1-4615-5529-2_5', '10.1007/s10994-007-5003-0', '10.1007/978-3-540-45167-9_28', '10.1109/TIT.2004.833339', '10.1137/S0097539703432542', '10.1109/TIT.2007.911292', '10.1016/j.jcss.2012.01.001', '10.1109/TIT.2005.847729', '10.1007/978-3-642-41136-6_16', '10.1145/1541880.1541882', '10.1007/s10791-009-9109-9', '10.1109/ICDM.2009.36', '10.1142/9789814293501_0015', '10.1162/0899766052530848', '10.1007/3-540-70659-3_2', '10.3115/1557690.1557757', '10.1109/TNN.2011.2160459', '10.1007/BF00114265', '10.1006/jcss.1997.1504', '10.1006/game.1999.0738', '10.1023/A:1007662407062', '10.1023/A:1007330508534', '10.1016/0898-1221(76)90003-1', '10.1023/A:1019271201970', '10.1007/11430919_49', '10.1145/3065950', '10.1145/3308558.3313503', '10.1002/sam.11217', '10.1609/aaai.v25i1.7910', '10.1109/TKDE.2003.1198387', '10.1109/TKDE.2013.184', '10.1109/TPAMI.2007.70771', '10.1145/3201604', '10.1007/s10994-007-5016-8', '10.1561/2400000013', '10.1145/267460.267502', '10.1111/1467-9965.00058', '10.1049/cp:19991091', '10.1145/2351316.2351329', '10.1007/s10994-012-5319-2', '10.1109/TPAMI.2011.270', '10.1609/aaai.v29i1.9587', '10.1109/WGEC.2008.32', '10.1007/978-3-642-16108-7_31', '10.1613/jair.301', '10.1145/1390156.1390212', '10.1016/j.jcss.2004.10.016', '10.1287/moor.12.2.262', '10.1109/TSP.2004.830991', '10.1145/225058.225121', '10.1145/3097983.3098079', '10.1109/MC.2009.263', '10.1007/s10115-010-0342-8', '10.1088/0305-4470/20/11/013', '10.1016/j.patcog.2011.03.019', '10.1016/0196-8858(85)90002-8', '10.1109/TPAMI.2006.56', '10.1007/3-540-49430-8_2', '10.1145/2512962', '10.1145/1961189.1961193', '10.1016/j.artint.2015.01.006', '10.1145/2435209.2435213', '10.1007/s10994-012-5281-z', '10.1109/TKDE.2013.139', '10.1016/j.neucom.2019.07.031', '10.1109/ICC.2019.8761303', '10.1109/ACCESS.2019.2900698', '10.1016/j.ins.2019.07.093', '10.1023/A:1012435301888', '10.1109/TNN.2006.880583', '10.1007/BF00116827', '10.1016/B978-0-08-094829-4.50022-2', '10.1006/inco.1994.1009', '10.1609/aaai.v30i1.10257', '10.1007/s10994-016-5599-z', '10.1145/3156684', '10.1007/s10994-016-5555-y', '10.1109/TSP.2013.2254478', '10.1007/s10107-007-0149-x', '10.24963/ijcai.2017/354', '10.1016/j.jfranklin.2019.12.039', '10.1137/1.9781611972825.85', '10.1080/14697688.2011.570368', '10.1109/TKDE.2009.191', '10.1016/j.neunet.2019.01.012', '10.1162/neco.1991.3.2.213', '10.7551/mitpress/1130.003.0016', '10.1109/5326.983933', '10.1109/TFUZZ.2019.2939993', '10.1016/j.ins.2019.04.055', '10.1109/TKDE.2016.2626441', '10.1007/978-3-642-34156-4_29', '10.1109/FSKD.2009.553', '10.1007/978-1-4612-5110-1_13', '10.1037/h0042519', '10.1126/science.290.5500.2323', '10.1007/978-3-642-04747-3_23', '10.1287/moor.1100.0446', '10.1145/3299875', '10.24963/ijcai.2018/369', '10.1109/MCI.2009.932254', '10.1002/asmb.874', '10.1145/3097983.3098025', '10.1561/2200000018', '10.1007/s10994-007-5014-x', '10.1007/s10107-010-0420-4', '10.1145/2556270', '10.3233/IDA-2009-0373', '10.1155/2009/421425', '10.1007/s00521-013-1362-6', '10.1126/science.290.5500.2319', '10.1093/biomet/25.3-4.285', '10.5244/C.26.87', '10.1007/s10791-005-6991-7', '10.1145/1552303.1552305', '10.1016/j.patrec.2011.11.022', '10.1007/978-3-642-16108-7_30', '10.1109/72.788640', '10.1007/11564096_42', '10.1145/279943.279947', '10.1145/2647868.2654948', '10.1145/1552303.1552307', '10.3115/v1/P15-1163', '10.1109/TKDE.2013.157', '10.1109/TKDE.2013.32', '10.1109/ICDM.2012.116', '10.1006/ijhc.2001.0499', '10.1162/neco.1989.1.2.270', '10.1109/TKDE.2015.2477296', '10.1016/j.neucom.2017.03.077', '10.1145/3070646', '10.1109/TPAMI.2013.149', '10.1016/j.patcog.2020.107566', '10.1007/11503415_12', '10.1016/j.artint.2014.06.003', '10.1109/TKDE.2018.2826011', '10.1007/978-3-031-01548-9'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Multimodal AI', 'NLP', 'Multimedia', 'Computer Vision'], conference_acronym='Neurocomputing (Amsterdam)', publisher=None, query_handler=None),\n",
       " 'Semisupervised svm batch mode active learning with applications to image retrieval': Paper(DOI='10.1145/1508850.1508854', crossref_json=None, google_schorlar_metadata=None, title='Semisupervised svm batch mode active learning with applications to image retrieval', authors=['Steven CH Hoi', 'Rong Jin', 'Jianke Zhu', 'Michael R Lyu'], abstract='Support vector machine (SVM) active learning is one popular and successful technique for relevance feedback in content-based image retrieval (CBIR). Despite the success, conventional SVM active learning has two main drawbacks. First, the performance of SVM is usually limited by the number of labeled examples. It often suffers a poor performance for the small-sized labeled examples, which is the case in relevance feedback. Second, conventional approaches do not take into account the redundancy among examples, and could select multiple examples that are similar (or even identical). In this work, we propose a novel scheme for explicitly addressing the drawbacks. It first learns a kernel function from a mixture of labeled and unlabeled data, and therefore alleviates the problem of small-sized training data. The kernel will then be used for a batch mode active learning method to identify the most informative and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1017/CBO9780511804441', '10.7551/mitpress/9780262033589.001.0001', '10.1007/11788034_13', '10.1145/1027527.1027664', '10.1145/1143844.1143897', '10.1145/1027527.1027533', '10.1145/1135777.1135870', '10.1109/CVPR.2005.44', '10.1145/1150402.1150426', '10.1109/TKDE.2006.53', '10.1016/S0031-3203(03)00043-8', '10.1145/1126004.1126005', '10.1109/TIP.2006.881938', '10.1109/34.531803', '10.1007/BF01588971', '10.1007/s11042-006-0043-1', '10.1109/76.718510', '10.1145/1102351.1102455', '10.1109/34.895972', '10.1109/TKDE.2007.1003', '10.5555/1018428.1020883', '10.1109/TMM.2005.861375', '10.1145/500141.500159', '10.1145/957013.957087', '10.1007/s00530-002-0070-3'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Multimodal AI', 'NLP', 'Multimedia', 'Computer Vision'], conference_acronym='ACM transactions on information systems', publisher=None, query_handler=None),\n",
       " 'Robust subspace segmentation by low-rank representation': Paper(DOI='10.1109/tcyb.2013.2286106', crossref_json=None, google_schorlar_metadata=None, title='Robust subspace segmentation by low-rank representation', authors=['Guangcan Liu', 'Zhouchen Lin', 'Yong Yu'], abstract='We propose low-rank representation (LRR) to segment data drawn from a union of multiple linear (or affine) subspaces. Given a set of data vectors, LRR seeks the lowestrank representation among all the candidates that represent all vectors as the linear combination of the bases in a dictionary. Unlike the well-known sparse representation (SR), which computes the sparsest representation of each data vector individually, LRR aims at finding the lowest-rank representation of a collection of vectors jointly. LRR better captures the global structure of data, giving a more effective tool for robust subspace segmentation from corrupted data. Both theoretical and experimental results show that LRR is a promising tool for subspace segmentation.', conference=None, journal=None, year=None, reference_list=['10.1109/TIP.2012.2235849', '10.1109/CVPR.2011.5995393', '10.1609/aaai.v24i1.7654', '10.1109/TPAMI.2010.220', '10.1162/NECO_a_00155', '10.1109/JPROC.2009.2035722', '10.1214/11-AOS914', '10.1109/34.868688', '10.1109/MSP.2010.939739', '10.1109/TPAMI.2005.244', '10.1023/A:1008000628999', '10.1109/CVPR.2004.1315101', '10.1111/1467-9868.00196', '10.1145/358669.358692', '10.1109/CVPR.2009.5206547', '10.1007/s11263-008-0178-9', '10.1002/cpa.20132', '10.1016/j.neucom.2009.12.032', '10.1109/TPAMI.2008.79', '10.1109/TIP.2010.2103949', '10.1109/TPAMI.2009.191', '10.1109/TPAMI.2007.1085', '10.1109/CVPR.2011.5995328', '10.1109/TSMCB.2011.2167322', '10.1145/1970392.1970395', '10.1023/A:1008026310903', '10.1109/TSMCB.2011.2168953', '10.1109/TSMCB.2012.2185490', '10.1109/CVPR.2011.5995365', '10.1137/080738970', '10.1109/ICCV.2011.6126422', '10.1007/BF01581204', '10.1109/TPAMI.2012.88'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'computer vision', 'image processing', 'numerical optimization'], conference_acronym='IEEE transactions on cybernetics (Print)', publisher=None, query_handler=None),\n",
       " 'Linearized alternating direction method with adaptive penalty for low-rank representation': Paper(DOI='10.1007/s11263-013-0611-6', crossref_json=None, google_schorlar_metadata=None, title='Linearized alternating direction method with adaptive penalty for low-rank representation', authors=['Zhouchen Lin', 'Risheng Liu', 'Zhixun Su'], abstract='Many machine learning and signal processing problems can be formulated as linearly constrained convex programs, which could be efficiently solved by the alternating direction method (ADM). However, usually the subproblems in ADM are easily solvable only when the linear mappings in the constraints are identities. To address this issue, we propose a linearized ADM (LADM) method by linearizing the quadratic penalty term and adding a proximal term when solving the subproblems. For fast convergence, we also allow the penalty to change adaptively according a novel update rule. We prove the global convergence of LADM with adaptive penalty (LADMAP). As an example, we apply LADMAP to solve low-rank representation (LRR), which is an important subspace clustering technique yet suffers from high computation cost. By combining LADMAP with a skinny SVD representation technique, we are able to reduce the complexity  of the original ADM based method to , where  and  are the rank and size of the representation matrix, respectively, hence making LRR possible for large scale applications. Numerical experiments verify that for LRR our LADMAP based methods are much faster than state-of-the-art algorithms.', conference=None, journal=None, year=None, reference_list=['10.1137/080738970', '10.1145/1970392.1970395', '10.1007/BF01582566', '10.21236/ADA567407', '10.1109/18.382009', '10.1002/cpa.20132', '10.1137/S0895479895290954', '10.1137/110822347', '10.1007/BF02612715', '10.1007/s10107-012-0584-1', '10.1137/090777761', '10.1109/ICCV.2011.6126388', '10.1109/CVPR.2011.5995548', '10.1109/ICDAR.2013.86', '10.1007/s11263-012-0515-x'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'computer vision', 'image processing', 'numerical optimization'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Recurrent squeeze-and-excitation context aggregation net for single image deraining': Paper(DOI='10.1007/978-3-030-01234-2_16', crossref_json=None, google_schorlar_metadata=None, title='Recurrent squeeze-and-excitation context aggregation net for single image deraining', authors=['Xia Li', 'Jianlong Wu', 'Zhouchen Lin', 'Hong Liu', 'Hongbin Zha'], abstract='Rain streaks can severely degrade the visibility, which causes many current computer vision algorithms fail to work. So it is necessary to remove the rain from images. We propose a novel deep network architecture based on deep convolutional and recurrent neural networks for single image deraining. As contextual information is very important for rain removal, we first adopt the dilated convolutional neural network to acquire large receptive field. To better fit the rain removal task, we also modify the network. In heavy rain, rain streaks have various directions and shapes, which can be regarded as the accumulation of multiple rain streak layers. We assign different alpha-values to various rain streak layers according to the intensity and transparency by incorporating the squeeze-and-excitation block. Since rain streak layers overlap with each other, it is not easy to remove the rain in one stage. So we further decompose the rain removal into multiple stages. Recurrent neural network is incorporated to preserve the useful information in previous stages and benefit the rain removal in later stages. We conduct extensive experiments on both synthetic and real-world datasets. Our proposed method outperforms the state-of-the-art approaches under all evaluation metrics. method outperforms the state-of-the-art approaches under all evaluation metrics.', conference=None, journal=None, year=None, reference_list=['10.1109/ICME.2006.262572', '10.1007/s11263-006-0028-6', '10.1007/s11263-014-0759-8', '10.1007/s11760-012-0373-6', '10.1109/ICCV.2015.388', '10.1109/ICCV.2017.191', '10.1109/CVPR.2016.299', '10.1109/CVPR.2017.186', '10.1109/CVPR.2017.183', '10.1109/ICME.2012.92', '10.1109/ICCV.2017.273', '10.1007/s11263-008-0200-2', '10.1109/ICCV.2005.253', '10.1007/s11263-011-0421-7', '10.1145/1553374.1553463', '10.1006/dspr.1999.0361', '10.1109/TPAMI.2012.88', '10.1109/TIP.2011.2179057', '10.1109/TIP.2017.2708502', '10.1109/ICCV.2017.189', '10.1109/ICCV.2017.276', '10.1109/TIP.2017.2691802', '10.1007/978-81-322-3691-7_2', '10.1109/CVPR.2018.00745', '10.1002/047084535X', '10.3115/v1/D14-1179', '10.1049/el:20080522', '10.1109/TIP.2003.819861'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'computer vision', 'image processing', 'numerical optimization'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Fast, automatic and fine-grained tampered JPEG image detection via DCT coefficient analysis': Paper(DOI='10.1016/j.patcog.2009.03.019', crossref_json=None, google_schorlar_metadata=None, title='Fast, automatic and fine-grained tampered JPEG image detection via DCT coefficient analysis', authors=['Zhouchen Lin', 'Junfeng He', 'Xiaoou Tang', 'Chi-Keung Tang'], abstract='The quick advance in image/video editing techniques has enabled people to synthesize realistic images/videos conveniently. Some legal issues may arise when a tampered image cannot be distinguished from a real one by visual examination. In this paper, we focus on JPEG images and propose detecting tampered images by examining the double quantization effect hidden among the discrete cosine transform (DCT) coefficients. To our knowledge, our approach is the only one to date that can automatically locate the tampered region, while it has several additional advantages: fine-grained detection at the scale of 8×8 DCT blocks, insensitivity to different kinds of forgery methods (such as alpha matting and inpainting, in addition to simple image cut/paste), the ability to work without fully decompressing the JPEG images, and the fast speed. Experimental results on JPEG images are promising.', conference=None, journal=None, year=None, reference_list=['10.1145/344779.344972', '10.1145/882262.882264', '10.1145/566570.566572', '10.1145/882262.882269', '10.1145/1015706.1015721', '10.1145/1073204.1073274', '10.1145/1015706.1015719', '10.1145/1073170.1073171', '10.1109/TSP.2005.855406', '10.1109/TSP.2004.839932', '10.1117/12.640109', '10.1109/83.951529', '10.1145/1161366.1161376', '10.1109/ICME.2006.262447', '10.1109/ICME.2007.4284578', '10.1109/ICME.2007.4284574'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'computer vision', 'image processing', 'numerical optimization'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'Relay backpropagation for effective learning of deep convolutional neural networks': Paper(DOI='10.1007/978-3-319-46478-7_29', crossref_json=None, google_schorlar_metadata=None, title='Relay backpropagation for effective learning of deep convolutional neural networks', authors=['Li Shen', 'Zhouchen Lin', 'Qingming Huang'], abstract=' Learning deeper convolutional neural networks has become a tendency in recent years. However, many empirical evidences suggest that performance improvement cannot be attained by simply stacking more layers. In this paper, we consider the issue from an information theoretical perspective, and propose a novel method Relay Backpropagation, which encourages the propagation of effective information through the network in training stage. By virtue of the method, we achieved the first place in ILSVRC 2015 Scene Classification Challenge. Extensive experiments on two large scale challenging datasets demonstrate the effectiveness of our method is not restricted to a specific dataset or network architecture.', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2014.81', '10.1109/ICCV.2015.123', '10.1007/978-3-319-10578-9_23', '10.1109/CVPR.2016.90', '10.1145/2647868.2654889', '10.1142/4224', '10.1109/CVPR.2014.223', '10.1007/978-3-642-35289-8_3', '10.1007/s11263-015-0816-y', '10.1109/CVPR.2015.7298594', '10.1109/CVPR.2016.308', '10.1109/CVPR.2014.220', '10.1109/ITW.2015.7133169', '10.1007/s11263-014-0748-y', '10.1007/978-3-319-10590-1_53'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia Analysis and Retrieval', 'Image and Video Processing', 'Pattern Recognition', 'Computer Vision', 'Video Coding'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Accelerated proximal gradient methods for nonconvex programming': Paper(DOI='10.1007/s10107-015-0871-8', crossref_json=None, google_schorlar_metadata=None, title='Accelerated proximal gradient methods for nonconvex programming', authors=['Huan Li', 'Zhouchen Lin'], abstract='Nonconvex and nonsmooth problems have recently received considerable attention in signal/image processing, statistics and machine learning. However, solving the nonconvex and nonsmooth optimization problems remains a big challenge. Accelerated proximal gradient (APG) is an excellent method for convex programming. However, it is still unknown whether the usual APG can ensure the convergence to a critical point in nonconvex programming. To address this issue, we introduce a monitor-corrector step and extend APG for general nonconvex and nonsmooth programs. Accordingly, we propose a monotone APG and a non-monotone APG. The latter waives the requirement on monotonic reduction of the objective function and needs less computation in each iteration. To the best of our knowledge, we are the first to provide APG-type algorithms for general nonconvex and nonsmooth problems ensuring that every accumulation point is a critical point, and the convergence rates remain  when the problems are convex, in which k is the number of iterations. Numerical results testify to the advantage of our algorithms in speed.', conference=None, journal=None, year=None, reference_list=['10.1109/WSC.1998.744910', '10.1137/080716542', '10.1137/090774100', '10.1007/s10107-012-0613-0', '10.1198/016214501753382273', '10.1287/ijoc.14.3.192.113', '10.1137/110848864', '10.1137/110848876', '10.1137/120880811', '10.1007/978-3-662-06409-2', '10.1007/s10107-010-0434-y', '10.1007/s10107-012-0588-x', '10.1145/1553374.1553463', '10.1137/070704277', '10.1007/978-1-4419-8853-9', '10.1007/s10107-004-0552-5', '10.1007/s10107-012-0629-5', '10.1137/0330046', '10.1214/aoms/1177729586', '10.1137/050623012', '10.1002/0471722138'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'computer vision', 'image processing', 'numerical optimization'], conference_acronym='Mathematical programming (Print)', publisher=None, query_handler=None),\n",
       " 'Laplacian regularized low-rank representation and its applications': Paper(DOI='10.1109/tpami.2015.2462360', crossref_json=None, google_schorlar_metadata=None, title='Laplacian regularized low-rank representation and its applications', authors=['Ming Yin', 'Junbin Gao', 'Zhouchen Lin'], abstract='Low-rank representation (LRR) has recently attracted a great deal of attention due to its pleasing efficacy in exploring low-dimensional subspace structures embedded in data. For a given set of observed data corrupted with sparse errors, LRR aims at learning a lowest-rank representation of all data jointly. LRR has broad applications in pattern recognition, computer vision and signal processing. In the real world, data often reside on low-dimensional manifolds embedded in a high-dimensional ambient space. However, the LRR method does not take into account the non-linear geometric structures within data, thus the locality and similarity information among data may be missing in the learning process. To improve LRR in this regard, we propose a general Laplacian regularized low-rank representation framework for data representation where a hypergraph Laplacian regularizer can be readily introduced into, i.e., a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1137/100781894', '10.1109/34.868688', '10.1109/ICCV.2011.6126422', '10.1109/TPAMI.2012.88', '10.1126/science.290.5500.2323', '10.1007/978-3-642-33460-3_32', '10.1109/TGRS.2012.2226730', '10.1162/089976603321780317', '10.1109/TSP.2012.2197748', '10.1109/TKDE.2010.259', '10.1109/TPAMI.2005.55', '10.1145/1553374.1553434', '10.1137/07069239X', '10.1109/TPAMI.2012.274', '10.1016/j.neucom.2013.06.013', '10.1109/TIP.2010.2090535', '10.1145/1970392.1970395', '10.1126/science.290.5500.2319', '10.1137/S003614450037906X', '10.1109/TIP.2009.2038764', '10.5948/UPO9781614440222', '10.1109/TIT.2006.871582', '10.1162/neco.2007.11-06-397', '10.1109/TPAMI.2012.63', '10.1109/CVPR.2006.100', '10.1109/DICTA.2012.6411718', '10.1109/TPAMI.2010.231', '10.1109/ICDM.2007.89', '10.1137/080738970', '10.1109/ICIP.2013.6738777', '10.1016/S0218-0014(00)00018-0', '10.1007/s10208-009-9045-5', '10.1109/JPROC.2010.2044470', '10.1049/iet-ipr.2012.0322', '10.1109/TPAMI.2007.250598', '10.1109/TPAMI.2008.216', '10.1007/s12532-010-0017-1'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'computer vision', 'image processing', 'numerical optimization'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Co-occurrence feature learning for skeleton based action recognition using regularized deep LSTM networks': Paper(DOI='10.1609/aaai.v30i1.10451', crossref_json=None, google_schorlar_metadata=None, title='Co-occurrence feature learning for skeleton based action recognition using regularized deep LSTM networks', authors=['Wentao Zhu', 'Cuiling Lan', 'Junliang Xing', 'Wenjun Zeng', 'Yanghao Li', 'Li Shen', 'Xiaohui Xie'], abstract='Skeleton based action recognition distinguishes human actions using the trajectories of skeleton joints, which provide a very good representation for describing actions. Considering that recurrent neural networks (RNNs) with Long Short-Term Memory (LSTM) can learn feature representations and model long-term temporal dependencies automatically, we propose an end-to-end fully connected deep LSTM network for skeleton based action recognition. Inspired by the observation that the co-occurrences of the joints intrinsically characterize human actions, we take the skeleton as the input at each time slot and introduce a novel regularization scheme to learn the co-occurrence features of skeleton joints. To train the deep LSTM network effectively, we propose a new dropout algorithm which simultaneously operates on the gates, cells, and output responses of the LSTM neurons. Experimental results on three human action recognition datasets consistently demonstrate the effectiveness of the proposed model.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks': Paper(DOI='10.1016/j.neunet.2021.10.021', crossref_json=None, google_schorlar_metadata=None, title='Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks', authors=['Jie Hu', 'Li Shen', 'Samuel Albanie', 'Gang Sun', 'Andrea Vedaldi'], abstract='While the use of bottom-up local operators in convolutional neural networks (CNNs) matches well some of the statistics of natural images, it may also prevent such models from capturing contextual long-range feature interactions. In this work, we propose a simple, lightweight approach for better context exploitation in CNNs. We do so by introducing a pair of operators: gather, which efficiently aggregates feature responses from a large spatial extent, and excite, which redistributes the pooled information to local features. The operators are cheap, both in terms of number of added parameters and computational complexity, and can be integrated directly in existing architectures to improve their performance. Experiments on several datasets show that gather-excite can bring benefits comparable to increasing the depth of a CNN at a fraction of the cost. For example, we find ResNet-50 with gather-excite operators is able to outperform its 101-layer counterpart on ImageNet with no additional learnable parameters. We also propose a parametric gather-excite operator pair which yields further performance gains, relate it to the recently-introduced Squeeze-and-Excitation Networks, and analyse the effects of these changes to the CNN feature activation statistics.', conference=None, journal=None, year=None, reference_list=['10.1007/s11042-020-10486-4', '10.1016/j.ins.2021.08.042', '10.1016/j.future.2021.03.003', '10.1080/15472450902858368', '10.1145/3385414', '10.1109/ACCESS.2020.3008848', '10.1007/s00521-019-04485-2', '10.1007/s11704-020-9194-x', '10.1016/j.eswa.2018.08.057', '10.1109/ACCESS.2019.2939902', '10.1109/CVPR.2016.90', '10.1007/s10955-014-1024-9', '10.1016/j.isprsjprs.2019.09.008', '10.1109/TGRS.2018.2890705', '10.1016/j.isprsjprs.2018.10.006', '10.1016/j.aej.2020.09.038', '10.1109/TITS.2013.2267735', '10.1016/j.future.2021.04.019', '10.1609/aaai.v33i01.33018561', '10.1007/s11704-011-1192-6', '10.1109/TITS.2013.2247040', '10.3390/s17040818', '10.1109/TITS.2013.2262376', '10.1016/j.aej.2020.01.015', '10.1109/TITS.2006.869623', '10.1061/(ASCE)0733-947X(2003)129:6(664)', '10.1162/neco.1989.1.2.270', '10.1007/s41019-020-00151-z', '10.1609/aaai.v34i01.5470', '10.1109/TMECH.2019.2952552', '10.1007/s11071-020-05615-5', '10.1016/j.artint.2018.03.002', '10.1109/TKDE.2019.2891537'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Recognition and Generation', 'Neural Networks', 'Representation Learning', 'Computer Vision'], conference_acronym='Neural networks (Print)', publisher=None, query_handler=None),\n",
       " 'Multi-level discriminative dictionary learning with application to large scale image classification': Paper(DOI='10.1109/tip.2015.2438548', crossref_json=None, google_schorlar_metadata=None, title='Multi-level discriminative dictionary learning with application to large scale image classification', authors=['Li Shen', 'Gang Sun', 'Qingming Huang', 'Shuhui Wang', 'Zhouchen Lin', 'Enhua Wu'], abstract='The sparse coding technique has shown flexibility and capability in image representation and analysis. It is a powerful tool in many visual applications. Some recent work has shown that incorporating the properties of task (such as discrimination for classification task) into dictionary learning is effective for improving the accuracy. However, the traditional supervised dictionary learning methods suffer from high computation complexity when dealing with large number of categories, making them less satisfactory in large scale applications. In this paper, we propose a novel multi-level discriminative dictionary learning method and apply it to large scale image classification. Our method takes advantage of hierarchical category correlation to encode multi-level discriminative information. Each internal node of the category hierarchy is associated with a discriminative dictionary and a classification model. The dictionaries at\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2010.5539964', '10.1109/CVPR.2008.4587652', '10.1109/CVPR.2013.56', '10.1109/TPAMI.2013.189', '10.1145/2001269.2001295', '10.1109/CVPR.2008.4587504', '10.1109/CVPR.2011.5995732', '10.1109/CVPR.2007.383198', '10.1109/CVPR.2011.5995477', '10.7551/mitpress/7287.001.0001', '10.1109/CVPR.2012.6248040', '10.1109/CVPR.2011.5995720', '10.1109/CVPR.2011.5995474', '10.1109/MSP.2010.939537', '10.1016/S0042-6989(97)00169-7', '10.1109/CVPR.2008.4587410', '10.1007/s11263-014-0722-8', '10.1109/CVPR.2010.5540027', '10.1109/TPAMI.2011.119', '10.1145/1273496.1273646', '10.1007/s11222-007-9033-z', '10.1007/s10994-009-5108-8', '10.1109/CVPR.2010.5539970', '10.1007/978-0-387-31256-9', '10.1109/ICCV.2009.5459169', '10.1109/ICCV.2009.5459183', '10.1109/TIP.2013.2262292', '10.1109/TSP.2012.2226445', '10.1109/ICCV.2011.6126286', '10.1109/CVPR.2006.68', '10.1109/TSP.2006.881199', '10.1109/CVPR.2010.5539963', '10.1109/CVPR.2010.5539958', '10.1109/TPAMI.2011.156', '10.1109/CVPR.2010.5540018', '10.1007/s11263-013-0636-x', '10.1109/TIP.2006.881969', '10.1109/CVPR.2013.114', '10.1109/TIP.2010.2050625', '10.1145/1015330.1015427', '10.1145/1015330.1015374', '10.1109/CVPR.2011.5995354'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Recognition and Generation', 'Neural Networks', 'Representation Learning', 'Computer Vision'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'AlphaGAN: Fully Differentiable Architecture Search for Generative Adversarial Networks': Paper(DOI='10.1109/tpami.2021.3099829', crossref_json=None, google_schorlar_metadata=None, title='AlphaGAN: Fully Differentiable Architecture Search for Generative Adversarial Networks', authors=['Yuesong Tian', 'Li Shen', 'Li Shen', 'Guinan Su', 'Zhifeng Li', 'Wei Liu'], abstract='Generative Adversarial Networks (GANs) are formulated as minimax game problems that generative networks attempt to approach real data distributions by adversarial learning against discriminators which learn to distinguish generated samples from real ones, of which the intrinsic problem complexity poses challenges to performance and robustness. In this work, we aim to boost model learning from the perspective of network architectures, by incorporating recent progress on automated architecture search into GANs. Specially we propose a fully differentiable search framework, dubbed  alphaGAN , where the searching process is formalized as solving a bi-level minimax optimization problem. The outer-level objective aims for seeking an optimal network architecture towards pure Nash Equilibrium conditioned on the network parameters of generators and discriminators optimized with a traditional adversarial loss within\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2018.00907', '10.1073/pnas.36.1.48', '10.1109/CVPR.2016.90', '10.1109/ICCV.2019.00461', '10.1109/CVPR42600.2020.00782', '10.1109/ICCV.2015.425', '10.1109/ICCV.2019.00332', '10.1007/978-3-030-58574-7_6', '10.1007/978-3-030-58571-6_11', '10.1109/CVPR42600.2020.00821', '10.18653/v1/D17-1230', '10.1109/CVPR.2018.00577', '10.1109/ICCV.2017.304', '10.1109/CVPR42600.2020.00813', '10.1109/CVPR.2019.00453', '10.1109/ICCV.2017.244', '10.1109/CVPR.2017.632', '10.1109/CVPR.2018.00917', '10.1609/aaai.v33i01.33014780', '10.1109/CVPR.2019.00720', '10.1109/ICCV.2017.167'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Recognition and Generation', 'Neural Networks', 'Representation Learning', 'Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Learning Meta Representations for Agents in Multi-Agent Reinforcement Learning': Paper(DOI='10.1007/s10458-013-9235-z', crossref_json=None, google_schorlar_metadata=None, title='Learning Meta Representations for Agents in Multi-Agent Reinforcement Learning', authors=['Shenao Zhang', 'Li Shen', 'Lei Han', 'Li Shen'], abstract='In multi-agent reinforcement learning, the behaviors that agents learn in a single Markov Game (MG) are typically confined to the given agent number (i.e., population size). Every single MG induced by varying population sizes may possess distinct optimal joint strategies and game-specific knowledge, which are modeled independently in modern multi-agent algorithms. In this work, we focus on creating agents that generalize across population-varying MGs. Instead of learning a unimodal policy, each agent learns a policy set that is formed by effective strategies across a variety of games. We propose Meta Representations for Agents (MRA) that explicitly models the game-common and game-specific strategic knowledge. By representing the policy sets with multi-modal latent policies, the common strategic knowledge and diverse strategic modes are discovered with an iterative optimization procedure. We prove that as an approximation to a constrained mutual information maximization objective, the learned policies can reach Nash Equilibrium in every evaluation MG under the assumption of Lipschitz game on a sufficiently large latent space. When deploying it at practical latent models with limited size, fast adaptation can be achieved by leveraging the first-order gradient information. Extensive experiments show the effectiveness of MRA on both training performance and generalization ability in hard and unseen games.', conference=None, journal=None, year=None, reference_list=['10.1016/0025-5564(71)90051-4', '10.1007/s10994-007-5040-8', '10.1613/jair.731', '10.1016/S0004-3702(00)00033-3', '10.1023/A:1007379606734', '10.1145/1553374.1553406', '10.1016/0004-3702(94)90047-7', '10.1177/1059712308092835', '10.1177/1059712310397633', '10.1109/DEVLRN.2008.4640832', '10.1023/A:1017944732463', '10.1177/1059712310391484', '10.1109/ICMLA.2009.33', '10.1016/j.neunet.2010.01.001', '10.1109/ISIC.1992.225046', '10.1007/978-3-642-15880-3_36', '10.1145/1553374.1553442', '10.1145/1143844.1143906', '10.1109/ROBOT.1991.131810', '10.1145/1390156.1390225', '10.1613/jair.3384', '10.1007/978-3-642-17537-4_51', '10.1137/0111030', '10.1145/1273496.1273572', '10.1016/B978-1-55860-335-6.50030-1', '10.1007/s10994-008-5061-y', '10.1145/1390156.1390251', '10.1016/S0921-8890(97)00041-9', '10.1007/978-3-642-11688-9_3', '10.1016/B978-1-55860-335-6.50042-8', '10.1145/1830483.1830671', '10.1109/CIRA.2003.1222152', '10.1145/1329125.1329170', '10.1007/11564096_40', '10.1613/jair.1190', '10.1145/1273496.1273624'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Recognition and Generation', 'Neural Networks', 'Representation Learning', 'Computer Vision'], conference_acronym='Autonomous agents and multi-agent systems', publisher=None, query_handler=None),\n",
       " 'Adaptive Sharing for Image Classification': Paper(DOI='10.1016/j.compchemeng.2008.10.017', crossref_json=None, google_schorlar_metadata=None, title='Adaptive Sharing for Image Classification', authors=['Li Shen', 'Gang Sun', 'Zhouchen Lin', 'Qingming Huang', 'Enhua Wu'], abstract='In this paper, we formulate the image classification problem in a multi-task learning framework. We propose a novel method to adaptively share information among tasks (classes). Different from imposing strong assumptions or discovering specific structures, the key insight in our method is to selectively extract and exploit the shared information among classes while capturing respective disparities simultaneously. It is achieved by estimating a composite of two sets of parameters with different regularization. Besides applying it for learning classifiers on pre-computed features, we also integrate the adaptive sharing with deep neural networks, whose discriminative power can be augmented by encoding class relationship. We further develop two strategies for solving the optimization problems in the two scenarios. Empirical results demonstrate that our method can significantly improve the classification performance by transferring knowledge appropriately.', conference=None, journal=None, year=None, reference_list=['10.3233/AIC-1994-7104', '10.2307/2685469', '10.1002/(SICI)1098-1098(1997)8:2<214::AID-IMA8>3.0.CO;2-D', '10.1016/S0167-8655(99)00091-4', '10.1109/72.159059', '10.1016/S0003-2670(03)00338-6', '10.1016/S0952-1976(99)00041-X', '10.1006/jcss.1997.1504', '10.1109/72.501732', '10.1007/BFb0020609', '10.1016/S0933-3657(96)00361-2', '10.1007/BFb0056325', '10.1097/00004424-199404000-00020', '10.1007/3-540-44593-5_28', '10.1016/S0165-0114(02)00060-X', '10.1007/3-540-48508-2_38', '10.1007/3-540-44527-7_41', '10.1007/3-540-44593-5_3', '10.1016/S0952-1976(02)00020-9', '10.1016/j.compchemeng.2005.06.008', '10.1016/j.compchemeng.2008.10.013'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Recognition and Generation', 'Neural Networks', 'Representation Learning', 'Computer Vision'], conference_acronym='Computers & chemical engineering', publisher=None, query_handler=None),\n",
       " 'High-Resolution Volumetric Reconstruction for Clothed Humans': Paper(DOI='10.1145/3606032', crossref_json=None, google_schorlar_metadata=None, title='High-Resolution Volumetric Reconstruction for Clothed Humans', authors=['Sicong Tang', 'Guangyuan Wang', 'Qing Ran', 'Lingzhi Li', 'Li Shen', 'Ping Tan'], abstract='We present a novel method for reconstructing clothed humans from a sparse set of, e.g., 1–6 RGB images. Despite impressive results from recent works employing deep implicit representation, we revisit the volumetric approach and demonstrate that better performance can be achieved with proper system design. The volumetric representation offers significant advantages in leveraging 3D spatial context through 3D convolutions, and the notorious quantization error is largely negligible with a reasonably large yet affordable volume resolution, e.g., 512. To handle memory and computation costs, we propose a sophisticated coarse-to-fine strategy with voxel culling and subspace sparse convolution. Our method starts with a discretized visual hull to compute a coarse shape and then focuses on a narrow band nearby the coarse shape for refinement. Once the shape is reconstructed, we adopt an image-based\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2019.00238', '10.1109/CVPR52688.2022.00156', '10.1145/1073204.1073207', '10.1109/ICCV.2019.00552', '10.1007/978-3-319-46454-1_34', '10.1109/CVPR.2019.00609', '10.1109/CVPR42600.2020.00700', '10.1145/2766945', '10.1145/2897824.2925969', '10.1109/CVPR.2017.602', '10.1007/978-3-030-01252-6_35', '10.1109/CVPR.2018.00961', '10.1145/3355089.3356571', '10.1109/CVPR.2019.01114', '10.1109/ICCV48922.2021.01086', '10.1109/CVPR46437.2021.00060', '10.1109/3DV.2017.00055', '10.1007/978-3-030-01270-0_21', '10.1109/CVPR42600.2020.00316', '10.1109/CVPR42600.2020.00604', '10.1109/TPAMI.2017.2782743', '10.1109/CVPR.2018.00868', '10.1109/CVPR.2018.00744', '10.1109/CVPR42600.2020.00530', '10.1109/ICCV.2019.00234', '10.1007/978-3-030-58592-1_4', '10.1109/ICCV.2019.00445', '10.1145/2816795.2818013', '10.1145/37402.37422', '10.1109/CVPR.2019.00459', '10.1007/978-3-030-58452-8_24', '10.1109/CVPR.2015.7298631', '10.1109/ISMAR.2011.6092378', '10.1007/978-3-319-46484-8_29', '10.1109/3DV.2018.00062', '10.1109/CVPR.2019.00025', '10.1109/CVPR.2019.01123', '10.1109/CVPR.2018.00055', '10.1007/978-3-030-58580-8_31', '10.1109/CVPR46437.2021.00894', '10.1109/ICCV.2019.00239', '10.1109/CVPR42600.2020.00016', '10.1109/CVPR52688.2022.01541', '10.1007/978-3-030-01234-2_2', '10.5555/3295222.3295349', '10.1145/1618452.1618520', '10.1007/978-3-319-10602-1_54', '10.1109/CVPR52688.2022.01294', '10.1109/ICCV.2019.00785', '10.1109/CVPR46437.2021.00455', '10.1109/ICCV.2017.104', '10.1109/TPAMI.2019.2928296', '10.1109/CVPR46437.2021.00569', '10.1109/ICCV48922.2021.00618', '10.1109/TPAMI.2021.3050505', '10.1109/ICCV.2019.00783'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Recognition and Generation', 'Neural Networks', 'Representation Learning', 'Computer Vision'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " 'Barren plateaus in quantum neural network training landscapes': Paper(DOI='10.1038/s41467-018-07090-4', crossref_json=None, google_schorlar_metadata=None, title='Barren plateaus in quantum neural network training landscapes', authors=['Jarrod R McClean', 'Sergio Boixo', 'Vadim N Smelyanskiy', 'Ryan Babbush', 'Hartmut Neven'], abstract='Many experimental proposals for noisy intermediate scale quantum devices involve training a parameterized quantum circuit with a classical optimization loop. Such hybrid quantum-classical algorithms are popular for applications in quantum simulation, optimization, and machine learning. Due to its simplicity and hardware efficiency, random circuits are often proposed as initial guesses for exploring the space of quantum states. We show that the exponential dimension of Hilbert space and the gradient estimation complexity make this choice unsuitable for hybrid quantum-classical algorithms run on more than a few qubits. Specifically, we show that for a wide class of reasonable parameterized quantum circuits, the probability that the gradient along any reasonable direction is non-zero to some fixed precision is exponentially small as a function of the number of qubits. We argue that this is related to the 2-design\\xa0…', conference=None, journal=None, year=None, reference_list=['10.22331/q-2018-08-06-79', '10.1038/ncomms5213', '10.1088/1367-2630/18/2/023023', '10.1103/PhysRevA.95.042308', '10.1103/PhysRevA.92.042303', '10.1103/PhysRevA.95.020501', '10.1038/nature23879', '10.1126/sciadv.aap9646', '10.1103/PhysRevLett.120.210501', '10.1103/PhysRevA.94.022309', '10.1103/PhysRevA.97.022304', '10.1088/2058-9565/aab822', '10.1088/2058-9565/aa8072', '10.1038/nature23474', '10.1038/nature14539', '10.1021/jz501649m', '10.1088/2058-9565/aad3e4', '10.1088/1367-2630/aab919', '10.1103/PhysRevLett.120.110501', '10.1038/s41567-018-0124-x', '10.1103/PhysRevA.75.012328', '10.1038/nphys444', '10.1103/PhysRevLett.102.190502', '10.1103/PhysRevLett.102.190501', '10.1090/surv/089', '10.1063/1.1737053', '10.1103/PhysRevA.80.012304', '10.1007/s00220-009-0873-6', '10.1007/JHEP04(2017)121', '10.1162/neco.2006.18.7.1527', '10.1109/CVPR.2016.90', '10.7551/mitpress/7503.003.0024'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Quantum Computing', 'Machine Learning', 'Computer Vision', 'Neuroscience', 'Robotics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Characterizing quantum supremacy in near-term devices': Paper(DOI='10.1038/s41567-018-0124-x', crossref_json=None, google_schorlar_metadata=None, title='Characterizing quantum supremacy in near-term devices', authors=['Sergio Boixo', 'Sergei V Isakov', 'Vadim N Smelyanskiy', 'Ryan Babbush', 'Nan Ding', 'Zhang Jiang', 'Michael J Bremner', 'John M Martinis', 'Hartmut Neven'], abstract='A critical question for quantum computing in the near future is whether quantum devices without error correction can perform a well-defined computational task beyond the capabilities of supercomputers. Such a demonstration of what is referred to as quantum supremacy requires a reliable evaluation of the resources required to solve tasks with classical approaches. Here, we propose the task of sampling from the output distribution of random quantum circuits as a demonstration of quantum supremacy. We extend previous results in computational complexity to argue that this sampling task must take exponential time in a classical computer. We introduce cross-entropy benchmarking to obtain the experimental fidelity of complex multiqubit dynamics. This can be estimated and extrapolated to give a success metric for a quantum supremacy demonstration. We study the computational cost of relevant classical\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1126/science.1090790', '10.1088/0305-4470/39/43/002', '10.1103/PhysRevLett.98.130502', '10.1103/PhysRevA.78.062329', '10.1103/PhysRevE.78.046211', '10.1007/s00220-009-0873-6', '10.1103/PhysRevA.78.052332', '10.1103/PhysRevLett.111.127205', '10.1007/JHEP02(2016)004', '10.1098/rspa.2005.1546', '10.1098/rspa.2010.0301', '10.1145/1993636.1993682', '10.1088/1367-2630/aa5fdb', '10.1007/s00037-017-0162-2', '10.1103/PhysRevLett.117.080501', '10.1038/nature13171', '10.1038/nature14270', '10.1103/PhysRevA.30.1610', '10.1103/PhysRevLett.71.525', '10.1016/j.physrep.2006.09.003', '10.1103/RevModPhys.69.731', '10.1103/PhysRev.104.483', '10.1007/978-1-4899-3698-1_38', '10.22331/q-2017-04-25-8', '10.1137/050644756', '10.1103/PhysRevLett.102.190502', '10.1103/PhysRevLett.116.250501', '10.1088/1464-4266/7/10/021', '10.1103/PhysRevA.77.012307', '10.1103/PhysRevLett.106.180504', '10.1103/PhysRevA.86.032324', '10.1038/ncomms8654', '10.1103/PhysRevLett.100.100503', '10.1103/PhysRevLett.106.230501'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Quantum Computing', 'Machine Learning', 'Computer Vision', 'Neuroscience', 'Robotics'], conference_acronym='Nature physics (Print)', publisher=None, query_handler=None),\n",
       " 'Scalable quantum simulation of molecular energies': Paper(DOI='10.1080/08927020802635145', crossref_json=None, google_schorlar_metadata=None, title='Scalable quantum simulation of molecular energies', authors=['Peter JJ O’Malley', 'Ryan Babbush', 'Ian D Kivlichan', 'Jonathan Romero', 'Jarrod R McClean', 'Rami Barends', 'Julian Kelly', 'Pedram Roushan', 'Andrew Tranter', 'Nan Ding', 'Brooks Campbell', 'Yu Chen', 'Zijun Chen', 'Ben Chiaro', 'Andrew Dunsworth', 'Austin G Fowler', 'Evan Jeffrey', 'Erik Lucero', 'Anthony Megrant', 'Josh Y Mutus', 'Matthew Neeley', 'Charles Neill', 'Chris Quintana', 'Daniel Sank', 'Amit Vainsencher', 'Jim Wenner', 'Ted C White', 'Peter V Coveney', 'Peter J Love', 'Hartmut Neven', 'Alain Aspuru-Guzik', 'John M Martinis'], abstract='We report the first electronic structure calculation performed on a quantum computer without exponentially costly precompilation. We use a programmable array of superconducting qubits to compute the energy surface of molecular hydrogen using two distinct quantum algorithms. First, we experimentally execute the unitary coupled cluster method using the variational quantum eigensolver. Our efficient implementation predicts the correct dissociation energy to within chemical accuracy of the numerically exact result. Second, we experimentally demonstrate the canonical quantum algorithm for chemistry, which consists of Trotterization and quantum phase estimation. We compare the experimental performance of these approaches to show clear evidence that the variational quantum eigensolver is robust to certain errors. This error tolerance inspires hope that variational quantum simulations of classically intractable\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1088/0953-8984/18/35/L01', '10.1088/0953-8984/8/36/005', '10.1103/RevModPhys.73.33', '10.1103/PhysRevB.59.7413', '10.1103/PhysRev.136.B864', '10.1103/PhysRev.140.A1133', '10.1103/PhysRevB.54.11169', '10.1103/PhysRevLett.100.126404', '10.1039/b613676a', '10.1103/PhysRevB.46.6671', '10.1103/PhysRevLett.77.3865', '10.1063/1.1888569', '10.1063/1.1829049'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Quantum Computing', 'Machine Learning', 'Computer Vision', 'Neuroscience', 'Robotics'], conference_acronym='Molecular simulation (Print)', publisher=None, query_handler=None),\n",
       " 'What is the computational value of finite-range tunneling?': Paper(DOI='10.1103/physrevx.6.031015', crossref_json=None, google_schorlar_metadata=None, title='What is the computational value of finite-range tunneling?', authors=['Vasil S Denchev', 'Sergio Boixo', 'Sergei V Isakov', 'Nan Ding', 'Ryan Babbush', 'Vadim Smelyanskiy', 'John Martinis', 'Hartmut Neven'], abstract='Quantum annealing (QA) has been proposed as a quantum enhanced optimization heuristic exploiting tunneling. Here, we demonstrate how finite-range tunneling can provide considerable computational advantage. For a crafted problem designed to have tall and narrow energy barriers separating local minima, the D-Wave 2X quantum annealer achieves significant runtime advantages relative to simulated annealing (SA). For instances with 945 variables, this results in a time-to-99%-success-probability that is∼ 1 0 8 times faster than SA running on a single processor core. We also compare physical QA with the quantum Monte Carlo algorithm, an algorithm that emulates quantum tunneling on classical processors. We observe a substantial constant overhead against physical QA: D-Wave 2X again runs up to∼ 1 0 8 times faster than an optimized implementation of the quantum Monte Carlo algorithm on a single\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1126/science.220.4598.671', '10.1103/PhysRevE.58.5355', '10.1126/science.284.5415.779', '10.1021/jp991868i', '10.1126/science.1057726', '10.1126/science.1068774', '10.1007/11526216', '10.1103/RevModPhys.80.1061', '10.1103/PhysRevB.82.024511', '10.1038/nature10012', '10.1016/j.physrep.2012.10.002', '10.1038/ncomms3067', '10.1038/ncomms2920', '10.1038/nphys2900', '10.1103/PhysRevX.4.021041', '10.1088/1367-2630/16/4/045006', '10.1126/science.1252319', '10.1038/srep05703', '10.1103/PhysRevA.91.042314', '10.2200/S00585ED1V01Y201407QMC008', '10.1103/PhysRevX.5.031040', '10.1140/epjst/e2015-02346-0', '10.1103/PhysRevA.90.042317', '10.1103/PhysRevA.92.042310', '10.1103/PhysRevA.92.042325', '10.1103/PhysRevLett.115.230501', '10.1103/PhysRevA.92.062328', '10.1103/PhysRevX.5.031026', '10.1038/srep22318', '10.1038/ncomms10327', '10.1103/PhysRevX.6.021028', '10.1073/pnas.1002116107', '10.1103/PhysRevA.86.052334', '10.1103/PhysRevB.81.134510', '10.1016/j.cpc.2015.02.015', '10.1103/PhysRevX.4.021008', '10.1007/s100510050761', '10.1103/PhysRevB.91.024201', '10.1103/PhysRevLett.115.077201', '10.1103/PhysRevD.15.2929', '10.1070/PU1982v025n04ABEH004533', '10.1017/CBO9780511524219', '10.1016/0003-4916(69)90153-5', '10.1063/1.532563', '10.1016/0003-4916(83)90202-6', '10.1103/PhysRevLett.100.197001', '10.1103/PhysRevLett.101.117003', '10.1103/PhysRevLett.81.4281', '10.1016/S0304-3975(01)00153-0', '10.1103/PhysRevLett.84.1347', '10.1287/moor.21.1.85', '10.1103/PhysRevE.67.056701', '10.1080/00401706.1990.10484616', '10.1126/sciadv.1500838', '10.1142/S0219749903000383', '10.1080/00107151031000110776', '10.1103/PhysRevB.93.024202'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Quantum Computing', 'Machine Learning', 'Computer Vision', 'Neuroscience', 'Robotics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Quantum approximate optimization of non-planar graph problems on a planar superconducting processor': Paper(DOI='10.22331/q-2021-08-30-532', crossref_json=None, google_schorlar_metadata=None, title='Quantum approximate optimization of non-planar graph problems on a planar superconducting processor', authors=['Matthew P Harrigan', 'Kevin J Sung', 'Matthew Neeley', 'Kevin J Satzinger', 'Frank Arute', 'Kunal Arya', 'Juan Atalaya', 'Joseph C Bardin', 'Rami Barends', 'Sergio Boixo', 'Michael Broughton', 'Bob B Buckley', 'David A Buell', 'Brian Burkett', 'Nicholas Bushnell', 'Yu Chen', 'Zijun Chen', 'Ben Chiaro', 'Roberto Collins', 'William Courtney', 'Sean Demura', 'Andrew Dunsworth', 'Daniel Eppens', 'Austin Fowler', 'Brooks Foxen', 'Craig Gidney', 'Marissa Giustina', 'Rob Graff', 'Steve Habegger', 'Alan Ho', 'Sabrina Hong', 'Trent Huang', 'LB Ioffe', 'Sergei V Isakov', 'Evan Jeffrey', 'Zhang Jiang', 'Cody Jones', 'Dvir Kafri', 'Kostyantyn Kechedzhi', 'Julian Kelly', 'Seon Kim', 'Paul V Klimov', 'Alexander N Korotkov', 'Fedor Kostritsa', 'David Landhuis', 'Pavel Laptev', 'Mike Lindmark', 'Martin Leib', 'Orion Martin', 'John M Martinis', 'Jarrod R McClean', 'Matt McEwen', 'Anthony Megrant', 'Xiao Mi', 'Masoud Mohseni', 'Wojciech Mruczkiewicz', 'Josh Mutus', 'Ofer Naaman', 'Charles Neill', 'Florian Neukart', 'Murphy Yuezhen Niu', 'Thomas E O’Brien', 'Bryan O’Gorman', 'Eric Ostby', 'Andre Petukhov', 'Harald Putterman', 'Chris Quintana', 'Pedram Roushan', 'Nicholas C Rubin', 'Daniel Sank', 'Andrea Skolik', 'Vadim Smelyanskiy', 'Doug Strain', 'Michael Streif', 'Marco Szalay', 'Amit Vainsencher', 'Theodore White', 'Z Jamie Yao', 'Ping Yeh', 'Adam Zalcman', 'Leo Zhou', 'Hartmut Neven', 'Dave Bacon', 'Erik Lucero', 'Edward Farhi', 'Ryan Babbush'], abstract='Faster algorithms for combinatorial optimization could prove transformative for diverse areas such as logistics, finance and machine learning. Accordingly, the possibility of quantum enhanced optimization has driven much interest in quantum technologies. Here we demonstrate the application of the Google Sycamore superconducting qubit quantum processor to combinatorial optimization problems with the quantum approximate optimization algorithm (QAOA). Like past QAOA experiments, we study performance for problems defined on the planar connectivity graph native to our hardware; however, we also apply the QAOA to the Sherrington–Kirkpatrick model and MaxCut, non-native problems that require extensive compilation to implement. For hardware-native problems, which are classically efficient to solve on average, we obtain an approximation ratio that is independent of problem size and observe that\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1038/s41567-020-01105-y', '10.1038/s41586-019-1666-5', '10.1126/science.1113479', '10.1103/PhysRevX.6.031007', '10.1038/nature23474', '10.1038/s41467-018-06598-z', '10.1126/science.273.5278.1073', '10.1140/epjqt9', '10.1103/PhysRevE.58.5355', '10.1073/pnas.2006373117', '10.1088/1367-2630/18/2/023023', '10.1103/PhysRevA.103.L030401', '10.1007/s11128-020-02748-9', '10.1007/s11128-019-2171-3', '10.1103/PhysRevA.97.022304', '10.1103/PhysRevA.95.062317', '10.1103/PhysRevA.98.062333', '10.1103/PhysRevLett.124.090504', '10.1007/s11128-020-02692-8', '10.1038/s41928-020-00498-1', '10.1103/PhysRevApplied.14.034010', '10.1038/s41566-018-0236-y', '10.1515/eng-2019-0059'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Quantum Computing', 'Machine Learning', 'Computer Vision', 'Neuroscience', 'Robotics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Image base inquiry system for search engines for mobile telephones with integrated camera': Paper(DOI='10.1117/12.2034348', crossref_json=None, google_schorlar_metadata=None, title='Image base inquiry system for search engines for mobile telephones with integrated camera', authors=['Hartmut Neven Sr'], abstract='An increasing number of mobile telephones and computers are being equipped with a camera. Thus, instead of simple text strings, it is also possible to send images as queries to search engines or databases. Moreover, advances in image recognition allow a greater degree of automated recognition of objects, strings of letters, or symbols in digital images. This makes it possible to convert the graphical information into a symbolic format, for example, plain text, in order to then access information about the object shown.', conference=None, journal=None, year=None, reference_list=['10.1109/CISP.2010.5648266', '10.1145/5666.5673', '10.1145/63039.63043'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Quantum Computing', 'Machine Learning', 'Computer Vision', 'Neuroscience', 'Robotics'], conference_acronym='Proceedings of SPIE, the International Society for Optical Engineering', publisher=None, query_handler=None),\n",
       " 'Mobile image-based information retrieval system': Paper(DOI='10.17950/ijer/v3s6/607', crossref_json=None, google_schorlar_metadata=None, title='Mobile image-based information retrieval system', authors=['Hartmut Neven', 'Hartmut Neven Sr'], abstract='An image-based information retrieval system, including a mobile telephone, a remote recognition server, and a remote media server, the mobile telephone having a built-in camera and a communication link for transmitting an image from the built-in camera to the remote recognition server and for receiving mobile media content from the remote media server, the remote recognition server for matching an image from the mobile telephone with an object representation in a database and forwarding an associated text identifier to the remote server, and the remote media server for forwarding mobile media content to the mobile telephone based on the associated text identifier.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Quantum Computing', 'Machine Learning', 'Computer Vision', 'Neuroscience', 'Robotics'], conference_acronym='International journal of engineering research', publisher=None, query_handler=None),\n",
       " 'Spectroscopic signatures of localization with interacting photons in superconducting qubits': Paper(DOI='10.1126/science.aao1401', crossref_json=None, google_schorlar_metadata=None, title='Spectroscopic signatures of localization with interacting photons in superconducting qubits', authors=['Pedram Roushan', 'Charles Neill', 'J Tangpanitanon', 'Victor M Bastidas', 'A Megrant', 'Rami Barends', 'Yu Chen', 'Z Chen', 'B Chiaro', 'A Dunsworth', 'A Fowler', 'B Foxen', 'Marissa Giustina', 'E Jeffrey', 'J Kelly', 'Erik Lucero', 'J Mutus', 'Matthew Neeley', 'Chris Quintana', 'D Sank', 'Amit Vainsencher', 'James Wenner', 'T White', 'H Neven', 'DG Angelakis', 'J Martinis'], abstract='Quantized eigenenergies and their associated wave functions provide extensive information for predicting the physics of quantum many-body systems. Using a chain of nine superconducting qubits, we implement a technique for resolving the energy levels of interacting photons. We benchmark this method by capturing the main features of the intricate energy spectrum predicted for two-dimensional electrons in a magnetic field—the Hofstadter butterfly. We introduce disorder to study the statistics of the energy levels of the system as it undergoes the transition from a thermalized to a localized phase. Our work introduces a many-body spectroscopy technique to study quantum phases of matter.', conference=None, journal=None, year=None, reference_list=['10.1016/j.aop.2005.11.014', '10.1146/annurev-conmatphys-031214-014726', '10.1146/annurev-conmatphys-031214-014701', '10.1103/PhysRevLett.113.243002', '10.1126/science.aaa7432', '10.1038/nphys3783', '10.1126/science.aaf6725', '10.1126/science.aaf8834', '10.1103/PhysRevB.82.174411', '10.1103/PhysRevB.89.144201', '10.1103/PhysRevLett.113.147204', '10.1103/PhysRevLett.113.107204', '10.1103/PhysRevX.5.041047', '10.1038/ncomms8341', '10.1103/PhysRevLett.117.027201', '10.1103/PhysRevB.94.214206', '10.1103/PhysRevLett.115.100501', '10.1126/science.1251422', '10.1007/978-3-319-52025-4', '10.1088/0034-4885/80/1/016401', '10.1103/PhysRevB.14.2239', '10.1088/1367-2630/5/1/356', '10.1038/nature12187', '10.1038/nature12186', '10.1126/science.1237240', '10.1103/PhysRevLett.111.185302', '10.1103/PhysRevLett.111.185301', '10.1103/PhysRevB.75.155111', '10.1103/PhysRevLett.110.084101', '10.1103/PhysRevLett.52.1', '10.1142/9789814299084', '10.1103/PhysRevB.87.134202', '10.1103/PhysRevB.93.014203', '10.1103/PhysRevLett.114.117401', '10.1103/PhysRevB.90.064203', '10.1103/PhysRevLett.42.673', '10.1103/PhysRevLett.113.220502', '10.1209/0295-5075/98/66002', '10.1103/PhysRevA.92.012320', '10.1103/PhysRevB.87.134202', '10.1103/PhysRevLett.109.106402', '10.1103/PhysRevLett.115.186601', '10.1103/PhysRevLett.113.045304', '10.1038/nphys3830'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Quantum Computing', 'Machine Learning', 'Computer Vision', 'Neuroscience', 'Robotics'], conference_acronym='Science (New York, N.Y.)', publisher=None, query_handler=None),\n",
       " 'Chiral ground-state currents of interacting photons in a synthetic magnetic field': Paper(DOI='10.1038/nphys3930', crossref_json=None, google_schorlar_metadata=None, title='Chiral ground-state currents of interacting photons in a synthetic magnetic field', authors=['Pedram Roushan', 'Charles Neill', 'Anthony Megrant', 'Yu Chen', 'Ryan Babbush', 'Rami Barends', 'Brooks Campbell', 'Zijun Chen', 'Ben Chiaro', 'Andrew Dunsworth', 'Austin Fowler', 'Evan Jeffrey', 'Julian Kelly', 'Erik Lucero', 'Josh Mutus', 'PJJ O’Malley', 'Matthew Neeley', 'Chris Quintana', 'Daniel Sank', 'Amit Vainsencher', 'Jim Wenner', 'Ted White', 'Eliot Kapit', 'Hartmut Neven', 'John Martinis'], abstract='The intriguing many-body phases of quantum matter arise from the interplay of particle interactions, spatial symmetries, and external fields. Generating these phases in an engineered system could provide deeper insight into their nature. Using superconducting qubits, we simultaneously realize synthetic magnetic fields and strong particle interactions, which are among the essential elements for studying quantum magnetism and fractional quantum Hall phenomena. The artificial magnetic fields are synthesized by sinusoidally modulating the qubit couplings. In a closed loop formed by the three qubits, we observe the directional circulation of photons, a signature of broken time-reversal symmetry. We demonstrate strong interactions through the creation of photon vacancies, or ‘holes’, which circulate in the opposite direction. The combination of these key elements results in chiral ground-state currents. Our work\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1126/science.177.4047.393', '10.1038/nphys2259', '10.1038/nature08609', '10.1103/PhysRevLett.111.185302', '10.1103/PhysRevLett.111.185301', '10.1038/nature13915', '10.1038/nphoton.2013.274', '10.1038/nature12066', '10.1038/nphoton.2014.248', '10.1038/nphoton.2014.177', '10.1103/PhysRevA.93.042302', '10.1103/PhysRevLett.48.1559', '10.1103/PhysRevLett.50.1395', '10.1103/RevModPhys.86.153', '10.1038/nphys2275', '10.1038/nphys2253', '10.1038/nphys2251', '10.1126/science.1177838', '10.1016/S0038-1098(03)00314-4', '10.1103/PhysRevLett.101.246809', '10.1103/PhysRevLett.108.223602', '10.1103/PhysRevA.86.053804', '10.1103/PhysRevB.90.060503', '10.1103/PhysRevB.92.174305', '10.1103/RevModPhys.80.1083', '10.1142/S021797921330017X', '10.1103/PhysRevLett.105.215303', '10.1088/1367-2630/5/1/356', '10.1103/PhysRevLett.109.145301', '10.1103/PhysRevA.82.043811', '10.1088/1367-2630/13/9/095008', '10.1038/nphoton.2012.236', '10.1103/PhysRevA.92.012302', '10.1103/PhysRevLett.113.220502', '10.1103/PhysRevB.87.060301', '10.1038/nphys3134', '10.1103/PhysRevApplied.4.034002', '10.1103/PhysRevA.78.033834', '10.1038/nature08293', '10.1038/nmat3520', '10.1103/PhysRevA.89.023619', '10.1103/PhysRevA.89.061601', '10.1038/nphys2998', '10.1103/PhysRevLett.110.163605', '10.1103/PhysRevA.81.061801'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Quantum Computing', 'Machine Learning', 'Computer Vision', 'Neuroscience', 'Robotics'], conference_acronym='Nature physics (Print)', publisher=None, query_handler=None),\n",
       " 'Low-depth quantum simulation of materials': Paper(DOI='10.1103/physrevx.8.011044', crossref_json=None, google_schorlar_metadata=None, title='Low-depth quantum simulation of materials', authors=['Ryan Babbush', 'Nathan Wiebe', 'Jarrod McClean', 'James McClain', 'Hartmut Neven', 'Garnet Kin-Lic Chan'], abstract='Quantum simulation of the electronic structure problem is one of the most researched applications of quantum computing. The majority of quantum algorithms for this problem encode the wavefunction using N Gaussian orbitals, leading to Hamiltonians with O (N 4) second-quantized terms. We avoid this overhead and extend methods to condensed phase materials by utilizing a dual form of the plane wave basis which diagonalizes the potential operator, leading to a Hamiltonian representation with O (N 2) second-quantized terms. Using this representation, we can implement single Trotter steps of the Hamiltonians with linear gate depth on a planar lattice. Properties of the basis allow us to deploy Trotter-and Taylor-series-based simulations with respective circuit depths of O (N 7/2) and O (N 8/3) for fixed charge densities. Variational algorithms also require significantly fewer measurements in this basis, ameliorating\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF02650179', '10.1126/science.273.5278.1073', '10.1103/PhysRevLett.79.2586', '10.1126/science.1113479', '10.1090/S0002-9939-1959-0108732-6', '10.1016/0375-9601(93)90701-Z', '10.1038/srep06603', '10.1088/1367-2630/18/3/033032', '10.1073/pnas.0808245105', '10.1088/1751-8121/aa77b8', '10.1088/2058-9565/aa9463', '10.1038/ncomms5213', '10.1088/1367-2630/18/2/023023', '10.1080/00268976.2011.552441', '10.1103/PhysRevA.90.022305', '10.26421/QIC15.5-6-1', '10.26421/QIC15.1-2-1', '10.1103/PhysRevA.91.022311', '10.1021/jz501649m', '10.1063/1.4768229', '10.1002/qua.24969', '10.1103/PhysRevA.94.030301', '10.1103/PhysRevA.95.032332', '10.1038/ncomms7979', '10.1038/ncomms7983', '10.1038/nature14270', '10.1038/nature17658', '10.1038/nphys3930', '10.1038/543171a', '10.1038/nchem.483', '10.1038/srep00001', '10.1021/acsnano.5b01651', '10.1103/PhysRevA.95.020501', '10.1103/PhysRevX.6.031007', '10.1038/nature23879', '10.1103/PhysRevA.92.042303', '10.1038/nchem.2248', '10.1007/BF01331938', '10.1006/aphy.2002.6254', '10.1103/PhysRevA.71.022316', '10.1073/pnas.1619152114', '10.1063/1.2790019', '10.1063/1.4961301', '10.1063/1.2790019', '10.1017/CBO9780511805769', '10.1063/1.1467901', '10.1063/1.4818753', '10.1021/cr200168z', '10.1080/00268979000101451', '10.1021/acs.jctc.7b00049', '10.1103/PhysRevB.66.035119', '10.1063/1.1839852', '10.1016/0009-2614(82)83051-0', '10.1016/0021-9991(84)90009-3', '10.1063/1.462100', '10.1080/00268976.2016.1176262', '10.1007/BF02199356', '10.1103/PhysRevE.55.5261', '10.1002/(SICI)1521-3978(199811)46:6/8&lt;877::AID-PROP877&gt;3.0.CO;2-A', '10.1016/S0167-2789(98)00042-6', '10.1103/PhysRevE.57.54', '10.1103/PhysRevA.79.032316', '10.1103/PhysRevLett.113.010401', '10.1090/S0025-5718-1965-0178586-1', '10.1146/annurev-physchem-032210-103512', '10.1088/1367-2630/14/11/115023', '10.1007/s00220-006-0150-x', '10.1088/1751-8113/43/6/065203', '10.1103/PhysRevLett.114.090502', '10.26421/QIC14.1-2-1', '10.1103/PhysRevLett.118.010501', '10.26421/QIC17.7-8-5', '10.1109/TCAD.2005.855930', '10.1103/PhysRevA.95.042308', '10.1126/sciadv.aap9646', '10.1017/CBO9780511619915', '10.1103/RevModPhys.82.1743', '10.1103/RevModPhys.65.677', '10.1103/PhysRev.46.1002', '10.1142/1584', '10.1103/PhysRevLett.45.566', '10.1139/p80-159', '10.1103/PhysRevB.45.13244', '10.1103/PhysRevB.39.5005', '10.1103/PhysRevE.66.036703', '10.1103/PhysRevLett.88.256601', '10.1103/PhysRevB.88.085121', '10.1103/PhysRevLett.102.126402', '10.1103/PhysRev.106.364', '10.1103/PhysRevB.15.5512', '10.1103/PhysRevB.85.081103', '10.1063/1.4720076', '10.1088/0953-8984/7/28/001', '10.1063/1.4934666', '10.1103/PhysRev.56.340', '10.1016/j.physleta.2010.12.029', '10.1063/1.462811', '10.1103/PhysRevB.86.035111', '10.1002/cpa.3160100201', '10.1063/1.473863', '10.1063/1.469351', '10.1016/S0009-2614(98)00111-0', '10.1103/PhysRevB.77.045136', '10.1016/0166-1280(86)80068-9', '10.1002/qua.560510612', '10.1080/00268970210133206', '10.1080/002689797170220', '10.1017/CBO9780511609633', '10.1103/PhysRevB.73.205119', '10.1103/PhysRevB.87.165122', '10.1103/PhysRevB.73.233103'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Quantum Computing', 'Machine Learning', 'Computer Vision', 'Neuroscience', 'Robotics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond': Paper(DOI='10.1007/s11263-019-01158-4', crossref_json=None, google_schorlar_metadata=None, title='Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond', authors=['Dimitrios Kollias', 'Panagiotis Tzirakis', 'Mihalis A Nicolaou', 'Athanasios Papaioannou', 'Guoying Zhao', 'Björn Schuller', 'Irene Kotsia', 'Stefanos Zafeiriou'], abstract=' Automatic understanding of human affect using visual signals is of great importance in everyday human–machine interactions. Appraising human emotional states, behaviors and reactions displayed in real-world settings, can be accomplished using latent continuous dimensions (e.g., the circumplex model of affect). Valence (i.e., how positive or negative is an emotion) and arousal (i.e., power of the activation of the emotion) constitute popular and effective representations for affect. Nevertheless, the majority of collected datasets this far, although containing naturalistic emotional states, have been captured in highly controlled recording conditions. In this paper, we introduce the Aff-Wild benchmark for training and evaluating affect recognition algorithms. We also report on the results of the First Affect-in-the-wild Challenge (Aff-Wild Challenge) that was recently organized in conjunction with CVPR 2017 on the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/2647868.2654890', '10.1007/s11263-017-0999-5', '10.1109/TPAMI.2016.2515606', '10.1016/S0167-6393(02)00071-7', '10.4018/jse.2012010101', '10.1109/CVPR.2009.5206848', '10.1145/3136755.3143004', '10.1145/2993148.2997638', '10.1145/2663204.2666275', '10.1145/2522848.2531739', '10.1016/j.imavis.2009.08.002', '10.1109/CVPR.2016.90', '10.1162/neco.1997.9.8.1735', '10.1145/3136755.3143009', '10.1109/ICCV.2015.341', '10.1109/T-AFFC.2011.15', '10.1109/CVPRW.2017.247', '10.1016/j.imavis.2017.02.001', '10.2307/2532051', '10.1109/CVPRW.2017.244', '10.1109/CVPRW.2010.5543262', '10.1109/FG.2011.5771462', '10.1007/978-3-319-10593-2_47', '10.1109/T-AFFC.2011.20', '10.1109/ICME.2005.1521424', '10.5244/C.29.41', '10.1145/2733373.2806408', '10.1037/0022-3514.36.10.1152', '10.1109/TPAMI.2014.2366127', '10.1145/2388676.2388776', '10.1109/T-AFFC.2011.26', '10.1109/T-AFFC.2011.25', '10.1109/34.908962', '10.1145/2988257.2988258', '10.1145/2661806.2661807', '10.1145/2512530.2512533', '10.1109/CVPRW.2017.248', '10.1109/TPAMI.2008.52'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Affective Computing', 'Artificial Intelligence', 'Computer Vision', 'Pattern Recognition'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Automatic sleep stage scoring with single-channel EEG using convolutional neural networks': Paper(DOI='10.1515/bmt-2016-0156', crossref_json=None, google_schorlar_metadata=None, title='Automatic sleep stage scoring with single-channel EEG using convolutional neural networks', authors=['Orestis Tsinalis', 'Paul M Matthews', 'Yike Guo', 'Stefanos Zafeiriou'], abstract=\"We used convolutional neural networks (CNNs) for automatic sleep stage scoring based on single-channel electroencephalography (EEG) to learn task-specific filters for classification without using prior domain knowledge. We used an openly available dataset from 20 healthy young adults for evaluation and applied 20-fold cross-validation. We used class-balanced random sampling within the stochastic gradient descent (SGD) optimization of the CNN to avoid skewed performance in favor of the most represented sleep stages. We achieved high mean F1-score (81%, range 79-83%), mean accuracy across individual sleep stages (82%, range 80-84%) and overall accuracy (74%, range 71-76%) over all subjects. By analyzing and visualizing the filters that our CNN learns, we found that rules learned by the filters correspond to sleep scoring criteria in the American Academy of Sleep Medicine (AASM) manual that human experts follow. Our method's performance is balanced across classes and our results are comparable to state-of-the-art methods with hand-engineered features. We show that, without using prior domain knowledge, a CNN can automatically learn to distinguish among different normal sleep stages.\", conference=None, journal=None, year=None, reference_list=['10.1007/978-3-642-20353-4_1', '10.1016/j.eswa.2012.09.022', '10.1016/j.neucom.2008.04.006', '10.1016/j.neucom.2015.03.017', '10.7551/mitpress/7496.003.0016', '10.1109/TPAMI.2010.125', '10.1016/j.compbiomed.2011.04.001', '10.1177/001316446002000104', '10.1016/j.artmed.2004.04.004', '10.1016/j.cmpb.2011.11.005', '10.1007/1-4020-4486-0_1', '10.1016/0167-8655(94)90029-9', '10.1161/01.CIR.101.23.e215', '10.1109/TBME.2014.2375292', '10.1007/BF02513332', '10.1016/j.neucom.2012.11.003', '10.1113/jphysiol.1968.sp008455', '10.1113/jphysiol.1959.sp006308', '10.1007/BF00354982', '10.1023/A:1016359216961', '10.1109/TBME.2015.2468589', '10.1016/j.compbiomed.2012.09.012', '10.1016/j.artmed.2011.06.004', '10.5220/0004725600200030', '10.1109/TSMCA.2012.2192264', '10.1109/5.726791', '10.1109/TIM.2012.2187242', '10.1007/s40846-015-0036-5', '10.1109/MLSP.2008.4685487', '10.1093/sleep/32.2.139', '10.1007/3-540-44989-2_118', '10.1162/08997660460734001', '10.1016/j.smrv.2011.06.003', '10.5664/jcsm.27124', '10.1016/j.smrv.2011.01.007', '10.1007/11941439_114', '10.1007/978-3-642-29491-4_7', '10.1088/1741-2560/8/3/036015', '10.1109/TNN.2005.844906', '10.1109/MSP.2010.939038', '10.1016/j.bspc.2007.05.005'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Deep Learning', 'Statistical Machine Learning', 'Pattern Recognition', 'Biometrics'], conference_acronym='Biomedizinische Technik (Berlin. Zeitschrift. Internet)', publisher=None, query_handler=None),\n",
       " 'Geodesic active contours and level sets for the detection and tracking of moving objects': Paper(DOI='10.1109/34.841758', crossref_json=None, google_schorlar_metadata=None, title='Geodesic active contours and level sets for the detection and tracking of moving objects', authors=['Nikos Paragios', 'Rachid Deriche'], abstract='This paper presents a new variational framework for detecting and tracking multiple moving objects in image sequences. Motion detection is performed using a statistical framework for which the observed interframe difference density function is approximated using a mixture model. This model is composed of two components, namely, the static (background) and the mobile (moving objects) one. Both components are zero-mean and obey Laplacian or Gaussian law. This statistical framework is used to provide the motion detection boundaries. Additionally, the original frame is used to provide the moving object boundaries. Then, the detection and the tracking problem are addressed in a common framework that employs a geodesic active contour objective function. This function is minimized using a gradient descent method. A new approach named Hermes is proposed, which exploits aspects from the well-known\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF00127170', '10.1006/jcph.1995.1098', '10.1016/0021-9991(88)90002-2', '10.1016/1049-9660(91)90028-N', '10.1007/BF00133570', '10.1007/BF00379537', '10.1109/83.661183', '10.1109/34.216733', '10.1016/0004-3702(88)90080-X', '10.1109/ICCV.1995.466882', '10.1017/S0962492900002671', '10.1109/CVPR.1996.517056', '10.1006/jcph.1993.1092', '10.1007/BF01539538', '10.1073/pnas.93.4.1591', '10.1109/83.334981', '10.1109/ICCV.1995.466855', '10.1109/ICCV.1995.466871', '10.1109/ICCV.1995.466925', '10.1006/ciun.1994.1042', '10.1109/ICCV.1999.791292', '10.1109/CVPR.1999.784648', '10.1006/jvci.1995.1029', '10.1016/0923-5965(95)00003-F', '10.1109/TPAMI.1986.4767755', '10.1016/S0923-5965(98)00011-3', '10.1016/0923-5965(91)90028-Z', '10.1016/0734-189X(89)90079-0', '10.1007/BFb0028363', '10.1109/34.250841', '10.1007/3-540-48236-9_5', '10.1007/3-540-48236-9_4', '10.1109/ICCV.1998.710859', '10.1109/34.368173', '10.1137/S0036142994275044'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['artificial intelligence', 'computer vision', 'machine learning', 'medical imaging'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'A radiomics approach to assess tumour-infiltrating CD8 cells and response to anti-PD-1 or anti-PD-L1 immunotherapy: an imaging biomarker, retrospective multicohort study': Paper(DOI='10.1016/s1470-2045(18)30413-3', crossref_json=None, google_schorlar_metadata=None, title='A radiomics approach to assess tumour-infiltrating CD8 cells and response to anti-PD-1 or anti-PD-L1 immunotherapy: an imaging biomarker, retrospective multicohort study', authors=['Roger Sun', 'Elaine Johanna Limkin', 'Maria Vakalopoulou', 'Laurent Dercle', 'Stéphane Champiat', 'Shan Rong Han', 'Loïc Verlingue', 'David Brandao', 'Andrea Lancia', 'Samy Ammari', 'Antoine Hollebecque', 'Jean-Yves Scoazec', 'Aurélien Marabelle', 'Christophe Massard', 'Jean-Charles Soria', 'Charlotte Robert', 'Nikos Paragios', 'Eric Deutsch', 'Charles Ferté'], abstract='BackgroundBecause responses of patients with cancer to immunotherapy can vary in success, innovative predictors of response to treatment are urgently needed to improve treatment outcomes. We aimed to develop and independently validate a radiomics-based biomarker of tumour-infiltrating CD8 cells in patients included in phase 1 trials of anti-programmed cell death protein (PD)-1 or anti-programmed cell death ligand 1 (PD-L1) monotherapy. We also aimed to evaluate the association between the biomarker, and tumour immune phenotype and clinical outcomes of these patients.MethodsIn this retrospective multicohort study, we used four independent cohorts of patients with advanced solid tumours to develop and validate a radiomic signature predictive of immunotherapy response by combining contrast-enhanced CT images and RNA-seq genomic data from tumour biopsies to assess CD8 cell tumour\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1093/annonc/mdx034', '10.1016/j.crad.2010.04.005', '10.1038/ncomms5006', '10.1016/j.ijrobp.2016.03.038', '10.1056/NEJMoa1503093', '10.1002/ajh.24226', '10.1056/NEJMoa1606774', '10.1038/nature21349', '10.1038/nature14011', '10.1093/annonc/mdw217', '10.1016/j.cell.2016.02.065', '10.1158/1078-0432.CCR-15-1507', '10.1038/nature13954', '10.1158/2159-8290.CD-16-1396', '10.1016/S0950-3552(98)80061-8', '10.1158/1078-0432.CCR-16-1741', '10.1016/j.ejca.2017.07.033', '10.1186/s13059-016-1070-5', '10.1007/s10278-013-9622-7', '10.1038/ng.2764', '10.1158/2326-6066.CIR-16-0031', '10.2967/jnumed.114.144055', '10.1038/s41598-017-10371-5', '10.1038/srep11075', '10.1038/nmeth.4197', '10.1148/radiol.2502071879', '10.1111/j.1467-9868.2005.00503.x', '10.1136/amiajnl-2012-001469', '10.1200/JCO.2008.19.5081', '10.1001/archotol.126.11.1329', '10.1186/s13058-017-0846-1', '10.1056/NEJMoa1801005', '10.1056/NEJMoa1709937', '10.1016/S0146-664X(75)80008-6', '10.1148/radiol.2016152771', '10.1158/2159-8290.CD-17-1320', '10.3389/fimmu.2017.00830', '10.4161/onci.28976', '10.1172/JCI80005', '10.1093/annonc/mdx238', '10.1158/2326-6066.CIR-16-0325', '10.1038/s41598-018-20471-5', '10.7554/eLife.23421', '10.1038/s41598-017-18489-2', '10.1016/j.immuni.2018.03.023', '10.1016/j.celrep.2018.03.086', '10.1126/science.aaf8399', '10.1016/j.ccell.2018.02.005'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['artificial intelligence', 'computer vision', 'machine learning', 'medical imaging'], conference_acronym='Lancet oncology (Print)', publisher=None, query_handler=None),\n",
       " 'Geodesic active regions: A new framework to deal with frame partition problems in computer vision': Paper(DOI='10.1006/jvci.2001.0475', crossref_json=None, google_schorlar_metadata=None, title='Geodesic active regions: A new framework to deal with frame partition problems in computer vision', authors=['Nikos Paragios', 'Rachid Deriche'], abstract='This paper presents a novel variational framework for dealing with frame partition problems in computer vision by the propagation of curves. This framework integrates boundary- and region-based frame partition modules under a curve-based objective function, which aims at finding a set of minimal length curves that preserve three main properties: (i) they are regular and smooth, (ii) they are attracted by the boundary points (boundary-based information), (iii) and they create a partition that is optimal according to the expected region properties of the different hypotheses (region-based information). The defined objective function is minimized using a gradient descent method. According to the obtained motion equations, the set of initial curves is propagated toward the best partition under the influence of boundary- and region-based forces, and is constrained by a regularity force. The changes of topology are naturally\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.295913', '10.1109/83.277898', '10.1007/BF01385685', '10.1109/ICCV.1995.466871', '10.1023/A:1007979827043', '10.1109/42.544503', '10.1007/3-540-48236-9_13', '10.1016/1049-9660(91)90028-N', '10.1007/BF00115697', '10.1109/TPAMI.1984.4767596', '10.1007/BF00133570', '10.1109/ICCV.1995.466855', '10.1137/0731015', '10.1109/34.368173', '10.1016/0021-9991(88)90002-2', '10.1109/ICCV.1999.790347', '10.1007/3-540-48236-9_27', '10.1006/jcph.1996.0167', '10.1109/34.537343'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['artificial intelligence', 'computer vision', 'machine learning', 'medical imaging'], conference_acronym='Journal of visual communication and image representation (Print)', publisher=None, query_handler=None),\n",
       " 'Promises and challenges for the implementation of computational medical imaging (radiomics) in oncology': Paper(DOI='10.1093/annonc/mdx034', crossref_json=None, google_schorlar_metadata=None, title='Promises and challenges for the implementation of computational medical imaging (radiomics) in oncology', authors=['Elaine Johanna Limkin', 'Roger Sun', 'Laurent Dercle', 'Evangelia I Zacharaki', 'Charlotte Robert', 'Sylvain Reuzé', 'Antoine Schernberg', 'Nikos Paragios', 'Eric Deutsch', 'Charles Ferté'], abstract='Medical image processing and analysis (also known as Radiomics) is a rapidly growing discipline that maps digital medical images into quantitative data, with the end goal of generating imaging biomarkers as decision support tools for clinical practice. The use of imaging data from routine clinical work-up has tremendous potential in improving cancer care by heightening understanding of tumor biology and aiding in the implementation of precision medicine. As a noninvasive method of assessing the tumor and its microenvironment in their entirety, radiomics allows the evaluation and monitoring of tumor characteristics such as temporal and spatial heterogeneity. One can observe a rapid increase in the number of computational medical imaging publications—milestones that have highlighted the utility of imaging biomarkers in oncology. Nevertheless, the use of radiomics as clinical biomarkers still necessitates\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1148/radiol.2015151169', '10.1016/j.mri.2012.06.010', '10.1016/j.ejca.2011.11.036', '10.1038/nature12626', '10.1158/1078-0432.CCR-14-0990', '10.3109/0284186X.2010.498437', '10.1038/srep23428', '10.1007/s10278-014-9716-x', '10.1371/journal.pone.0102107', '10.1016/j.radonc.2012.09.023', '10.1038/srep03529', '10.54294/yxytcv', '10.1109/TPAMI.1986.4767776', '10.1038/srep11075', '10.1371/journal.pone.0145063', '10.2967/jnumed.112.116715', '10.1007/s11307-016-0940-2', '10.1016/j.patcog.2008.08.011', '10.1109/TSMC.1973.4309314', '10.1016/S0146-664X(75)80008-6', '10.1109/TBME.2013.2284600', '10.1016/0734-189X(83)90032-4', '10.21037/tcr.2016.06.31', '10.1118/1.3151811', '10.1016/j.patcog.2013.05.001', '10.1038/clpt.2008.161', '10.1038/nrc2294', '10.1158/1078-0432.CCR-12-3937', '10.1002/sim.4780090710', '10.1016/j.cmpb.2016.12.018', '10.1038/srep13087', '10.1016/j.jtho.2016.07.002', '10.3174/ajnr.A2939', '10.1007/s11548-011-0559-3', '10.1002/mrm.22147', '10.1158/1078-0432.CCR-15-2997', '10.1016/j.ultras.2016.08.004', '10.1109/TMI.2016.2536809', '10.1038/srep24454', '10.1109/TMI.2016.2532122', '10.1148/radiographics.19.3.g99ma14745', '10.1016/j.ijrobp.2016.03.038', '10.1016/S1470-2045(05)01737-7', '10.1016/j.semradonc.2010.10.001', '10.1371/journal.pone.0118261', '10.1038/npjbcancer.2016.12', '10.1148/radiol.12112428', '10.1073/pnas.0801279105', '10.1148/radiol.14132641', '10.1148/radiol.12111607', '10.1148/radiol.13130078', '10.1097/RCT.0000000000000239', '10.18632/oncotarget.7129', '10.1016/S0092-8674(00)81683-9', '10.1016/j.cell.2011.02.013', '10.1102/1470-7330.2010.0021', '10.1016/j.radonc.2015.06.013', '10.1038/srep17787', '10.1371/journal.pone.0025451', '10.1109/TBME.2009.2035305', '10.1038/nbt1306', '10.1056/NEJMoa1102873', '10.3389/fonc.2016.00071', '10.1097/JTO.0b013e3182843721', '10.1016/j.ejrad.2007.12.005', '10.1016/j.radonc.2015.02.015', '10.1038/srep11044', '10.3109/0284186X.2015.1061214', '10.1371/journal.pone.0136557', '10.1148/radiol.2016160845', '10.1148/radiol.2016151455', '10.1038/srep33860', '10.1186/s12885-015-1563-8', '10.1002/jmri.23971', '10.1148/radiol.2015141309', '10.1148/radiol.11110264', '10.1200/JCO.2015.61.6870', '10.1038/nature13954', '10.1183/09031936.00047908', '10.1016/j.ijrobp.2015.07.019', '10.1016/j.tranon.2016.01.008', '10.1186/1878-5085-4-7', '10.1038/clpt.2009.227', '10.1186/1741-7015-10-51', '10.1038/ng1201-365', '10.1097/RLI.0000000000000180', '10.1371/journal.pone.0164924', '10.1007/s10278-013-9622-7', '10.1593/tlo.13832', '10.1073/pnas.1505935112'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['artificial intelligence', 'computer vision', 'machine learning', 'medical imaging'], conference_acronym='Annals of oncology', publisher=None, query_handler=None),\n",
       " 'DRAMMS: Deformable registration via attribute matching and mutual-saliency weighting': Paper(DOI='10.1016/j.media.2010.07.002', crossref_json=None, google_schorlar_metadata=None, title='DRAMMS: Deformable registration via attribute matching and mutual-saliency weighting', authors=['Yangming Ou', 'Aristeidis Sotiras', 'Nikos Paragios', 'Christos Davatzikos'], abstract='A general-purpose deformable registration algorithm referred to as “DRAMMS” is presented in this paper. DRAMMS bridges the gap between the traditional voxel-wise methods and landmark/feature-based methods with primarily two contributions. First, DRAMMS renders each voxel relatively distinctively identifiable by a rich set of attributes, therefore largely reducing matching ambiguities. In particular, a set of multi-scale and multi-orientation Gabor attributes are extracted and the optimal components are selected, so that they form a highly distinctive morphological signature reflecting the anatomical and geometric context around each voxel. Moreover, the way in which the optimal Gabor attributes are constructed is independent of the underlying image modalities or contents, which renders DRAMMS generally applicable to diverse registration tasks. A second contribution of DRAMMS is that it modulates the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF00158167', '10.1016/j.neuroimage.2007.07.007', '10.1088/0031-9155/54/9/008', '10.1007/11569541_40', '10.1109/34.24792', '10.1007/11505730_11', '10.1006/gmod.2000.0531', '10.1088/0031-9155/39/3/022', '10.1016/S1077-3142(03)00009-2', '10.1097/00004728-199403000-00005', '10.1259/bjr/25329214', '10.1016/j.mri.2007.03.004', '10.1016/S1361-8415(03)00039-2', '10.1016/j.jneumeth.2007.04.017', '10.1109/42.481446', '10.1016/j.patrec.2005.03.035', '10.1155/2009/281615', '10.1109/TMI.2006.886812', '10.1007/11566465_11', '10.1002/hbm.460030303', '10.1097/00004728-199303000-00011', '10.1117/12.185179', '10.1016/j.neuroimage.2009.04.024', '10.1002/jmri.1139', '10.1016/j.media.2008.03.006', '10.1006/nimg.2001.0786', '10.1109/PROC.1979.11328', '10.1007/BFb0056296', '10.1088/0031-9155/46/3/201', '10.1109/TMI.2007.904691', '10.1016/0031-3203(91)90143-S', '10.1007/11505730_42', '10.1109/83.855431', '10.1016/j.neuroimage.2004.07.068', '10.1023/A:1012460413855', '10.1016/j.cviu.2008.06.007', '10.1109/TIP.2003.813139', '10.1016/j.neuroimage.2007.06.041', '10.1109/TPAMI.2005.151', '10.1109/TMI.2007.892646', '10.1016/S0031-3203(98)00095-8', '10.1007/978-3-642-02498-6_14', '10.1016/j.compmedimag.2009.03.004', '10.1109/TMI.2002.1009382', '10.1016/j.neuroimage.2004.04.020', '10.1016/j.media.2009.05.003', '10.1016/j.patcog.2007.04.002', '10.1109/42.563664', '10.1109/ISBI.2008.4541197', '10.1016/S1361-8415(01)80026-8', '10.1109/34.531803', '10.1109/42.585761', '10.1016/S1361-8415(97)85010-4', '10.2310/7290.2006.00002', '10.1007/978-1-84882-299-3_13', '10.3109/10929080209146523', '10.1016/j.media.2009.05.002', '10.1016/j.media.2005.03.006', '10.1016/j.media.2005.03.008', '10.1109/TMI.2003.815867', '10.1016/j.neuroimage.2005.11.044', '10.1016/j.media.2005.05.007', '10.1109/42.929618', '10.1109/42.796284', '10.1007/11866763_86', '10.1016/j.patcog.2006.08.012', '10.1109/TMI.2002.803111', '10.1016/j.neuroimage.2003.12.015', '10.1109/42.921475', '10.1109/42.832958', '10.1007/978-3-642-04268-3_83', '10.1016/j.patcog.2009.04.022', '10.1016/S1361-8415(98)80022-4', '10.1259/BJR/20005470', '10.1109/42.511745', '10.1007/978-3-540-75759-7_39', '10.1016/j.neuroimage.2008.10.040', '10.1016/j.acra.2008.01.029', '10.1016/j.acra.2006.05.017', '10.1016/S1361-8415(01)80004-9', '10.1109/TMI.2006.879320', '10.1007/978-3-540-73273-0_14', '10.1016/S1361-8415(03)00067-7', '10.1007/978-3-540-85988-8_41', '10.1109/TMI.2004.834616', '10.1016/j.neuroimage.2005.09.054', '10.1007/978-3-540-79982-5_2', '10.1117/12.812318', '10.1007/978-3-540-85990-1_109', '10.1016/j.neuroimage.2009.04.055', '10.1109/TMI.2008.916954', '10.1109/TMI.2005.862744', '10.1016/j.acra.2007.07.018', '10.1007/978-3-540-30136-3_106', '10.1016/S0262-8856(03)00137-9', '10.1007/11569541_30', '10.1148/radiology.198.3.8628891'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['artificial intelligence', 'computer vision', 'machine learning', 'medical imaging'], conference_acronym='Medical image analysis (Print)', publisher=None, query_handler=None),\n",
       " 'A level set approach for shape-driven segmentation and tracking of the left ventricle': Paper(DOI='10.1109/tmi.2003.814785', crossref_json=None, google_schorlar_metadata=None, title='A level set approach for shape-driven segmentation and tracking of the left ventricle', authors=['Nikos Paragios'], abstract='Knowledge-based segmentation has been explored significantly in medical imaging. Prior anatomical knowledge can be used to define constraints that can improve performance of segmentation algorithms to physically corrupted and incomplete data. In this paper, the objective is to introduce such knowledge-based constraints while preserving the ability of dealing with local deformations. Toward this end, we propose a variational level set framework that can account for global shape consistency as well as for local deformations. In order to improve performance, the problems of segmentation and tracking of the structure of interest are dealt with simultaneously by introducing the notion of time in the process and looking for a solution that satisfies that prior constraints while being consistent along consecutive frames. Promising experimental results in magnetic resonance and ultrasonic cardiac images demonstrate\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/0021-9991(88)90002-2', '10.1023/A:1020882509893', '10.1006/jvci.2001.0475', '10.1023/A:1020874308076', '10.1109/MMBIA.2001.991698', '10.1006/jcph.1996.0167', '10.1109/42.650882', '10.1109/ICCV.1995.466855', '10.1007/BF00133570', '10.1109/34.368173', '10.1109/CVPR.2000.855835', '10.1023/A:1020878408985', '10.1023/A:1007979827043', '10.1007/b97541'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['artificial intelligence', 'computer vision', 'machine learning', 'medical imaging'], conference_acronym='IEEE transactions on medical imaging (Print)', publisher=None, query_handler=None),\n",
       " 'MRF energy minimization and beyond via dual decomposition': Paper(DOI='10.1109/tpami.2010.108', crossref_json=None, google_schorlar_metadata=None, title='MRF energy minimization and beyond via dual decomposition', authors=['Nikos Komodakis', 'Nikos Paragios', 'Georgios Tziritas'], abstract='This paper introduces a new rigorous theoretical framework to address discrete MRF-based optimization in computer vision. Such a framework exploits the powerful technique of Dual Decomposition. It is based on a projected subgradient scheme that attempts to solve an MRF optimization problem by first decomposing it into a set of appropriately chosen subproblems, and then combining their solutions in a principled way. In order to determine the limits of this method, we analyze the conditions that these subproblems have to satisfy and demonstrate the extreme generality and flexibility of such an approach. We thus show that by appropriately choosing what subproblems to use, one can design novel and very powerful MRF optimization algorithms. For instance, in this manner we are able to derive algorithms that: 1) generalize and extend state-of-the-art message-passing methods, 2) optimize very tight LP\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1613/jair.1933', '10.1023/A:1026501619075', '10.1023/B:VISI.0000042934.15159.49', '10.7551/mitpress/3348.001.0001', '10.1109/TPAMI.2007.1036', '10.1162/089976601750541769', '10.1109/18.910585', '10.1109/TIT.2005.850085', '10.1109/CVPR.2008.4587355', '10.1016/0167-6377(96)00019-3', '10.1007/978-3-642-82118-9', '10.1109/ICCV.2003.1238444', '10.1109/TIT.2005.856938', '10.1109/CVPR.2003.1211409', '10.1109/TPAMI.2007.70844', '10.1137/070708111', '10.1137/S1052623499362111', '10.1109/CVPR.2005.160', '10.1145/1390156.1390257', '10.1109/CVPR.2007.383203', '10.1109/ICCV.2005.110', '10.1007/s101070050090', '10.1145/1201775.882264', '10.1109/ICCV.2007.4408890', '10.1109/TPAMI.2006.200', '10.1109/CVPR.2005.249', '10.1109/CVPR.2007.383095', '10.1109/CVPR.2006.141', '10.1109/TPAMI.2007.1128', '10.1007/s101070050002', '10.1137/0311049', '10.1109/34.969114', '10.1002/rsa.20057', '10.1007/BFb0120697', '10.1023/A:1023713602895'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['deep learning', 'computer vision', 'machine learning', 'medical image analysis', 'artificial intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Wider or deeper: Revisiting the resnet model for visual recognition': Paper(DOI='10.1016/j.patcog.2019.01.006', crossref_json=None, google_schorlar_metadata=None, title='Wider or deeper: Revisiting the resnet model for visual recognition', authors=['Zifeng Wu', 'Chunhua Shen', 'Anton Van Den Hengel'], abstract='The community has been going deeper and deeper in designing one cutting edge network after another, yet some works are there suggesting that we may have gone too far in this dimension. Some researchers unravelled a residual network into an exponentially wider one, and assorted the success of residual networks to fusing a large amount of relatively shallow models. Since some of their early claims are still not settled, we in this paper dig more on this topic, i.e., the unravelled view of residual networks. Based on that, we try to find a good compromise between the depth and width. Afterwards, we walk through a typical pipeline of developing a deep-learning-based algorithm. We start from a group of relatively shallow networks, which perform as well or even better than the current (much deeper) state-of-the-art models on the ImageNet classification dataset. Then, we initialize fully convolutional networks (FCNs\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.patcog.2016.12.005', '10.1016/j.patcog.2018.03.015', '10.1016/j.patcog.2018.03.003', '10.1007/s11263-015-0816-y', '10.1109/TPAMI.2017.2699184', '10.1109/TPAMI.2016.2572683', '10.1109/TPAMI.2016.2577031', '10.1016/j.cviu.2017.05.007', '10.1007/s11263-014-0733-5', '10.1109/TPAMI.2017.2723009', '10.1109/TPAMI.2017.2708714'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Visual Question Answering'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'Refuge challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs': Paper(DOI='10.1016/j.media.2019.101570', crossref_json=None, google_schorlar_metadata=None, title='Refuge challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs', authors=['José Ignacio Orlando', 'Huazhu Fu', 'João Barbosa Breda', 'Karel Van Keer', 'Deepti R Bathula', 'Andrés Diaz-Pinto', 'Ruogu Fang', 'Pheng-Ann Heng', 'Jeyoung Kim', 'JoonHo Lee', 'Joonseok Lee', 'Xiaoxiao Li', 'Peng Liu', 'Shuai Lu', 'Balamurali Murugesan', 'Valery Naranjo', 'Sai Samarth R Phaye', 'Sharath M Shankaranarayana', 'Apoorva Sikka', 'Jaemin Son', 'Anton van den Hengel', 'Shujun Wang', 'Junyan Wu', 'Zifeng Wu', 'Guanghui Xu', 'Yongli Xu', 'Pengshuai Yin', 'Fei Li', 'Xiulan Zhang', 'Yanwu Xu', 'Hrvoje Bogunović'], abstract='Glaucoma is one of the leading causes of irreversible but preventable blindness in working age populations. Color fundus photography (CFP) is the most cost-effective imaging modality to screen for retinal disorders. However, its application to glaucoma has been limited to the computation of a few related biomarkers such as the vertical cup-to-disc ratio. Deep learning approaches, although widely applied for medical image analysis, have not been extensively used for glaucoma assessment due to the limited size of the available data sets. Furthermore, the lack of a standardize benchmark strategy makes difficult to compare existing methods in a uniform way. In order to overcome these issues we set up the Retinal Fundus Glaucoma Challenge, REFUGE (https://refuge.grand-challenge.org), held in conjunction with MICCAI 2018. The challenge consisted of two primary tasks, namely optic disc/cup segmentation and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/RBME.2010.2084567', '10.1038/s41746-018-0040-6', '10.3390/sym10040087', '10.1155/2015/180972', '10.1001/jamaophthalmol.2017.3782', '10.1016/j.artmed.2008.04.005', '10.1613/jair.953', '10.1038/s41598-018-35044-9', '10.1038/s41591-018-0107-6', '10.5566/ias.1155', '10.2307/2531595', '10.1136/bjophthalmol-2016-EGSguideline.001', '10.1145/1360612.1360666', '10.1109/TMI.2018.2791488', '10.1364/BOE.10.000892', '10.1001/jama.2016.17216', '10.1016/j.cmpb.2018.07.012', '10.1016/j.compmedimag.2013.09.005', '10.1117/1.JMI.4.1.014503', '10.1109/TMI.2011.2106509', '10.1038/s41598-019-43385-2', '10.1016/j.ophtha.2017.10.011', '10.1109/5.726791', '10.1016/j.ophtha.2018.01.023', '10.1016/j.media.2017.07.005', '10.1016/j.ogla.2018.04.002', '10.1109/TMI.2003.823261', '10.1038/s41467-018-07619-7', '10.1109/TMI.2015.2412881', '10.1109/TMI.2009.2033909', '10.1049/iet-ipr.2012.0455', '10.1109/TBME.2016.2535311', '10.3346/jkms.2018.33.e239', '10.1038/s41551-018-0195-0', '10.3390/data3030025', '10.1159/000329603', '10.1016/j.ins.2018.01.051', '10.1016/j.ophtha.2011.09.054', '10.1007/s11263-015-0816-y', '10.1016/j.preteyeres.2018.07.004', '10.1134/S1054661817030269', '10.1186/s12880-015-0068-x', '10.1016/j.bspc.2018.01.014', '10.1016/j.ophtha.2014.05.013', '10.1007/978-3-030-00949-6_33', '10.1167/iovs.12-10347', '10.1186/1471-2105-9-265', '10.1016/j.patcog.2019.01.006', '10.1167/iovs.12-9576'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Visual Question Answering'], conference_acronym='Medical image analysis (Print)', publisher=None, query_handler=None),\n",
       " 'Image captioning and visual question answering based on attributes and external knowledge': Paper(DOI='10.1109/tpami.2017.2708709', crossref_json=None, google_schorlar_metadata=None, title='Image captioning and visual question answering based on attributes and external knowledge', authors=['Qi Wu', 'Chunhua Shen', 'Peng Wang', 'Anthony Dick', 'Anton Van Den Hengel'], abstract='Much of the recent progress in Vision-to-Language problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to progress directly from image features to text. In this paper we first propose a method of incorporating high-level concepts into the successful CNN-RNN approach, and show that it achieves a significant improvement on the state-of-the-art in both image captioning and visual question answering. We further show that the same mechanism can be used to incorporate external knowledge, which is critically important for answering high level visual questions. Specifically, we design a visual question answering model that combines an internal representation of the content of an image with information extracted from a general knowledge base to\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-319-10602-1_48', '10.3115/v1/P15-2017', '10.1109/CVPR.2015.7299087', '10.1109/CVPR.2015.7298932', '10.1007/978-3-642-15561-1_2', '10.1109/CVPR.2015.7298754', '10.1109/TPAMI.2012.162', '10.1109/JPROC.2010.2050411', '10.1109/CVPR.2015.7298752', '10.1109/TPAMI.2015.2491929', '10.1613/jair.3994', '10.1109/TPAMI.2016.2537320', '10.5244/C.30.141', '10.1109/ICCV.2015.169', '10.1007/978-3-319-10593-2_35', '10.1109/CVPR.2016.251', '10.1162/neco.1997.9.8.1735', '10.3115/v1/D14-1179', '10.1007/s11263-006-8614-1', '10.1007/s11263-012-0529-4', '10.1007/978-3-642-35749-7_5', '10.1109/CVPR.2016.503', '10.1109/CVPR.2015.7298917', '10.1609/aimag.v31i3.2303', '10.1007/978-3-540-76298-0_52', '10.1145/1376616.1376746', '10.1109/CVPR.2016.11', '10.1109/CVPR.2016.10', '10.1109/CVPR.2015.7298594', '10.1109/ICCV.2015.279', '10.1109/ICCV.2015.9', '10.3115/981732.981751', '10.1109/CVPR.2016.29', '10.1109/CVPR.2016.500', '10.18653/v1/N16-1181', '10.1109/CVPR.2009.5206772', '10.1109/CVPR.2015.7298878', '10.1109/CVPR.2015.7298856', '10.1109/5.726791', '10.1073/pnas.1422953112', '10.1109/MMUL.2014.29', '10.1007/978-3-319-46478-7_28', '10.1109/CVPR.2016.540', '10.1109/ICCV.2015.277', '10.1109/ICCV.2013.61'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Visual Question Answering'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'A haplotype map of the human genome': Paper(DOI='10.1152/physiolgenomics.00178.2002', crossref_json=None, google_schorlar_metadata=None, title='A haplotype map of the human genome', authors=['International HapMap Consortium Altshuler David altshuler@ molbio. mgh. harvard. edu Donnelly Peter donnelly@ stats. ox. ac. uk'], abstract='Inherited genetic variation has a critical but as yet largely uncharacterized role in human disease. Here we report a public database of common variation in the human genome: more than one million single nucleotide polymorphisms (SNPs) for which accurate and complete genotypes have been obtained in 269 DNA samples from four populations, including ten 500-kilobase regions in which essentially all information about common DNA variation has been extracted. These data document the generality of recombination hotspots, a block-like structure of linkage disequilibrium and low haplotype diversity, leading to substantial correlations of SNPs with many of their neighbours. We show how the HapMap resource can guide the design and analysis of genetic association studies, shed light on structural variation and recombination, and identify loci that may have been subject to natural selection during human evolution.', conference=None, journal=None, year=None, reference_list=['10.1086/316944', '10.1086/301977', '10.1126/science.278.5343.1580', '10.1126/science.298.5595.941a', '10.1038/ng1001-229', '10.1007/s00439-002-0809-0', '10.1126/science.1069424', '10.1086/302727', '10.1038/35079107', '10.1038/ng1001-217', '10.1038/ng910', '10.1086/339258', '10.1126/science.2570460', '10.1038/9642', '10.1038/85776', '10.1126/science.274.5287.536', '10.1086/344347', '10.1073/pnas.042680999', '10.1086/303003', '10.1086/316940', '10.1038/35079114', '10.1006/geno.2001.6646', '10.1126/science.1065573', '10.1016/0092-8674(94)90016-7', '10.1038/35075590', '10.1093/clinchem/48.9.1597', '10.1038/ng1001-223', '10.1038/35057149', '10.1086/342217', '10.1126/science.1059431', '10.1038/77100', '10.1006/geno.2000.6393', '10.1086/302699', '10.1086/344780', '10.1073/pnas.012672899'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Computer Vision', 'Pattern Recognition'], conference_acronym='Physiological genomics (Print)', publisher=None, query_handler=None),\n",
       " 'Skin damage among health care workers managing coronavirus disease-2019': Paper(DOI='10.1016/j.jaad.2020.04.003', crossref_json=None, google_schorlar_metadata=None, title='Skin damage among health care workers managing coronavirus disease-2019', authors=['Jiajia Lan', 'Zexing Song', 'Xiaoping Miao', 'Hang Li', 'Yan Li', 'Liyun Dong', 'Jing Yang', 'Xiangjie An', 'Yamin Zhang', 'Liu Yang', 'Nuoya Zhou', 'Jun Li', 'JingJiang Cao', 'Jianxiu Wang', 'Juan Tao'], abstract='From January to February 2020, self-administered online questionnaires were distributed to 700 individuals, consisting of physicians and nurses who worked in the designated departments of tertiary hospitals in Hubei, China. The questionnaire included questions about the condition of skin damage and the frequency or duration of several infection-prevention measures(Supplemental Material 1, available via Mendeley at https://data. mendeley. com/datasets/zknvry83v5/2). Finally, 542 individuals (Supplemental Material 2) completed the study (response rate, 77.4%), with 71.4%(387 of 542) working in isolation wards and 28.6%(155 of 542) working in fever clinics.The general prevalence rate of skin damage caused by enhanced infection-prevention measures was 97.0%(526 of 542) among first-line health care workers. The affected sites included the nasal bridge, hands, cheek, and forehead, with the nasal bridge\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.jaad.2020.03.014', '10.12968/jowc.2019.28.5.284'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym='Journal of the American Academy of Dermatology', publisher=None, query_handler=None),\n",
       " 'Genome-wide association study of esophageal squamous cell carcinoma in Chinese subjects identifies a susceptibility locus at PLCE1': Paper(DOI='10.1016/j.canep.2013.04.009', crossref_json=None, google_schorlar_metadata=None, title='Genome-wide association study of esophageal squamous cell carcinoma in Chinese subjects identifies a susceptibility locus at PLCE1', authors=['Li-Dong Wang', 'Fu-You Zhou', 'Xue-Min Li', 'Liang-Dan Sun', 'Xin Song', 'Yan Jin', 'Jiang-Man Li', 'Guo-Qiang Kong', 'Hong Qi', 'Juan Cui', 'Lian-Qun Zhang', 'Jie-Zhi Yang', 'Ji-Lin Li', 'Xing-Chuan Li', 'Jing-Li Ren', 'Zhi-Cai Liu', 'Wen-Jun Gao', 'Ling Yuan', 'Wu Wei', 'Yan-Rui Zhang', 'Wei-Peng Wang', 'Ilyar Sheyhidin', 'Feng Li', 'Bao-Ping Chen', 'Shu-Wei Ren', 'Bin Liu', 'Dan Li', 'Jian-Wei Ku', 'Zong-Min Fan', 'Sheng-Li Zhou', 'Zhi-Gang Guo', 'Xue-Ke Zhao', 'Na Liu', 'Yong-Hong Ai', 'Fang-Fang Shen', 'Wen-Yan Cui', 'Shuang Song', 'Tao Guo', 'Jing Huang', 'Chao Yuan', 'Jia Huang', 'Yue Wu', 'Wen-Bin Yue', 'Chang-Wei Feng', 'Hong-Lei Li', 'Yan Wang', 'Jin-Ya Tian', 'Yue Lu', 'Yi Yuan', 'Wen-Liang Zhu', 'Min Liu', 'Wen-Jing Fu', 'Xia Yang', 'Han-Jing Wang', 'Suo-Li Han', 'Jie Chen', 'Min Han', 'Hai-Yan Wang', 'Peng Zhang', 'Xiu-Min Li', 'Jin-Cheng Dong', 'Guo-Lan Xing', 'Ran Wang', 'Ming Guo', 'Zhi-Wei Chang', 'Hai-Lin Liu', 'Li Guo', 'Zhi-Qing Yuan', 'Hai Liu', 'Qin Lu', 'Liu-Qin Yang', 'Fu-Guo Zhu', 'Xiu-Feng Yang', 'Xiao-Shan Feng', 'Zhou Wang', 'Yin Li', 'She-Gan Gao', 'Qirenwang Qige', 'Long-Tang Bai', 'Wen-Jun Yang', 'Guang-Yan Lei', 'Zhong-Ying Shen', 'Long-Qi Chen', 'En-Min Li', 'Li-Yan Xu', 'Zhi-Yong Wu', 'Wei-Ke Cao', 'Jian-Po Wang', 'Zhi-Qin Bao', 'Ji-Li Chen', 'Guang-Cheng Ding', 'Xiang Zhuang', 'Ying-Fa Zhou', 'Hou-Feng Zheng', 'Zheng Zhang', 'Xian-Bo Zuo', 'Zi-Ming Dong', 'Dong-Mei Fan', 'Xin He', 'Jin Wang', 'Qi Zhou', 'Qin-Xian Zhang', 'Xin-Ying Jiao', 'Shi-Yong Lian', 'Ai-Fang Ji', 'Xiao-Mei Lu', 'Jin-Sheng Wang', 'Fu-Bao Chang', 'Chang-Dong Lu', 'Zhi-Guo Chen', 'Jian-Jun Miao', 'Zeng-Lin Fan', 'Ruo-Bai Lin', 'Tai-Jiang Liu', 'Jin-Chang Wei', 'Qing-Peng Kong', 'Yu Lan', 'Yu-Jing Fan', 'Fu-Sheng Gao', 'Tian-Yun Wang', 'Dong Xie', 'Shu-Qing Chen', 'Wan-Cai Yang', 'Jun-Yan Hong', 'Liang Wang', 'Song-Liang Qiu', 'Zhi-Ming Cai', 'Xue-Jun Zhang'], abstract='We performed a genome-wide association study of esophageal squamous cell carcinoma (ESCC) by genotyping 1,077 individuals with ESCC and 1,733 control subjects of Chinese Han descent. We selected 18 promising SNPs for replication in an additional 7,673 cases of ESCC and 11,013 control subjects of Chinese Han descent and 303 cases of ESCC and 537 control subjects of Chinese Uygur-Kazakh descent. We identified a previously unknown susceptibility locus for ESCC: PLCE1 at 10q23 (PHan combined for ESCC = 7.46 × 10−56, odds ratio (OR) = 1.43; PUygur-Kazakh for ESCC = 5.70 × 10−4, OR = 1.53). We also confirmed association in 2,766 cases of gastric cardia adenocarcinoma and the same 11,013 control subjects (PLCE1, PHan for GCA = 1.74 × 10−39, OR = 1.55. PLCE1 has important biological implications for both ESCC and GCA. PLCE1 might regulate cell growth, differentiation\\xa0…', conference=None, journal=None, year=None, reference_list=['10.3322/canjclin.55.2.74', '10.3322/caac.20107', '10.1016/j.gtc.2009.01.008', '10.1111/j.1442-2050.2007.00776.x', '10.1002/ijc.20616', '10.1002/ijc.11300', '10.1371/journal.pone.0009668', '10.1086/302970', '10.1038/ng.648', '10.1038/ng.649', '10.1038/ng.849', '10.1124/mi.3.5.273', '10.1126/scisignal.3119tr3', '10.1093/carcin/bgp125', '10.1158/0008-5472.CAN-04-3143', '10.1016/j.cancergencyto.2010.01.021', '10.1074/jbc.M507734200', '10.1245/s10434-011-2160-y', '10.1186/1471-2407-11-258', '10.1080/01635580701883011', '10.1016/j.gtc.2009.01.004', '10.1002/gepi.20211'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym='Cancer epidemiology (Print)', publisher=None, query_handler=None),\n",
       " 'An international two–stage genome–wide search for schizophrenia susceptibility genes': Paper(DOI='10.1016/s0163-4453(98)80050-9', crossref_json=None, google_schorlar_metadata=None, title='An international two–stage genome–wide search for schizophrenia susceptibility genes', authors=['HW Moises', 'L Yang', 'H Kristbjarnarson', 'C Wiese', 'W Byerley', 'F Macciardi', 'V Arolt', 'D Blackwood', 'X Liu', 'B Sjögren', 'HN Aschauer', 'H-G Hwu', 'K Jang', 'WJ Livesley', 'JL Kennedy', 'T Zoega', 'O Ivarsson', 'M-T Bui', 'M-H Yu', 'B Havsteen', 'D Commenges', 'J Weissenbach', 'E Schwinger', 'II Gottesman', 'AJ Pakstis', 'L Wetterberg', 'KK Kidd', 'T Helgason'], abstract='Schizophrenia is thought to be a multifactorial disease with complex mode of inheritance1,2. Using a two-stage strategy for another complex disorder, a number of putative IDDM-susceptibility genes have recently been mapped3. We now report the results of a two-stage genome-wide search for genes conferring susceptibility to schizophrenia. In stage I, model-free linkage analyses of large pedigrees from Iceland, a geographical isolate, revealed 26 loci suggestive of linkage. In stage II, ten of these were followed-up in a second international collaborative study comprising families from Austria, Canada, Germany, Italy, Scotland, Sweden, Taiwan and the United States. Potential linkage findings of stage I on chromosomes 6p, 9 and 20 were observed again in the second sample. Furthermore, in a third sample from China, fine mapping of the 6p region by association studies also showed evidence for linkage or linkage\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym='The Journal of infection', publisher=None, query_handler=None),\n",
       " 'Particulate and trace gas emissions from open burning of wheat straw and corn stover in China': Paper(DOI='10.1021/es0705137', crossref_json=None, google_schorlar_metadata=None, title='Particulate and trace gas emissions from open burning of wheat straw and corn stover in China', authors=['Xinghua Li', 'Shuxiao Wang', 'Lei Duan', 'Jiming Hao', 'Chao Li', 'Yaosheng Chen', 'Liu Yang'], abstract='Field measurements were conducted to determine particulate emissions and trace gas emissions, including CO2, CO, CH4, NMHCs, NOx, NH3, N2O, and SO2, from open burning of wheat straw and maize stover, two major agricultural residues in China. The headfire ignition technique was adopted, and sampling was performed downwind from the agricultural fire. Particulate matter (PM) and gas emission factors were determined using the carbon mass-balance method. Particle mass size distributions show a prominent accumulation mode peak at 0.26−0.38 μm. Submicron particles dominate PM emissions. Most measured chemical species measured show a similar size distribution as PM. Chemical composition analysis indicates that PM2.5 is largely composed of carbon, K, and Cl. PM2.5 emission factors of wheat straw and maize stover are 7.6 ± 4.1 g/kg and 11.7 ± 1.0 g/kg, respectively. It also indicates that 12.1\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1021/es00003a746', '10.1029/2000GB001382', '10.1016/j.atmosenv.2003.11.037', '10.1016/j.atmosenv.2005.11.018', '10.1016/0004-6981(77)90042-7', '10.1080/00022470.1980.10465167', '10.1080/02786820119445', '10.1126/science.220.4602.1148', '10.1021/ef970031o', '10.1021/ef000104v', '10.1007/BF00708186', '10.1007/BF00708189', '10.1007/BF00694373'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym='Environmental science & technology', publisher=None, query_handler=None),\n",
       " 'Concentration, sources and ozone formation potential of volatile organic compounds (VOCs) during ozone episode in Beijing': Paper(DOI='10.1016/j.atmosres.2007.09.004', crossref_json=None, google_schorlar_metadata=None, title='Concentration, sources and ozone formation potential of volatile organic compounds (VOCs) during ozone episode in Beijing', authors=['Jingchun Duan', 'Jihua Tan', 'Liu Yang', 'Shan Wu', 'Jimin Hao'], abstract='Concentrations of carbonyl compounds and non-methane hydrocarbons (NMHCs) were measured at an urban site in Beijing from 16 to 19 August 2006 during an ozone episode. Of the six days monitored there were four days of which the 1-h maximum ozone concentration exceeding 160μg m−3 the Ambient Air Quality Standard (GB 3095—1996). Measurements of a variety of trace gases (O3, NOx, CO, volatile organic compounds (VOCs)) were carried out simultaneously. Principal component analysis/absolute principal component scores (PCA/APCS) was used to identify the dominant emission sources and evaluate their contribution to NMHCs and carbonyls. The possible sources for NMHCs in Beijing are combustion sources, solvent usage and biogenic sources, and for carbonyls are vehicle emission, cooking and biogenic sources. The ratios of NMHCs/NOx and Carbonyls/NOx are 11.8±3.9 and 2.7±0.8, which\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1029/2000GB001382', '10.1021/cr0206420', '10.1016/0269-7491(94)00059-M', '10.1016/S1352-2310(02)00302-3', '10.1016/j.atmosenv.2005.06.029', '10.1016/0004-6981(87)90341-6', '10.1016/S1352-2310(96)00226-9', '10.1080/1073161X.1994.10467290', '10.1016/S1352-2310(01)00405-8', '10.1016/S1352-2310(02)00097-3', '10.1016/j.chemosphere.2004.12.035', '10.1016/j.atmosenv.2004.10.009', '10.1021/es950592z', '10.1021/es950758w', '10.1021/es0111232', '10.1016/j.atmosenv.2004.05.004', '10.1016/S1352-2310(01)00570-2', '10.1016/0960-1686(90)90052-O', '10.1016/j.atmosenv.2003.11.026', '10.1021/es960535l', '10.1016/j.atmosenv.2006.05.044', '10.1016/1352-2310(96)00110-0', '10.1016/0960-1686(91)90280-K', '10.1016/S1352-2310(98)00345-8', '10.1021/es00002a014', '10.1016/j.scitotenv.2004.01.029', '10.1016/j.atmosenv.2006.08.046', '10.1016/S1352-2310(03)00177-8', '10.1016/S1352-2310(02)00640-4', '10.1016/S1352-2310(03)00462-X', '10.1016/0960-1686(90)90126-8', '10.1021/es9812406', '10.1021/es00050a020'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym='Atmospheric research (Print)', publisher=None, query_handler=None),\n",
       " 'Copper‐catalyzed direct C arylation of heterocycles with aryl bromides: discovery of fluorescent core frameworks': Paper(DOI='10.1002/chin.200935107', crossref_json=None, google_schorlar_metadata=None, title='Copper‐catalyzed direct C arylation of heterocycles with aryl bromides: discovery of fluorescent core frameworks', authors=['Dongbing Zhao', 'Wenhai Wang', 'Fei Yang', 'Jingbo Lan', 'Li Yang', 'Ge Gao', 'Jingsong You'], abstract=' A window of opportunity: A general copper‐catalyzed C\\uf8ffH bond‐activation path allows arylation of heterocycles with a wide range of aryl bromides (see scheme). The reaction shows excellent regioselectivity and exhibits good functional group tolerance. The 8‐aryl xanthines exhibit fluorescence in a variety of solvents and show promise as reagents for biological imaging.       ', conference=None, journal=None, year=None, reference_list=['10.1002/anie.200900413'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym='Chem Inform', publisher=None, query_handler=None),\n",
       " 'A comparison of semi-active damping control strategies for vibration isolation of harmonic disturbances': Paper(DOI='10.1016/j.jsv.2003.11.048', crossref_json=None, google_schorlar_metadata=None, title='A comparison of semi-active damping control strategies for vibration isolation of harmonic disturbances', authors=['Y Liu', 'TP Waters', 'MJ2115547 Brennan'], abstract='Active vibration isolation systems are less commonly used than passive systems due to their associated cost and power requirements. In principle, semi-active isolation systems can deliver the versatility, adaptability and higher performance of fully active systems for a fraction of the power consumption. Various semi-active control algorithms have been suggested in the past, many of which are of the “on–off” variety. This paper studies the vibration isolation characteristics of four established semi-active damping control strategies, which are based on skyhook control and balance control. A semi-active damper is incorporated into a single-degree-of-freedom (s.d.o.f.) system model subject to base excitation. Its performance is evaluated in terms of the root-mean-square (r.m.s.) acceleration transmissibility, and is compared with those of a passive damper and an ideal skyhook damper. The results show that the semi\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1006/jsvi.1996.0037', '10.1115/1.3269279', '10.1115/1.3267444', '10.1109/CDC.1988.194694', '10.1006/jsvi.1996.0901', '10.1061/(ASCE)0733-9399(2000)126:8(795)', '10.1115/1.2896163', '10.1016/0022-460X(88)90404-X', '10.1080/00423118408968771'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym='Journal of sound and vibration', publisher=None, query_handler=None),\n",
       " 'Targeting apoptosis pathways in cancer and perspectives with natural compounds from mother nature': Paper(DOI='10.1158/1940-6207.capr-14-0136', crossref_json=None, google_schorlar_metadata=None, title='Targeting apoptosis pathways in cancer and perspectives with natural compounds from mother nature', authors=['Faya M Millimouno', 'Jia Dong', 'Liu Yang', 'Jiang Li', 'Xiaomeng Li'], abstract=' Although the incidences are increasing day after day, scientists and researchers taken individually or by research group are trying to fight against cancer by several ways and also by different approaches and techniques. Sesquiterpenes, flavonoids, alkaloids, diterpenoids, and polyphenolic represent a large and diverse group of naturally occurring compounds found in a variety of fruits, vegetables, and medicinal plants with various anticancer properties. In this review, our aim is to give our perspective on the current status of the natural compounds belonging to these groups and discuss their natural sources, their anticancer activity, their molecular targets, and their mechanism of actions with specific emphasis on apoptosis pathways, which may help the further design and conduct of preclinical and clinical trials. Unlike pharmaceutical drugs, the selected natural compounds induce apoptosis by targeting\\xa0…', conference=None, journal=None, year=None, reference_list=['10.3322/CA.2007.0010', '10.3322/caac.21153', '10.3322/caac.20073', '10.1093/ecam/nem009', '10.1016/S1471-4914(02)00002-3', '10.2533/chimia.2012.320', '10.1155/2010/370835', '10.1021/jf9038056', '10.1155/2009/830616', '10.1200/JCO.2008.20.6235', '10.1016/j.jep.2005.05.011', '10.1080/01926230701320337', '10.1038/nrd1657', '10.1016/j.jep.2005.06.006', '10.1016/j.jep.2009.06.023', '10.1016/S0092-8674(02)00625-6', '10.1016/S0092-8674(00)00003-9', '10.1126/science.281.5381.1305', '10.1080/01635589209514201', '10.1016/j.canlet.2005.06.034', '10.1038/35037710', '10.1146/annurev.pharmtox.38.1.539', '10.1097/CAD.0b013e328356cad9', '10.1038/nrc1455', '10.1038/sj.onc.1205842', '10.1126/science.287.5459.1765', '10.1007/s10495-009-0330-1', '10.1016/S0959-437X(99)00038-6', '10.1038/35091170', '10.1007/s10565-010-9176-0', '10.1158/1535-7163.MCT-07-0217', '10.1152/ajpgi.00159.2011', '10.1089/ars.2009.2440', '10.1158/1078-0432.CCR-07-1926', '10.3748/wjg.v10.i15.2205', '10.1159/000350098', '10.1016/S1995-7645(13)60041-3', '10.1182/blood-2007-05-090621', '10.18388/abp.2008_3035', '10.1158/1535-7163.MCT-08-0661', '10.4161/cbt.25944', '10.1155/2013/719858', '10.1016/j.fct.2012.06.014', '10.1002/iub.1141', '10.1155/2013/248532', '10.1016/j.ccr.2004.09.003', '10.1016/S1471-4914(02)02375-4', '10.1038/nrd2781', '10.1007/s12026-008-8025-1', '10.1038/nri2423', '10.1038/nrc780', '10.1172/JCI31537', '10.1016/j.cell.2007.03.029', '10.1172/JCI26322', '10.1002/jcp.20785', '10.1186/1742-2094-7-16', '10.1074/jbc.M110773200', '10.1007/s10753-011-9346-0', '10.1038/sj.onc.1207995', '10.1016/j.acthis.2013.11.005', '10.1016/j.ejphar.2014.02.031', '10.1248/bpb.34.580', '10.1186/bcr3477', '10.1016/j.bbagen.2012.12.015', '10.4196/kjpp.2010.14.6.353', '10.1016/j.tiv.2009.11.019', '10.1080/10715760701762407', '10.3109/10715762.2012.700515', '10.1016/j.intimp.2012.08.011', '10.1007/s10495-013-0854-2', '10.3390/molecules181013061', '10.1016/j.jep.2011.01.014', '10.1002/jcb.22230', '10.1016/j.intimp.2007.08.028', '10.1038/cr.2008.30', '10.1158/0008-5472.CAN-12-4297', '10.3858/emm.2010.42.11.082', '10.1007/s00011-007-7015-4', '10.1289/ehp.11464', '10.1089/jmf.2009.0072', '10.1002/ptr.2521', '10.1093/emboj/18.17.4657', '10.1016/S0092-8674(01)00607-9', '10.1172/JCI3785', '10.1371/journal.pone.0043711', '10.1074/jbc.M112.442970', '10.1128/MCB.16.8.4117', '10.1038/385544a0', '10.1007/s12272-013-0217-0', '10.1111/j.1745-7254.2007.00667.x', '10.7150/ijbs.8.1', '10.1007/s12031-013-9964-0', '10.3892/ijo.2011.1277', '10.1097/CJI.0b013e3181a8efe6', '10.1111/j.1745-7254.2008.00725.x', '10.1177/1087057111398486', '10.1111/j.1574-6968.2011.02397.x', '10.1055/s-2002-33799', '10.1080/10286020802240301', '10.1016/S0163-7258(01)00137-1', '10.4161/cc.7.21.6965', '10.1248/bpb.24.303', '10.3390/molecules18021418', '10.1007/s10753-013-9707-y', '10.1371/journal.pone.0086369', '10.1248/bpb.b12-00352', '10.1016/S0140-6736(94)92211-X', '10.1089/ars.2011.4489', '10.1016/j.drudis.2006.04.003', '10.1371/journal.pone.0087061', '10.1155/2013/271602', '10.1016/0378-4274(95)03474-9', '10.7150/ijbs.3753', '10.1002/iub.1068', '10.1016/j.tiv.2010.06.007', '10.1038/35077213', '10.2174/0929867033456477', '10.1186/1742-2094-10-15', '10.1016/j.etap.2009.03.005', '10.1196/annals.1397.052', '10.1093/abbs/gms013', '10.1016/j.ygyno.2011.08.031', '10.3390/molecules18089382', '10.3892/mmr.2013.1640', '10.1016/j.intimp.2013.01.004', '10.1038/aps.2011.92', '10.1254/jphs.12269FP', '10.1016/j.tiv.2012.02.004', '10.1080/10715760701499356', '10.1080/10715760802112791', '10.4161/cbt.26726', '10.2174/156652412800619978', '10.1002/bies.10329', '10.1038/nrd726', '10.1158/1535-7163.MCT-09-0133', '10.1023/B:DRUG.0000026256.38560.be', '10.1016/j.exphem.2013.04.012', '10.1371/journal.pone.0021573', '10.2174/156652412803833508', '10.1021/jf400542m', '10.1016/j.bcp.2013.02.009', '10.4248/IJOS11014', '10.1016/j.bcp.2008.06.012', '10.1021/nn900785b', '10.1002/jssc.201200896', '10.1016/j.juro.2010.12.091', '10.1124/mol.112.078535', '10.1002/bmc.2724', '10.1007/s00277-008-0677-3', '10.3748/wjg.v17.i38.4298', '10.1038/aps.2010.178', '10.3892/ijo.2012.1582', '10.1080/10717540902738341', '10.1016/j.ejogrb.2008.02.023', '10.1002/pmic.200900649', '10.1016/j.ejphar.2008.06.026', '10.1111/j.1476-5381.2010.00804.x', '10.1002/jso.21936', '10.1002/jcp.22954', '10.3892/ijo.2012.1739', '10.1093/carcin/bgq186', '10.1007/s00011-010-0227-z', '10.3390/molecules15096466', '10.1016/j.fct.2012.12.026', '10.1155/2008/394802', '10.1248/bpb.32.1803', '10.1007/s11010-011-1164-z', '10.1038/bjp.2008.319', '10.1002/ptr.3553', '10.1186/1748-717X-6-56', '10.3390/molecules16086758', '10.4161/cbt.22952', '10.1111/j.1476-5381.2010.00749.x', '10.3892/mmr.2012.959', '10.1186/1756-9966-29-108', '10.1186/1471-2407-12-453', '10.1002/ijc.25587', '10.1158/0008-5472.CAN-09-4572', '10.1667/RR1394.1', '10.1111/j.1747-0285.2011.01257.x', '10.1007/s11033-012-2215-6', '10.1158/1541-7786.MCR-08-0410', '10.1111/j.1751-2980.2009.00382.x', '10.3892/ijo_00000782', '10.1007/s00432-011-1120-z', '10.1155/2012/957568', '10.1007/s00432-010-0784-0', '10.1038/aps.2009.20', '10.1080/10286020802573420', '10.1016/j.ejphar.2012.02.035', '10.1016/j.bcp.2012.05.014', '10.1002/ptr.4808', '10.1111/j.1745-7254.2007.00706.x', '10.3892/ijo.2012.1519', '10.4161/auto.5.3.7896', '10.1016/j.abb.2009.08.011', '10.1186/1471-2407-10-610', '10.4161/cbt.21460', '10.1371/journal.pone.0050753', '10.1254/jphs.08044FP', '10.1007/s00535-012-0612-1', '10.1016/j.biocel.2011.01.020', '10.1080/10286020701273866', '10.7150/ijbs.4554', '10.1111/j.1365-2184.2012.00849.x', '10.1016/j.colsurfb.2011.05.037', '10.3748/wjg.v18.i48.7166', '10.1142/S0192415X13500134', '10.1002/em.21712', '10.1016/j.canlet.2011.01.002', '10.1186/1476-4598-10-106', '10.1093/carcin/bgr075', '10.1016/j.niox.2008.04.017', '10.1038/sj.cdd.4401325', '10.1002/jcb.24220', '10.1016/j.brainres.2009.03.063', '10.1158/1078-0432.CCR-09-0298', '10.1002/ptr.3392', '10.1002/jcb.22716', '10.3390/molecules14051852', '10.1016/j.phymed.2012.02.003', '10.1016/j.canlet.2012.09.019', '10.1097/CAD.0b013e32833d26a9'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym='Cancer prevention research (Philadelphia, Pa.)', publisher=None, query_handler=None),\n",
       " 'The impact of transportation control measures on emission reductions during the 2008 Olympic Games in Beijing, China': Paper(DOI='10.1016/j.atmosenv.2009.10.040', crossref_json=None, google_schorlar_metadata=None, title='The impact of transportation control measures on emission reductions during the 2008 Olympic Games in Beijing, China', authors=['Yu Zhou', 'Ye Wu', 'Liu Yang', 'Lixin Fu', 'Kebin He', 'Shuxiao Wang', 'Jiming Hao', 'Jinchuan Chen', 'Chunyan Li'], abstract='Traffic congestion and air pollution were two major challenges for the planners of the 2008 Olympic Games in Beijing. The Beijing municipal government implemented a package of temporary transportation control measures during the event. In this paper, we report the results of a recent research project that investigated the effects of these measures on urban motor vehicle emissions in Beijing. Bottom–up methodology has been used to develop grid-based emission inventories with micro-scale vehicle activities and speed-dependent emission factors. The urban traffic emissions of volatile organic compounds (VOC), carbon monoxide (CO), nitrogen oxides (NOx) and particulate matter with an aerodynamic diameter of 10\\xa0μm or less (PM10) during the 2008 Olympics were reduced by 55.5%, 56.8%, 45.7% and 51.6%, respectively, as compared to the grid-based emission inventory before the Olympics. Emission\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1001/jama.285.7.897', '10.1080/10473289.2001.10464300', '10.1016/S1352-2310(02)00923-8', '10.1016/S1352-2310(99)00324-6', '10.1081/ESE-100102619', '10.1080/10473289.2005.10464726', '10.1146/annurev.energy.27.122001.083421', '10.1016/j.atmosenv.2004.11.044', '10.3155/1047-3289.57.8.968', '10.1016/j.atmosres.2008.02.005', '10.1016/S0048-9697(99)00196-5', '10.1016/j.atmosenv.2006.08.046', '10.1111/j.1477-8947.2007.00136.x', '10.1016/j.scitotenv.2008.11.008', '10.1016/j.atmosenv.2008.09.042', '10.3155/1047-3289.57.11.1379', '10.3155/1047-3289.57.9.1071'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym='Atmospheric environment (1994)', publisher=None, query_handler=None),\n",
       " 'Determination of oxidase enzyme substrates using cross‐flow thin‐layer amperometry': Paper(DOI='10.1002/elan.1140080803', crossref_json=None, google_schorlar_metadata=None, title='Determination of oxidase enzyme substrates using cross‐flow thin‐layer amperometry', authors=['Liu Yang', 'Peter T Kissinger'], abstract=' Thin‐layer cross‐flow amperometric detectors with enzyme modified working electrodes were developed for the determination of oxidase substrates in flowing streams. The enzyme was covalently immobilized in an osmium redox polymer film on the electrode surface. Electrodes modified with glucose oxidase or lactate oxidase were prepared for the continuous monitoring of glucose or lactate. A peroxidase electrode coupled with oxidase enzyme reactors was also explored for the determination of glucose, lactate, and acetylcholine/choline in continuous monitoring systems or following liquid chromatography. The effects of interferences, oxygen, mass transfer limiting membrane, and perfusate composition on electrode performance are discussed.', conference=None, journal=None, year=None, reference_list=['10.1016/S0378-4347(00)82938-2', '10.1002/elan.1140040403', '10.1016/0378-4347(94)00504-X', '10.1016/0021-9673(94)85109-3', '10.1016/0039-9140(92)80292-L', '10.1021/ac00104a005', '10.1016/0378-4347(95)00181-6', '10.1021/j100168a046', '10.1021/ac00087a008', '10.1002/elan.1140070704', '10.1021/ac00089a026', '10.1021/ac00103a015', '10.1002/elan.1140050917'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym='Electroanalysis (New York, N.Y.)', publisher=None, query_handler=None),\n",
       " 'A luminescent 2D coordination polymer for selective sensing of nitrobenzene': Paper(DOI='10.1039/c3dt51450a', crossref_json=None, google_schorlar_metadata=None, title='A luminescent 2D coordination polymer for selective sensing of nitrobenzene', authors=['Guan-Yao Wang', 'Le-Le Yang', 'Yue Li', 'Han Song', 'Wen-Juan Ruan', 'Ze Chang', 'Xian-He Bu'], abstract='A luminescent two-dimensional (2D) coordination polymer is demonstrated to be a selective sensing material for the straightforward detection of nitrobenzene via a redox fluorescence quenching mechanism.', conference=None, journal=None, year=None, reference_list=['10.1520/JFS14554J', '10.1021/cr9801014', '10.1039/b517953j', '10.1039/b809631g', '10.1063/1.1771493', '10.1002/(SICI)1097-0231(199602)10:3<287::AID-RCM429>3.0.CO;2-H', '10.1038/28728', '10.1038/nature03438', '10.1021/cr0501339', '10.1021/cr200324t', '10.1021/ol801030u', '10.1039/C2TA00635A', '10.1021/om701082y', '10.1021/ja982293q', '10.1021/ja9742996', '10.1039/c1cc13925h', '10.1021/ar9600502', '10.3109/10408448209089848', '10.1021/es010663w', '10.1021/tx970166m', '10.1016/S0043-1354(03)00436-6', '10.1002/anie.200804853', '10.1039/c0cc01236j', '10.1039/c3dt32851a', '10.1039/c3ta01118f', '10.1039/c0cc05166g', '10.1039/c2jm32066e', '10.1002/anie.201208885', '10.1021/ja106851d', '10.1039/c1cc15594f', '10.1039/c3dt00055a', '10.1002/adma.200601838', '10.1039/c0ce00135j', '10.1039/c1cc10897b', '10.1039/c2jm16344f', '10.1039/c2jm35273g', '10.1107/S0021889802022112', '10.1111/j.1556-4029.2006.00332.x', '10.1021/ja021214e'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym='Dalton transactions (2003. Print)', publisher=None, query_handler=None),\n",
       " 'The rat model of type 2 diabetic mellitus and its glycometabolism characters': Paper(DOI='10.1538/expanim.52.401', crossref_json=None, google_schorlar_metadata=None, title='The rat model of type 2 diabetic mellitus and its glycometabolism characters', authors=['Fanglin Zhang', 'Chuanzhong Ye', 'Guo Li', 'Wei Ding', 'Wenzhong Zhou', 'Hongda Zhu', 'Gang Chen', 'Tianhong Luo', 'Min Guang', 'Youping Liu', 'Di Zhang', 'Sheng Zheng', 'Jialin Yang', 'Yanyun Gu', 'Xiaoyan Xie', 'Min Luo'], abstract='To develop a rat model of type 2 diabetic mellitus that simulated the common manifestation of the metabolic abnormalities and resembled the natural history of a certain type 2 diabetes in human population, male Sprague-Dawley rats (4 months old) were injected with low-dose (15 mg/kg) STZ after high fat diet (30% of calories as fat) for two months (LSTZ/2HF). The functional and histochemical changes in the pancreatic islets were examined. Insulin-glucose tolerance test, islet immunohistochemistry and other corresponding tests were performed and the data in L-STZ/2HF group were compared with that of other groups, such as the model of type 1 diabetes (given 50 mg/kg STZ) and the model of obesity (high fat diet). The body weight of rats in the group of rats given 15 mg/kg STZ after high fat diet for two months increased significantly more than that of rats in the group of rats given 50 mg/kg STZ (the model of type 1 diabetes)(595±33 g vs. 352±32 g, p< 0.05). Fast blood glucose levels for L-STZ/2HF group were 16.92±1.68 mmol/l, versus 5.17±0.55 mmol/l in normal control and 5.59±0.61 mmol/l in rats given high fat diet only. Corresponding values for fast serum insulin were 0.66±0.15 ng/ml, 0.52±0.13 ng/ml, 0.29±0.11 ng/ml, respectively. Rats of type 2 diabetes (L-STZ/2HF) had elevated levels of triglyceride (TG, 3.82±0.88 mmol/l), and cholesterol (Ch, 2.38±0.55 mmol/l) compared with control (0.95±0.15 mmol/l and 1.31±0.3 mmol/l, respectively)(p< 0.05). The islet morphology as examined by immunocytochemistry using insulin antibodies in the L-STZ/2HF group was affected and quantitative analysis showed the islet insulin content was higher\\xa0…', conference=None, journal=None, year=None, reference_list=['10.2337/diabetes.51.10.2903', '10.1053/meta.2002.35195', '10.1007/s001250100029', '10.1053/meta.2000.17721', '10.2337/diab.33.9.901', '10.2337/diacare.23.8.1108', '10.1016/S0002-9343(00)00337-5', '10.1002/(SICI)1099-0895(199812)14:4<263::AID-DMR233>3.0.CO;2-C', '10.1016/S0002-9149(98)00848-0', '10.1079/PNS19990026', '10.1007/BF02374470', '10.1152/ajpendo.2001.280.5.E712', '10.1016/0168-8227(95)01070-T', '10.1056/NEJM199201023260104'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'A combined analysis of D22S278 marker alleles in affected sib‐pairs: support for a susceptibility locus for schizophrenia at chromosome 22q12': Paper(DOI='10.1016/s0920-9964(98)00048-6', crossref_json=None, google_schorlar_metadata=None, title='A combined analysis of D22S278 marker alleles in affected sib‐pairs: support for a susceptibility locus for schizophrenia at chromosome 22q12', authors=['Michael Gill', 'Homero Vallada', 'David Collier', 'Pak Sham', 'Peter Holmans', 'Robin Murray', 'Peter McGuffin', 'Shin Nanko', 'Mike Owen', 'Stylianos Antonarakis', 'David Housman', 'Haig Kazazian', 'Gerald Nestadt', 'Ann E Pulver', 'Richard E Straub', 'Charles J MacLean', 'Dermot Walsh', 'Kenneth S Kendler', 'Lynn DeLisi', 'Mihael Polymeropoulos', 'Hilary Coon', 'William Byerley', 'Ray Lofthouse', 'Elliot Gershon', 'Lynn Golden', 'Timothy Crow', 'William Byerley', 'Robert Freedman', 'Claudine Laurent', 'Sylvie Bodeau‐Pean', \"Thierry d'Amato\", 'Maurice Jay', 'Dominique Campion', 'Jacques Mallet', 'Dieter B Wildenauer', 'Bernard Lerer', 'Margot Albus', 'Manfred Ackenheil', 'Richard P Ebstein', 'Joachim Hallmayer', 'Wolfgang Maier', 'Hugh Gurling', 'David Curtis', 'Gusharon Kalsi', 'Jon Brynjolfsson', 'Thordur Sigmundson', 'Hannes Petursson', 'Douglas Blackwood', 'Walter Muir', 'David St. Clair', 'Lin He', 'Susan Maguire', 'Hans W Moises', 'Hai‐Gwo Hwu', 'Liu Yang', 'Claudia Wiese', 'Li Tao', 'Xiehe Liu', 'Helgi Kristbjarnason', 'Douglas F Levinson', 'Bryan J Mowry', 'Helen Donis‐Keller', 'Nicholas K Hayward', 'Raymond R Crowe', 'Jeremy M Silverman', 'Derek J Nancarrow', 'Christina M Read'], abstract='Several groups have reported weak evidence for linkage between schizophrenia and genetic markers located on chromosome 22q using the lod score method of analysis. However these findings involved different genetic markers and methods of analysis, and so were not directly comparable. To resolve this issue we have performed a combined analysis of genotypic data from the marker D22S278 in multiply affected schizophrenic families derived from 11 independent research groups worldwide. This marker was chosen because it showed maximum evidence for linkage in three independent datasets (Vallada et al., Am J Med Genet 60: 139-146, 1995; Polymeropoulos et al., Neuropsychiatr Genet 54: 93-99, 1994; Lasseter et al., Am J Med Genet, 60: 172-173, 1995). Using the affected sib-pair method as implemented by the program ESPA, the combined dataset showed 252 alleles shared compared with 188\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym='Schizophrenia research (Print)', publisher=None, query_handler=None),\n",
       " 'LincRNA-Gm4419 knockdown ameliorates NF-κB/NLRP3 inflammasome-mediated inflammation in diabetic nephropathy': Paper(DOI='10.1038/cddis.2016.451', crossref_json=None, google_schorlar_metadata=None, title='LincRNA-Gm4419 knockdown ameliorates NF-κB/NLRP3 inflammasome-mediated inflammation in diabetic nephropathy', authors=['Hong Yi', 'Rui Peng', 'Lu-yu Zhang', 'Yan Sun', 'Hui-min Peng', 'Han-deng Liu', 'Li-juan Yu', 'Ai-ling Li', 'Ya-juan Zhang', 'Wen-hao Jiang', 'Zheng Zhang'], abstract='Diabetic nephropathy (DN) as the primary cause of end-stage kidney disease is a common complication of diabetes. Recent researches have shown the activation of nuclear factor kappa light-chain enhancer of activated B cells (NF-κB) and NACHT, LRR and PYD domain-containing protein 3 (NLRP3) inflammasome are associated with inflammation in the progression of DN, but the exact mechanism is unclear. Long noncoding RNAs (lncRNAs) have roles in the development of many diseases including DN. However, the relationship between lncRNAs and inflammation in DN remains largely unknown. Our previous study has revealed that 14 lncRNAs are abnormally expressed in DN by RNA sequencing and real-time quantitative PCR (qRT-PCR) in the renal tissues of db/db DN mice. In this study, these lncRNAs were verified their expressions by qRT-PCR in mesangial cells (MCs) cultured under high-and low\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1056/NEJMe1313104', '10.1111/j.1755-5922.2010.00218.x', '10.1016/j.mce.2016.06.004', '10.3109/10799893.2015.1061002', '10.1038/ki.2015.250', '10.1016/j.intimp.2016.06.024', '10.1042/CS20120198', '10.1073/pnas.1200081109', '10.1172/JCI28804', '10.1016/j.mce.2015.09.024', '10.1016/j.coi.2007.09.002', '10.1016/j.bbamcr.2014.07.001', '10.1038/ki.2014.271', '10.1038/ki.2014.322', '10.1007/s00125-013-3115-6', '10.1016/j.lfs.2016.06.002', '10.1002/bies.200900112', '10.1093/hmg/ddq362', '10.1038/431915a', '10.1016/j.cell.2009.02.006', '10.1101/gr.132159.111', '10.1016/j.ygeno.2008.11.009', '10.1101/gr.6036807', '10.1038/cddis.2014.466', '10.1016/j.mce.2015.09.024', '10.1007/s12020-016-0950-5', '10.1016/j.mce.2016.02.020', '10.1016/j.bbrc.2015.11.023', '10.1371/journal.pone.0077468', '10.1371/journal.pone.0018671', '10.2337/db07-0675', '10.2337/db06-1072', '10.4103/1319-2442.132190', '10.18632/oncotarget.10521', '10.1016/j.ccell.2015.02.004', '10.1126/science.1192002', '10.1038/nature05519', '10.1038/8432', '10.1155/2015/948417', '10.1016/j.etap.2015.01.019', '10.1038/ni.1831', '10.1038/ki.2014.322', '10.1186/s13567-016-0359-4', '10.1159/000453208', '10.1016/j.biopha.2016.04.043', '10.1016/j.celrep.2016.06.103', '10.1016/j.lfs.2016.06.002', '10.1371/journal.pone.0104771', '10.1016/j.febslet.2012.02.045', '10.1128/MCB.14.6.3981', '10.1146/annurev.immunol.14.1.649', '10.1128/MCB.12.10.4412', '10.1074/jbc.M116.719302', '10.1111/j.1600-065X.2012.01097.x', '10.1177/1932296816636894', '10.1016/j.ejca.2012.03.023', '10.1016/j.bbadis.2012.10.001', '10.1002/ijc.24526', '10.1038/mt.2015.31', '10.1016/j.gene.2016.04.041', '10.1093/nar/gkv1176', '10.1038/nprot.2011.409', '10.1093/nar/gkh383', '10.3892/or.2016.4984', '10.1074/jbc.M115.678326', '10.1182/blood-2014-09-601542', '10.1016/j.trsl.2013.10.002', '10.3748/wjg.v11.i7.990', '10.1097/SHK.0000000000000563'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Thermal environment and sleep quality: A review': Paper(DOI='10.1016/j.enbenv.2023.06.010', crossref_json=None, google_schorlar_metadata=None, title='Thermal environment and sleep quality: A review', authors=['Li Lan', 'K Tsuzuki', 'YF Liu', 'ZW Lian'], abstract='Thermal environment in bedrooms is still a largely neglected topic in thermal comfort research, although a thermal comfortable environment is important for sleep maintenance. Studies confirm that human body is sensitive to air temperature during sleep; even moderate heat or cold exposure decrease sleep quality significantly. In the present paper we reviewed air temperatures measured in bedroom and the effects of heat and cold exposure on sleep quality, and then proposed 5 aspects of approaches or technologies that could improve sleeping thermal environment at a low energy consumption. We concluded that there are two important research topics in sleeping thermal environment. One is to develop sleeping-mode control strategy for air conditioner used in bedroom to get slight increase or to avoid decrease in room air temperature when approaching morning. The other is to control bed micro-environment\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1186/1471-2148-9-8', '10.1016/j.enbuild.2017.05.043', '10.1016/j.jtherbio.2018.09.012', '10.5664/jcsm.6576', '10.1007/s12273-022-0972-2', '10.1378/chest.08-0812', '10.1016/j.enbuild.2020.110392', '10.1038/nature04283', '10.1016/j.enbuild.2022.112137', '10.1016/j.buildenv.2020.107406', '10.1016/j.enbuild.2018.05.034', '10.1111/ina.13034', '10.1016/j.buildenv.2022.109369', '10.1016/j.enbuild.2018.10.040', '10.1016/j.enbuild.2020.110097', '10.1016/j.buildenv.2021.108161', '10.1016/j.jtherbio.2017.10.017', '10.1016/j.buildenv.2023.110008', '10.1007/s00484-011-0424-7', '10.1081/CBI-100101050', '10.1016/j.jtherbio.2008.02.008', '10.1016/j.enbuild.2010.07.030', '10.1016/j.jtherbio.2013.03.001', '10.1186/1880-6805-31-14', '10.1016/j.buildenv.2006.11.026', '10.1016/j.buildenv.2011.12.020', '10.1016/j.buildenv.2018.02.004', '10.1016/j.buildenv.2014.09.024', '10.1177/1420326X11425967', '10.1016/j.buildenv.2013.11.024', '10.1016/j.buildenv.2018.11.035', '10.1016/j.buildenv.2019.106585', '10.1016/j.enbuild.2016.08.089', '10.1016/j.buildenv.2013.05.018', '10.1016/j.buildenv.2018.05.061', '10.1016/j.buildenv.2022.109478', '10.1016/j.enbuild.2019.109562', '10.1016/j.sleep.2015.02.1377', '10.1016/j.jtherbio.2004.08.024', '10.1016/j.jtherbio.2005.11.027', '10.1016/j.jtherbio.2014.10.003', '10.1016/j.buildenv.2022.109888', '10.1016/S0306-4565(97)00041-7', '10.1016/S0304-3940(96)12936-0', '10.1016/j.buildenv.2021.108666', '10.1016/j.jtherbio.2003.10.003', '10.1016/j.jtherbio.2018.12.016', '10.1093/sleep/22.3.313', '10.1016/j.enbuild.2016.09.020', '10.1016/S1087-0792(03)00023-6', '10.1038/43366', '10.1016/B978-1-4160-6645-3.00028-1', '10.1046/j.1365-2869.1998.00112.x', '10.1152/jappl.1977.42.1.50', '10.1007/s10286-002-0043-9', '10.1053/smrv.2001.0177', '10.1016/j.enbuild.2021.111462', '10.1016/j.buildenv.2021.108252', '10.1016/j.buildenv.2018.04.007', '10.1016/j.enbuild.2016.08.001', '10.1016/j.buildenv.2016.03.030', '10.1093/sleep/30.6.797', '10.1016/j.physbeh.2004.09.009', '10.1016/j.enbuild.2016.12.066', '10.1016/j.buildenv.2018.05.040', '10.1201/9781439824351.ch16', '10.1016/S0893-133X(01)00315-3', '10.1016/j.buildenv.2018.06.052', '10.1016/j.apergo.2020.103249', '10.1016/j.buildenv.2006.11.036', '10.1016/j.buildenv.2015.04.015', '10.1016/j.buildenv.2020.107108', '10.1016/j.buildenv.2019.01.058', '10.1111/ina.13122', '10.1016/j.jtherbio.2019.102480', '10.1111/ina.12599', '10.1016/j.enbuild.2019.109687', '10.1016/j.enbuild.2012.11.028', '10.1016/j.jtherbio.2022.103357', '10.1016/j.smrv.2020.101276', '10.1016/j.diabet.2020.07.001', '10.1016/0924-977X(96)87845-X', '10.1016/j.jtherbio.2022.103401', '10.1016/j.jtherbio.2021.102977', '10.1016/j.sleep.2013.11.791', '10.1111/ina.12748', '10.1016/j.buildenv.2021.108096', '10.1016/j.buildenv.2021.108060', '10.1016/j.buildenv.2022.109417', '10.1016/j.enbuild.2021.111174', '10.1016/j.proeng.2017.10.333', '10.1016/j.csite.2023.102826', '10.1111/ina.12861', '10.1016/j.buildenv.2022.109788', '10.1016/S0140-6736(01)05344-2', '10.1016/j.physbeh.2006.09.005', '10.1016/j.physbeh.2018.06.026', '10.1016/j.enbuild.2020.109789', '10.1016/j.enbuild.2019.04.003', '10.1016/j.buildenv.2017.05.008', '10.1016/j.buildenv.2023.109981', '10.1016/j.pmedr.2018.06.013', '10.1016/j.buildenv.2016.04.016', '10.1038/nrn1141', '10.1016/j.esd.2022.10.006', '10.1016/j.enbuild.2017.06.052', '10.1016/j.buildenv.2012.08.010'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Clinical validation of SPECT attenuation correction using x-ray computed tomography–derived attenuation maps: multicenter clinical trial with angiographic correlation': Paper(DOI='10.1016/j.nuclcard.2005.08.006', crossref_json=None, google_schorlar_metadata=None, title='Clinical validation of SPECT attenuation correction using x-ray computed tomography–derived attenuation maps: multicenter clinical trial with angiographic correlation', authors=['Yasmin Masood', 'Yi-Hwa Liu', 'Gordon DePuey', 'Raymond Taillefer', 'Luis I Araujo', 'Steven Allen', 'Dominique Delbeke', 'Frank Anstett', 'Aharon Peretz', 'Mary-Jo Zito', 'Vera Tsatkin', 'J Frans'], abstract='BACKGROUND: Nonuniform attenuation artifacts cause suboptimal specificity of stress single photon emission computed tomography (SPECT) myocardial perfusion images. In phantoms, normal subjects, and patients suspected of having coronary artery disease (CAD), we evaluated a new hybrid attenuation correction (AC) system that combines x-ray computed tomography (CT) with conventional stress SPECT imaging. METHODS AND RESULTS: The effect of CT-based AC was evaluated in phantoms by assessing homogeneity of normal cardiac inserts. AC improved homogeneity of normal cardiac phantoms from 11% ± 2% to 5% ± 1% (P < .001). Attenuation-corrected normal patient files were created from 37 normal subjects with a low likelihood (<3%) of CAD. The diagnostic performance of AC for detection of CAD was evaluated in 118 patients who had stress technetium 99m sestamibi or tetrofosmin stress\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Molucular biology'], conference_acronym='Journal of nuclear cardiology', publisher=None, query_handler=None),\n",
       " 'Improving the fisher kernel for large-scale image classification': Paper(DOI='10.1007/978-3-642-15561-1_11', crossref_json=None, google_schorlar_metadata=None, title='Improving the fisher kernel for large-scale image classification', authors=['Florent Perronnin', 'Jorge Sánchez', 'Thomas Mensink'], abstract=' The Fisher kernel (FK) is a generic framework which combines the benefits of generative and discriminative approaches. In the context of image classification the FK was shown to extend the popular bag-of-visual-words (BOV) by going beyond count statistics. However, in practice, this enriched representation has not yet shown its superiority over the BOV. In the first part we show that with several well-motivated modifications over the original framework we can boost the accuracy of the FK. On PASCAL VOC 2007 we increase the Average Precision (AP) from 47.9% to 58.3%. Similarly, we demonstrate state-of-the-art accuracy on CalTech 256. A major advantage is that these results are obtained using only SIFT descriptors and costless linear classifiers. Equipped with this representation, we can now explore image classification on a larger scale. In the second part, as an application, we compare two\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s11263-006-9794-4', '10.1109/ICCVW.2009.5457703', '10.1007/s11263-009-0275-4', '10.1145/1150402.1150429', '10.1145/1273496.1273598', '10.1109/ICCV.2009.5459203', '10.1109/CVPR.2010.5539914', '10.1109/CVPR.2010.5539949', '10.1109/CVPR.2008.4587598', '10.1109/CVPR.2007.383266', '10.1109/CVPR.2009.5206848', '10.1023/B:VISI.0000029664.99615.94', '10.1109/CVPRW.2009.5206663', '10.1109/ICCV.2009.5459257', '10.1109/TPAMI.2008.128'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'artificial intelligence'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Image classification with the fisher vector: Theory and practice': Paper(DOI='10.1007/s11263-013-0636-x', crossref_json=None, google_schorlar_metadata=None, title='Image classification with the fisher vector: Theory and practice', authors=['Jorge Sánchez', 'Florent Perronnin', 'Thomas Mensink', 'Jakob Verbeek'], abstract=' A standard approach to describe an image for classification and retrieval purposes is to extract a set of local patch descriptors, encode them into a high dimensional vector and pool them into an image-level signature. The most common patch encoding strategy consists in quantizing the local descriptors into a finite set of prototypical elements. This leads to the popular Bag-of-Visual words representation. In this work, we propose to use the Fisher Kernel framework as an alternative patch encoding strategy: we describe patches by their deviation from an “universal” generative Gaussian mixture model. This representation, which we call Fisher vector has many advantages: it is efficient to compute, it leads to excellent results even with efficient linear classifiers, and it can be compressed with a minimal loss of accuracy using product quantization. We report experimental results on five standard datasets\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2012.6248040', '10.1162/neco.1995.7.1.108', '10.1109/CVPR.2008.4587598', '10.1109/CVPR.2010.5539963', '10.1109/ICCV.2011.6126555', '10.1109/72.80298', '10.5244/C.25.76', '10.1109/CVPR.2012.6247926', '10.1109/CVPR.2009.5206848', '10.1007/978-3-642-15555-0_6', '10.1007/s11263-009-0275-4', '10.1109/CVPR.2011.5995370', '10.1109/ICCV.2009.5459169', '10.1109/18.720541', '10.1109/CVPR.2010.5540120', '10.1109/ICCV.2009.5459257', '10.1109/CVPR.2009.5206609', '10.1109/CVPR.2010.5540039', '10.1109/TPAMI.2010.57', '10.1109/TPAMI.2011.235', '10.1109/ICCV.2011.6126406', '10.1109/CVPR.2011.5995701', '10.1109/CVPR.2006.68', '10.1109/CVPR.2011.5995477', '10.1023/B:VISI.0000029664.99615.94', '10.1109/ICCV.2009.5459203', '10.1109/CVPR.2008.4587630', '10.1007/978-3-642-33709-3_35', '10.1109/CVPR.2007.383266', '10.1007/11744085_36', '10.1109/CVPR.2010.5540009', '10.1109/CVPR.2010.5539914', '10.1007/978-3-642-15561-1_11', '10.1109/CVPR.2012.6248090', '10.1109/TASSP.1984.1164346', '10.1109/CVPR.2011.5995504', '10.1016/j.patrec.2012.07.019', '10.1145/1273496.1273598', '10.1109/ICCV.2003.1238663', '10.1090/S0002-9939-97-03900-2', '10.1214/ECP.v12-1294', '10.1109/CVPR.2011.5995347', '10.1109/CVPRW.2009.5206663', '10.1109/TPAMI.2009.154', '10.1109/TPAMI.2009.132', '10.1109/CVPR.2010.5539949', '10.1109/CVPR.2012.6247943', '10.1109/ICCV.2003.1238351', '10.1109/ICCV.2009.5459167', '10.1109/CVPR.2010.5540018', '10.1109/ICCV.2005.171', '10.1109/CVPR.2010.5539970', '10.1007/s11263-006-9794-4', '10.1007/978-3-642-15555-0_11'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'artificial intelligence'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Distance-based image classification: Generalizing to new classes at near-zero cost': Paper(DOI='10.1109/tpami.2013.83', crossref_json=None, google_schorlar_metadata=None, title='Distance-based image classification: Generalizing to new classes at near-zero cost', authors=['Thomas Mensink', 'Jakob Verbeek', 'Florent Perronnin', 'Gabriela Csurka'], abstract='We study large-scale image classification methods that can incorporate new classes and training images continuously over time at negligible cost. To this end, we consider two distance-based classifiers, the k-nearest neighbor (k-NN) and nearest class mean (NCM) classifiers, and introduce a new metric learning approach for the latter. We also introduce an extension of the NCM classifier to allow for richer class representations. Experiments on the ImageNet 2010 challenge dataset, which contains over 10 6  training images of 1,000 classes, show that, surprisingly, the NCM classifier compares favorably to the more flexible k-NN classifier. Moreover, the NCM performance is comparable to that of linear SVMs which obtain current state-of-the-art performance. Experimentally, we study the generalization performance to classes that were not used to learn the metrics. Using a metric learned on 1,000 classes, we show\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-7908-2604-3_16', '10.1109/TPAMI.2010.57', '10.1109/18.720541', '10.1002/0470854774', '10.1109/CVPR.2006.264', '10.1109/89.279278', '10.1109/CVPR.2012.6248035', '10.1023/B:VISI.0000029664.99615.94', '10.1109/CVPR.2012.6248090', '10.1017/CBO9780511804441', '10.1007/978-3-642-33718-5_10', '10.1109/CVPR.2011.5995627', '10.1109/TPAMI.2005.182', '10.1109/CVPR.2008.4587598', '10.1109/ICCV.2009.5459266', '10.1145/1459359.1459391', '10.1109/CVPR.2011.5995504', '10.1109/CVPR.2009.5206594', '10.1109/TPAMI.2006.79', '10.5244/C.23.80', '10.1007/s11263-006-9794-4', '10.1007/978-3-642-15561-1_11', '10.1109/ICCV.2009.5459197', '10.1109/CVPR.2007.382969', '10.1109/TPAMI.2011.235', '10.1016/j.sigpro.2009.06.015', '10.1007/s10791-009-9117-9'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Artificial Intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Metric learning for large scale image classification: Generalizing to new classes at near-zero cost': Paper(DOI='10.1007/978-3-642-33709-3_35', crossref_json=None, google_schorlar_metadata=None, title='Metric learning for large scale image classification: Generalizing to new classes at near-zero cost', authors=['Thomas Mensink', 'Jakob Verbeek', 'Florent Perronnin', 'Gabriela Csurka'], abstract=' We are interested in large-scale image classification and especially in the setting where images corresponding to new or existing classes are continuously added to the training set. Our goal is to devise classifiers which can incorporate such images and classes on-the-fly at (near) zero cost. We cast this problem into one of learning a metric which is shared across all classes and explore k-nearest neighbor (k-NN) and nearest class mean (NCM) classifiers. We learn metrics on the ImageNet 2010 challenge data set, which contains more than 1.2M training images of 1K classes. Surprisingly, the NCM classifier compares favorably to the more flexible k-NN classifier, and has comparable performance to linear SVMs. We also study the generalization performance, among others by using the learned metric on the ImageNet-10K dataset, and we obtain competitive performance. Finally, we explore zero-shot\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2009.5206848', '10.1007/978-3-642-15555-0_6', '10.1109/CVPR.2010.5539949', '10.1109/CVPR.2011.5995627', '10.1109/TPAMI.2010.57', '10.1109/CVPR.2011.5995504', '10.1109/TPAMI.2011.235', '10.1109/CVPR.2011.5995477', '10.1109/CVPR.2012.6248090', '10.1109/ICCV.2009.5459266', '10.1002/0470854774', '10.1109/TPAMI.2005.182', '10.1145/1459359.1459391', '10.1007/978-3-7908-2604-3_16', '10.1109/18.720541', '10.1007/978-3-642-15561-1_11', '10.1007/s11263-006-9794-4', '10.1109/CVPR.2007.382969', '10.1016/j.sigpro.2009.06.015', '10.1109/TPAMI.2006.79', '10.1109/CVPRW.2009.5206594', '10.5244/C.23.80', '10.1007/s10791-009-9117-9', '10.1109/89.279278'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Artificial Intelligence'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Adapted vocabularies for generic visual categorization': Paper(DOI='10.1109/tpami.2007.70755', crossref_json=None, google_schorlar_metadata=None, title='Adapted vocabularies for generic visual categorization', authors=['Florent Perronnin', 'Christopher Dance', 'Gabriela Csurka', 'Marco Bressan'], abstract=' Several state-of-the-art Generic Visual Categorization (GVC) systems are built around a vocabulary of visual terms and characterize images with one histogram of visual word counts. We propose a novel and practical approach to GVC based on a universal vocabulary, which describes the content of all the considered classes of images, and class vocabularies obtained through the adaptation of the universal vocabulary using class-specific data. An image is characterized by a set of histograms – one per class – where each histogram describes whether the image content is best modeled by the universal vocabulary or the corresponding class vocabulary. It is shown experimentally on three very different databases that this novel representation outperforms those approaches which characterize an image with a single histogram.', conference=None, journal=None, year=None, reference_list=['10.1007/s11263-005-4635-4', '10.1109/ICCV.2005.137', '10.1109/ICCV.2003.1238663', '10.1109/ICCV.2005.77', '10.1109/ICCV.2005.171', '10.1109/ICCV.2003.1238476', '10.1109/89.279278', '10.1023/A:1007617005950', '10.1109/ICCV.2005.66', '10.1109/TPAMI.2005.127', '10.5244/C.20.98', '10.1006/dspr.1999.0361', '10.1162/153244303322533214', '10.1007/11553595_75', '10.1109/ICCV.1999.790379', '10.1023/B:VISI.0000029664.99615.94', '10.1007/s11263-005-3848-x', '10.1109/TPAMI.2005.188'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Mechanism Design', 'Markov Decision Processes', 'Computer Vision', 'Image Processing'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Universal and adapted vocabularies for generic visual categorization': Paper(DOI='10.1109/tpami.2007.70755', crossref_json=None, google_schorlar_metadata=None, title='Universal and adapted vocabularies for generic visual categorization', authors=['Florent Perronnin'], abstract='Generic visual categorization (GVC) is the pattern classification problem that consists in assigning labels to an image based on its semantic content. This is a challenging task as one has to deal with inherent object/scene variations, as well as changes in viewpoint, lighting, and occlusion. Several state-of-the-art GVC systems use a vocabulary of visual terms to characterize images with a histogram of visual word counts. We propose a novel practical approach to GVC based on a universal vocabulary, which describes the content of all the considered classes of images, and class vocabularies obtained through the adaptation of the universal vocabulary using class-specific data. The main novelty is that an image is characterized by a set of histograms - one per class - where each histogram describes whether the image content is best modeled by the universal vocabulary or the corresponding class vocabulary. This\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s11263-005-4635-4', '10.1109/ICCV.2005.137', '10.1109/ICCV.2003.1238663', '10.1109/ICCV.2005.77', '10.1109/ICCV.2005.171', '10.1109/ICCV.2003.1238476', '10.1109/89.279278', '10.1023/A:1007617005950', '10.1109/ICCV.2005.66', '10.1109/TPAMI.2005.127', '10.5244/C.20.98', '10.1006/dspr.1999.0361', '10.1162/153244303322533214', '10.1007/11553595_75', '10.1109/ICCV.1999.790379', '10.1023/B:VISI.0000029664.99615.94', '10.1007/s11263-005-3848-x', '10.1109/TPAMI.2005.188'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'artificial intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Handwritten word-spotting using hidden Markov models and universal vocabularies': Paper(DOI='10.1016/j.patcog.2009.02.005', crossref_json=None, google_schorlar_metadata=None, title='Handwritten word-spotting using hidden Markov models and universal vocabularies', authors=['José A Rodríguez-Serrano', 'Florent Perronnin'], abstract='Handwritten word-spotting is traditionally viewed as an image matching task between one or multiple query word-images and a set of candidate word-images in a database. This is a typical instance of the query-by-example paradigm. In this article, we introduce a statistical framework for the word-spotting problem which employs hidden Markov models (HMMs) to model keywords and a Gaussian mixture model (GMM) for score normalization. We explore the use of two types of HMMs for the word modeling part: continuous HMMs (C-HMMs) and semi-continuous HMMs (SC-HMMs), i.e. HMMs with a shared set of Gaussians. We show on a challenging multi-writer corpus that the proposed statistical framework is always superior to a traditional matching system which uses dynamic time warping (DTW) for word-image distance computation. A very important finding is that the SC-HMM is superior when labeled training\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s10032-006-0027-8', '10.1109/34.824821', '10.1007/s100440070020', '10.1007/s10032-006-0024-y', '10.1109/5.18626', '10.1016/j.specom.2004.12.004', '10.1002/j.1538-7305.1981.tb00243.x', '10.1109/34.308482', '10.1016/j.patrec.2004.02.014', '10.1109/TIP.2003.821114', '10.21236/ADA477875', '10.1109/34.982898', '10.1098/rspb.1991.0045', '10.1007/s10032-006-0023-z', '10.1109/TPAMI.2005.127', '10.1016/S0167-8655(01)00042-3', '10.1142/S0218001401000848', '10.1109/TPAMI.2004.14', '10.1109/TPAMI.2006.103', '10.1023/B:VISI.0000029664.99615.94', '10.1109/34.291449', '10.1016/0031-3203(95)00013-P', '10.1109/34.494644', '10.1109/34.667887', '10.1006/cviu.1998.0685', '10.1109/34.784288', '10.1214/aoms/1177697196', '10.1109/29.61531', '10.1006/dspr.1999.0361'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'machine learning', 'artificial intelligence'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'Negative margin matters: Understanding margin in few-shot classification': Paper(DOI='10.1007/978-3-030-58548-8_26', crossref_json=None, google_schorlar_metadata=None, title='Negative margin matters: Understanding margin in few-shot classification', authors=['Bin Liu', 'Yue Cao', 'Yutong Lin', 'Qi Li', 'Zheng Zhang', 'Mingsheng Long', 'Han Hu'], abstract=' This paper introduces a negative margin loss to metric learning based few-shot learning methods. The negative margin loss significantly outperforms regular softmax loss, and achieves state-of-the-art accuracy on three standard few-shot classification benchmarks with few bells and whistles. These results are contrary to the common practice in the metric learning field, that the margin is zero or positive. To understand why the negative margin loss performs well for the few-shot classification, we analyze the discriminability of learned features w.r.t different margins for training and novel classes, both empirically and theoretically. We find that although negative margin reduces the feature discriminability for training classes, it may also avoid falsely mapping samples of the same novel class to multiple peaks or clusters, and thus benefit the discrimination of novel classes. Code is available at                  https\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2017.502', '10.1109/ICCV.2017.89', '10.1109/CVPR.2009.5206848', '10.1109/CVPR.2019.00482', '10.1109/ICCV.2019.00815', '10.1109/ICCV.2017.328', '10.1109/CVPR.2016.90', '10.1214/aos/1015362183', '10.1109/CVPR.2019.01091', '10.1145/3240508.3240516', '10.1109/CVPR.2017.713', '10.1109/WACV45572.2020.9093338', '10.1109/ICASSP.2019.8682255', '10.1109/CVPR.2018.00610', '10.1109/CVPR.2018.00755', '10.1109/CVPR.2015.7298682', '10.1109/CVPR.2018.00131', '10.1109/CVPR.2018.00552', '10.1109/CVPR.2018.00760', '10.1007/978-3-030-01234-2_42', '10.5244/C.30.87', '10.1109/CVPR.2018.00534'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Deep Learning', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Learning Laplacian Matrix in Smooth Graph Signal Representations': Paper(DOI='10.1109/tsp.2016.2602809', crossref_json=None, google_schorlar_metadata=None, title='Learning Laplacian Matrix in Smooth Graph Signal Representations', authors=['Xiaowen Dong', 'Dorina Thanou', 'Pascal Frossard', 'Pierre Vandergheynst'], abstract='The construction of a meaningful graph plays a crucial role in the success of many graph-based representations and algorithms for handling structured data, especially in the emerging field of graph signal processing. However, a meaningful graph is not always readily available from the data, nor easy to define depending on the application domain. In particular, it is often desirable in graph signal processing applications that a graph is chosen such that the data admit certain regularity or smoothness on the graph. In this paper, we address the problem of learning graph Laplacians, which is equivalent to learning graph topologies, such that the input data form graph signals with smooth variations on the resulting topology. To this end, we adopt a factor analysis model for the graph signals and impose a Gaussian probabilistic prior on the latent variables that control these signals. We show that the Gaussian prior leads\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1093/biostatistics/kxm045', '10.1016/j.neuroimage.2013.07.019', '10.1109/ISBI.2011.5872835', '10.1145/1553374.1553400', '10.1093/biomet/asm018', '10.1214/009053606000000281', '10.1007/978-3-642-15939-8_17', '10.1145/1273496.1273520', '10.1126/science.286.5439.509', '10.1109/LSP.2012.2230165', '10.1016/j.jcss.2007.08.006', '10.1109/ISBI.2013.6556550', '10.1109/TSP.2013.2238935', '10.1109/MSP.2012.2235192', '10.1109/TIT.2013.2252233', '10.1007/978-3-642-38868-2_1', '10.1109/PCS.2010.5702565', '10.1109/TSP.2014.2345355', '10.1109/ICASSP.2013.6638232', '10.1016/j.acha.2006.03.004', '10.1007/11503415_32', '10.1109/TSP.2011.2107908', '10.1007/978-1-84800-155-8_7', '10.1017/CBO9780511809071', '10.1561/2200000016', '10.1017/CBO9780511804441', '10.1017/CBO9780511810800', '10.1111/j.1467-9868.2005.00503.x', '10.1109/ICASSP.2012.6288775', '10.1016/j.acha.2006.04.004', '10.1214/08-EJS176', '10.1016/j.acha.2010.04.005', '10.1109/TSP.2011.2158428', '10.1109/TSP.2012.2188718', '10.1109/TSP.2015.2424203', '10.1109/ICASSP.2012.6288639', '10.1109/TSP.2014.2332441', '10.1016/j.acha.2015.02.005', '10.1007/978-3-540-45167-9_12', '10.1109/ICASSP.2015.7178669', '10.1109/TSP.2012.2212886', '10.1109/TPAMI.2013.50', '10.1007/s11222-007-9033-z', '10.1109/SSP.2012.6319640', '10.1002/9781119970583', '10.1111/1467-9868.00196', '10.1002/9780470316894', '10.1201/9780203492024', '10.1214/13-AOS1162'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'image processing', 'computer vision', 'multimedia communications'], conference_acronym='IEEE transactions on signal processing', publisher=None, query_handler=None),\n",
       " 'Analysis of classifiers’ robustness to adversarial perturbations': Paper(DOI='10.1007/s10994-017-5663-3', crossref_json=None, google_schorlar_metadata=None, title='Analysis of classifiers’ robustness to adversarial perturbations', authors=['Alhussein Fawzi', 'Omar Fawzi', 'Pascal Frossard'], abstract=' The goal of this paper is to analyze the intriguing instability of classifiers to adversarial perturbations (Szegedy et al., in: International conference on learning representations (ICLR), 2014). We provide a theoretical framework for analyzing the robustness of classifiers to adversarial perturbations, and show fundamental upper bounds on the robustness of classifiers. Specifically, we establish a general upper bound on the robustness of classifiers to adversarial perturbations, and then illustrate the obtained upper bound on two practical classes of classifiers, namely the linear and quadratic classifiers. In both cases, our upper bound depends on a distinguishability measure that captures the notion of difficulty of the classification task. Our results for both classes imply that in tasks involving small distinguishability, no classifier in the considered set will be robust to adversarial perturbations, even if a good accuracy is\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2016.173', '10.1145/1961189.1961199', '10.1145/1014052.1014066', '10.1007/s10994-009-5124-8', '10.5244/C.29.106', '10.1109/5.726791', '10.1007/s10107-014-0806-9', '10.1023/B:VISI.0000029664.99615.94', '10.1109/18.312167', '10.1137/0804021', '10.1007/BF01582210', '10.1007/978-1-4613-0039-7', '10.1109/CVPR.2016.282', '10.1023/B:JOTA.0000004873.30548.ca', '10.1109/SP.2014.20'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'image processing', 'computer vision', 'multimedia communications'], conference_acronym='Machine learning', publisher=None, query_handler=None),\n",
       " 'Learning graphs from data: A signal representation perspective': Paper(DOI='10.1109/msp.2018.2887284', crossref_json=None, google_schorlar_metadata=None, title='Learning graphs from data: A signal representation perspective', authors=['Xiaowen Dong', 'Dorina Thanou', 'Michael Rabbat', 'Pascal Frossard'], abstract='The construction of a meaningful graph topology plays a crucial role in the effective representation, processing, analysis, and visualization of structured data. When a natural choice of the graph is not readily available from the data sets, it is thus desirable to infer or learn a graph topology from the data. In this article, we survey solutions to the problem of graph learning, including classical viewpoints from statistics and physics, and more recent approaches that adopt a graph signal processing (GSP) perspective. We further emphasize the conceptual similarities and differences between classical and GSP-based graph-inference methods and highlight the potential advantage of the latter in a number of theoretical and practical scenarios. We conclude with several open issues and challenges that are keys to the design of future signal processing and machine-learning algorithms for learning graphs from data.', conference=None, journal=None, year=None, reference_list=['10.1109/JPROC.2018.2798928', '10.1109/ICIP.2017.8296567', '10.1109/ICASSP.2017.7952672', '10.1109/DSW.2018.8439913', '10.1016/j.acha.2016.06.007', '10.1109/JSEN.2017.2733767', '10.1109/TSP.2016.2602809', '10.1145/2623330.2623760', '10.1109/TSP.2014.2332441', '10.1109/TSP.2018.2870386', '10.1109/JSTSP.2017.2726979', '10.1103/RevModPhys.87.925', '10.1109/TSP.2014.2304431', '10.1109/JSAC.2006.884020', '10.21236/ADA637131', '10.1109/ICASSP.2012.6288639', '10.1016/j.acha.2010.04.005', '10.1109/SSP.2016.7551716', '10.4135/9781452226576', '10.1109/TSP.2016.2628354', '10.1002/hbm.460020104', '10.23919/EUSIPCO.2017.8081186', '10.1126/science.290.5500.2323', '10.1016/j.compbiomed.2011.09.004', '10.1371/journal.pone.0128136', '10.1109/JPROC.2018.2804318', '10.1109/JPROC.2018.2799702', '10.1214/088342304000000422', '10.1109/TIP.2014.2378055', '10.1109/APSIPA.2015.7415334', '10.1016/j.physrep.2009.11.002', '10.1111/j.1467-9868.2005.00532.x', '10.1214/09-AOS691', '10.2307/2322600', '10.1137/1016079', '10.1016/j.laa.2014.04.020', '10.1145/1553374.1553400', '10.1109/ICASSP.2017.7953413', '10.1109/JPROC.2010.2040551', '10.1016/j.mri.2003.08.026', '10.1016/j.neuroimage.2004.11.017', '10.1109/TSP.2011.2129515', '10.1109/TSP.2016.2634543', '10.1002/hbm.460020107', '10.1109/MSP.2010.939537', '10.1109/TSP.2013.2238935', '10.1109/JPROC.2018.2820126', '10.1214/009053606000000281', '10.1093/biostatistics/kxm045', '10.1109/TSP.2018.2813337', '10.1109/TVCG.2017.2746080', '10.1109/TSP.2013.2295553', '10.2307/2528966', '10.23919/EUSIPCO.2017.8081187', '10.1109/MSP.2012.2233865', '10.1109/MSP.2012.2235192', '10.1109/TSP.2016.2628343', '10.1109/GlobalSIP.2016.7905863', '10.1145/1835804.1835933', '10.1109/TSIPN.2018.2872157', '10.1109/TSIPN.2016.2605763', '10.1145/3097983.3098037', '10.1017/nws.2014.3', '10.1109/TSIPN.2017.2742940', '10.1109/TSIPN.2017.2731051', '10.1109/TSIPN.2017.2731164', '10.1109/ICASSP.2017.7953410', '10.1109/JSTSP.2017.2726975', '10.1145/1458082.1458115', '10.1073/pnas.0708838104'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'image processing', 'computer vision', 'multimedia communications'], conference_acronym='IEEE signal processing magazine (Print)', publisher=None, query_handler=None),\n",
       " 'Graph-based compression of dynamic 3D point cloud sequences': Paper(DOI='10.1109/tip.2016.2529506', crossref_json=None, google_schorlar_metadata=None, title='Graph-based compression of dynamic 3D point cloud sequences', authors=['Dorina Thanou', 'Philip A Chou', 'Pascal Frossard'], abstract='This paper addresses the problem of compression of 3D point cloud sequences that are characterized by moving 3D positions and color attributes. As temporally successive point cloud frames share some similarities, motion estimation is key to effective compression of these sequences. It, however, remains a challenging problem as the point cloud frames have varying numbers of points without explicit correspondence information. We represent the time-varying geometry of these sequences with a set of graphs, and consider 3D positions and color attributes of the point clouds as signals on the vertices of the graphs. We then cast motion estimation as a feature-matching problem between successive graphs. The motion is estimated on a sparse set of representative vertices using new spectral graph wavelet descriptors. A dense motion field is eventually interpolated by solving a graph-based regularization problem\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/37401.37422', '10.1109/TCSVT.2014.2329376', '10.1109/TCSVT.2014.2319631', '10.1109/TCSVT.2003.817625', '10.1109/TCSVT.2007.903810', '10.1109/ICASSP.2014.6854786', '10.1145/2766945', '10.1145/566570.566589', '10.1111/1467-8659.00433', '10.1109/2945.764870', '10.1111/j.1467-8659.2010.01659.x', '10.1109/TVCG.2007.70441', '10.1145/2492045.2492053', '10.1007/978-3-642-15558-1_26', '10.1109/ICIP.2011.6116679', '10.1109/CVPR.2009.5206748', '10.1016/j.patrec.2007.02.009', '10.1016/j.jvcir.2005.03.001', '10.1145/2527267', '10.1109/TSP.2014.2332441', '10.1109/DCC.2006.5', '10.1109/ICASSP.2012.6288775', '10.1142/S0129054190000291', '10.1111/cgf.12305', '10.1109/TMM.2005.858410', '10.1109/MSP.2012.2235192', '10.1109/TMM.2008.917349', '10.1109/JSTSP.2014.2370752', '10.1007/3-540-44480-7_18', '10.1023/B:VISI.0000029664.99615.94', '10.5244/C.16.36', '10.1145/1291233.1291311', '10.1109/ICIP.2015.7351401', '10.1109/ICRA.2012.6224647', '10.1016/j.acha.2010.04.005', '10.1109/ICIP.2014.7025414', '10.1109/VISUAL.2000.885711', '10.1109/TPAMI.2013.148', '10.1109/ICCVW.2011.6130444', '10.1109/CVPR.2013.278', '10.1109/CVPR.2014.296', '10.1109/LSP.2012.2230165', '10.1109/CVPR.2010.5539838', '10.1111/j.1467-8659.2009.01515.x'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'image processing', 'computer vision', 'multimedia communications'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'The robustness of deep networks: A geometrical perspective': Paper(DOI='10.1109/msp.2017.2740965', crossref_json=None, google_schorlar_metadata=None, title='The robustness of deep networks: A geometrical perspective', authors=['Alhussein Fawzi', 'Seyed-Mohsen Moosavi-Dezfooli', 'Pascal Frossard'], abstract=\"Deep neural networks have recently shown impressive classification performance on a diverse set of visual tasks. When deployed in real-world (noise-prone) environments, it is equally important that these classifiers satisfy robustness guarantees: small perturbations applied to the samples should not yield significant loss to the performance of the predictor. The goal of this article is to discuss the robustness of deep networks to a diverse set of perturbations that may affect the samples in practice, including adversarial perturbations, random noise, and geometric transformations. This article further discusses the recent works that build on the robustness analysis to provide geometric insights on the classifier's decision surface, which help in developing a better understanding of deep networks. Finally, we present recent solutions that attempt to increase the robustness of deep networks. We hope this review article will\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1109/SP.2016.41', '10.1080/10556780600883791', '10.1109/ICISIP.2004.1287696', '10.1007/s10994-017-5663-3', '10.1109/CVPRW.2016.58', '10.1145/2647868.2654889', '10.1109/CVPR.2015.7298594', '10.1007/s11263-015-0816-y', '10.1145/1273496.1273556', '10.1016/j.media.2017.07.005', '10.1109/CVPR.2015.7298640', '10.5244/C.29.106', '10.1109/CVPR.2016.282', '10.1109/ICIP.2016.7533048', '10.5244/C.30.137', '10.1145/3065386', '10.1007/978-3-642-40994-3_25', '10.1145/2976749.2978392', '10.1109/CVPR.2017.17', '10.1109/CVPR.2016.481', '10.7551/mitpress/10761.003.0012'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'image processing', 'computer vision', 'multimedia communications'], conference_acronym='IEEE signal processing magazine (Print)', publisher=None, query_handler=None),\n",
       " 'Joint source/FEC rate selection for quality-optimal MPEG-2 video delivery': Paper(DOI='10.1109/83.974566', crossref_json=None, google_schorlar_metadata=None, title='Joint source/FEC rate selection for quality-optimal MPEG-2 video delivery', authors=['Pascal Frossard', 'Olivier Verscheure'], abstract='This paper deals with the optimal allocation of MPEG-2 encoding and media-independent forward error correction (FEC) rates under a total given bandwidth. The optimality is defined in terms of minimum perceptual distortion given a set of video and network parameters. We first derive the set of equations leading to the residual loss process parameters. That is, the packet loss ratio (PLR) and the average burst length after FEC decoding. We then show that the perceptual source distortion decreases exponentially with the increasing MPEG-2 source rate. We also demonstrate that the perceptual distortion due to data loss is directly proportional to the number of lost macroblocks, and therefore decreases with the amount of channel protection. Finally, we derive the global set of equations that lead to the optimal dynamic rate allocation. The optimal distribution is shown to outperform classical FEC scheme, thanks to its\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/52324.52356', '10.17487/rfc2330', '10.1109/PROC.1978.11013', '10.1109/TIT.1974.1055148', '10.1002/j.1538-7305.1960.tb03959.x', '10.1109/4234.913160', '10.1007/978-1-4615-3298-9', '10.1145/263109.263155', '10.1109/5.664283', '10.1117/12.235432', '10.1109/83.334987', '10.1109/76.486416', '10.1109/76.554439', '10.1109/76.709411', '10.1117/12.235440', '10.1109/49.553683', '10.1109/INFCOM.1990.91241', '10.1109/76.946516', '10.1109/49.611154', '10.1002/j.1538-7305.1965.tb04139.x', '10.1016/S0923-5965(99)00025-9', '10.1145/99508.99566', '10.1109/65.730750', '10.1016/0165-1684(95)00111-3', '10.1109/83.748887', '10.1109/65.752641', '10.1016/S0165-1684(99)00062-6', '10.1006/rtim.1999.0175', '10.1145/62044.62050', '10.1145/263876.263881'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'image processing', 'computer vision', 'multimedia communications'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Clustering with multi-layer graphs: A spectral perspective': Paper(DOI='10.1109/tsp.2012.2212886', crossref_json=None, google_schorlar_metadata=None, title='Clustering with multi-layer graphs: A spectral perspective', authors=['Xiaowen Dong', 'Pascal Frossard', 'Pierre Vandergheynst', 'Nikolai Nefedov'], abstract='Observational data usually comes with a multimodal nature, which means that it can be naturally represented by a multi-layer graph whose layers share the same set of vertices (objects) with different edges (pairwise relationships). In this paper, we address the problem of combining different layers of the multi-layer graph for an improved clustering of the vertices compared to using layers independently. We propose two novel methods, which are based on a joint matrix factorization and a graph regularization framework respectively, to efficiently combine the spectrum of the multiple graph layers, namely the eigenvectors of the graph Laplacian matrices. In each case, the resulting combination, which we call a “joint spectrum” of multiple layers, is used for clustering the vertices. We evaluate our approaches by experiments with several real world social network datasets. Results demonstrate the superior or competitive\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1835804.1835877', '10.1002/0470073047', '10.1137/S0895479895281484', '10.1017/CBO9780511809071', '10.1145/1273496.1273642', '10.1109/ICDM.2009.125', '10.1007/978-3-642-02124-4_15', '10.1109/34.868688', '10.1145/1988688.1988763', '10.1007/s00779-005-0046-3', '10.1103/PhysRevE.69.066133', '10.1073/pnas.0900282106', '10.1088/1742-5468/2008/10/P10008', '10.1007/s11222-007-9033-z', '10.1073/pnas.0913678107', '10.1016/j.cosrev.2007.05.001', '10.1109/GRC.2009.5255152', '10.1109/ICDM.2010.156', '10.1145/1864708.1864731', '10.1103/PhysRevE.69.026113', '10.1109/ICDM.2010.112', '10.1073/pnas.122653799'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'image processing', 'computer vision', 'multimedia communications'], conference_acronym='IEEE transactions on signal processing', publisher=None, query_handler=None),\n",
       " 'Low-rate and flexible image coding with redundant representations': Paper(DOI='10.1109/tip.2005.860596', crossref_json=None, google_schorlar_metadata=None, title='Low-rate and flexible image coding with redundant representations', authors=['Rosa M Figueras i Ventura', 'Pierre Vandergheynst', 'Pascal Frossard'], abstract=None, conference=None, journal=None, year=None, reference_list=['10.1109/76.499834', '10.1109/ICASSP.2001.941280', '10.1109/TSP.2003.821105', '10.1137/S0036141093256265', '10.1117/12.157984', '10.1109/78.668789', '10.1109/83.366473', '10.1145/214762.214771', '10.1006/acha.1998.0255', '10.1109/83.334984', '10.1109/TIP.2002.1014998', '10.1137/S1064827596304010', '10.1109/18.959265', '10.1109/TIT.2002.801410', '10.1109/TIT.2003.820031', '10.1109/78.258082', '10.1007/BF02124742', '10.1214/aos/1176350382', '10.1038/381607a0', '10.1109/76.554427', '10.1109/ICIP.2001.958939', '10.1017/S0962492900002816', '10.1109/ICIP.2001.958941', '10.1109/18.720544', '10.1109/TIP.2004.840710', '10.1109/ICIP.2002.1038034', '10.1117/12.173207', '10.1109/ACSSC.1993.342465', '10.1117/12.505452', '10.1109/TIT.2004.834793', '10.1109/76.946520', '10.1109/ICIP.1995.529037', '10.1109/TIT.2005.860474'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'image processing', 'computer vision', 'multimedia communications'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Learning heat diffusion graphs': Paper(DOI='10.1007/s00373-021-02424-4', crossref_json=None, google_schorlar_metadata=None, title='Learning heat diffusion graphs', authors=['Dorina Thanou', 'Xiaowen Dong', 'Daniel Kressner', 'Pascal Frossard'], abstract='Information analysis of data often boils down to properly identifying their hidden structure. In many cases, the data structure can be described by a graph representation that supports signals in the dataset. In some applications, this graph may be partly determined by design constraints or predetermined sensing arrangements. In general though, the data structure is not readily available nor easily defined. In this paper, we propose to represent structured data as a sparse combination of localized functions that live on a graph. This model is more appropriate to represent local data arrangements than the classical global smoothness prior. We focus on the problem of inferring the connectivity that best explains the data samples at different vertices of a graph that is a priori unknown. We concentrate on the case where the observed data are actually the sum of heat diffusion processes, which is a widely used model for data\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-1-4614-1939-6', '10.1016/0024-3795(94)00346-F', '10.1016/S0012-365X(98)00085-5', '10.1016/j.laa.2016.09.043', '10.1142/S1005386719000075', '10.1007/s10801-011-0287-3', '10.1017/S0004972719001102', '10.1214/17-ECP84', '10.1214/16-ECP4392'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['machine learning', 'image processing', 'computer vision', 'multimedia communications'], conference_acronym='Graphs and combinatorics', publisher=None, query_handler=None),\n",
       " 'Multimodal Unsupervised Image-to-Image Translation': Paper(DOI='10.18178/ijml.2023.13.2.1132', crossref_json=None, google_schorlar_metadata=None, title='Multimodal Unsupervised Image-to-Image Translation', authors=['Xun Huang', 'Ming-Yu Liu', 'Serge Belongie', 'Jan Kautz'], abstract='Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Video-to-video synthesis': Paper(DOI='10.2200/s00061ed1v01y200707ivm010', crossref_json=None, google_schorlar_metadata=None, title='Video-to-video synthesis', authors=['Ting-Chun Wang', 'Ming-Yu Liu', 'Jun-Yan Zhu', 'Guilin Liu', 'Andrew Tao', 'Jan Kautz', 'Bryan Catanzaro'], abstract='We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image synthesis problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without understanding temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a novel video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generator and discriminator architectures, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our approach to future video prediction, outperforming several state-of-the-art competing systems.', conference=None, journal=None, year=None, reference_list=['10.1109/5.899055', '10.1109/TIP.2005.860353', '10.1109/TMM.2005.850989', '10.1109/TCSVT.2002.800309', '10.1109/TIP.2005.860600', '10.1016/j.image.2005.02.002', '10.1109/TCSVT.2003.814966', '10.1109/5.904503', '10.1109/6046.944477', '10.1109/ICASSP.2002.1006157', '10.1109/79.855913', '10.1109/83.217221', '10.1109/83.334987', '10.1109/79.733495', '10.1007/978-1-4757-2566-7', '10.1002/j.1538-7305.1948.tb01338.x', '10.17487/rfc2474', '10.1109/JPROC.2002.802000', '10.1109/JSAC.2003.815015', '10.1109/6046.909598', '10.1109/JSAC.2003.815231', '10.1109/JPROC.2004.839603', '10.1109/MCOM.2003.1235598', '10.1109/JSAC.2003.816445', '10.1109/MWC.2005.1497855', '10.1002/wcm.469', '10.17487/rfc3550', '10.1145/99517.99553', '10.17487/rfc2736', '10.17487/rfc2038', '10.17487/rfc3016', '10.17487/rfc2032', '10.17487/rfc3984', '10.1109/76.911161', '10.1109/76.867930', '10.1109/TMM.2005.864313', '10.1109/6046.909591', '10.1109/TCSVT.2003.819186', '10.1109/TCSVT.2004.831966', '10.1201/9781482290097', '10.1109/26.848555', '10.1109/5.664283', '10.1109/49.848249', '10.1109/5.790632', '10.1109/49.848250', '10.1109/49.848251', '10.1109/TIP.2002.802507', '10.1109/ICME.2003.1221569', '10.1109/6046.909594', '10.1109/ICME.2003.1220841', '10.1016/0169-7552(95)00051-8', '10.17487/rfc1633', '10.1109/65.238150', '10.1109/83.388074', '10.1109/TIP.2003.819861', '10.1109/TIP.2006.881959', '10.1109/TCSVT.2002.800313', '10.1109/49.848255', '10.1109/ICIP.2005.1530203', '10.1002/0471200611', '10.1109/18.53739', '10.1109/5.790634', '10.1109/JPROC.2004.839621', '10.1109/83.826773', '10.1109/TIP.2003.809006', '10.1109/JSAC.2003.815394', '10.1109/ICC.2004.1312726', '10.1109/TCSVT.2003.815165', '10.1109/78.258085', '10.1109/76.499834', '10.1109/76.744279', '10.1109/76.157160', '10.1109/83.334985', '10.1109/83.743851', '10.1016/j.image.2004.05.002', '10.1016/j.image.2004.05.006', '10.1109/TIP.2003.819433', '10.1109/6046.966110', '10.1109/18.556670', '10.1109/83.908500', '10.1109/TCSVT.2003.814969', '10.1109/76.488825', '10.1109/79.733497', '10.1109/83.902290', '10.1109/ICASSP.2007.366039', '10.1109/90.392383', '10.1109/25.293655', '10.1109/26.2763', '10.1002/j.1538-7305.1960.tb03959.x', '10.1109/25.350282', '10.1109/26.803503', '10.1002/dac.580', '10.1137/0108018', '10.1109/MSP.2004.1311137', '10.1117/12.453072', '10.1109/9780470546345', '10.1109/25.820695', '10.1117/12.532294', '10.1109/49.848250', '10.1109/JSAC.2003.816455', '10.1109/76.499834', '10.1109/TCSVT.2002.800336', '10.1109/JSAC.2003.815228', '10.1109/TCOMM.2004.836436', '10.1109/TWC.2005.847026', '10.1109/JSAC.2007.070511', '10.1109/ICME.2007.4284811'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Synthesis lectures on image, video, and multimedia processing', publisher=None, query_handler=None),\n",
       " 'A closed-form solution to photorealistic image stylization': Paper(DOI='10.1007/978-3-030-01219-9_28', crossref_json=None, google_schorlar_metadata=None, title='A closed-form solution to photorealistic image stylization', authors=['Yijun Li', 'Ming-Yu Liu', 'Xueting Li', 'Ming-Hsuan Yang', 'Jan Kautz'], abstract='Photorealistic image stylization concerns transferring style of a reference photo to a content photo with the constraint that the stylized photo should remain photorealistic. While several photorealistic image stylization methods exist, they tend to generate spatially inconsistent stylizations with noticeable artifacts. In this paper, we propose a method to address these issues. The proposed method consists of a stylization step and a smoothing step. While the stylization step transfers the style of the reference photo to the content photo, the smoothing step ensures spatially consistent stylizations. Each of the steps has a closed-form solution and can be computed efficiently. We conduct extensive experimental validations. The results show that the proposed method generates photorealistic stylization outputs that are more preferred by human subjects as compared to those by the competing methods while running much faster. Source code and additional results are available at https://github. com/NVIDIA/FastPhotoStyle.', conference=None, journal=None, year=None, reference_list=['10.1109/38.946629', '10.1109/ICCV.2005.166', '10.1145/1778765.1778862', '10.1145/1141911.1141935', '10.1145/2601097.2601101', '10.1145/2601097.2601137', '10.1109/CVPR.2016.265', '10.1109/CVPR.2016.265', '10.1109/CVPR.2017.740', '10.1109/CVPR.2010.5540201', '10.1145/2508363.2508419', '10.1111/cgf.12008', '10.1145/2897824.2925942', '10.1109/CVPR.2016.272', '10.1007/978-3-319-46475-6_43', '10.1109/CVPR.2017.36', '10.5244/C.31.114', '10.1109/ICCV.2017.167', '10.1145/3123266.3123425', '10.5244/C.31.153', '10.1109/CVPR.2017.632', '10.1109/CVPR.2018.00917', '10.1109/CVPR.2017.241', '10.1109/ICCV.2017.244', '10.1007/978-3-030-01219-9_11', '10.1007/978-3-319-10590-1_53', '10.1109/ICCV.2015.178', '10.1109/CVPR.2013.407', '10.1109/34.868688', '10.1109/TPAMI.2007.1177', '10.1007/978-3-319-10602-1_48', '10.1109/CVPR.2017.397', '10.1109/ICCV.2015.164', '10.1109/TPAMI.2012.213'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Few-shot video-to-video synthesis': Paper(DOI='10.1109/tcsvt.2023.3282777', crossref_json=None, google_schorlar_metadata=None, title='Few-shot video-to-video synthesis', authors=['Ting-Chun Wang', 'Ming-Yu Liu', 'Andrew Tao', 'Guilin Liu', 'Jan Kautz', 'Bryan Catanzaro'], abstract='Video-to-video synthesis (vid2vid) aims at converting an input semantic video, such as videos of human poses or segmentation masks, to an output photorealistic video. While the state-of-the-art of vid2vid has advanced significantly, existing approaches share two major limitations. First, they are data-hungry. Numerous images of a target human subject or a scene are required for training. Second, a learned model has limited generalization capability. A pose-to-human vid2vid model can only synthesize poses of the single person in the training set. It does not generalize to other humans that are not in the training set. To address the limitations, we propose a few-shot vid2vid framework, which learns to synthesize videos of previously unseen subjects or scenes by leveraging few example images of the target at test time. Our model achieves this few-shot generalization capability via a novel network weight generation module utilizing an attention mechanism. We conduct extensive experimental validations with comparisons to strong baselines using several large-scale video datasets including human-dancing videos, talking-head videos, and street-scene videos. The experimental results verify the effectiveness of the proposed framework in addressing the two limitations of existing vid2vid approaches.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='IEEE transactions on circuits and systems for video technology (Print)', publisher=None, query_handler=None),\n",
       " 'Graphvae: Towards generation of small graphs using variational autoencoders': Paper(DOI='10.1007/978-3-030-01418-6_41', crossref_json=None, google_schorlar_metadata=None, title='Graphvae: Towards generation of small graphs using variational autoencoders', authors=['Martin Simonovsky', 'Nikos Komodakis'], abstract=' Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of molecule generation.', conference=None, journal=None, year=None, reference_list=['10.18653/v1/K16-1002', '10.1109/MSP.2017.2693418', '10.1109/CVPR.2014.268', '10.1021/ci3001277', '10.1186/s13321-017-0235-x', '10.1038/sdata.2014.22', '10.1021/acscentsci.7b00512', '10.1109/CVPR.2017.11', '10.1109/CVPR.2016.255', '10.1162/neco.1989.1.2.270', '10.1109/CVPR.2017.330', '10.1609/aaai.v31i1.10804'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['deep learning', 'computer vision', 'machine learning', 'medical image analysis', 'artificial intelligence'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Image completion using efficient belief propagation via priority scheduling and dynamic pruning': Paper(DOI='10.1109/tip.2007.906269', crossref_json=None, google_schorlar_metadata=None, title='Image completion using efficient belief propagation via priority scheduling and dynamic pruning', authors=['Nikos Komodakis', 'Georgios Tziritas'], abstract='In this paper, a new exemplar-based framework is presented, which treats image completion, texture synthesis, and image inpainting in a unified manner. In order to be able to avoid the occurrence of visually inconsistent results, we pose all of the above image-editing tasks in the form of a discrete global optimization problem. The objective function of this problem is always well-defined, and corresponds to the energy of a discrete Markov random field (MRF). For efficiently optimizing this MRF, a novel optimization scheme, called priority belief propagation (BP), is then proposed, which carries two very important extensions over the standard BP algorithm: ldquopriority-based message schedulingrdquo and ldquodynamic label pruning.rdquo These two extensions work in cooperation to deal with the intolerable computational cost of BP, which is caused by the huge number of labels associated with our MRF. Moreover\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/245.247', '10.1109/CVPR.2003.1211410', '10.1109/CVPR.2003.1211409', '10.1109/CVPR.2004.1315041', '10.1145/882262.882269', '10.1109/TPAMI.2006.200', '10.1109/TIT.2005.856938', '10.1109/CVPR.2003.1211536', '10.1109/ICIP.2002.1038113', '10.1006/jvci.2001.0487', '10.1109/ICCV.1999.790383', '10.1145/344779.345009', '10.1145/258734.258882', '10.1145/1201775.882264', '10.1145/501786.501787', '10.1145/1015706.1015730', '10.1145/364338.364405', '10.1145/383259.383295', '10.1109/CVPR.2006.141', '10.1109/ICCV.2001.937584', '10.1109/CVPR.2003.1211538', '10.1109/ICCV.2001.937658', '10.1109/CVPR.2003.1211463', '10.1023/A:1026501619075', '10.1109/ICIP.1996.560871', '10.1109/CVPR.2001.990497', '10.1145/344779.344972', '10.1145/218380.218446', '10.1109/83.935036', '10.1023/A:1026553619983', '10.1145/383259.383296', '10.1145/1186822.1073274', '10.1145/344779.345012', '10.1109/CVPR.2003.1211414', '10.1145/566570.566635', '10.1145/882262.882267', '10.1145/1073204.1073263', '10.1109/CVPR.2004.1315022'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['deep learning', 'computer vision', 'machine learning', 'medical image analysis', 'artificial intelligence'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Playing with duality: An overview of recent primal? dual approaches for solving large-scale optimization problems': Paper(DOI='10.1109/msp.2014.2377273', crossref_json=None, google_schorlar_metadata=None, title='Playing with duality: An overview of recent primal? dual approaches for solving large-scale optimization problems', authors=['Nikos Komodakis', 'Jean-Christophe Pesquet'], abstract='Optimization methods are at the core of many problems in signal/image processing, computer vision, and machine learning. For a long time, it has been recognized that looking at the dual of an optimization problem may drastically simplify its solution. However, deriving efficient strategies that jointly bring into play the primal and dual problems is a more recent idea that has generated many important new contributions in recent years. These novel developments are grounded in the recent advances in convex analysis, discrete optimization, parallel processing, and nonsmooth optimization with an emphasis on sparsity issues. In this article, we aim to present the principles of primal-dual approaches while providing an overview of the numerical methods that have been proposed in different contexts. Last but not least, primal-dual methods lead to algorithms that are easily parallelizable. Today, such parallel algorithms\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF01582566', '10.1137/120901106', '10.1109/TIP.2010.2076294', '10.1109/TIP.2010.2053941', '10.1109/SSP.2009.5278459', '10.1137/080725891', '10.1109/JSTSP.2007.910264', '10.1007/BF01581204', '10.1016/0898-1221(76)90003-1', '10.1051/0004-6361:20047011', '10.1007/3-540-31247-1', '10.1109/MSP.2007.914731', '10.1007/978-3-642-04900-2', '10.1016/j.sigpro.2013.09.026', '10.1016/0167-2789(92)90242-F', '10.1080/02331934.2012.733883', '10.1137/S0363012998338806', '10.1109/ICASSP.2012.6288075', '10.1088/0266-5611/23/4/008', '10.1007/s11228-010-0147-7', '10.1088/0266-5611/29/2/025011', '10.1088/0266-5611/27/12/125007', '10.1109/ICIP.2014.7025841', '10.1137/050626090', '10.1515/9781400873173', '10.1561/2200000015', '10.1007/978-1-4419-9467-7', '10.1109/ICCV.2011.6126441', '10.1017/CBO9780511804441', '10.24033/bsmf.1625', '10.1007/s10851-010-0251-1', '10.1007/s10444-011-9254-8', '10.1137/100814494', '10.1137/09076934X', '10.1002/cpa.20042', '10.1007/s10957-009-9522-7', '10.1007/s10957-012-0245-9', '10.1016/j.media.2008.03.006', '10.1016/j.cviu.2008.06.007', '10.1007/s11263-011-0512-5', '10.1007/978-3-642-33715-4_1', '10.1109/ICCV.2007.4408890', '10.1109/42.796284', '10.1109/TPAMI.2007.1061', '10.1146/annurev-bioeng-071910-124649', '10.1109/TPAMI.2007.1036', '10.1007/978-3-540-88690-7_59', '10.1109/TPAMI.2006.200', '10.1007/978-3-642-33715-4_2', '10.1109/CVPR.2009.5206846', '10.1109/TIT.2010.2079014', '10.1137/10081602X', '10.1007/s10851-013-0486-8', '10.1137/130904160', '10.1137/130950616', '10.1088/0266-5611/24/6/065014', '10.1016/j.jvcir.2009.10.006', '10.1016/j.dam.2011.10.026', '10.7551/mitpress/8579.001.0001', '10.1016/j.dam.2012.06.009', '10.1109/CVPR.2013.175', '10.1007/978-3-642-38267-3_11', '10.1109/TSP.2015.2415759', '10.1016/j.cam.2015.02.030', '10.1007/s10851-011-0324-9', '10.1109/TSP.2014.2331614', '10.1137/090746379', '10.1016/j.sigpro.2014.03.014', '10.1109/TIP.2011.2128335', '10.1137/130920058', '10.1007/s11760-013-0472-z', '10.1109/ICASSP.2013.6638822', '10.1109/34.969114', '10.1016/j.cviu.2011.06.012', '10.1109/TPAMI.2007.70844', '10.1137/120895068', '10.1016/j.media.2010.08.001', '10.1088/0266-5611/29/3/035007', '10.1007/978-1-4419-9569-8_10', '10.1007/s11228-011-0191-y', '10.1016/j.cviu.2008.06.007', '10.1109/TPAMI.2010.108', '10.1109/TIT.2005.856938', '10.1109/TPAMI.2012.105', '10.1016/j.cviu.2013.07.004', '10.1109/CVPR.2011.5995361', '10.1109/CVPR.2013.20', '10.1109/CVPR.2011.5995513', '10.1109/CVPR.2009.5206604', '10.1109/ICCV.2009.5459287', '10.7551/mitpress/8579.003.0015', '10.1109/ICCV.2009.5459434', '10.1109/CVPR.2008.4587562', '10.1109/CVPR.2011.5995652', '10.1109/CVPR.2010.5540072', '10.1007/978-3-642-37431-9_28', '10.1016/j.disopt.2009.04.006', '10.1007/978-3-540-88690-7_60', '10.1016/S0166-218X(01)00341-9', '10.1109/TPAMI.2004.1262177'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['deep learning', 'computer vision', 'machine learning', 'medical image analysis', 'artificial intelligence'], conference_acronym='IEEE signal processing magazine (Print)', publisher=None, query_handler=None),\n",
       " 'Approximate labeling via graph cuts based on linear programming': Paper(DOI='10.1109/tpami.2007.1061', crossref_json=None, google_schorlar_metadata=None, title='Approximate labeling via graph cuts based on linear programming', authors=['Nikos Komodakis', 'Georgios Tziritas'], abstract='A new framework is presented for both understanding and developing graph-cut-based combinatorial algorithms suitable for the approximate optimization of a very wide class of Markov Random Fields (MRFs) that are frequently encountered in computer vision. The proposed framework utilizes tools from the duality theory of linear programming in order to provide an alternative and more general view of state-of-the-art techniques like the \\\\alpha-expansion algorithm, which is included merely as a special case. Moreover, contrary to \\\\alpha-expansion, the derived algorithms generate solutions with guaranteed optimality properties for a much wider class of problems, for example, even for MRFs with nonmetric potentials. In addition, they are capable of providing per-instance suboptimality bounds in all occasions, including discrete MRFs with an arbitrary potential function. These bounds prove to be very tight in practice\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2005.14', '10.1109/CVPR.1998.698598', '10.1145/335305.335397', '10.1109/ICCV.1998.710763', '10.1109/CVPR.2004.1315196', '10.1109/ICCV.2003.1238444', '10.1023/A:1014573219977', '10.1023/B:VISI.0000045324.43199.43', '10.1006/cviu.1996.0006', '10.1109/34.969114', '10.1145/585265.585268', '10.1109/CVPR.2005.130', '10.1109/ICCV.2005.110'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['deep learning', 'computer vision', 'machine learning', 'medical image analysis', 'artificial intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Markov random field modeling, inference & learning in computer vision & image understanding: A survey': Paper(DOI='10.1016/j.cviu.2013.07.004', crossref_json=None, google_schorlar_metadata=None, title='Markov random field modeling, inference & learning in computer vision & image understanding: A survey', authors=['Chaohui Wang', 'Nikos Komodakis', 'Nikos Paragios'], abstract='In this paper, we present a comprehensive survey of Markov Random Fields (MRFs) in computer vision and image understanding, with respect to the modeling, the inference and the learning. While MRFs were introduced into the computer vision field about two decades ago, they started to become a ubiquitous tool for solving visual perception problems around the turn of the millennium following the emergence of efficient inference methods. During the past decade, a variety of MRF models as well as inference and learning methods have been developed for addressing numerous low, mid and high-level vision problems. While most of the literature concerns pairwise MRFs, in recent years we have also witnessed significant progress in higher-order MRFs, which substantially enhances the expressiveness of graph-based models and expands the domain of solvable problems. This survey provides a compact and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/11585978_11', '10.1007/s11263-006-7007-9', '10.1007/s11263-006-0016-x', '10.1109/CVPR.2007.383191', '10.1016/j.cviu.2008.07.002', '10.1109/CVPR.2012.6247958', '10.1111/j.2517-6161.1974.tb00999.x', '10.1109/CVPR.2003.1211410', '10.1145/1831407.1831431', '10.1109/34.969114', '10.1109/TPAMI.2003.1233908', '10.1109/TPAMI.2004.1262177', '10.1109/TIT.2005.856938', '10.1109/TPAMI.2007.1128', '10.1109/TPAMI.2006.200', '10.1016/j.cviu.2008.06.007', '10.1007/978-3-642-15552-9_38', '10.1007/3-540-47977-5_6', '10.1016/j.media.2008.03.006', '10.1007/s11263-007-0120-6', '10.1109/TPAMI.2007.70844', '10.1007/s11263-006-7934-5', '10.1109/18.910572', '10.1109/TPAMI.1984.4767596', '10.1145/1015706.1015720', '10.1023/B:VISI.0000042934.15159.49', '10.1146/annurev-bioeng-071910-124649', '10.1109/ICCV.2007.4408890', '10.1007/11585978_10', '10.1023/A:1026501619075', '10.1109/38.988747', '10.1023/A:1013961817285', '10.1109/ICCV.1998.710763', '10.1109/TPAMI.2007.70712', '10.1109/34.250841', '10.1109/ICPR.2000.903724', '10.1109/CVPR.2008.4587562', '10.1109/TPAMI.2010.147', '10.1007/978-3-642-02498-6_45', '10.1109/TPAMI.2003.1206509', '10.1016/j.cviu.2008.06.006', '10.1109/ICCV.2003.1238310', '10.1109/CVPR.2009.5206669', '10.1109/CVPR.2010.5539890', '10.1007/978-3-642-15555-0_16', '10.1109/ICCV.2011.6126393', '10.1007/BF02980577', '10.1017/S0305004100027419', '10.1109/CVPR.1998.698673', '10.1109/TPAMI.1986.4767807', '10.1109/34.9105', '10.1007/978-3-540-88690-7_34', '10.1016/j.imavis.2008.02.006', '10.1007/978-3-642-33718-5_43', '10.1109/TPAMI.2009.120', '10.1109/ICCV.2009.5459175', '10.1007/978-3-642-15552-9_35', '10.1145/1778765.1778839', '10.1109/CVPR.2011.5995513', '10.1007/s11263-009-0273-6', '10.1109/T-C.1973.223602', '10.1109/CVPR.2006.180', '10.5244/C.23.3', '10.1109/CVPR.2009.5206754', '10.1109/CVPR.2013.82', '10.1109/TPAMI.2009.167', '10.1109/CVPR.2005.329', '10.1007/978-3-642-15558-1_53', '10.1109/CVPR.2010.5539951', '10.1109/ICCV.2009.5459303', '10.1016/j.media.2009.05.004', '10.1109/TMI.2007.896924', '10.1109/CVPR.2009.5206649', '10.1109/CVPR.2009.5206714', '10.1109/ISBI.2011.5872733', '10.1109/CVPR.2005.160', '10.1007/s11263-008-0197-6', '10.1109/CVPR.2007.383204', '10.1109/TPAMI.2008.217', '10.1109/CVPR.2008.4587417', '10.1007/s11263-008-0202-0', '10.1109/TPAMI.2009.131', '10.1109/CVPR.2009.5206846', '10.1007/BF00133570', '10.1109/34.57681', '10.1007/978-3-540-88682-2_29', '10.1007/978-3-642-15558-1_20', '10.1007/978-3-642-32717-9_5', '10.1109/ICCV.2009.5459262', '10.1109/TPAMI.2012.110', '10.1007/978-3-642-15711-0_24', '10.1109/ICCV.2011.6126258', '10.1109/CVPR.2008.4587440', '10.1109/CVPR.2009.5206567', '10.1109/CVPR.2010.5539897', '10.1007/s11263-011-0437-z', '10.1109/34.537343', '10.1007/978-3-642-15555-0_18', '10.1007/s11263-012-0583-y', '10.1561/2200000013', '10.1109/ICCV.2001.937505', '10.1109/ICCV.2009.5459248', '10.1007/978-3-642-15561-1_31', '10.1007/s11263-007-0109-1', '10.1007/978-3-642-33718-5_26', '10.1109/CVPR.2013.317', '10.1109/CVPR.2013.217', '10.1007/BF01890546', '10.1109/18.825794', '10.1109/36.662728', '10.1007/BF00054995', '10.1109/18.910585', '10.1007/s11263-006-7899-4', '10.1109/CVPR.1998.698598', '10.1109/TPAMI.2007.1031', '10.1109/CVPR.2007.383203', '10.1109/TPAMI.2007.1061', '10.1109/CVPR.2007.383095', '10.1109/TPAMI.2007.1036', '10.1109/TPAMI.2010.108', '10.1145/129712.129736', '10.1145/48014.61051', '10.1016/S0166-218X(01)00341-9', '10.1109/ICCV.1999.791261', '10.1007/978-3-540-40899-4_28', '10.1109/CVPR.2000.855839', '10.1109/ICCV.1999.791245', '10.1109/CVPR.2007.383249', '10.1007/s11263-011-0491-6', '10.1007/978-3-642-33712-3_61', '10.1007/BF02612354', '10.1145/1390156.1390217', '10.1109/TPAMI.2009.143', '10.1109/ICCV.2005.81', '10.1109/CVPR.2006.47', '10.1109/CVPR.2008.4587402', '10.1109/TPAMI.2009.194', '10.1007/978-3-540-45243-0_52', '10.1109/CVPR.2011.5995449', '10.1109/TPAMI.2010.135', '10.1023/B:STCO.0000021412.33763.d5', '10.1109/ICCV.2003.1238444', '10.1007/11744085_35', '10.1007/978-3-540-69321-5_5', '10.1016/S0734-189X(86)80047-0', '10.1109/ICCV.2011.6126392', '10.1561/2200000001', '10.1109/CVPR.2013.175', '10.1007/978-3-540-88690-7_60', '10.1109/TPAMI.2009.134', '10.1007/978-3-540-88688-4_44', '10.1109/ICCV.2009.5459287', '10.1109/CVPR.2010.5539886', '10.1109/CVPR.2008.4587401', '10.1007/11744047_21', '10.1016/j.cviu.2008.05.007', '10.1109/CVPR.2009.5206689', '10.1109/ICCV.2011.6126347', '10.1007/978-3-540-88690-7_8', '10.1109/CVPR.2005.143', '10.1109/TPAMI.2010.91', '10.1109/CVPR.2011.5995452', '10.1007/s11263-012-0531-x', '10.1109/CVPR.2007.383094', '10.1109/TPAMI.2011.121', '10.1109/CVPR.2008.4587355', '10.1109/CVPR.2010.5540189', '10.1109/CVPR.2009.5206739', '10.1109/CVPR.2010.5539858', '10.1109/CVPR.2013.223', '10.1109/CVPR.2009.5206590', '10.1007/978-3-540-88688-4_43', '10.1145/1390156.1390195', '10.1109/CVPR.2008.4587699', '10.1109/CVPR.2005.133', '10.1109/CVPR.2011.5995375', '10.1145/1553374.1553523', '10.1109/ICCV.2011.6126227', '10.1109/CVPR.2009.5206774'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['deep learning', 'computer vision', 'machine learning', 'medical image analysis', 'artificial intelligence'], conference_acronym='Computer vision and image understanding (Print)', publisher=None, query_handler=None),\n",
       " 'Deep Hashing Network for Efficient Similarity Retrieval': Paper(DOI='10.1609/aaai.v30i1.10235', crossref_json=None, google_schorlar_metadata=None, title='Deep Hashing Network for Efficient Similarity Retrieval', authors=['Han Zhu', 'Mingsheng Long', 'Jianmin Wang', 'Yue Cao'], abstract='Due to the storage and retrieval efficiency, hashing has been widely deployed to approximate nearest neighbor search for large-scale multimedia retrieval. Supervised hashing, which improves the quality of hash coding by exploiting the semantic similarity on data pairs, has received increasing attention recently. For most existing supervised hashing methods for image retrieval, an image is first represented as a vector of hand-crafted or machine-learned features, followed by another separate quantization step that generates binary codes. However, suboptimal hash coding may be produced, because the quantization error is not statistically minimized and the feature representation is not optimally compatible with the binary coding. In this paper, we propose a novel Deep Hashing Network (DHN) architecture for supervised hashing, in which we jointly learn good image representation tailored to hash coding and formally control the quantization error. The DHN model constitutes four key components:(1) a sub-network with multiple convolution-pooling layers to capture image representations;(2) a fully-connected hashing layer to generate compact binary hash codes;(3) a pairwise cross-entropy loss layer for similarity-preserving learning; and (4) a pairwise quantization loss for controlling hashing quality. Extensive experiments on standard image retrieval datasets show the proposed DHN model yields substantial boosts over latest state-of-the-art hashing methods.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Transferable representation learning with deep adaptation networks': Paper(DOI='10.1109/tpami.2018.2868685', crossref_json=None, google_schorlar_metadata=None, title='Transferable representation learning with deep adaptation networks', authors=['Mingsheng Long', 'Yue Cao', 'Zhangjie Cao', 'Jianmin Wang', 'Michael I Jordan'], abstract='Domain adaptation studies learning algorithms that generalize across source domains and target domains that exhibit different distributions. Recent studies reveal that deep neural networks can learn transferable features that generalize well to similar novel tasks. However, as deep features eventually transition from general to specific along the network, feature transferability drops significantly in higher task-specific layers with increasing domain discrepancy. To formally reduce the effects of this discrepancy and enhance feature transferability in task-specific layers, we develop a novel framework for deep adaptation networks that extends deep convolutional neural networks to domain adaptation problems. The framework embeds the deep features of all task-specific layers into reproducing kernel Hilbert spaces (RKHSs) and optimally matches different domain distributions. The deep features are made more\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2013.50', '10.1109/CVPR.2017.316', '10.1109/ICCV.2011.6126344', '10.1109/ICCV.2015.463', '10.1109/TKDE.2009.191', '10.1109/CVPR.2014.81', '10.1109/TPAMI.2013.167', '10.1109/TPAMI.2011.114', '10.7551/mitpress/9780262033589.001.0001', '10.1145/2647868.2654889', '10.1109/ICCV.2013.368', '10.1109/CVPR.2011.5995347', '10.1109/TKDE.2014.2373376', '10.1007/s10994-009-5152-4', '10.1109/CVPR.2015.7298594', '10.1109/CVPR.2016.90', '10.1007/s11263-015-0816-y', '10.1109/CVPR.2014.222', '10.1109/ICCV.2013.274', '10.1109/TNN.2010.2091281', '10.1214/13-AOS1140', '10.1109/CVPR.2017.18', '10.1109/CVPR.2017.241'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Negative Margin Matters: Understanding Margin in Few-shot Classification': Paper(DOI='10.1007/978-3-030-58548-8_26', crossref_json=None, google_schorlar_metadata=None, title='Negative Margin Matters: Understanding Margin in Few-shot Classification', authors=['Bin Liu', 'Yue Cao', 'Yutong Lin', 'Qi Li', 'Zheng Zhang', 'Mingsheng Long', 'Han Hu'], abstract=' This paper introduces a negative margin loss to metric learning based few-shot learning methods. The negative margin loss significantly outperforms regular softmax loss, and achieves state-of-the-art accuracy on three standard few-shot classification benchmarks with few bells and whistles. These results are contrary to the common practice in the metric learning field, that the margin is zero or positive. To understand why the negative margin loss performs well for the few-shot classification, we analyze the discriminability of learned features w.r.t different margins for training and novel classes, both empirically and theoretically. We find that although negative margin reduces the feature discriminability for training classes, it may also avoid falsely mapping samples of the same novel class to multiple peaks or clusters, and thus benefit the discrimination of novel classes. Code is available at                  https\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2017.502', '10.1109/ICCV.2017.89', '10.1109/CVPR.2009.5206848', '10.1109/CVPR.2019.00482', '10.1109/ICCV.2019.00815', '10.1109/ICCV.2017.328', '10.1109/CVPR.2016.90', '10.1214/aos/1015362183', '10.1109/CVPR.2019.01091', '10.1145/3240508.3240516', '10.1109/CVPR.2017.713', '10.1109/WACV45572.2020.9093338', '10.1109/ICASSP.2019.8682255', '10.1109/CVPR.2018.00610', '10.1109/CVPR.2018.00755', '10.1109/CVPR.2015.7298682', '10.1109/CVPR.2018.00131', '10.1109/CVPR.2018.00552', '10.1109/CVPR.2018.00760', '10.1007/978-3-030-01234-2_42', '10.5244/C.30.87', '10.1109/CVPR.2018.00534'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Disentangled Non-Local Neural Networks': Paper(DOI='10.1117/1.jei.30.5.053029', crossref_json=None, google_schorlar_metadata=None, title='Disentangled Non-Local Neural Networks', authors=['Minghao Yin', 'Zhuliang Yao', 'Yue Cao', 'Xiu Li', 'Zheng Zhang', 'Stephen Lin', 'Han Hu'], abstract=' The non-local block is a popular module for strengthening the context modeling ability of a regular convolutional neural network. This paper first studies the non-local block in depth, where we find that its attention computation can be split into two terms, a whitened pairwise term accounting for the relationship between two pixels and a unary term representing the saliency of every pixel. We also observe that the two terms trained alone tend to model different visual clues, e.g. the whitened pairwise term learns within-region relationships while the unary term learns salient boundaries. However, the two terms are tightly coupled in the non-local block, which hinders the learning of each. Based on these findings, we present the disentangled non-local block, where the two terms are decoupled to facilitate learning for both terms. We demonstrate the effectiveness of the decoupled design on various tasks, such as\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision'], conference_acronym='Journal of electronic imaging (Print)', publisher=None, query_handler=None),\n",
       " 'Deep learning of transferable representation for scalable domain adaptation': Paper(DOI='10.1109/tkde.2016.2554549', crossref_json=None, google_schorlar_metadata=None, title='Deep learning of transferable representation for scalable domain adaptation', authors=['Mingsheng Long', 'Jianmin Wang', 'Yue Cao', 'Jiaguang Sun', 'S Yu Philip'], abstract='Domain adaptation generalizes a learning model across source domain and target domain that are sampled from different distributions. It is widely applied to cross-domain data mining for reusing labeled information and mitigating labeling consumption. Recent studies reveal that deep neural networks can learn abstract feature representation, which can reduce, but not remove, the cross-domain discrepancy. To enhance the invariance of deep representation and make it more transferable across domains, we propose a unified deep adaptation framework for jointly learning transferable representation and classifier to enable scalable domain adaptation, by taking the advantages of both deep learning and optimal two-sample matching. The framework constitutes two inter-dependent paradigms, unsupervised pre-training for effective training of deep models using deep denoising autoencoders, and supervised fine\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1126/science.1127647', '10.1109/TKDE.2013.111', '10.1214/13-AOS1140', '10.1007/978-3-642-15561-1_16', '10.1109/TKDE.2009.191', '10.1109/TKDE.2014.2373376', '10.1145/2339530.2339730', '10.1109/TPAMI.2013.50', '10.1007/s10994-009-5152-4', '10.1145/1772690.1772767', '10.1109/TNN.2010.2091281', '10.1109/TPAMI.2011.114', '10.1109/TNNLS.2011.2178556', '10.1109/ICCV.2013.274'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Deep Learning', 'Computer Vision'], conference_acronym='IEEE transactions on knowledge and data engineering (Print)', publisher=None, query_handler=None),\n",
       " 'Rethinking the value of network pruning': Paper(DOI='10.3390/jimaging8030064', crossref_json=None, google_schorlar_metadata=None, title='Rethinking the value of network pruning', authors=['Zhuang Liu', 'Mingjie Sun', 'Tinghui Zhou', 'Gao Huang', 'Trevor Darrell'], abstract='Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned \"important\" weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited \"important\" weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the \"Lottery Ticket Hypothesis\" (Frankle & Carbin 2019), and find that with optimal learning rate, the \"winning ticket\" initialization as used in Frankle & Carbin (2019) does not bring\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/3065386', '10.1109/ICASSP40776.2020.9053986', '10.1109/72.248452', '10.1109/CVPR.2018.00958', '10.1109/72.80236', '10.1103/PhysRevA.39.6600', '10.1038/nature14539', '10.1109/TC.2019.2914438', '10.1038/s41467-018-04316-3', '10.1162/neco.1992.4.4.473', '10.1145/3005348', '10.1109/ICCV.2017.155', '10.1109/WACV.2018.00083', '10.1007/s10589-008-9218-1', '10.1109/TNNLS.2021.3063265', '10.1609/aimag.v29i3.2157'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Everybody dance now': Paper(DOI='10.1093/combul/bwl126', crossref_json=None, google_schorlar_metadata=None, title='Everybody dance now', authors=['Caroline Chan', 'Shiry Ginosar', 'Tinghui Zhou', 'Alexei A Efros'], abstract='This paper presents a simple method for\" do as I do\" motion transfer: given a source video of a person dancing, we can transfer that performance to a novel (amateur) target after only a few minutes of the target subject performing standard moves. We approach this problem as video-to-video translation using pose as an intermediate representation. To transfer the motion, we extract poses from the source subject and apply the learned pose-to-appearance mapping to generate the target subject. We predict two consecutive frames for temporally coherent video results and introduce a separate pipeline for realistic face synthesis. Although our method is quite simple, it produces surprisingly compelling results (see video). This motivates us to also provide a forensics tool for reliable synthetic content detection, which is able to distinguish videos synthesized by our system from real data. In addition, we release a first-of-its-kind open-source dataset of videos that can be legally used for training and motion transfer.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='ITNOW (Online)', publisher=None, query_handler=None),\n",
       " 'View synthesis by appearance flow': Paper(DOI='10.1111/cgf.13860', crossref_json=None, google_schorlar_metadata=None, title='View synthesis by appearance flow', authors=['Tinghui Zhou', 'Shubham Tulsiani', 'Weilun Sun', 'Jitendra Malik', 'Alexei A Efros'], abstract=' We address the problem of novel view synthesis: given an input image, synthesizing new images of the same object or scene observed from arbitrary viewpoints. We approach this as a learning task but, critically, instead of learning to synthesize pixels from scratch, we learn to copy them from the input image. Our approach exploits the observation that the visual appearance of different views of the same instance is highly correlated, and such correlation could be explicitly learned by training a convolutional neural network (CNN) to predict appearance flows – 2-D coordinate vectors specifying which pixels in the input view could be used to reconstruct the target view. Furthermore, the proposed framework easily generalizes to multiple input views by learning how to optimally combine single-view predictions. We show that for both objects and scenes, our approach is able to synthesize novel views of higher\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/383259.383309', '10.1109/CVPR.2019.00382', '10.1145/2487228.2487238', '10.1109/ICCV.2017.168', '10.1109/CVPR.2009.5206848', '10.1109/CVPR.2015.7298761', '10.1109/CVPR.2016.595', '10.1007/s11263-005-6643-9', '10.1109/CVPR.2012.6248074', '10.1145/3272127.3275084', '10.1007/978-3-319-46475-6_43', '10.1109/CVPR.2018.00938', '10.1561/0600000012', '10.1145/2980179.2980251', '10.1145/1531326.1531350', '10.1145/237170.237199', '10.1109/CVPR.2018.00485', '10.1109/ICCV.2017.478', '10.1109/CVPR.2018.00059', '10.1109/CVPR.2018.00183', '10.1109/CVPR.2017.244', '10.1109/ICCV.2017.37', '10.1145/3130800.3130855', '10.1145/325165.325242', '10.1109/CVPR.2019.00254', '10.1109/CVPR.2018.00931', '10.1007/978-3-319-46487-9_31', '10.1007/978-3-319-46478-7_20', '10.1145/3306346.3323035', '10.1109/TIP.2003.819861', '10.1016/j.image.2003.07.001', '10.1109/CVPR.2018.00068', '10.1007/978-3-319-46454-1_36', '10.1145/1015706.1015766', '10.1145/3197517.3201323', '10.1007/978-3-319-46493-0_18'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Computer graphics forum (Print)', publisher=None, query_handler=None),\n",
       " 'Multi-view relighting using a geometry-aware network.': Paper(DOI='10.1145/3306346.3323013', crossref_json=None, google_schorlar_metadata=None, title='Multi-view relighting using a geometry-aware network.', authors=['Julien Philip', 'Michaël Gharbi', 'Tinghui Zhou', 'Alexei A Efros', 'George Drettakis'], abstract='Changing the illumination of an outdoor image is a notoriously difficult problem that requires the lighting to be modified consistently across the image, and shadows to be removed and resynthesized for the new sun position [Duchêne et al. 2015; Tchou et al. 2004; Yu et al. 1999]. Cast shadows are particularly challenging because an occluder can be arbitrarily far from the point it shadows, or even out of view.The basic premise of our approach is to use multi-view information and approximate 3D geometry to reason about non-local lighting interactions and guide the relighting task. We introduce the first learning-based algorithm that can relight multi-view datasets of outdoor scenes (Fig. 1), which have become a commodity thanks to smartphone cameras, large-scale internet photo collections and drone cinematography. Our model uses a neural network designed to exploit geometric cues. It includes a careful treatment of cast shadows and is trained solely on realistic synthetic renderings.', conference=None, journal=None, year=None, reference_list=['10.1145/383259.383309', '10.1109/38.988744', '10.1145/344779.344855', '10.1145/2756549', '10.1109/TPAMI.2006.18', '10.1109/ICCV.2007.4408933', '10.1145/2732407', '10.1109/CVPR.2011.5995725', '10.1109/CVPR.2016.90', '10.1109/CVPR.2017.255', '10.1145/2185520.2185591', '10.1109/CVPR.2017.632', '10.1007/978-3-319-46475-6_43', '10.1145/3272127.3275104', '10.1145/2070781.2024191', '10.1145/2601097.2601209', '10.1145/1409060.1409069', '10.1145/2366145.2366221', '10.1007/978-3-030-01267-0_11', '10.1109/ICCV.2009.5459163', '10.1145/1618452.1618477', '10.5555/1888028.1888053', '10.1364/JOSA.61.000001', '10.1145/882262.882315', '10.1109/MCG.2007.30', '10.1145/1276377.1276442', '10.1016/j.patcog.2011.10.001', '10.1145/2508363.2508419', '10.1109/CVPR.2017.578', '10.1145/1141911.1141964', '10.1145/1029949.1029977', '10.1145/1276377.1276504', '10.1145/1186223.1186323', '10.1109/CVPR.2018.00192', '10.1109/TPAMI.2008.244', '10.1109/ICCV.2001.937606', '10.1145/1073204.1073258', '10.1145/1243980.1243982', '10.1145/3197517.3201313', '10.1145/311535.311559', '10.1109/ICCV.2017.244'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " 'Learning to factorize and relight a city': Paper(DOI='10.1007/978-3-030-58548-8_32', crossref_json=None, google_schorlar_metadata=None, title='Learning to factorize and relight a city', authors=['Andrew Liu', 'Shiry Ginosar', 'Tinghui Zhou', 'Alexei A Efros', 'Noah Snavely'], abstract=' We propose a learning-based framework for disentangling outdoor scenes into temporally-varying illumination and permanent scene factors. Inspired by the classic intrinsic image decomposition, our learning signal builds upon two insights: 1) combining the disentangled factors should reconstruct the original image, and 2) the permanent factors should stay constant across multiple temporal samples of the same scene. To facilitate training, we assemble a city-scale dataset of outdoor timelapse imagery from Google Street View, where the same locations are captured repeatedly through time. This data represents an unprecedented scale of spatio-temporal outdoor imagery. We show that our learned disentangled factors can be used to manipulate novel images in realistic ways, such as changing lighting effects and scene geometry. Please visit                  http://factorize-a-city.github.io/                                 for\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1017/CBO9780511984037.014', '10.1109/TVCG.2014.2346446', '10.1109/TPAMI.2014.2377712', '10.1145/2601097.2601206', '10.1109/ICCV.2013.122', '10.1145/2185520.2185597', '10.1073/pnas.1700035114', '10.1109/CVPR.2013.122', '10.1109/CVPR.2017.255', '10.1109/ICCV.2007.4408858', '10.1109/CVPR.2017.632', '10.1109/CVPR.2007.383258', '10.1007/978-3-319-46475-6_43', '10.1007/978-3-030-01267-0_23', '10.1109/ICCV.2015.57', '10.1145/2601097.2601101', '10.1145/1618452.1618477', '10.1109/ICCPHOT.2015.7168368', '10.1007/978-3-030-01219-9_23', '10.1109/CVPR.2018.00942', '10.1109/CVPR42600.2020.00255', '10.1145/2766903', '10.1109/CVPR.2019.00704', '10.1109/CVPRW.2014.121', '10.1109/CVPR.2019.00244', '10.1109/CVPR.2011.5995374', '10.1109/ICCV.2019.00869', '10.1109/CVPR.2018.00659', '10.1145/1275808.1276504', '10.1007/978-3-319-46448-0_30', '10.1109/CVPR.2018.00917', '10.1109/CVPR.2019.00327', '10.1109/ICCV.2015.396', '10.1007/978-3-319-46484-8_16'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Exploring Simple and Transferable Recognition-Aware Image Processing': Paper(DOI='10.1109/tpami.2022.3183243', crossref_json=None, google_schorlar_metadata=None, title='Exploring Simple and Transferable Recognition-Aware Image Processing', authors=['Zhuang Liu', 'Hung-Ju Wang', 'Tinghui Zhou', 'Zhiqiang Shen', 'Bingyi Kang', 'Evan Shelhamer', 'Trevor Darrell'], abstract=\"Recent progress in image recognition has stimulated the deployment of vision systems at an unprecedented scale. As a result, visual data are now often consumed not only by humans but also by machines. Existing image processing methods only optimize for better human perception, yet the resulting images may not be accurately recognized by machines. This can be undesirable, e.g., the images can be improperly handled by search engines or recommendation systems. In this work, we examine simple approaches to improve machine recognition of processed images: optimizing the recognition loss directly on the image processing network or through an intermediate input transformation model. Interestingly, the processing model's ability to enhance recognition quality can transfer when evaluated on models of different architectures, recognized categories, tasks and training datasets. This makes the methods applicable even when we do not have the knowledge of future recognition models, e.g., when uploading processed images to the Internet. We conduct experiments on multiple image processing tasks paired with ImageNet classification and PASCAL VOC detection as recognition tasks. With these simple yet effective methods, substantial accuracy gain can be achieved with strong transferability and minimal image quality loss. Through a user study we further show that the accuracy gain can transfer to a black-box cloud model. Finally, we try to explain this transferability phenomenon by demonstrating the similarities of different models' decision boundaries. Code is available at https://github.com/liuzhuang13/Transferable_RA .\", conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2018.00262', '10.1016/0167-2789(92)90242-F', '10.1109/MSP.2003.1203207', '10.1109/CVPR.2018.00194', '10.1109/CVPR.2016.302', '10.1109/CVPR.2018.00179', '10.1109/TIT.2005.862083', '10.1109/CVPR.2016.91', '10.1109/CVPR.2017.632', '10.1109/CVPR.2016.182', '10.1007/978-3-319-46475-6_25', '10.1109/CVPR.2016.181', '10.1109/CVPR.2016.207', '10.1109/CVPR.2017.243', '10.1109/ICCV.2017.244', '10.1609/aaai.v32i1.12287', '10.1109/ICCV48922.2021.00055', '10.1109/CVPR.2016.90', '10.1109/CVPR.2018.00068', '10.1109/CVPR.2017.19', '10.1007/978-3-319-46475-6_43', '10.1007/978-3-319-46487-9_40', '10.1109/ICCV.2013.84', '10.1109/ICCV.2017.514', '10.1109/TIP.2003.819861', '10.1109/CVPR.2018.00424', '10.1109/ICCV.2019.00153', '10.1109/CVPR.2018.00010', '10.1109/CVPR.2018.00328', '10.1145/3394171.3413937', '10.1007/978-3-319-10593-2_13', '10.1109/CVPR.2018.00547', '10.1109/ICCV.2017.481', '10.1109/CVPR.2016.518', '10.1109/ICCVW.2019.00131', '10.1109/CVPR.2017.618', '10.1007/978-3-319-46493-0_35', '10.1109/CVPR.2018.00333'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'High-resolution stereo datasets with subpixel-accurate ground truth': Paper(DOI='10.1007/978-3-319-11752-2_3', crossref_json=None, google_schorlar_metadata=None, title='High-resolution stereo datasets with subpixel-accurate ground truth', authors=['Daniel Scharstein', 'Heiko Hirschmüller', 'York Kitajima', 'Greg Krathwohl', 'Nera Nešić', 'Xi Wang', 'Porter Westling'], abstract=' We present a structured lighting system for creating high-resolution stereo datasets of static indoor scenes with highly accurate ground-truth disparities. The system includes novel techniques for efficient 2D subpixel correspondence search and self-calibration of cameras and projectors with modeling of lens distortion. Combining disparity estimates from multiple projector positions we are able to achieve a disparity accuracy of 0.2 pixels on most observed surfaces, including in half-occluded regions. We contribute 33 new 6-megapixel datasets obtained with our system and demonstrate that they present new challenges for the next generation of stereo algorithms.', conference=None, journal=None, year=None, reference_list=['10.1016/S0031-3203(97)00074-5', '10.1109/TPAMI.2003.1217603', '10.1109/TPAMI.2005.37', '10.1109/CVPR.2012.6248074', '10.1007/978-3-642-19315-6_3', '10.1007/s11263-012-0554-3', '10.1109/CVPR.2012.6247753', '10.1109/CVPR.2012.6247784', '10.1109/CVPR.2009.5206493', '10.1109/TPAMI.2008.221', '10.1109/TPAMI.2007.1166', '10.1023/A:1014554110407', '10.1109/TPAMI.2012.46', '10.1145/2047196.2047270', '10.1080/15599610802438680', '10.1145/344779.344849', '10.1145/1486525.1486527', '10.1109/CVPR.1996.517099', '10.1016/j.patcog.2010.03.004', '10.1023/A:1014573219977', '10.1109/CVPR.2008.4587706', '10.1007/BFb0028345'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Robotics'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Evaluation of stereo matching costs on images with radiometric differences': Paper(DOI='10.1109/tpami.2008.221', crossref_json=None, google_schorlar_metadata=None, title='Evaluation of stereo matching costs on images with radiometric differences', authors=['Heiko Hirschmuller', 'Daniel Scharstein'], abstract='Stereo correspondence methods rely on matching costs for computing the similarity of image locations. We evaluate the insensitivity of different costs for passive binocular stereo methods with respect to radiometric variations of the input images. We consider both pixel-based and window-based variants like the absolute difference, the sampling-insensitive absolute difference, and normalized cross correlation, as well as their zero-mean versions. We also consider filters like LoG, mean, and bilateral background subtraction (BilSub) and nonparametric measures like Rank, SoftRank, Census, and Ordinal. Finally, hierarchical mutual information (HMI) is considered as pixelwise cost. Using stereo data sets with ground-truth disparities taken under controlled changes of exposure and lighting, we evaluate the costs with a local, a semiglobal, and a global stereo method. We measure the performance of all costs in the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.677269', '10.1007/s11263-006-8671-5', '10.1109/ICCV.2007.4408933', '10.1023/A:1020149707513', '10.1109/CVPR.2007.383159', '10.1023/A:1014573219977', '10.1109/TPAMI.2003.1217603', '10.1109/3DPVT.2006.78', '10.1177/02783640122067525', '10.1109/RATFG.1999.799242', '10.1109/TPAMI.2006.70', '10.1109/TDPVT.2004.1335420', '10.1145/1186562.1015766', '10.1109/TSMCB.2007.890584', '10.1109/ICPR.2002.1048459', '10.1109/TPAMI.2007.1171', '10.1109/CVPR.2006.260', '10.1109/TPAMI.2007.1166', '10.1023/A:1023713602895', '10.1109/TPAMI.2005.158', '10.1109/ROBOT.1999.770390', '10.1080/01621459.1987.10478480', '10.1109/FPGA.1997.624620', '10.1023/A:1007958904918', '10.1109/ICCV.1998.710815', '10.1007/BFb0028345', '10.1109/34.677275', '10.1109/CVPR.1997.609427', '10.1023/A:1014554110407', '10.1109/TDPVT.2004.1335273', '10.1109/TPAMI.2007.70844', '10.1109/CVPR.2003.1211354', '10.1109/TPAMI.2004.60', '10.1006/cviu.1996.0040', '10.1023/A:1008150329890', '10.1109/34.969114', '10.1109/CVPR.1997.609447', '10.1109/TPAMI.2004.1262177'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Robotics'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Stereo matching with nonlinear diffusion': Paper(DOI='10.1049/iet-cvi.2011.0231', crossref_json=None, google_schorlar_metadata=None, title='Stereo matching with nonlinear diffusion', authors=['Daniel Scharstein', 'Richard Szeliski'], abstract=' One of the central problems in stereo matching (and other image registration tasks) is the selection of optimal window sizes for comparing image regions. This paper addresses this problem with some novel algorithms based on iteratively diffusing support at different disparity hypotheses, and locally controlling the amount of diffusion based on the current quality of the disparity estimate. It also develops a novel Bayesian estimation technique, which significantly outperforms techniques based on area-based matching (SSD) and regular diffusion. We provide experimental results on both synthetic and real stereo image pairs.', conference=None, journal=None, year=None, reference_list=['10.1023/A:1014573219977', '10.1109/34.310690', '10.1109/TPAMI.2002.1114859', '10.1109/TPAMI.2006.70', '10.1023/A:1008150329890', '10.1109/34.969114', '10.1023/A:1008015117424', '10.1109/TPAMI.2003.1206509', '10.1109/34.865184', '10.1007/BF00363999', '10.1109/CVPR.2007.383248'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Robotics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'View synthesis using stereo vision': Paper(DOI='10.1007/3-540-48725-5_3', crossref_json=None, google_schorlar_metadata=None, title='View synthesis using stereo vision', authors=['Daniel Scharstein'], abstract='Image-based rendering, as an area of overlap between computer graphics and computer vision, uses computer vision techniques to aid in sythesizing new views of scenes. Image-based rendering methods are having a substantial impact on the field of computer graphics, and also play an important role in the related field of multimedia systems, for applications such as teleconferencing, remote instruction and surgery, virtual reality and entertainment. The book develops a novel way of formalizing the view synthesis problem under the full perspective model, yielding a clean, linear warping equation. It shows new techniques for dealing with visibility issues such as partial occlusion and\" holes\". Furthermore, the author thoroughly re-evaluates the requirements that view synthesis places on stereo algorithms and introduces two novel stereo algorithms specifically tailored to the application of view synthesis.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Robotics'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Sampling the disparity space image': Paper(DOI='10.7763/ijcte.2014.v6.902', crossref_json=None, google_schorlar_metadata=None, title='Sampling the disparity space image', authors=['Richard Szeliski', 'Daniel Scharstein'], abstract='A central issue in stereo algorithm design is the choice of matching cost. Many algorithms simply use squared or absolute intensity differences based on integer disparity steps. In this paper, we address potential problems with such approaches. We begin with a careful analysis of the properties of the continuous disparity space image (DSI) and propose several new matching cost variants based on symmetrically matching interpolated image signals. Using stereo images with ground truth, we empirically evaluate the performance of the different cost variants and show that proper sampling can yield improved matching performance.', conference=None, journal=None, year=None, reference_list=['10.1023/A:1008150329890', '10.1109/6046.748168'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Robotics'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Every pixel counts++: Joint learning of geometry and motion with 3d holistic understanding': Paper(DOI='10.1109/tpami.2019.2930258', crossref_json=None, google_schorlar_metadata=None, title='Every pixel counts++: Joint learning of geometry and motion with 3d holistic understanding', authors=['Chenxu Luo', 'Zhenheng Yang', 'Peng Wang', 'Yang Wang', 'Wei Xu', 'Ram Nevatia', 'Alan Yuille'], abstract='Learning to estimate 3D geometry in a single frame and optical flow from consecutive frames by watching unlabeled videos via deep convolutional network has made significant progress recently. Current state-of-the-art (SoTA) methods treat the two tasks independently. One typical assumption of the existing depth estimation methods is that the scenes contain no independent moving objects. while object moving could be easily modeled using optical flow. In this paper, we propose to address the two tasks as a whole, i.e., to jointly understand per-pixel 3D geometry and motion. This eliminates the need of static scene assumption and enforces the inherent geometrical consistency during the learning process, yielding significantly improved results for both tasks. We call our method as “Every Pixel Counts++” or “EPC++”. Specifically, during training, given two consecutive frames from a video, we adopt three parallel\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2017.238', '10.1109/CVPR.2015.7299035', '10.1109/CVPR.2016.438', '10.5244/C.28.21', '10.1007/s11263-018-1122-2', '10.1109/ICCV.2015.304', '10.1109/TPAMI.2017.2662005', '10.1109/CVPR.2015.7298652', '10.1109/ICRA.2018.8460564', '10.1109/CVPR.2015.7298720', '10.1007/s11263-006-0031-y', '10.1109/3DV.2016.23', '10.1109/ICCV.2017.498', '10.1109/CVPR.2010.5540002', '10.1109/3DV.2014.56', '10.1109/ICCV.2013.51', '10.1109/ICCV.2015.401', '10.1007/0-387-28831-7_23', '10.1109/CVPR.2017.291', '10.1109/ICCV.2015.316', '10.1109/CVPR.2018.00931', '10.1109/CVPR.2019.00826', '10.1109/TRO.2015.2463671', '10.1109/ICCV.1999.790293', '10.1109/TPAMI.2005.63', '10.1007/s11263-013-0684-2', '10.1109/ICCV.2017.281', '10.1109/ICCV.2013.174', '10.1109/CVPR.2016.423', '10.1109/ICCV.2011.6126513', '10.1109/ICCV.2019.00393', '10.1109/TPAMI.2007.70752', '10.1109/TPAMI.2008.132', '10.1109/CVPR.2012.6248074', '10.5244/C.25.14', '10.1007/978-3-642-33783-3_44', '10.1109/CVPR.2017.179', '10.1006/cviu.1996.0006', '10.1109/TIP.2003.819861', '10.1109/ICCV.2017.322', '10.1609/aaai.v33i01.33018001', '10.1109/CVPR.2018.00594', '10.1109/ICRA.2018.8461251', '10.1109/CVPR.2018.00216', '10.1109/CVPR.2017.700', '10.1109/3DV.2016.32', '10.1016/0004-3702(81)90024-2', '10.1109/CVPR.2018.00216', '10.1109/CVPR.2017.238', '10.1109/CVPR.2018.00212', '10.1109/CVPR.2015.7298965', '10.1109/CVPR.2019.01252', '10.1109/CVPR.2015.7298925', '10.1109/CVPR.2018.00031', '10.1109/34.982903', '10.1109/CVPR.2018.00513', '10.1109/CVPR.2016.352', '10.1109/CVPR.2017.699', '10.1109/CVPR.2015.7298897', '10.1109/CVPR.2015.7299152', '10.1109/ICCV.2017.365', '10.1109/CVPR.2014.19', '10.1109/TPAMI.2014.2316835'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['artificial intelligence', 'deep learning', 'NLP', 'computer vision', 'reinforcement learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Softcuts: a soft edge smoothness prior for color image super-resolution': Paper(DOI='10.1109/tip.2009.2012908', crossref_json=None, google_schorlar_metadata=None, title='Softcuts: a soft edge smoothness prior for color image super-resolution', authors=['Shengyang Dai', 'Mei Han', 'Wei Xu', 'Ying Wu', 'Yihong Gong', 'Aggelos K Katsaggelos'], abstract=' Designing effective image priors is of great interest to image super-resolution (SR), which is a severely under-determined problem. An edge smoothness prior is favored since it is able to suppress the jagged edge artifact effectively. However, for soft image edges with gradual intensity transitions, it is generally difficult to obtain analytical forms for evaluating their smoothness. This paper characterizes soft edge smoothness based on a novel SoftCuts metric by generalizing the Geocuts method  . The proposed soft edge smoothness measure can approximate the average length of all level lines in an intensity image. Thus, the total length of all level lines can be minimized effectively by integrating this new form of prior. In addition, this paper presents a novel combination of this soft edge smoothness prior and the alpha matting technique for color image SR, by adaptively normalizing image edges according to their \\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1276377.1276389', '10.1109/TIP.2007.916051', '10.1109/83.902288', '10.1016/0167-2789(92)90242-F', '10.1109/34.969114', '10.1109/ICIP.2006.312943', '10.1109/ICCV.2007.4408922', '10.1109/CVPR.2007.383029', '10.1109/ICIP.2008.4711836', '10.1109/TIP.2005.860336', '10.1109/ICIP.1996.560768', '10.1109/83.951537', '10.1109/ICIP.2005.1530224', '10.1109/CVPR.2001.990494', '10.1109/CVPR.2005.422', '10.1109/TIP.2003.816007', '10.1109/CVPR.2007.383028', '10.1006/jvci.1993.1030', '10.1109/TPAMI.2004.1261081', '10.1109/TPAMI.2004.60', '10.1109/TPAMI.2002.1033210', '10.1109/83.650118', '10.1109/CVPR.2007.383147', '10.1109/TIP.2004.834669', '10.1109/ICCV.2003.1238310', '10.1109/38.988747', '10.1023/A:1026501619075', '10.5244/C.20.57'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['artificial intelligence', 'deep learning', 'NLP', 'computer vision', 'reinforcement learning'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Spatio-temporal lstm with trust gates for 3d human action recognition': Paper(DOI='10.1007/978-3-319-46487-9_50', crossref_json=None, google_schorlar_metadata=None, title='Spatio-temporal lstm with trust gates for 3d human action recognition', authors=['Jun Liu', 'Amir Shahroudy', 'Dong Xu', 'Gang Wang'], abstract=' 3D action recognition – analysis of human actions based on 3D skeleton data – becomes popular recently due to its succinctness, robustness, and view-invariant representation. Recent attempts on this problem suggested to develop RNN-based learning methods to model the contextual dependency in the temporal domain. In this paper, we extend this idea to spatio-temporal domains to analyze the hidden sources of action-related information within the input data over both domains concurrently. Inspired by the graphical structure of the human skeleton, we further propose a more powerful tree-structure based traversal method. To handle the noise and occlusion in 3D skeleton data, we introduce new gating mechanism within LSTM to learn the reliability of the sequential input data and accordingly adjust its effect on updating the long-term context information stored in the memory cell. Our method achieves\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.cviu.2017.01.011', '10.1016/j.imavis.2016.06.007', '10.1109/CVPRW.2012.6239233', '10.1109/ICPR.2014.772', '10.1109/CVPR.2014.82', '10.1109/ICCV.2013.227', '10.1109/CVPRW.2013.76', '10.1109/ICASSP.2011.5947611', '10.21437/Interspeech.2012-65', '10.21437/Interspeech.2013-596', '10.1109/CVPR.2015.7298935', '10.1109/CVPR.2015.7299101', '10.1109/CVPR.2016.216', '10.1109/CVPR.2016.573', '10.1109/CVPR.2016.110', '10.1109/CVPR.2016.516', '10.1109/CVPR.2016.217', '10.1109/CVPR.2016.214', '10.1109/CVPR.2016.116', '10.1007/978-3-319-46478-7_13', '10.1007/978-3-319-46478-7_9', '10.1007/978-3-319-46484-8_48', '10.21236/ADA623249', '10.1145/2911996.2912001', '10.1145/2733373.2806222', '10.1109/ICCV.2015.460', '10.1109/CVPR.2016.115', '10.1007/978-3-319-04561-0_2', '10.1109/TPAMI.2015.2505295', '10.1109/ICCV.2013.334', '10.1109/WACV.2014.6836044', '10.1109/ISCCSP.2014.6877819', '10.1109/CVPR.2016.289', '10.1109/CVPR.2015.7298860', '10.1109/CVPR.2016.218', '10.1007/978-3-319-46448-0_17', '10.1109/ICASSP.2016.7472170', '10.1109/CVPR.2016.167', '10.1016/j.imavis.2016.04.004', '10.1109/ICCVW.2015.48', '10.1109/TPAMI.2015.2505295', '10.1609/aaai.v30i1.10451', '10.1162/neco.1997.9.8.1735', '10.1007/978-3-642-24797-2_2', '10.1109/CVPR.2011.5995741', '10.1109/ICASSP.2013.6638947', '10.1109/CVPRW.2012.6239234', '10.1109/WACV.2013.6474999', '10.1109/CVPR.2015.7299172', '10.1007/s11263-015-0876-z', '10.1109/CVPRW.2013.78', '10.1109/ICMEW.2014.6890714', '10.1109/ICCV.2015.505', '10.1109/TCYB.2014.2350774', '10.1109/CVPR.2015.7298934', '10.1109/CVPRW.2010.5543273', '10.1109/NCVPRIPG.2013.6776204', '10.1109/ISSNIP.2014.6827664'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Multimedia', 'Transfer Learning', 'Compression'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'A siamese long short-term memory architecture for human re-identification': Paper(DOI='10.1007/978-3-319-46478-7_9', crossref_json=None, google_schorlar_metadata=None, title='A siamese long short-term memory architecture for human re-identification', authors=['Rahul Rama Varior', 'Bing Shuai', 'Jiwen Lu', 'Dong Xu', 'Gang Wang'], abstract=' Matching pedestrians across multiple camera views known as human re-identification (re-identification) is a challenging problem in visual surveillance. In the existing works concentrating on feature extraction, representations are formed locally and independent of other regions. We present a novel siamese Long Short-Term Memory (LSTM) architecture that can process image regions sequentially and enhance the discriminative capability of local feature representation by leveraging contextual information. The feedback connections and internal gating mechanism of the LSTM cells enable our model to memorize the spatial dependencies and selectively propagate relevant contextual information through the network. We demonstrate improved performance compared to the baseline algorithm with no LSTM units and promising results compared to state-of-the-art methods on Market-1501, CUHK03 and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2015.7299016', '10.1007/978-3-642-35289-8_26', '10.1109/72.279181', '10.1109/ICASSP.2013.6638244', '10.1109/CVPR.2015.7298977', '10.1109/CVPR.2016.142', '10.1109/CVPR.2016.149', '10.5244/C.25.68', '10.1145/1273496.1273523', '10.1109/CVPR.2010.5539926', '10.1007/978-3-540-74695-9_23', '10.1109/IJCNN.2000.861302', '10.1049/cp:19991218', '10.1016/j.neunet.2005.06.042', '10.1109/ICCV.2009.5459197', '10.1162/neco.1997.9.8.1735', '10.1109/CVPR.2015.7298932', '10.1109/CVPR.2012.6247939', '10.1109/TPAMI.2012.246', '10.1007/978-3-642-37331-2_3', '10.1109/CVPR.2014.27', '10.1109/CVPR.2014.27', '10.1109/CVPR.2013.463', '10.1109/ICCV.2015.420', '10.1109/CVPR.2015.7298832', '10.1109/CVPR.2010.5539817', '10.1023/B:VISI.0000029664.99615.94', '10.5244/C.26.57', '10.1109/CVPR.2016.152', '10.1109/CVPR.2016.148', '10.1109/TPAMI.2002.1017623', '10.1109/CVPR.2015.7298794', '10.1109/CVPR.2013.426', '10.1109/CVPR.2013.426', '10.1109/ICCV.2015.366', '10.1109/CVPR.2015.7299046', '10.1109/ICCV.2015.426', '10.1007/978-3-319-46478-7_9', '10.1109/CVPR.2016.144', '10.1109/CVPR.2007.383218', '10.1109/5.58337', '10.1109/CVPR.2016.140', '10.1007/978-3-319-10584-0_1', '10.1109/ICPR.2014.328', '10.1007/978-3-319-10590-1_35', '10.1109/ICPR.2014.16', '10.1109/CVPR.2016.139', '10.1109/CVPR.2016.143', '10.1007/978-3-319-16199-0_9', '10.1109/ICCV.2013.314', '10.1109/CVPR.2013.460', '10.1109/CVPR.2014.26', '10.1109/CVPR.2015.7298783', '10.1109/ICCV.2015.133', '10.1109/CVPRW.2015.7301268'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Multimedia', 'Transfer Learning', 'Compression'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Advanced Deep-Learning Techniques for Salient and Category-Specific Object Detection: A Survey': Paper(DOI='10.1109/msp.2017.2749125', crossref_json=None, google_schorlar_metadata=None, title='Advanced Deep-Learning Techniques for Salient and Category-Specific Object Detection: A Survey', authors=['Junwei Han', 'Dingwen Zhang', 'Gong Cheng', 'Nian Liu', 'Dong Xu'], abstract='Object detection, including objectness detection (OD), salient object detection (SOD), and category-specific object detection (COD), is one of the most fundamental yet challenging problems in the computer vision community. Over the last several decades, great efforts have been made by researchers to tackle this problem, due to its broad range of applications for other computer vision tasks such as activity or event recognition, content-based image retrieval and scene understanding, etc. While numerous methods have been presented in recent years, a comprehensive review for the proposed high-quality object detection techniques, especially for those based on advanced deep-learning techniques, is still lacking. To this end, this article delves into the recent progress in this research field, including 1) definitions, motivations, and tasks of each subdirection; 2) modern techniques and essential research trends; 3\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Multimedia', 'Transfer Learning', 'Compression'], conference_acronym='IEEE signal processing magazine (Print)', publisher=None, query_handler=None),\n",
       " 'Skeleton-based action recognition using spatio-temporal LSTM network with trust gates': Paper(DOI='10.1109/tpami.2017.2771306', crossref_json=None, google_schorlar_metadata=None, title='Skeleton-based action recognition using spatio-temporal LSTM network with trust gates', authors=['Jun Liu', 'Amir Shahroudy', 'Dong Xu', 'Alex C Kot', 'Gang Wang'], abstract=\"Skeleton-based human action recognition has attracted a lot of research attention during the past few years. Recent works attempted to utilize recurrent neural networks to model the temporal dependencies between the 3D positional configurations of human body joints for better analysis of human activities in the skeletal data. The proposed work extends this idea to spatial domain as well as temporal domain to better analyze the hidden sources of action-related information within the human skeleton sequences in both of these domains simultaneously. Based on the pictorial structure of Kinect's skeletal data, an effective tree-structure based traversal framework is also proposed. In order to deal with the noise in the skeletal data, a new gating mechanism within LSTM module is introduced, with which the network can learn the reliability of the sequential data and accordingly adjust the effect of the input data on the\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2016.115', '10.1109/ICCV.2015.460', '10.1145/2911996.2912001', '10.1109/CVPR.2015.7298878', '10.1109/CVPR.2015.7299101', '10.1109/CVPR.2017.751', '10.1109/CVPR.2017.783', '10.1109/CVPR.2017.453', '10.1145/2733373.2806222', '10.1109/CVPR.2016.116', '10.1109/CVPR.2017.140', '10.1109/CVPR.2016.573', '10.1109/CVPR.2016.216', '10.1109/CVPR.2016.516', '10.1109/CVPR.2016.110', '10.1016/j.jvcir.2014.04.007', '10.1109/CVPR.2016.214', '10.1109/ISSNIP.2014.6827664', '10.1109/CVPR.2016.217', '10.1109/ICASSP.2016.7472170', '10.1016/j.imavis.2016.04.004', '10.1109/CVPR.2017.55', '10.1109/CVPR.2017.498', '10.1109/CVPR.2017.137', '10.1109/ICCVW.2015.48', '10.1016/j.patrec.2016.05.032', '10.1109/TMM.2015.2505089', '10.1109/FG.2015.7284883', '10.1016/j.patcog.2015.11.019', '10.1016/j.imavis.2016.06.007', '10.1109/CVPR.2017.51', '10.1016/j.cviu.2017.01.011', '10.1016/j.jvcir.2013.03.001', '10.1109/ICCV.2013.396', '10.1109/CVPRW.2012.6239233', '10.1109/CVPR.2014.82', '10.1109/ISCCSP.2014.6877819', '10.1109/WACV.2014.6836044', '10.1109/CVPR.2014.109', '10.1109/CVPR.2015.7298860', '10.1109/TPAMI.2015.2505295', '10.1109/TPAMI.2013.198', '10.1109/CVPR.2016.484', '10.1109/ICCV.2013.334', '10.1145/2522848.2532595', '10.1109/CVPR.2015.7299172', '10.1109/CVPRW.2012.6239234', '10.1007/978-3-642-24797-2', '10.1049/ic.2016.0063', '10.1109/CVPRW.2010.5543273', '10.1109/WACV.2013.6474999', '10.1109/CVPR.2016.167', '10.1109/CVPRW.2013.78', '10.1109/CVPR.2016.333', '10.1162/neco.1997.9.8.1735', '10.1016/j.patcog.2008.12.024', '10.1109/CVPR.2011.5995741', '10.1109/CVPR.2011.5995407', '10.1109/TPAMI.2016.2558148', '10.1007/978-3-319-10599-4_52', '10.1145/2522848.2532589', '10.1109/CVPR.2014.247', '10.1109/CVPR.2015.7299176', '10.1109/ACPR.2015.7486568', '10.1016/j.jvcir.2013.04.007', '10.1109/NCVPRIPG.2013.6776204', '10.1109/CVPR.2016.218', '10.1109/ICPR.2014.772', '10.1109/CVPRW.2013.76', '10.1109/ICCV.2013.227', '10.1109/ICASSP.2013.6638947', '10.1109/ICASSP.2011.5947611', '10.21437/Interspeech.2012-65', '10.21437/Interspeech.2013-596', '10.1016/j.patcog.2014.08.011', '10.1109/TCYB.2014.2350774', '10.1109/TPAMI.2016.2587640', '10.1109/CVPR.2015.7298934', '10.1109/ICCV.2015.505', '10.1016/j.image.2015.02.004', '10.1109/CVPR.2016.289', '10.1109/ICMEW.2014.6890714'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Multimedia', 'Transfer Learning', 'Compression'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation': Paper(DOI='10.1109/tpami.2013.167', crossref_json=None, google_schorlar_metadata=None, title='Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation', authors=['Wen Li', 'Lixin Duan', 'Dong Xu', 'Ivor W Tsang'], abstract='In this paper, we study the heterogeneous domain adaptation (HDA) problem, in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions. By introducing two different projection matrices, we first transform the data from two domains into a common subspace such that the similarity between samples across different domains can be measured. We then propose a new feature mapping function for each domain, which augments the transformed samples with their original features and zeros. Existing supervised learning methods ( e.g.,  SVM and SVR) can be readily employed by incorporating our newly proposed augmented feature representations for supervised HDA. As a showcase, we propose a novel method called Heterogeneous Feature Augmentation (HFA) based on SVM. We show that the proposed formulation can be equivalently\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/72.788641', '10.1109/ICCV.2011.6126478', '10.1109/ICDM.2010.65', '10.1109/CVPR.2011.5995702', '10.1109/TNN.2010.2091281', '10.1109/TPAMI.2011.265', '10.1109/TNNLS.2011.2178556', '10.1109/TPAMI.2011.114', '10.1109/CVPR.2013.344', '10.3115/1687878.1687880', '10.3115/1610075.1610094', '10.1080/10556780802712889'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Multimedia', 'Transfer Learning', 'Compression'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Learning with Augmented Features for Heterogeneous Domain Adaptation': Paper(DOI='10.1109/tpami.2013.167', crossref_json=None, google_schorlar_metadata=None, title='Learning with Augmented Features for Heterogeneous Domain Adaptation', authors=['Lixin Duan', 'Dong Xu', 'Ivor Tsang'], abstract='We propose a new learning method for heterogeneous domain adaptation (HDA), in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions. Using two different projection matrices, we first transform the data from two domains into a common subspace in order to measure the similarity between the data from two domains. We then propose two new feature mapping functions to augment the transformed data with their original features and zeros. The existing learning methods (e.g., SVM and SVR) can be readily incorporated with our newly proposed augmented feature representations to effectively utilize the data from both domains for HDA. Using the hinge loss function in SVM as an example, we introduce the detailed objective function in our method called Heterogeneous Feature Augmentation (HFA) for a linear case and also describe its kernelization in order to efficiently cope with the data with very high dimensions. Moreover, we also develop an alternating optimization algorithm to effectively solve the nontrivial optimization problem in our HFA method. Comprehensive experiments on two benchmark datasets clearly demonstrate that HFA outperforms the existing HDA methods.', conference=None, journal=None, year=None, reference_list=['10.1109/72.788641', '10.1109/ICCV.2011.6126478', '10.1109/ICDM.2010.65', '10.1109/CVPR.2011.5995702', '10.1109/TNN.2010.2091281', '10.1109/TPAMI.2011.265', '10.1109/TNNLS.2011.2178556', '10.1109/TPAMI.2011.114', '10.1109/CVPR.2013.344', '10.3115/1687878.1687880', '10.3115/1610075.1610094', '10.1080/10556780802712889'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Multimedia', 'Transfer Learning', 'Compression'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Gait components and their application to gender recognition': Paper(DOI='10.1109/tsmcc.2007.913886', crossref_json=None, google_schorlar_metadata=None, title='Gait components and their application to gender recognition', authors=['Xuelong Li', 'Stephen J Maybank', 'Shuicheng Yan', 'Dacheng Tao', 'Dong Xu'], abstract='Human gait is a promising biometrics resource. In this paper, the information about gait is obtained from the motions of the different parts of the silhouette. The human silhouette is segmented into seven components, namely head, arm, trunk, thigh, front-leg, back-leg, and feet. The leg silhouettes for the front-leg and the back-leg are considered separately because, during walking, the left leg and the right leg are in front or at the back by turns. Each of the seven components and a number of combinations of the components are then studied with regard to two useful applications: human identification (ID) recognition and gender recognition. More than 500 different experiments on human ID and gender recognition are carried out under a wide range of circumstances. The effectiveness of the seven human gait components for ID and gender recognition is analyzed.', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2003.1251144', '10.1109/ICCV.2003.1238660', '10.1109/CVPR.2004.1315244', '10.1109/TPAMI.2005.39', '10.1109/ICPR.2002.1044731', '10.1109/CVPR.1994.323868', '10.1109/CVPR.2004.1315243', '10.1109/ICPR.2004.1333798', '10.1109/CVPR.2006.138', '10.1109/CVPR.2000.854929', '10.1007/978-1-4615-9197-9_8', '10.1109/CVPR.1997.609439', '10.1006/cviu.1998.0716', '10.1109/TPAMI.2005.55', '10.1109/TSMCC.2004.829274', '10.1109/ICPR.2004.1334590', '10.1038/scientificamerican0675-76', '10.1109/TIP.2004.832865', '10.1109/34.1000244', '10.1109/CVPR.2001.990924', '10.1006/cviu.2000.0897', '10.1097/00002060-196602000-00002', '10.1109/AFGR.2002.1004181', '10.1016/j.cviu.2004.04.004', '10.1109/ICPR.2002.1047474', '10.1016/S1077-3142(03)00008-0', '10.1006/cviu.1998.0744', '10.1109/AFGR.2002.1004148', '10.1109/ICCV.2003.1238411', '10.1109/ICCV.1998.710746', '10.1109/ICPR.2004.1333741'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'image processing', 'applications of probability'], conference_acronym='IEEE transactions on systems, man and cybernetics. Part C, Applications and reviews', publisher=None, query_handler=None),\n",
       " 'PatchDock and SymmDock: servers for rigid and symmetric docking': Paper(DOI='10.1093/nar/gki481', crossref_json=None, google_schorlar_metadata=None, title='PatchDock and SymmDock: servers for rigid and symmetric docking', authors=['Dina Schneidman-Duhovny', 'Yuval Inbar', 'Ruth Nussinov', 'Haim J Wolfson'], abstract=' Here, we describe two freely available web servers for molecular docking. The PatchDock method performs structure prediction of protein–protein and protein–small molecule complexes. The SymmDock method predicts the structure of a homomultimer with cyclic symmetry given the structure of the monomeric unit. The inputs to the servers are either protein PDB codes or uploaded protein structures. The services are available at http://bioinfo3d.cs.tau.ac.il . The methods behind the servers are very efficient, allowing large-scale docking experiments. ', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Principles of docking: An overview of search algorithms and a guide to scoring functions': Paper(DOI='10.1002/prot.10115', crossref_json=None, google_schorlar_metadata=None, title='Principles of docking: An overview of search algorithms and a guide to scoring functions', authors=['Inbal Halperin', 'Buyong Ma', 'Haim Wolfson', 'Ruth Nussinov'], abstract=' The docking field has come of age. The time is ripe to present the principles of docking, reviewing the current state of the field. Two reasons are largely responsible for the maturity of the computational docking area. First, the early optimism that the very presence of the “correct” native conformation within the list of predicted docked conformations signals a near solution to the docking problem, has been replaced by the stark realization of the extreme difficulty of the next scoring/ranking step. Second, in the last couple of years more realistic approaches to handling molecular flexibility in docking schemes have emerged. As in folding, these derive from concepts abstracted from statistical mechanics, namely, populations. Docking and folding are interrelated. From the purely physical standpoint, binding and folding are analogous processes, with similar underlying principles. Computationally, the tools developed for\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/S0167-7799(01)01666-3', '10.1016/S1367-5931(00)00217-9', '10.1126/science.257.5073.1078', '10.1016/S0959-440X(96)80061-3', '10.1016/S0959-440X(00)00105-6', '10.1124/mol.57.3.474', '10.1006/meth.1999.0922', '10.1006/jmbi.2000.4453', '10.1107/S0365110X53001964', '10.1016/0022-2836(71)90324-X', '10.1126/science.6879170', '10.1107/S0021889883010985', '10.1016/S0022-2836(77)80200-3', '10.1016/0022-2836(82)90153-X', '10.1016/S0022-5193(84)80193-9', '10.1002/bip.360250705', '10.1002/bip.360240814', '10.1021/jm00161a004', '10.1016/0022-2836(78)90302-9', '10.1016/0079-6107(87)90008-3', '10.1021/jm00145a002', '10.1002/(SICI)1099-1352(199601)9:1<1::AID-JMR241>3.0.CO;2-6', '10.1016/0076-6879(86)31042-5', '10.1016/0003-9861(92)90538-8', '10.1021/ja980065q', '10.1021/bi010582c', '10.1002/pro.5560070301', '10.1016/S1359-0278(98)00032-7', '10.1073/pnas.96.18.9970', '10.1002/(SICI)1096-987X(19981115)19:14<1623::AID-JCC8>3.0.CO;2-L', '10.1002/(SICI)1096-987X(19981115)19:14<1612::AID-JCC7>3.0.CO;2-M', '10.1021/jm001044l', '10.1002/1097-0134(20001101)41:2<173::AID-PROT30>3.0.CO;2-3', '10.1002/jcc.540160409', '10.1002/prot.340180111', '10.1016/0263-7855(96)00030-6', '10.1002/(SICI)1097-0134(199708)28:4<556::AID-PROT9>3.0.CO;2-7', '10.1002/bip.360340711', '10.2174/1386207302666220204193837', '10.1093/protein/7.1.39', '10.1006/jmbi.1995.0493', '10.1016/S0022-2836(95)80063-8', '10.1002/(SICI)1097-0134(19980215)30:3<321::AID-PROT11>3.0.CO;2-H', '10.1002/(SICI)1097-0134(20000501)39:2<178::AID-PROT8>3.0.CO;2-6', '10.1002/prot.1107', '10.1093/protein/12.4.271', '10.1006/jmbi.1998.2090', '10.1002/(SICI)1097-0134(19990801)36:2<147::AID-PROT2>3.0.CO;2-3', '10.1073/pnas.96.18.10118', '10.1002/1097-0134(2000)41:4 <63::AID-PROT60>3.0.CO;2-6', '10.1002/prot.1124', '10.1021/bi00188a001', '10.1073/pnas.44.2.98', '10.1006/jmbi.2001.4757', '10.1073/pnas.160259697', '10.1006/jmbi.1999.3110', '10.1007/978-3-642-51499-9', '10.1002/(SICI)1097-0134(19990501)35:2<153::AID-PROT2>3.0.CO;2-E', '10.1006/jmbi.2001.4551', '10.1126/science.287.5456.1279', '10.1042/0300-5127:0280517', '10.1093/emboj/19.7.1505', '10.1016/S1074-5521(99)80077-5', '10.1110/ps.9.1.10', '10.1016/S0959-440X(00)00216-5', '10.1002/(SICI)1099-1352(199901/02)12:1<1::AID-JMR449>3.0.CO;2-P', '10.1110/ps.8.6.1181', '10.1093/protein/12.9.713', '10.1002/1097-0134(20010201)42:2<279::AID-PROT150>3.0.CO;2-U', '10.1110/ps.8.5.1134', '10.1016/S1367-5931(97)80038-5', '10.1016/0022-2836(91)90859-5', '10.1002/prot.1070', '10.1073/pnas.96.15.8477', '10.1016/S0021-9258(17)46181-3', '10.1002/(SICI)1097-0134(20000515)39:3<261::AID-PROT90>3.0.CO;2-4', '10.1016/0022-2836(91)80074-5', '10.1016/S0959-440X(05)80162-9', '10.1126/science.272.5260.337b', '10.1038/nsb0197-8', '10.1002/pro.5560050311', '10.1016/S1367-5931(98)80014-8', '10.1021/cc000055x', '10.1002/(SICI)1521-3773(20000117)39:2<290::AID-ANIE290>3.0.CO;2-1', '10.1002/(SICI)1097-0134(19990701)36:1<87::AID-PROT8>3.0.CO;2-R', '10.1073/pnas.92.8.3288', '10.1002/(SICI)1097-0134(1997)1 <215::AID-PROT29>3.0.CO;2-Q', '10.1002/bip.360320107', '10.1016/0263-7855(93)87001-L', '10.1006/jmbi.1996.0776', '10.1128/AAC.42.12.3225', '10.1016/S1093-3263(00)00036-X', '10.1007/BF00119865', '10.1002/(SICI)1097-461X(1999)72:1<73::AID-QUA7>3.0.CO;2-O', '10.1007/3-540-61258-0_20', '10.1002/(SICI)1097-0134(19980801)32:2<159::AID-PROT3>3.0.CO;2-G', '10.1089/cmb.1998.5.631', '10.1016/S0022-2836(05)80038-5', '10.1002/(SICI)1097-0134(19981101)33:2<227::AID-PROT7>3.0.CO;2-F', '10.1080/07391102.1991.10507882', '10.1006/jmbi.1993.1170', '10.1002/(SICI)1097-0134(19981101)33:2<295::AID-PROT12>3.0.CO;2-F', '10.1002/(SICI)1096-987X(19980115)19:1<21::AID-JCC2>3.0.CO;2-0', '10.1006/jmbi.1997.1203', '10.1002/(SICI)1097-0134(20000601)39:4<372::AID-PROT100>3.0.CO;2-Q', '10.1002/prot.1038', '10.1006/jmbi.1997.1519', '10.1002/pro.5560070411', '10.1002/prot.340080302', '10.1002/prot.340130305', '10.1038/358774a0', '10.1002/(SICI)1097-0134(199602)24:2<227::AID-PROT9>3.0.CO;2-F', '10.1021/jm00399a006', '10.1002/jcc.540130608', '10.1002/prot.340110104', '10.1002/prot.340230403', '10.1002/prot.340110409', '10.1007/BF00124387', '10.1016/0263-7855(92)80059-M', '10.1007/BF00141572', '10.1007/BF00123667', '10.1007/BF00124402', '10.1002/(SICI)1097-0134(199607)25:3<342::AID-PROT6>3.3.CO;2-3', '10.1006/jmbi.1996.0897', '10.1002/(SICI)1096-987X(19981115)19:14<1639::AID-JCC10>3.0.CO;2-B', '10.1002/(SICI)1097-0134(19981001)33:1<74::AID-PROT7>3.0.CO;2-L', '10.1023/A:1008737207775', '10.1002/jcc.540130311', '10.1002/pro.5560061011', '10.1110/ps.8701', '10.1145/224170.224218', '10.1074/jbc.274.53.38051', '10.1073/pnas.89.6.2195', '10.1016/0022-2836(92)90506-F', '10.1093/protein/8.4.371', '10.1002/(SICI)1097-0282(199609)39:3<455::AID-BIP16>3.0.CO;2-A', '10.1093/protein/9.9.741', '10.1002/prot.340200405', '10.1002/1097-0134(20000815)40:3<525::AID-PROT190>3.0.CO;2-F', '10.1002/9780470125823.ch7', '10.1038/330084a0', '10.1016/0263-7855(95)00073-9', '10.1006/jmbi.1996.0077', '10.1002/(SICI)1096-987X(19990715)20:9<983::AID-JCC9>3.0.CO;2-R', '10.1006/jmbi.1998.2411', '10.1073/pnas.88.23.10495', '10.1110/ps.07501', '10.1023/A:1011216130486', '10.1021/jm991090p', '10.1021/ja003834q', '10.1016/0898-5529(90)90159-6', '10.1021/ci00067a016', '10.1021/ci00005a017', '10.1021/ci00017a026', '10.1006/jmbi.1994.1656', '10.1006/jmbi.1996.0477', '10.1007/BF00124464', '10.1093/bioinformatics/15.3.243', '10.1002/prot.340190303', '10.1016/S0959-440X(96)80061-3', '10.1002/(SICI)1098-1128(199601)16:1<3::AID-MED1>3.0.CO;2-6', '10.1002/prot.1034', '10.1016/0263-7855(87)80045-0', '10.1021/ci00003a004', '10.1021/ci00040a009', '10.1016/S1093-3263(97)00080-6', '10.1016/S0734-189X(87)80147-0', '10.1007/BF00126666', '10.1021/jm00090a001', '10.1021/jm9806143', '10.1002/1097-0134(20000901)40:4<623::AID-PROT70>3.0.CO;2-I', '10.1021/jm990322h', '10.1021/bi9716074', '10.1038/353715a0', '10.1006/jmbi.1999.2659', '10.1006/jmbi.2000.3918', '10.1016/S0006-3495(92)81649-1', '10.1073/pnas.78.4.2179', '10.1002/1097-0134(20001115)41:3<415::AID-PROT130>3.0.CO;2-7', '10.1093/protein/7.6.761', '10.1073/pnas.96.5.1875', '10.1002/(SICI)1096-987X(199902)20:3<287::AID-JCC1>3.0.CO;2-H', '10.1002/prot.340110406', '10.1016/S0959-440X(05)80162-9', '10.1016/0022-2836(91)80222-G', '10.1002/jcc.540120612', '10.1006/jmbi.1994.1054', '10.1002/(SICI)1097-0134(199608)25:4<403::AID-PROT1>3.0.CO;2-E', '10.1110/ps.8.3.603', '10.1006/jmbi.1996.0634', '10.1002/pro.5560050406', '10.1093/protein/4.2.177', '10.1016/0022-2836(92)90936-E', '10.1006/jmbi.1994.1052', '10.1038/nsb0494-259', '10.1073/pnas.84.19.6611', '10.1002/jcc.540150503', '10.1016/0167-8655(91)90157-H', '10.1002/(SICI)1097-0134(199703)27:3<425::AID-PROT10>3.0.CO;2-N', '10.1006/jmbi.1998.2371', '10.1016/S0968-0004(98)01346-2', '10.1016/S0968-0004(98)01345-0', '10.1073/pnas.97.22.12038', '10.1073/pnas.95.21.12141', '10.1038/10850', '10.1002/pro.5560060909', '10.1110/ps.33301', '10.1021/jm0100279', '10.1002/(SICI)1097-0134(19990515)35:3<364::AID-PROT11>3.0.CO;2-4', '10.1016/S0006-3495(99)77281-4', '10.1110/ps.12901', '10.1089/106652701300312896', '10.1006/jmbi.1998.1843', '10.1126/science.7529940', '10.1006/jmbi.1998.1669', '10.1021/bi971884a', '10.1021/bi9808621', '10.1006/jmbi.2000.3945', '10.1002/(SICI)1097-0134(20000601)39:4<331::AID-PROT60>3.0.CO;2-A', '10.1021/ci990262o', '10.1073/pnas.75.1.303', '10.1002/(SICI)1097-0134(199605)25:1<120::AID-PROT10>3.3.CO;2-1', '10.1093/protein/12.8.639', '10.1016/0022-2836(92)90405-9', '10.1147/rd.453.0513', '10.1016/S0022-2836(83)80079-5', '10.1093/protein/12.8.657', '10.1073/pnas.96.18.9997', '10.1002/pro.5560030501', '10.1002/pro.5560060106', '10.1006/jmbi.1996.0868', '10.1002/pro.5560040923', '10.1021/ma00145a039', '10.1126/science.7761829', '10.1126/science.3589666', '10.1038/nsb0596-427', '10.1002/jcc.540120405', '10.1016/0010-4655(95)00043-F', '10.1021/jm990352k', '10.1021/jm001090l', '10.1006/jmbi.1999.3371', '10.1016/S0960-894X(01)00021-X', '10.1006/jmbi.2001.4870', '10.1002/prot.340040104', '10.1002/(SICI)1097-0134(19990201)34:2<232::AID-PROT9>3.0.CO;2-9', '10.1002/anie.199625881', '10.1002/prot.340070203', '10.1006/jmbi.1996.0077', '10.1002/prot.340130304', '10.1002/jcc.540161107', '10.1016/0076-6879(91)02020-A', '10.1016/S1359-0278(97)00011-4', '10.1038/nsb0396-233', '10.1002/(SICI)1097-0134(199703)27:3<410::AID-PROT9>3.0.CO;2-G', '10.1016/S0959-440X(97)80029-2', '10.1002/pro.5560041014', '10.1006/jmbi.1993.1648', '10.1016/S0065-3233(08)60654-3', '10.1016/S0969-2126(00)00167-2', '10.1002/1097-0134(20010501)43:2<113::AID-PROT1023>3.0.CO;2-T', '10.1002/1097-0134(20010215)42:3<296::AID-PROT20>3.0.CO;2-F', '10.1002/1097-0134(20010501)43:2<217::AID-PROT1032>3.0.CO;2-G'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym='Proteins (Print)', publisher=None, query_handler=None),\n",
       " 'Efficient unbound docking of rigid molecules': Paper(DOI='10.1007/3-540-45784-4_14', crossref_json=None, google_schorlar_metadata=None, title='Efficient unbound docking of rigid molecules', authors=['Dina Duhovny', 'Ruth Nussinov', 'Haim J Wolfson'], abstract=' We present a new algorithm for unbound (real life) docking of molecules, whether protein-protein or protein-drug. The algorithm carries out rigid docking, with surface variability/flexibility implicitly addressed through liberal intermolecular penetration. The high efficiency of the algorithm is the outcome of several factors: (i) focusing initial molecular surface fitting on localized, curvature based surface patches; (ii) use of Geometric Hashing and Pose Clustering for initial transformation detection; (iii) accurate computation of shape complementarity utilizing the Distance Transform; (iv) efficient steric clash detection and geometric fit scoring based on a multi-resolution shape representation; and (v) utilization of biological information by focusing on hot spot rich surface patches. The algorithm has been implemented and applied to a large number of cases.', conference=None, journal=None, year=None, reference_list=['10.1002/1097-0134(20000815)40:3<525::AID-PROT190>3.0.CO;2-F', '10.1002/prot.10092', '10.1107/S0021889883010985', '10.1126/science.6879170', '10.1002/bip.360250705', '10.1007/978-3-662-04245-8', '10.1023/A:1011115820450', '10.1006/jmbi.1997.1203', '10.1002/prot.1070', '10.1002/(SICI)1097-0134(20000101)38:1<79::AID-PROT9>3.0.CO;2-U', '10.1002/prot.10115', '10.1002/(SICI)1097-0134(20000601)39:4<331::AID-PROT60>3.0.CO;2-A', '10.1110/ps.8.3.603', '10.1016/0022-2836(91)90859-5', '10.1006/jmbi.1996.0897', '10.1073/pnas.89.6.2195', '10.1016/0022-2836(82)90153-X', '10.1145/267521.267547', '10.1002/prot.340180111', '10.1002/bip.360340711', '10.1006/jmbi.1995.0493', '10.1002/(SICI)1097-0134(20000601)39:4<372::AID-PROT100>3.0.CO;2-Q', '10.1002/(SICI)1097-0134(19980801)32:2<159::AID-PROT3>3.0.CO;2-G', '10.1016/S0734-189X(87)80147-0', '10.1093/protein/8.4.371', '10.1093/protein/9.9.741', '10.1073/pnas.96.15.8477', '10.1016/0022-2836(92)90506-F'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Geometric hashing: An overview': Paper(DOI='10.1109/99.641604', crossref_json=None, google_schorlar_metadata=None, title='Geometric hashing: An overview', authors=['Haim J Wolfson', 'Isidore Rigoutsos'], abstract='Geometric hashing, a technique originally developed in computer vision for matching geometric features against a database of such features, finds use in a number of other areas. Matching is possible even when the recognizable database objects have undergone transformations or when only partial information is present. The technique is highly efficient and of low polynomial complexity.', conference=None, journal=None, year=None, reference_list=['10.1177/027836498600500403', '10.1177/027836498700600203', '10.1109/ROBOT.1988.12264', '10.1109/99.326666', '10.1109/CVPR.1988.196257', '10.1109/ICCV.1993.378208', '10.1109/70.62047', '10.1016/1049-9660(92)90012-R', '10.1109/CCV.1988.589995', '10.1109/ICPR.1990.118057', '10.1109/ICPR.1990.119438', '10.1117/12.58489', '10.1109/2.121473', '10.1109/ICPR.1990.118101', '10.1073/pnas.88.23.10495', '10.1109/CVPR.1991.139655', '10.1006/cviu.1995.1038', '10.1016/0167-8655(91)90157-H', '10.1109/34.99233', '10.1109/CVPR.1991.139736', '10.1109/CVPR.1991.139696', '10.1016/S0022-2836(95)80063-8'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym='IEEE computational science & engineering (Print)', publisher=None, query_handler=None),\n",
       " 'FireDock: fast interaction refinement in molecular docking': Paper(DOI='10.1002/prot.21495', crossref_json=None, google_schorlar_metadata=None, title='FireDock: fast interaction refinement in molecular docking', authors=['Nelly Andrusier', 'Ruth Nussinov', 'Haim J Wolfson'], abstract=' Here, we present FireDock, an efficient method for the refinement and rescoring of rigid‐body docking solutions. The refinement process consists of two main steps: (1) rearrangement of the interface side‐chains and (2) adjustment of the relative orientation of the molecules. Our method accounts for the observation that most interface residues that are important in recognition and binding do not change their conformation significantly upon complexation. Allowing full side‐chain flexibility, a common procedure in refinement methods, often causes excessive conformational changes. These changes may distort preformed structural signatures, which have been shown to be important for binding recognition. Here, we restrict side‐chain movements, and thus manage to reduce the false‐positive rate noticeably. In the later stages of our procedure (orientation adjustments and scoring), we smooth the atomic radii. This\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1073/pnas.89.6.2195', '10.1016/S0022-2836(95)80063-8', '10.1007/3-540-45784-4_14', '10.1002/(SICI)1097-0134(20000501)39:2<178::AID-PROT8>3.0.CO;2-6', '10.1073/pnas.0401942101', '10.1016/j.jmb.2004.09.051', '10.1016/j.jmb.2005.01.058', '10.1006/jmbi.1996.0859', '10.1002/prot.10390', '10.1002/prot.20560', '10.1093/protein/15.10.779', '10.1038/356539a0', '10.1142/S0219720005000904', '10.1110/ps.03154503', '10.1007/11415770_32', '10.1093/bioinformatics/bti763', '10.1089/106652702760277336', '10.1093/bioinformatics/bti144', '10.1002/prot.340140208', '10.1063/1.1699114', '10.1006/jmbi.1994.1366', '10.1002/(SICI)1097-0282(199908)50:2<111::AID-BIP1>3.0.CO;2-N', '10.1080/07391102.1991.10507882', '10.1093/protein/8.4.363', '10.1016/0022-2836(91)90550-P', '10.1006/jmbi.1994.1052', '10.1021/ja026939x', '10.1016/S0022-2836(03)00670-3', '10.1110/ps.041222905', '10.1016/j.jmb.2003.10.069', '10.1006/jmbi.1997.1519', '10.1002/prot.10460', '10.1002/jcc.540040211', '10.1002/prot.10394', '10.1110/ps.0239303', '10.1002/(SICI)1097-0134(20000601)39:4<372::AID-PROT100>3.0.CO;2-Q', '10.1002/pro.5560060807', '10.1063/1.472061', '10.1073/pnas.202485799', '10.1002/prot.20417', '10.1016/j.jmb.2004.07.038', '10.1006/jmbi.1998.2401', '10.1093/bioinformatics/16.9.815', '10.1007/BF02579150', '10.1515/9781400884179', '10.1093/imamat/6.1.76', '10.1093/comjnl/13.3.317', '10.1002/prot.20551', '10.1002/prot.10389', '10.1093/bioinformatics/btg371'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym='Proteins (Print)', publisher=None, query_handler=None),\n",
       " 'FireDock: a web server for fast interaction refinement in molecular docking': Paper(DOI='10.1093/nar/gkn186', crossref_json=None, google_schorlar_metadata=None, title='FireDock: a web server for fast interaction refinement in molecular docking', authors=['Efrat Mashiach', 'Dina Schneidman-Duhovny', 'Nelly Andrusier', 'Ruth Nussinov', 'Haim J Wolfson'], abstract=' Structural details of protein–protein interactions are invaluable for understanding and deciphering biological mechanisms. Computational docking methods aim to predict the structure of a protein–protein complex given the structures of its single components. Protein flexibility and the absence of robust scoring functions pose a great challenge in the docking field. Due to these difficulties most of the docking methods involve a two-tier approach: coarse global search for feasible orientations that treats proteins as rigid bodies, followed by an accurate refinement stage that aims to introduce flexibility into the process. The FireDock web server, presented here, is the first web server for flexible refinement and scoring of protein–protein docking solutions. It includes optimization of side-chain conformations and rigid-body orientation and allows a high-throughput refinement. The server provides a user-friendly interface and a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1093/hmg/ddi018', '10.1016/j.sbi.2007.06.003', '10.1038/nrm1859', '10.1016/j.bbapap.2005.07.045', '10.1016/j.sbi.2006.02.002', '10.1002/prot.10115', '10.1016/j.crvi.2004.03.006', '10.1007/3-540-45784-4_14', '10.1002/(SICI)1097-0134(20000601)39:4<372::AID-PROT100>3.0.CO;2-Q', '10.1016/S0022-2836(03)00670-3', '10.1002/prot.10383', '10.1007/11415770_32', '10.1002/prot.21495', '10.1006/jmbi.1997.1519', '10.1002/prot.10460', '10.1073/pnas.181147798', '10.1002/prot.10389', '10.1093/nar/gki481', '10.1093/nar/gnh082', '10.1093/bioinformatics/btg371', '10.1002/prot.10394', '10.1110/ps.041222905', '10.1002/(SICI)1097-0134(20000501)39:2<178::AID-PROT8>3.0.CO;2-6', '10.1002/prot.10390', '10.1002/prot.20560', '10.1089/106652702760277336', '10.1093/bioinformatics/bti144', '10.1093/imamat/6.1.76', '10.1093/comjnl/13.3.317', '10.1093/nar/28.1.235', '10.1002/prot.10092', '10.1002/bmb.2006.494034042644'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Context-free attentional operators: The generalized symmetry transform': Paper(DOI='10.1007/bf01418978', crossref_json=None, google_schorlar_metadata=None, title='Context-free attentional operators: The generalized symmetry transform', authors=['Daniel Reisfeld', 'Haim Wolfson', 'Yehezkel Yeshurun'], abstract=' Active vision systems, and especially foveated vision systems, depend on efficient attentional mechanisms. We propose that machine visual attention should consist of both high-level, context-dependent components, and low-level, context free components. As a basis for the context-free component, we present an attention operator based on the intuitive notion of symmetry, which generalized many of the existing methods of detecting regions of interest. It is a low-level operator that can be applied successfully without a priori knowledge of the world. The resultingsymmetry edge map can be applied in various low, intermediate-and high- level tasks, such as extraction of interest points, grouping, and object recognition. In particular, we have implemented an algorithm that locates interest points in real time, and can be incorporated in active and purposive vision systems. The results agree with some\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CCV.1988.590034', '10.1109/TC.1985.1676605', '10.1037/h0054663', '10.1109/5.5968', '10.1016/B978-0-444-87137-4.50012-6', '10.1016/0031-3203(78)90025-0', '10.1109/ICCV.1993.378180', '10.1177/027836498400300302', '10.1007/3-540-55426-2_60', '10.1109/TSMC.1977.4309685', '10.1126/science.918670', '10.3758/BF03210527', '10.1037/0096-1523.4.4.565', '10.1109/34.23119', '10.1016/0004-3702(77)90006-6', '10.1146/annurev.ne.13.030190.000325', '10.1109/ICCV.1990.139494', '10.1109/ICPR.1992.201521', '10.1007/3-540-55426-2_59', '10.1109/ICPR.1990.119370', '10.1016/0022-0965(73)90128-8', '10.1016/0262-8856(90)80003-C', '10.1016/0010-0277(84)90023-4', '10.1109/34.42838', '10.1109/34.42860', '10.1016/0734-189X(90)90126-G', '10.1109/CVPR.1992.223196', '10.1162/neco.1989.1.1.68'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Studies of protein‐protein interfaces: a statistical analysis of the hydrophobic effect': Paper(DOI='10.1002/pro.5560060106', crossref_json=None, google_schorlar_metadata=None, title='Studies of protein‐protein interfaces: a statistical analysis of the hydrophobic effect', authors=['Chung‐Jung Tsai', 'Shuo Liang Lin', 'Haim J Wolfson', 'Ruth Nussinov'], abstract=' Data sets of 362 structurally nonredundant protein‐protein interfaces and of 57 symmetry‐related oligomeric interfaces have been used to explore whether the hydrophobic effect that guides protein folding is also the main driving force for protein‐protein associations. The buried nonpolar surface area has been used to measure the hydrophobic effect. Our analysis indicates that, although the hydrophobic effect plays a dominant role in protein‐protein binding, it is not as strong as that observed in the interior of protein monomers. Comparison of the interiors of the monomers with those of the interfaces reveals that, in general, the hydrophobic amino acids are more frequent in the interior of the monomers than in the interior of the protein‐protein interfaces. On the other hand, a higher proportion of charged and polar residues are buried at the interfaces, suggesting that hydrogen bonds and ion pairs contribute more to\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1002/pro.5560030601', '10.1016/S0022-2836(77)80200-3', '10.1002/jcc.540040211', '10.1038/357543a0', '10.1002/prot.340210406', '10.1016/0022-2836(87)90189-6', '10.1006/jmbi.1995.0481', '10.1016/0079-6107(87)90013-7', '10.1002/prot.340230413', '10.1073/pnas.93.1.13', '10.1002/prot.340090106', '10.1016/0022-2836(71)90324-X', '10.1006/jmbi.1995.0208', '10.1016/0022-2836(87)90038-6', '10.1002/pro.5560031202', '10.1016/0076-6879(94)28025-8', '10.1016/0022-2836(73)90011-9', '10.1006/jmbi.1996.0424', '10.3109/10409239609106582', '10.1002/prot.340200405', '10.1016/0022-2836(92)90506-F', '10.1002/pro.5560030501'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym='Protein science (Print)', publisher=None, query_handler=None),\n",
       " 'Affine invariant model-based object recognition': Paper(DOI='10.1109/70.62047', crossref_json=None, google_schorlar_metadata=None, title='Affine invariant model-based object recognition', authors=['Yehezkel Lamdan', 'Jacob T Schwartz', 'Haim J Wolfson'], abstract='New techniques are described for model-based recognition of the objects in 3-D space. The recognition is performed from single gray-scale images taken from unknown viewpoints. The objects in the scene may be overlapping and partially occluded. An efficient matching algorithm, which assumes affine approximation to the prospective viewing transformation, is proposed. The algorithm has an offline model preprocessing (shape representation) phase which is independent of the scene information and a recognition phase based on efficient indexing. It has a straightforward parallel implementation. The algorithm was successfully tested in recognition of industrial objects appearing in composite occluded scenes.< >', conference=None, journal=None, year=None, reference_list=['10.1109/ROBOT.1988.12264', '10.1177/027836498700600203', '10.1109/CVPR.1988.196257', '10.1109/TPAMI.1985.4767680', '10.1177/027836498600500403', '10.1109/TPAMI.1985.4767722', '10.1109/JRA.1986.1087034', '10.1145/4078.4081', '10.1007/978-1-4613-2551-2', '10.1109/ROBOT.1987.1088004', '10.1177/027836498200100304', '10.1109/TPAMI.1986.4767751', '10.1177/027836498400300301', '10.1177/027836498600500301', '10.1109/CCV.1988.589995', '10.1109/TPAMI.1987.4767935', '10.1145/6462.6464', '10.1109/TPAMI.1980.4767032', '10.1016/S0734-189X(87)80147-0', '10.1145/6490.6492'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym='IEEE transactions on robotics and automation', publisher=None, query_handler=None),\n",
       " 'A method for simultaneous alignment of multiple protein structures': Paper(DOI='10.1002/prot.10628', crossref_json=None, google_schorlar_metadata=None, title='A method for simultaneous alignment of multiple protein structures', authors=['Maxim Shatsky', 'Ruth Nussinov', 'Haim J Wolfson'], abstract=' Here, we present MultiProt, a fully automated highly efficient technique to detect multiple structural alignments of protein structures. MultiProt finds the common geometrical cores between input molecules. To date, most methods for multiple alignment start from the pairwise alignment solutions. This may lead to a small overall alignment. In contrast, our method derives multiple alignments from simultaneous superpositions of input molecules. Further, our method does not require that all input molecules participate in the alignment. Actually, it efficiently detects high scoring partial multiple alignments for all possible number of molecules in the input. To demonstrate the power of MultiProt, we provide a number of case studies. First, we demonstrate known multiple alignments of protein structures to illustrate the performance of MultiProt. Next, we present various biological applications. These include: (1) a partial\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1017/CBO9780511790492', '10.1073/pnas.84.13.4355', '10.1016/0022-2836(89)90084-3', '10.1016/0022-2836(90)90312-A', '10.1002/prot.340110107', '10.1073/pnas.88.23.10495', '10.1093/protein/11.9.739', '10.1023/A:1008194019144', '10.1089/106652701446152', '10.1016/S0304-3975(97)00278-8', '10.1002/prot.340140216', '10.1002/pro.5560031025', '10.1089/106652701300312896', '10.1002/prot.1034', '10.1016/S0022-2836(95)80063-8', '10.1002/(SICI)1097-0134(19990201)34:2<206::AID-PROT6>3.0.CO;2-N', '10.1093/bioinformatics/btg1012', '10.1110/ps.03200603', '10.1002/prot.10100', '10.1007/978-3-662-04245-8', '10.1007/3-540-45784-4_18', '10.1002/prot.340230309', '10.1006/jmbi.1993.1489', '10.1093/protein/6.3.279', '10.1093/nar/28.1.235', '10.1016/0022-2836(87)90521-3', '10.1089/cmb.1998.5.585', '10.1002/pro.5560071126', '10.1016/S0022-2836(05)80134-2', '10.1016/S0968-0004(00)01748-5', '10.1016/S0959-440X(99)00031-7'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym='Proteins (Print)', publisher=None, query_handler=None),\n",
       " 'Multiple diverse ligands binding at a single protein site: a matter of pre‐existing populations': Paper(DOI='10.1110/ps.21302', crossref_json=None, google_schorlar_metadata=None, title='Multiple diverse ligands binding at a single protein site: a matter of pre‐existing populations', authors=['Buyong Ma', 'Maxim Shatsky', 'Haim J Wolfson', 'Ruth Nussinov'], abstract=' Here, we comment on the steadily increasing body of data showing that proteins with specificity actually bind ligands of diverse shapes, sizes, and composition. Such a phenomenon is not surprising when one considers that binding is a dynamic process with populations in equilibrium and that the shape of the binding site is strongly influenced by the molecular partner. It derives implicitly from the concept of populations. All proteins, specific and nonspecific, exist in ensembles of substates. If the library of ligands in solution is large enough, favorably matching ligands with altered shapes and sizes can be expected to bind, with a redistribution of the protein populations. Point mutations at spatially distant sites may exert large conformational rearrangements and hinge effects, consistent with mutations away from the binding site leading to population shifts and (cross‐)drug resistance. A similar effect is observed in\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1002/prot.340080109', '10.1073/pnas.90.14.6796', '10.1038/380041a0', '10.1021/bi00121a034', '10.1016/S0022-2836(77)80200-3', '10.1006/jmbi.1998.1843', '10.1002/jcc.540040211', '10.1126/science.1060383', '10.1124/mol.57.3.474', '10.1126/science.7529940', '10.1006/jmbi.1998.1669', '10.1006/jmbi.2001.4551', '10.1016/0959-440X(94)90272-0', '10.1126/science.287.5456.1279', '10.1002/1099-1352(200101/02)14:1<42::AID-JMR518>3.0.CO;2-8', '10.1074/jbc.271.42.26157', '10.1093/nar/26.18.4280', '10.1006/jmbi.1993.1592', '10.1006/jmbi.1993.1048', '10.1021/bi00188a001', '10.1002/prot.10115', '10.1006/jmbi.1996.0550', '10.1016/S0959-440X(98)80015-8', '10.1002/(SICI)1097-0134(20000601)39:4<331::AID-PROT60>3.0.CO;2-A', '10.1006/jmbi.1997.1512', '10.1042/bst0151009', '10.1016/0079-6107(83)90003-2', '10.1126/science.2402636', '10.1016/S0006-3495(00)76756-7', '10.1006/jmbi.1996.0776', '10.1073/pnas.44.2.98', '10.1006/jmbi.1998.2296', '10.1002/1097-0134(20001201)41:4<485::AID-PROT60>3.0.CO;2-E', '10.1110/ps.9.1.10', '10.1002/(SICI)1097-0134(19990501)35:2<133::AID-PROT1>3.0.CO;2-N', '10.1038/2306', '10.1002/1097-0134(2000)41:4 <63::AID-PROT60>3.0.CO;2-6', '10.1093/protein/12.9.713', '10.1016/S0959-440X(00)00216-5', '10.1016/S0959-440X(96)80013-3', '10.1002/pro.5560070323', '10.1006/jmbi.1999.2924', '10.1021/bi00201a022', '10.1002/pro.5560070504', '10.1016/S0022-2836(05)80134-2', '10.2174/1386207302666220204195041', '10.2174/1386207302666220204195344', '10.1016/S0021-9258(18)82131-7', '10.1126/science.280.5364.708', '10.1016/0959-440X(95)80017-4', '10.1021/bi9716074', '10.1002/prot.340120209', '10.1002/(SICI)1097-0134(19980801)32:2<159::AID-PROT3>3.0.CO;2-G', '10.1089/cmb.1998.5.631', '10.1016/S0022-2836(05)80250-5', '10.1021/bi00159a003', '10.1073/pnas.92.2.452', '10.1073/pnas.93.19.10034', '10.1073/pnas.051399098', '10.1006/jmbi.1999.3086', '10.1002/pro.5560070702', '10.1016/S0969-2126(00)00167-2', '10.1110/ps.52201', '10.1016/S1074-5521(99)80077-5', '10.1073/pnas.96.18.9970', '10.1002/prot.1107', '10.1002/1097-0134(20001115)41:3<415::AID-PROT130>3.0.CO;2-7', '10.1002/(SICI)1099-1352(199901/02)12:1<1::AID-JMR449>3.0.CO;2-P', '10.1042/0300-5127:0280517', '10.1126/science.291.5512.2429', '10.1126/science.276.5319.1665', '10.1016/0076-6879(91)02020-A', '10.1110/ps.01601', '10.1006/jmbi.1999.3110', '10.1093/emboj/19.7.1505'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym='Protein science (Print)', publisher=None, query_handler=None),\n",
       " 'Mechanism of two classes of cancer mutations in the phosphoinositide 3-kinase catalytic subunit': Paper(DOI='10.1126/science.1135394', crossref_json=None, google_schorlar_metadata=None, title='Mechanism of two classes of cancer mutations in the phosphoinositide 3-kinase catalytic subunit', authors=['Nabil Miled', 'Ying Yan', 'Wai-Ching Hon', 'Olga Perisic', 'Marketa Zvelebil', 'Yuval Inbar', 'Dina Schneidman-Duhovny', 'Haim J Wolfson', 'Jonathan M Backer', 'Roger L Williams'], abstract='Many human cancers involve up-regulation of the phosphoinositide 3-kinase PI3Kα, with oncogenic mutations identified in both the p110α catalytic and the p85α regulatory subunits. We used crystallographic and biochemical approaches to gain insight into activating mutations in two noncatalytic p110α domains—the adaptor-binding and the helical domains. A structure of the adaptor-binding domain of p110α in a complex with the p85α inter–Src homology 2 (inter-SH2) domain shows that oncogenic mutations in the adaptor-binding domain are not at the inter-SH2 interface but in a polar surface patch that is a plausible docking site for other domains in the holo p110/p85 complex. We also examined helical domain mutations and found that the Glu545 to Lys545 (E545K) oncogenic mutant disrupts an inhibitory charge-charge interaction with the p85 N-terminal SH2 domain. These studies extend our understanding of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1126/science.296.5573.1655', '10.1146/annurev.biochem.70.1.535', '10.1016/S0165-6147(03)00163-9', '10.1016/S1535-6108(03)00248-4', '10.1158/0008-5472-CAN-04-3913', '10.2174/1381612043384402', '10.1016/j.ceb.2005.02.011', '10.1042/bst0320393', '10.1073/pnas.0700373104', '10.1128/MCB.18.3.1379', '10.1074/jbc.M205893200', '10.1016/S0021-9258(18)98375-4', '10.1002/j.1460-2075.1994.tb06289.x', '10.1074/jbc.273.46.30199', '10.1126/science.1096502', '10.1038/nrc1753', '10.1073/pnas.0408864102', '10.1158/0008-5472.CAN-04-4114', '10.1158/0008-5472.CAN-05-2612', '10.1073/pnas.0701005104', '10.1016/j.coph.2005.03.002', '10.1074/jbc.M011330200', '10.1038/46319', '10.1073/pnas.0535975100', '10.1093/nar/gki370', '10.1074/jbc.M506005200', '10.1038/nsb0496-364'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym='Science (New York, N.Y.)', publisher=None, query_handler=None),\n",
       " 'Conservation of polar residues as hot spots at protein interfaces': Paper(DOI='10.1002/(sici)1097-0134(20000601)39:4<331::aid-prot60>3.0.co;2-a', crossref_json=None, google_schorlar_metadata=None, title='Conservation of polar residues as hot spots at protein interfaces', authors=['Zengjian Hu', 'Buyong Ma', 'Haim Wolfson', 'Ruth Nussinov'], abstract=' A number of studies have addressed the question of which are the critical residues at protein‐binding sites. These studies examined either a single or a few protein–protein interfaces. The most extensive study to date has been an analysis of alanine‐scanning mutagenesis. However, although the total number of mutations was large, the number of protein interfaces was small, with some of the interfaces closely related. Here we show that although overall binding sites are hydrophobic, they are studded with specific, conserved polar residues at specific locations, possibly serving as energy “hot spots.” Our results confirm and generalize the alanine‐scanning data analysis, despite its limited size. Previously Trp, Arg, and Tyr were shown to constitute energetic hot spots. These were rationalized by their polar interactions and by their surrounding rings of hydrophobic residues. However, there was no compelling reason\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/0022-2836(88)90606-7', '10.1002/prot.340210105', '10.1073/pnas.93.1.13', '10.1006/jmbi.1996.0424', '10.1002/pro.5560060106', '10.1110/ps.8.6.1181', '10.1021/cr960387h', '10.1006/jmbi.1998.1843', '10.1093/protein/12.9.713', '10.1002/pro.5560060901', '10.1002/(SICI)1097-0134(19990815)36:3<307::AID-PROT5>3.0.CO;2-R', '10.1002/prot.340110406', '10.1016/S0959-440X(05)80162-9', '10.1002/pro.5560060707', '10.1002/prot.340230413', '10.1006/jmbi.1998.2439', '10.1002/pro.5560071211', '10.1002/pro.5560030501', '10.1002/prot.340200405', '10.1006/jmbi.1996.0712', '10.1126/science.7529940', '10.1016/0076-6879(91)02020-A', '10.1021/ja990935j', '10.1006/jmbi.1998.1669', '10.1016/S0022-2836(77)80200-3', '10.1006/jmbi.1997.1620', '10.1006/jmbi.1998.2296', '10.1073/pnas.88.23.10495', '10.1093/protein/6.3.279', '10.1002/pro.5560030506', '10.1002/prot.340090106', '10.1038/45170', '10.1016/S0092-8674(00)80790-4', '10.1073/pnas.95.9.4976', '10.1006/jmbi.1999.2920', '10.1073/pnas.95.11.5942', '10.1002/(SICI)1097-0134(19990201)34:2<255::AID-PROT10>3.0.CO;2-O', '10.1016/0959-440X(95)80017-4', '10.1038/71280', '10.1038/70057', '10.1038/70008'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym='Proteins (Print)', publisher=None, query_handler=None),\n",
       " 'Efficient detection of three-dimensional structural motifs in biological macromolecules by computer vision techniques.': Paper(DOI='10.1073/pnas.88.23.10495', crossref_json=None, google_schorlar_metadata=None, title='Efficient detection of three-dimensional structural motifs in biological macromolecules by computer vision techniques.', authors=['Ruth Nussinov', 'Haim J Wolfson'], abstract='Macromolecules carrying biological information often consist of independent modules containing recurring structural motifs. Detection of a specific structural motif within a protein (or DNA) aids in elucidating the role played by the protein (DNA element) and the mechanism of its operation. The number of crystallographically known structures at high resolution is increasing very rapidly. Yet, comparison of three-dimensional structures is a laborious time-consuming procedure that typically requires a manual phase. To date, there is no fast automated procedure for structural comparisons. We present an efficient O(n3) worst case time complexity algorithm for achieving such a goal (where n is the number of atoms in the examined structure). The method is truly three-dimensional, sequence-order-independent, and thus insensitive to gaps, insertions, or deletions. This algorithm is based on the geometric hashing paradigm\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym='Proceedings of the National Academy of Sciences of the United States of America', publisher=None, query_handler=None),\n",
       " 'Protein structure fitting and refinement guided by cryo-EM density': Paper(DOI='10.1016/j.str.2007.11.016', crossref_json=None, google_schorlar_metadata=None, title='Protein structure fitting and refinement guided by cryo-EM density', authors=['Maya Topf', 'Keren Lasker', 'Ben Webb', 'Haim Wolfson', 'Wah Chiu', 'Andrej Sali'], abstract='For many macromolecular assemblies, both a cryo-electron microscopy map and atomic structures of its component proteins are available. Here we describe a method for fitting and refining a component structure within its map at intermediate resolution (<15 Å). The atomic positions are optimized with respect to a scoring function that includes the crosscorrelation coefficient between the structure and the map as well as stereochemical and nonbonded interaction terms. A heuristic optimization that relies on a Monte Carlo search, a conjugate-gradients minimization, and simulated annealing molecular dynamics is applied to a series of subdivisions of the structure into progressively smaller rigid bodies. The method was tested on 15 proteins of known structure with 13 simulated maps and 3 experimentally determined maps. At ∼10 Å resolution, Cα rmsd between the initial and final structures was reduced on average\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1110/ps.04882105', '10.1006/jmbi.2000.3564', '10.1093/nar/gki070', '10.1126/science.1065659', '10.1371/journal.pcbi.0020146', '10.1093/nar/gkh121', '10.1038/nsb1295-1083', '10.1073/pnas.80.21.6571', '10.1107/S0108767394007130', '10.1016/j.jsb.2003.09.008', '10.1016/S0006-3495(01)76118-8', '10.1016/j.str.2004.12.016', '10.1002/0471140864.ps0209s50', '10.1016/j.str.2005.01.007', '10.1110/ps.9.9.1753', '10.1093/nar/gkj046', '10.1186/1471-2105-8-215', '10.1016/j.jsb.2006.06.010', '10.1016/j.str.2006.01.016', '10.1016/j.sbi.2005.08.004', '10.1093/nar/gkg460', '10.1002/bip.360221211', '10.1002/prot.10286', '10.1006/jmbi.2001.5133', '10.1016/j.str.2004.05.006', '10.1016/j.str.2005.02.002', '10.1021/jp973084f', '10.1093/nar/gkm236', '10.1073/pnas.112222299', '10.1146/annurev.biophys.35.040405.101950', '10.1016/S0022-2836(05)80134-2', '10.1529/biophysj.105.070045', '10.1002/jcc.20084', '10.1093/nar/gkj059', '10.1016/j.str.2005.01.005', '10.1016/j.sbi.2004.04.006', '10.1038/78923', '10.1006/jmbi.1993.1626', '10.1038/nature01513', '10.1145/355921.355933', '10.1110/ps.062416606', '10.1016/j.str.2004.05.020', '10.1107/S090744490602244X', '10.1016/S0022-2836(02)00627-7', '10.1016/j.jmb.2004.01.048', '10.1016/j.sbi.2005.08.001', '10.1016/j.jsb.2004.11.004', '10.1016/j.jmb.2006.01.062', '10.1038/nsb1003', '10.1016/j.str.2006.05.013', '10.1006/jsbi.1998.4080'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym='Structure (London)', publisher=None, query_handler=None),\n",
       " 'Recognition of functional sites in protein structures': Paper(DOI='10.1016/j.jmb.2004.04.012', crossref_json=None, google_schorlar_metadata=None, title='Recognition of functional sites in protein structures', authors=['Alexandra Shulman-Peleg', 'Ruth Nussinov', 'Haim J Wolfson'], abstract='Recognition of regions on the surface of one protein, that are similar to a binding site of another is crucial for the prediction of molecular interactions and for functional classifications. We first describe a novel method, SiteEngine, that assumes no sequence or fold similarities and is able to recognize proteins that have similar binding sites and may perform similar functions. We achieve high efficiency and speed by introducing a low-resolution surface representation via chemically important surface points, by hashing triangles of physico-chemical properties and by application of hierarchical scoring schemes for a thorough exploration of global and local similarities. We proceed to rigorously apply this method to functional site recognition in three possible ways: first, we search a given functional site on a large set of complete protein structures. Second, a potential functional site on a protein of interest is compared with\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/S0022-2836(02)00649-6', '10.2174/0929867043456287', '10.1023/A:1008194019144', '10.2174/0929867043456223', '10.1002/prot.10115', '10.1023/A:1020155510718', '10.1016/S1367-5931(00)00217-9', '10.2174/1381612013397861', '10.1093/nar/gkh413', '10.1093/emboj/18.13.3533', '10.1110/ps.21302', '10.1089/106652701446152', '10.1006/jmbi.1994.1657', '10.1145/321921.321925', '10.1021/ci0255984', '10.1002/pro.5560061104', '10.1002/pro.5560050603', '10.1109/CCV.1988.589995', '10.1073/pnas.88.23.10495', '10.1093/protein/6.3.279', '10.1093/bioinformatics/btg226', '10.1016/S0022-2836(03)00882-9', '10.1093/nar/gkg512', '10.1016/j.cbpa.2003.11.001', '10.1006/jmbi.1996.0591', '10.1002/prot.1093', '10.1093/protein/11.4.263', '10.1002/prot.340180111', '10.1016/0263-7855(96)00030-6', '10.1023/A:1011318527094', '10.1110/ps.0368703', '10.1145/362342.362367', '10.1107/S0021889883010985', '10.1016/S0022-2836(02)00811-2', '10.1016/S0022-2836(02)01408-0', '10.1126/science.6879170', '10.1007/3-540-45784-4_14', '10.1016/0263-7855(86)80086-8', '10.1016/S0734-189X(87)80147-0', '10.1107/S0108767397010325', '10.1107/S0567739478001680', '10.1002/prot.10422', '10.1073/pnas.95.26.15189', '10.1073/pnas.1835675100', '10.1126/science.1085658', '10.1016/S0065-3233(08)60639-7', '10.1016/S0022-2836(05)80134-2', '10.1007/3-540-45784-4_18', '10.1002/prot.10628', '10.1016/S0021-9258(19)61478-X', '10.1074/jbc.272.11.7140', '10.1016/0022-2836(82)90153-X', '10.1023/A:1008124202956', '10.1016/0263-7855(95)00073-9', '10.1002/pro.5560070905', '10.1016/0263-7855(95)00071-2', '10.1109/ICIP.1995.537694', '10.1093/nar/28.1.235', '10.1093/nar/28.1.254', '10.1093/nar/30.1.260', '10.1093/nar/gkh034', '10.1016/S0969-2126(97)00235-9', '10.1038/87610', '10.1038/10745', '10.1002/pro.5560040702'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computational Structural Biology', 'Structural  Bioinformatics', 'Computer Vision', 'Geometric Computing', 'Spatial Pattern Discovery'], conference_acronym='Journal of Molecular Biology', publisher=None, query_handler=None),\n",
       " 'Evaluating color descriptors for object and scene recognition': Paper(DOI='10.1109/tpami.2009.154', crossref_json=None, google_schorlar_metadata=None, title='Evaluating color descriptors for object and scene recognition', authors=['Koen EA Van De Sande', 'Theo Gevers', 'Cees GM Snoek'], abstract='Image category recognition is important to access visual information on the level of objects and scene types. So far, intensity-based descriptors have been widely used for feature extraction at salient points. To increase illumination invariance and discriminative power, color descriptors have been proposed. Because many different descriptors exist, a structured overview is required of color invariant descriptors in the context of image category recognition. Therefore, this paper studies the invariance properties and the distinctiveness of color descriptors (software to compute the color descriptors from this paper is available from http://www.colordescriptors.com) in a structured way. The analytical invariance properties of color descriptors are explored, using a taxonomy based on invariance properties with respect to photometric transformations, and tested experimentally using a data set with known illumination conditions\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2009.132', '10.1023/B:VISI.0000042993.50813.60', '10.1145/1282280.1282352', '10.1145/1180639.1180727', '10.1145/1178677.1178722', '10.1145/1290082.1290118', '10.1109/ICIP.2005.1530550', '10.1007/s11263-006-9794-4', '10.1002/col.5080100409', '10.1016/j.cviu.2003.10.011', '10.1007/s11263-006-8614-1', '10.1109/CVPR.2006.68', '10.1109/CVPR.2007.383120', '10.1109/ICCV.2005.142', '10.1016/j.imavis.2004.02.006', '10.1145/1348246.1348248', '10.1023/B:VISI.0000029664.99615.94', '10.1109/ICCV.2003.1238663', '10.1109/TPAMI.2007.70716', '10.1364/JOSAA.11.001553', '10.1109/TPAMI.2006.3', '10.1016/j.cviu.2008.07.003', '10.1109/CVPR.2003.1211479', '10.1109/CVPR.2006.95', '10.1023/A:1011126920638', '10.1109/34.977559', '10.1007/s11263-005-3848-x', '10.1561/0600000017', '10.1109/MMUL.2006.63', '10.5244/C.17.78', '10.1109/ICCV.2005.66', '10.1109/CVPRW.2006.177', '10.1214/aos/1176344552'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Object Recognition', 'HPC'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Boosting color saliency in image feature detection': Paper(DOI='10.1109/tpami.2006.3', crossref_json=None, google_schorlar_metadata=None, title='Boosting color saliency in image feature detection', authors=['Joost Van De Weijer', 'Theo Gevers', 'Andrew D Bagdanov'], abstract='The aim of salient feature detection is to find distinctive local events in images. Salient features are generally determined from the local differential structure of images. They focus on the shape-saliency of the local neighborhood. The majority of these detectors are luminance-based, which has the disadvantage that the distinctiveness of the local color information is completely ignored in determining salient image features. To fully exploit the possibilities of salient point detection in color images, color distinctiveness should be taken into account in addition to shape distinctiveness. In this paper, color distinctiveness is explicitly incorporated into the design of saliency detection. The algorithm, called color saliency boosting, is based on an analysis of the statistics of color image derivatives. Color saliency boosting is designed as a generic method easily adaptable to existing feature detectors. Results show that substantial\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF00318371', '10.1109/TIP.2005.860343', '10.1002/col.5080100409', '10.1109/CVPR.2003.1211479', '10.1016/j.imavis.2003.08.012', '10.1109/34.589215', '10.1023/B:VISI.0000027790.02288.f2', '10.1023/B:VISI.0000029664.99615.94', '10.1023/A:1008187804026', '10.1006/cviu.1999.0787', '10.1007/978-94-015-9664-0_2', '10.1109/CVPR.2003.1211478', '10.5244/C.2.23', '10.1109/TPAMI.2005.75', '10.1109/TPAMI.2004.29', '10.1109/72.788646', '10.1007/BF01418978', '10.1109/34.730558', '10.1023/A:1008199403446', '10.1109/34.954599', '10.1038/35058500', '10.1006/cviu.1997.0556'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Deep Learning', 'Continual Learning', 'Incremental Learning', 'Color Imaging'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Color constancy using natural image statistics and scene semantics': Paper(DOI='10.1109/tpami.2010.93', crossref_json=None, google_schorlar_metadata=None, title='Color constancy using natural image statistics and scene semantics', authors=['Arjan Gijsenij', 'Theo Gevers'], abstract='Existing color constancy methods are all based on specific assumptions such as the spatial and spectral characteristics of images. As a consequence, no algorithm can be considered as universal. However, with the large variety of available methods, the question is how to select the method that performs best for a specific image. To achieve selection and combining of color constancy algorithms, in this paper natural image statistics are used to identify the most important characteristics of color images. Then, based on these image characteristics, the proper color constancy algorithm (or best combination of algorithms) is selected for a specific image. To capture the image characteristics, the Weibull parameterization (e.g., grain size and contrast) is used. It is shown that the Weibull parameterization is related to the image attributes to which the used color constancy methods are sensitive. An MoG-classifier is used to\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/S0031-3203(98)00036-3', '10.1038/scientificamerican1277-108', '10.1109/CVPR.2007.383206', '10.1109/83.817602', '10.1023/A:1011139631724', '10.1109/CVPRW.2006.177', '10.1002/col.20226', '10.1007/BF00056770', '10.1364/JOSA.61.000001', '10.1109/TIP.2007.901808', '10.1016/0016-0032(80)90058-7', '10.1167/4.2.1', '10.1017/S0952523806233455', '10.1145/274644.274666', '10.1109/ICCV.2007.4409109', '10.5244/C.20.105', '10.1364/JOSAA.15.002036', '10.1109/TIP.2002.802529', '10.1002/col.10049', '10.1364/JOSAA.26.002243', '10.1364/JOSAA.17.002108', '10.1016/j.patrec.2005.07.020', '10.1109/34.93808', '10.1109/CVPR.2005.20', '10.1364/JOSAA.11.001553', '10.1007/BF00275077', '10.1109/34.969113', '10.1109/ICCV.2007.4409102', '10.1007/s11263-006-4100-z', '10.1007/s11263-008-0171-3', '10.1364/JOSAA.14.001393', '10.1109/TIP.2002.802531', '10.1364/JOSAA.23.001008', '10.1088/0954-898X/14/3/302', '10.1023/B:VISI.0000046586.95219.e7', '10.1023/A:1023052124951'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'image understanding', 'deep learning', 'object recognition', 'color'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Generalized gamut mapping using image derivative structures for color constancy': Paper(DOI='10.1007/s11263-008-0171-3', crossref_json=None, google_schorlar_metadata=None, title='Generalized gamut mapping using image derivative structures for color constancy', authors=['Arjan Gijsenij', 'Theo Gevers', 'Joost van de Weijer'], abstract=' The gamut mapping algorithm is one of the most promising methods to achieve computational color constancy. However, so far, gamut mapping algorithms are restricted to the use of pixel values to estimate the illuminant. Therefore, in this paper, gamut mapping is extended to incorporate the statistical nature of images. It is analytically shown that the proposed gamut mapping framework is able to include any linear filter output. The main focus is on the local n-jet describing the derivative structure of an image. It is shown that derivatives have the advantage over pixel values to be invariant to disturbing effects (i.e. deviations of the diagonal model) such as saturated colors and diffuse light. Further, as the n-jet based gamut mapping has the ability to use more information than pixel values alone, the combination of these algorithms are more stable than the regular gamut mapping algorithm. Different\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/3-540-45054-8_26', '10.1002/col.10049', '10.1117/12.705190', '10.1364/JOSAA.14.001393', '10.1016/0016-0032(80)90058-7', '10.1167/4.2.1', '10.1016/0734-189X(86)90223-9', '10.1109/34.541413', '10.1109/83.869188', '10.1109/34.969113', '10.1109/ICIP.2005.1530550', '10.1007/s11263-006-4100-z', '10.1007/BF00056770', '10.1017/S0952523806233455', '10.1109/CVPR.2008.4587765', '10.1109/83.817602', '10.1109/CVPR.2007.383206', '10.1002/col.20226', '10.1016/0734-189X(87)90043-0', '10.1007/BF00318371', '10.1038/scientificamerican1277-108', '10.1002/col.5080100409', '10.1109/TIP.2007.901808', '10.1109/ICCV.2007.4409109', '10.1145/274644.274666'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Deep Learning', 'Continual Learning', 'Incremental Learning', 'Color Imaging'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Accurate eye center location through invariant isocentric patterns': Paper(DOI='10.1109/tpami.2011.251', crossref_json=None, google_schorlar_metadata=None, title='Accurate eye center location through invariant isocentric patterns', authors=['Roberto Valenti', 'Theo Gevers'], abstract='Locating the center of the eyes allows for valuable information to be captured and used in a wide range of applications. Accurate eye center location can be determined using commercial eye-gaze trackers, but additional constraints and expensive hardware make these existing solutions unattractive and impossible to use on standard (i.e., visible wavelength), low-resolution images of eyes. Systems based solely on appearance are proposed in the literature, but their accuracy does not allow us to accurately locate and distinguish eye centers movements in these low-resolution settings. Our aim is to bridge this gap by locating the center of the eye within the area of the pupil on low-resolution images taken from a webcam or a similar device. The proposed method makes use of isophote properties to gain invariance to linear lighting changes (contrast and brightness), to achieve in-plane rotational invariance, and to\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1314303.1314306', '10.1023/B:VISI.0000013087.49260.fb', '10.1016/j.cviu.2006.08.008', '10.1016/j.patcog.2003.09.006', '10.1016/j.cviu.2004.07.012', '10.1109/CVPR.2005.196', '10.1023/B:VISI.0000029664.99615.94', '10.1016/j.cviu.2004.07.010', '10.1109/TPAMI.2003.1195991', '10.1109/34.879790', '10.1109/TMM.2011.2120600', '10.1007/BF01418978', '10.1145/1314303.1314305', '10.1109/ICPR.2006.136', '10.5244/C.20.20', '10.1109/CVPR.2010.5539773', '10.1109/CVPR.2008.4587529', '10.1145/1314303.1314304', '10.1109/34.927464', '10.5244/C.18.30', '10.5244/C.20.95', '10.1109/34.927467', '10.1109/AFGR.2004.1301514', '10.1016/0262-8856(92)90076-F', '10.1145/1386352.1386401', '10.1109/AFGR.2008.4813399', '10.1145/1314303.1314304', '10.1007/978-1-4612-1836-4', '10.1109/TPAMI.2005.179', '10.1109/4235.843496', '10.1023/A:1020276319925', '10.1016/j.cviu.2004.07.006'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'image understanding', 'deep learning', 'object recognition', 'color'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Are you really smiling at me? spontaneous versus posed enjoyment smiles': Paper(DOI='10.1007/978-3-642-33712-3_38', crossref_json=None, google_schorlar_metadata=None, title='Are you really smiling at me? spontaneous versus posed enjoyment smiles', authors=['Hamdi Dibeklioğlu', 'Albert Ali Salah', 'Theo Gevers'], abstract=' Smiling is an indispensable element of nonverbal social interaction. Besides, automatic distinction between spontaneous and posed expressions is important for visual analysis of social signals. Therefore, in this paper, we propose a method to distinguish between spontaneous and posed enjoyment smiles by using the dynamics of eyelid, cheek, and lip corner movements. The discriminative power of these movements, and the effect of different fusion levels are investigated on multiple databases. Our results improve the state-of-the-art. We also introduce the largest spontaneous/posed enjoyment smile database collected to date, and report new empirical and conceptual findings on smile dynamics. The collected database consists of 1240 samples of 400 subjects. Moreover, it has the unique property of having an age range from 8 to 76 years. Large scale experiments on the new database indicate that\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s10919-008-0059-5', '10.1111/j.1469-8986.1981.tb02919.x', '10.1007/BF00987191', '10.1142/S021969130400041X', '10.1007/s10919-008-0058-6', '10.1037/a0017844', '10.3389/fpsyg.2011.00143', '10.1145/1322192.1322202', '10.1109/ICCVW.2011.6130343', '10.1145/1873951.1874056', '10.1016/S0301-0511(03)00098-X', '10.1109/TPAMI.2005.159', '10.1109/TMM.2010.2060716', '10.1109/TIP.2011.2163162', '10.1007/s10919-005-0003-x', '10.1145/2393347.2393382'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'image understanding', 'deep learning', 'object recognition', 'color'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Color constancy for multiple light sources': Paper(DOI='10.2352/issn.2470-1173.2020.10.ipas-135', crossref_json=None, google_schorlar_metadata=None, title='Color constancy for multiple light sources', authors=['Arjan Gijsenij', 'Rui Lu', 'Theo Gevers'], abstract='Color constancy algorithms are generally based on the simplifying assumption that the spectral distribution of a light source is uniform across scenes. However, in reality, this assumption is often violated due to the presence of multiple light sources. In this paper, we will address more realistic scenarios where the uniform light-source assumption is too restrictive. First, a methodology is proposed to extend existing algorithms by applying color constancy locally to image patches, rather than globally to the entire image. After local (patch-based) illuminant estimation, these estimates are combined into more robust estimations, and a local correction is applied based on a modified diagonal model. Quantitative and qualitative experiments on spectral and real images show that the proposed methodology reduces the influence of two light sources simultaneously present in one scene. If the chromatic difference between\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'image understanding', 'deep learning', 'object recognition', 'color'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Multimodal approaches for emotion recognition: a survey': Paper(DOI='10.1016/j.neucom.2023.126693', crossref_json=None, google_schorlar_metadata=None, title='Multimodal approaches for emotion recognition: a survey', authors=['Nicu Sebe', 'Ira Cohen', 'Theo Gevers', 'Thomas S Huang'], abstract=\"Recent technological advances have enabled human users to interact with computers in ways previously unimaginable. Beyond the confines of the keyboard and mouse, new modalities for human-computer interaction such as voice, gesture, and force-feedback are emerging. Despite important advances, one necessary ingredient for natural interaction is still missing-emotions. Emotions play an important role in human-to-human communication and interaction, allowing people to express themselves beyond the verbal domain. The ability to understand human emotions is desirable for the computer in several applications. This paper explores new ways of human-computer interaction that enable the computer to be more aware of the user's emotional and attentional expressions. We present the basic research in the field and the recent advances into the emotion recognition from facial, voice, and physiological\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1016/j.inffus.2017.02.003', '10.3390/s18113886', '10.7603/s40601-013-0038-5', '10.1590/1516-4446-2016-1988', '10.1007/s10916-020-01676-6', '10.1016/j.jjcc.2014.03.012', '10.1371/journal.pone.0252408', '10.3389/fpsyg.2018.01303', '10.3389/fpsyg.2019.00508', '10.1016/j.procs.2017.05.025', '10.51735/ijiccn/001/30', '10.1186/s13638-021-02015-0', '10.3389/fpsyg.2019.01389', '10.1109/ICIECS.2009.5362730', '10.1007/978-3-642-30157-5_5', '10.1080/02699939208411068', '10.1037/0022-3514.74.4.967', '10.4018/jse.2010101605', '10.1112/jlms/s2-30.3.419', '10.1109/ICCV.2019.00699', '10.1109/LSP.2016.2603342', '10.1109/IHMSC.2018.00084', '10.1109/CVPR.2015.7298682', '10.1109/FG.2017.23', '10.1016/j.patrec.2021.01.029', '10.25046/aj030437', '10.1109/ICASSP.2017.7952552', '10.21437/Interspeech.2019-2594', '10.1109/LSP.2018.2860246', '10.1109/ICASSP43922.2022.9747460', '10.1002/eng2.12189', '10.1007/978-3-540-71496-5_53', '10.1007/s41060-018-0096-z', '10.1109/WI-IAT.2012.170', '10.1609/aaai.v32i1.12021', '10.3115/v1/D14-1162', '10.1109/JPROC.2015.2460697', '10.1109/T-AFFC.2011.15', '10.1109/ACCESS.2019.2891579', '10.1038/s41597-019-0209-0', '10.1109/T-AFFC.2011.20', '10.1109/FG.2013.6553805', '10.1109/TAFFC.2015.2392932', '10.1609/aaai.v32i1.12024', '10.1109/WACV.2016.7477679', '10.1109/MIS.2016.94', '10.1145/2818346.2820747', '10.1109/CCCM.2009.5267459', '10.1007/s11263-017-0997-7', '10.18653/v1/P17-1081', '10.1109/ACCESS.2020.3026823', '10.1016/j.inffus.2020.08.006', '10.18653/v1/N19-1423', '10.1007/978-3-319-46478-7_21', '10.18653/v1/D17-1115', '10.1016/j.eswa.2021.115507', '10.18653/v1/W18-3308', '10.1609/aaai.v33i01.33016892', '10.1016/j.knosys.2018.07.041', '10.1109/SLT48900.2021.9383618', '10.18653/v1/N19-1034', '10.1145/3394171.3413678', '10.1109/ICCV48922.2021.00148', '10.1609/aaai.v35i3.16330', '10.1109/JSTSP.2017.2764438', '10.1609/aaai.v36i8.20872', '10.1109/ACCESS.2021.3070212', '10.1109/MIS.2013.34', '10.1145/2070481.2070509', '10.1145/2663204.2663260', '10.18653/v1/P18-1208'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'image understanding', 'deep learning', 'object recognition', 'color'], conference_acronym='Neurocomputing (Amsterdam)', publisher=None, query_handler=None),\n",
       " 'Effects of steam explosion pretreatment on the bioactive components and characteristics of rapeseed and rapeseed products': Paper(DOI='10.1016/j.lwt.2021.111172', crossref_json=None, google_schorlar_metadata=None, title='Effects of steam explosion pretreatment on the bioactive components and characteristics of rapeseed and rapeseed products', authors=['Weijun Wang', 'Bo Yang', 'Wenlin Li', 'Qi Zhou', 'Changsheng Liu', 'Chang Zheng'], abstract='The objectives of this study were to obtain the optimal technical parameters of steam explosion for rapeseed and investigate the effects of pretreatment on the bioactive components and characteristics of three diverse samples of Brassica seed (Brassica napus, B. juncea and B. rapa) and their products. Under the optimal conditions, Brassica napus formed the largest amount of canolol, namely, 1210.10\\xa0mg/kg; B. rapa formed the smallest amount, namely, 82.70\\xa0mg/kg; and the canolol content was 2110.00\\xa0mg/kg in Brassica napus oil. Meanwhile, compared to the traditional cold-pressed rapeseed oil, the total tocopherol and phytosterol contents in Brassica napus, B. rapa and B. juncea oil increased by 5.3%, 4.8%, and 7.4% and 2.1%, 3.2%, and 5.1%, respectively. Steam explosion pretreatment also significantly affected the total phenolic contents and antioxidant capacities of the three types of rapeseeds and their\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.tifs.2010.11.002', '10.1016/j.tifs.2020.03.044', '10.1016/j.foodchem.2010.02.006', '10.1016/j.tet.2007.07.035', '10.1021/jf010701v', '10.1016/j.biortech.2010.11.052', '10.1021/jf403092b', '10.1016/j.psep.2011.08.004', '10.1016/j.lwt.2019.03.048', '10.1016/j.jfca.2019.103394', '10.1021/ie801474q', '10.1016/j.ijhydene.2006.09.027', '10.1016/j.imbio.2014.01.007', '10.1002/ejlt.201300425', '10.1007/s11746-011-1852-6', '10.1016/j.foodchem.2008.09.097', '10.1016/j.biombioe.2014.02.002', '10.1007/s12161-017-1030-z', '10.1016/j.indcrop.2018.04.045', '10.1515/HF.2002.047', '10.1007/s11746-009-1486-0', '10.1007/s00217-003-0721-4', '10.1021/ie801542g', '10.1016/j.biortech.2015.06.114', '10.1016/j.foodhyd.2019.02.042', '10.1007/s11746-008-1268-0', '10.1007/s11746-002-0555-x', '10.3390/antiox8080313', '10.1016/j.biombioe.2013.12.003', '10.1016/j.jfca.2007.07.012', '10.1002/ejlt.201400378', '10.1002/jsfa.2740100110', '10.1002/ejlt.200900292', '10.1016/j.foodchem.2011.01.040', '10.1016/j.talanta.2008.04.055', '10.1016/j.procbio.2009.09.014', '10.3390/molecules25174015', '10.1111/ijfs.12749', '10.2174/157340130904131122094946', '10.1002/lite.201300249', '10.1271/bbb.69.1568', '10.1016/j.lwt.2016.01.013', '10.1007/s11947-012-0987-2', '10.1016/j.jfca.2012.08.009', '10.1021/jf4054287', '10.1016/j.lwt.2017.05.048', '10.1016/j.foodres.2020.109160', '10.1002/ejlt.201300229'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Robust object tracking using constellation model with superpixel': Paper(DOI='10.1007/978-3-642-37431-9_15', crossref_json=None, google_schorlar_metadata=None, title='Robust object tracking using constellation model with superpixel', authors=['Weijun Wang', 'Ramakant Nevatia'], abstract=' Tracking objects under occlusion or non-rigid deformation poses a major problem: appearance variation of target makes existing bounding rectangle based representation vulnerable to background noise imported during adaptive appearance update. We address the object tracking problem by exploring superpixel based visual information around the target. Instead of representing each object with a single holistic appearance model, we propose to track each target with multiple related parts and model the tracking system as a Dynamic Bayesian Network(DBN). Based on visual features from superpixels, we propose a constellation appearance model with multiple parts which is adaptable to appearance variations. A particle-based approximate inference algorithm over the DBN is proposed for tracking. Experimental results show that the proposed algorithm performs favorably against existing object trackers\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-540-88688-4_58', '10.1007/s11263-006-0027-7', '10.1109/TPAMI.2003.1195991', '10.1109/CVPR.2010.5539821', '10.1109/TPAMI.2007.35', '10.1109/CVPR.2009.5206737', '10.1109/CVPR.2010.5540231', '10.1109/CVPR.2007.383177', '10.1109/CVPR.2010.5539893', '10.1007/978-3-642-15555-0_20', '10.1007/978-3-642-15549-9_32', '10.1109/CVPR.2011.6044588', '10.1109/CVPR.2011.5995366', '10.1016/S0262-8856(02)00129-4'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'MOSAIC: Mobile Segmentation via decoding Aggregated Information and encoded Context': Paper(DOI='10.1155/2023/9872576', crossref_json=None, google_schorlar_metadata=None, title='MOSAIC: Mobile Segmentation via decoding Aggregated Information and encoded Context', authors=['Weijun Wang', 'Andrew Howard'], abstract='We present a next-generation neural network architecture, MOSAIC, for efficient and accurate semantic image segmentation on mobile devices. MOSAIC is designed using commonly supported neural operations by diverse mobile hardware platforms for flexible deployment across various mobile platforms. With a simple asymmetric encoder-decoder structure which consists of an efficient multi-scale context encoder and a light-weight hybrid decoder to recover spatial details from aggregated information, MOSAIC achieves new state-of-the-art performance while balancing accuracy and computational cost. Deployed on top of a tailored feature extraction backbone based on a searched classification network, MOSAIC achieves a 5% absolute accuracy gain surpassing the current industry standard MLPerf models and state-of-the-art architectures.', conference=None, journal=None, year=None, reference_list=['10.1155/2022/9433661'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Robust 3d action recognition with random occupancy patterns': Paper(DOI='10.1007/978-3-642-33709-3_62', crossref_json=None, google_schorlar_metadata=None, title='Robust 3d action recognition with random occupancy patterns', authors=['Jiang Wang', 'Zicheng Liu', 'Jan Chorowski', 'Zhuoyuan Chen', 'Ying Wu'], abstract=' We study the problem of action recognition from depth sequences captured by depth cameras, where noise and occlusion are common problems because they are captured with a single commodity camera. In order to deal with these issues, we extract semi-local features called random occupancy pattern (ROP) features, which employ a novel sampling scheme that effectively explores an extremely large sampling space. We also utilize a sparse coding approach to robustly encode these features. The proposed approach does not require careful parameter tuning. Its training is very fast due to the use of the high-dimensional integral image, and it is robust to the occlusions. Our technique is evaluated on two datasets captured by commodity depth cameras: an action dataset and a hand gesture dataset. Our classification results are superior to those obtained by the state of the art approaches on both datasets.', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2011.5995316', '10.1109/ICCV.2011.6126509', '10.1109/ICCV.2011.6126356', '10.1109/ICCV.2011.6126270', '10.1023/B:VISI.0000013087.49260.fb', '10.1007/3-540-59119-2_166', '10.1109/ICCV.2007.4408849', '10.1162/neco.1997.9.7.1545', '10.1109/CVPR.2011.5995368', '10.1007/s13042-011-0019-y', '10.1109/CVPRW.2010.5543273', '10.1007/978-3-642-33275-3_31', '10.1109/CVPRW.2012.6239232', '10.1145/2393347.2396382', '10.1016/j.patrec.2010.10.007', '10.1111/j.1467-9868.2005.00503.x', '10.1007/s11263-005-1838-7', '10.1109/18.661502'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Learning actionlet ensemble for 3D human action recognition': Paper(DOI='10.1007/978-3-319-04561-0_2', crossref_json=None, google_schorlar_metadata=None, title='Learning actionlet ensemble for 3D human action recognition', authors=['Jiang Wang', 'Zicheng Liu', 'Ying Wu', 'Junsong Yuan'], abstract='Human action recognition is an important yet challenging task. Human actions usually involve human-object interactions, highly articulated motions, high intra-class variations, and complicated temporal structures. The recently developed commodity depth sensors open up new possibilities of dealing with this problem by providing 3D depth data of the scene. This information not only facilitates a rather powerful human motion capturing technique, but also makes it possible to efficiently model human-object interactions and intra-class variations. In this paper, we propose to characterize the human actions with a novel actionlet ensemble model, which represents the interaction of a subset of human joints. The proposed model is robust to noise, invariant to translational and temporal misalignment, and capable of characterizing both the human motion and the human-object interactions. We evaluate the proposed\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2011.5995316', '10.1109/CVPRW.2010.5543273', '10.1007/s11263-005-1838-7', '10.1109/CVPR.2008.4587756', '10.1007/11744085_28', '10.1016/j.imavis.2009.08.003', '10.1007/978-3-540-88688-4_31', '10.1145/1178782.1178808', '10.1007/978-3-642-35289-8_27', '10.1109/CVPR.2007.383274', '10.1109/CVPR.2011.5995476', '10.1109/CVPR.2010.5540234', '10.1109/ICCV.2009.5459303', '10.1007/978-3-642-33765-9_12', '10.1109/CVPR.2011.5995631', '10.1007/978-3-642-33709-3_62', '10.1007/978-3-642-33275-3_31', '10.1109/CVPRW.2012.6239232', '10.1145/2393347.2396382', '10.1109/CVPRW.2012.6239234', '10.1145/2019406.2019426', '10.1109/CVPRW.2013.153', '10.1145/358669.358692', '10.1023/A:1012450327387', '10.1214/07-AOAS148', '10.1109/CVPRW.2012.6239233', '10.1177/0278364913478446'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='SpringerBriefs in computer science (Print)', publisher=None, query_handler=None),\n",
       " 'Stop: Space-time occupancy patterns for 3d action recognition from depth map sequences': Paper(DOI='10.1007/978-3-642-33275-3_31', crossref_json=None, google_schorlar_metadata=None, title='Stop: Space-time occupancy patterns for 3d action recognition from depth map sequences', authors=['Antonio W Vieira', 'Erickson R Nascimento', 'Gabriel L Oliveira', 'Zicheng Liu', 'Mario FM Campos'], abstract=' This paper presents Space-Time Occupancy Patterns (STOP), a new visual representation for 3D action recognition from sequences of depth maps. In this new representation, space and time axes are divided into multiple segments to define a 4D grid for each depth map sequence. The advantage of STOP is that it preserves spatial and temporal contextual information between space-time cells while being flexible enough to accommodate intra-action variations. Our visual representation is validated with experiments on a public 3D human action dataset. For the challenging cross-subject test, we significantly improved the recognition accuracy from the previously reported 74.7% to 84.8%. Furthermore, we present an automatic segmentation and time alignment method for online recognition of depth sequences.', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-540-71457-6_23', '10.1109/TPAMI.2007.70711', '10.1109/TSMCB.2010.2043526', '10.1016/j.imavis.2009.08.003', '10.1109/TCSVT.2008.2005597', '10.1109/CVPRW.2010.5543273', '10.1109/CVPR.2007.383131', '10.1109/ICRA.2012.6224785', '10.1016/j.cviu.2006.07.013'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Foveated wide-angle imaging system and method for capturing and viewing wide-angle images in real time': Paper(DOI='10.3788/col202220.011201', crossref_json=None, google_schorlar_metadata=None, title='Foveated wide-angle imaging system and method for capturing and viewing wide-angle images in real time', authors=['Zicheng Liu', 'Michael Cohen'], abstract='The present invention includes a foveated wide-angle imaging system and method for capturing a wide-angle image and for viewing the captured wide-angle image in real time. In general, the foveated wide-angle imaging system includes a foveated wide-angle camera system having multiple cameras for capturing a scene and outputting raw output images, a foveated wide-angle stitching system for generating a stitch table, and a real-time wide-angle image correction system that creates a composed warp table from the stitch table and processes the raw output images using the composed warp table to correct distortion and perception problems. The foveated wide-angle imaging method includes using a foveated wide-angle camera system to capture a plurality of raw output images, generating a composed warp table, and processing the plurality of raw output images using the composed warp table to generate a\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Learning multi-scale block local binary patterns for face recognition': Paper(DOI='10.3724/sp.j.1087.2010.00964', crossref_json=None, google_schorlar_metadata=None, title='Learning multi-scale block local binary patterns for face recognition', authors=['Shengcai Liao', 'Xiangxin Zhu', 'Zhen Lei', 'Lun Zhang', 'Stan Z Li'], abstract=' In this paper, we propose a novel representation, called Multi-scale Block Local Binary Pattern (MB-LBP), and apply it to face recognition. The Local Binary Pattern (LBP) has been proved to be effective for image representation, but it is too local to be robust. In MB-LBP, the computation is done based on average values of block subregions, instead of individual pixels. In this way, MB-LBP code presents several advantages: (1) It is more robust than LBP; (2) it encodes not only microstructures but also macrostructures of image patterns, and hence provides a more complete image representation than the basic LBP operator; and (3) MB-LBP can be computed very efficiently using integral images. Furthermore, in order to reflect the uniform appearance of MB-LBP, we redefine the uniform patterns via statistical analysis. Finally, AdaBoost learning is applied to select most effective uniform MB-LBP features and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1023/A:1009715923555'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Face Recognition', 'Biometrics', 'Computer Vision', 'Pattern Recogniton', 'Artificial Intelligence'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'UA-DETRAC: A new benchmark and protocol for multi-object detection and tracking': Paper(DOI='10.1016/j.cviu.2020.102907', crossref_json=None, google_schorlar_metadata=None, title='UA-DETRAC: A new benchmark and protocol for multi-object detection and tracking', authors=['Longyin Wen', 'Dawei Du', 'Zhaowei Cai', 'Zhen Lei', 'Ming-Ching Chang', 'Honggang Qi', 'Jongwoo Lim', 'Ming-Hsuan Yang', 'Siwei Lyu'], abstract='Effective multi-object tracking (MOT) methods have been developed in recent years for a wide range of applications including visual surveillance and behavior understanding. Existing performance evaluations of MOT methods usually separate the tracking step from the detection step by using one single predefined setting of object detection for comparisons. In this work, we propose a new University at Albany DEtection and TRACking (UA-DETRAC) dataset for comprehensive performance evaluation of MOT systems especially on detectors. The UA-DETRAC benchmark dataset consists of 100 challenging videos captured from real-world traffic scenes (over 140,000 frames with rich annotations, including illumination, vehicle type, occlusion, truncation ratio, and vehicle bounding boxes) for multi-object detection and tracking. We evaluate complete MOT systems constructed from combinations of state-of-the-art object\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/AVSS.2017.8078518', '10.1109/CVPR.2008.4587583', '10.1109/CVPR.2011.5995311', '10.1109/CVPR.2012.6247893', '10.1109/CVPR.2014.159', '10.1109/CVPR.2011.5995667', '10.1109/TPAMI.2011.21', '10.1109/AVSS.2017.8078516', '10.1109/TPAMI.2010.232', '10.1109/CVPR.2011.5995395', '10.1007/978-3-319-46493-0_22', '10.1007/11744047_33', '10.1109/CVPR.2015.7299036', '10.1007/s11263-011-0437-z', '10.1109/CVPR.2009.5206848', '10.1109/ICCV.2013.286', '10.1109/TPAMI.2014.2300479', '10.1109/TPAMI.2011.155', '10.1007/978-3-030-01249-6_23', '10.1109/ICCV.2007.4409092', '10.1007/s11263-014-0733-5', '10.1109/TPAMI.2009.167', '10.1109/PETS-WINTER.2009.5399556', '10.1109/JOE.1983.1145560', '10.1109/TIP.2019.2922095', '10.1109/TPAMI.2013.185', '10.1109/CVPR.2012.6248074', '10.1109/ICCV.2015.169', '10.1109/CVPR.2014.81', '10.1109/TPAMI.2015.2389824', '10.1109/CVPR.2015.7299034', '10.1109/CVPR.2018.00745', '10.1109/CVPRW.2018.00141', '10.1109/TPAMI.2012.159', '10.1109/CVPR.2015.7298706', '10.1023/A:1008078328650', '10.1007/978-3-642-33783-3_8', '10.1109/CVPR.2007.383180', '10.1109/TPAMI.2005.223', '10.1109/ICCV.2015.533', '10.1109/CVPR.2008.4587586', '10.1109/CVPR.2014.453', '10.1109/TAC.2008.2008327', '10.1109/CVPR.2009.5206735', '10.1109/CVPR.2019.00533', '10.1109/AVSS.2018.8639089', '10.1109/AVSS.2017.8078560', '10.1177/0278364916679498', '10.1109/CVPR.2009.5206661', '10.1109/CVPRW.2013.111', '10.1109/IVS.2008.4621297', '10.1109/ICCV.2009.5459260', '10.1109/CVPR.2011.5995604', '10.1007/978-3-319-46493-0_32', '10.1109/CVPR.2016.91', '10.1109/TAC.1979.1102177', '10.1109/TPAMI.2016.2577031', '10.1109/ICCV.2015.349', '10.1109/ICCV.2011.6126456', '10.1109/CVPR.2014.450', '10.1109/ICCV.2003.1238663', '10.1109/AVSS.2015.7301755', '10.1007/978-3-540-69568-4_1', '10.1109/CVPR.2015.7299138', '10.1007/s11263-013-0620-5', '10.1023/B:VISI.0000013087.49260.fb', '10.1109/ICME.2017.8019461', '10.1109/TPAMI.2015.2509979', '10.1109/CVPR.2014.167', '10.1109/CVPRW.2009.5206638', '10.1109/CVPRW.2014.39', '10.1007/s11263-019-01157-5', '10.1109/CVPR.2011.5995468', '10.1109/CVPR.2014.320', '10.1109/CVPR.2014.169', '10.1109/CVPR.2016.596', '10.1007/978-3-642-33718-5_35', '10.1109/TPAMI.2017.2732350', '10.1007/978-3-642-33709-3_25', '10.1007/978-3-540-74549-5_2', '10.1109/CVPR.2018.00442', '10.1109/CVPR.2015.7298809', '10.1007/978-3-319-10602-1_26'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Face Recognition', 'Biometrics', 'Computer Vision', 'Pattern Recogniton', 'Artificial Intelligence'], conference_acronym='Computer vision and image understanding (Print)', publisher=None, query_handler=None),\n",
       " 'Occlusion-aware R-CNN: Detecting pedestrians in a crowd': Paper(DOI='10.1007/978-3-030-01219-9_39', crossref_json=None, google_schorlar_metadata=None, title='Occlusion-aware R-CNN: Detecting pedestrians in a crowd', authors=['Shifeng Zhang', 'Longyin Wen', 'Xiao Bian', 'Zhen Lei', 'Stan Z Li'], abstract='Pedestrian detection in crowded scenes is a challenging problem since the pedestrians often gather together and occlude each other. In this paper, we propose a new occlusion-aware R-CNN (OR-CNN) to improve the detection accuracy in the crowd. Specifically, we design a new aggregation loss to enforce proposals to be close and locate compactly to the corresponding objects. Meanwhile, we use a new part occlusion-aware region of interest (PORoI) pooling unit to replace the RoI pooling layer in order to integrate the prior structure information of human body with visibility prediction into the network to handle occlusion. Our detector is trained in an end-to-end fashion, which achieves state-of-the-art results on three pedestrian detection datasets, ie, CityPersons, ETH, and INRIA, and performs on-pair with the state-of-the-arts on Caltech.', conference=None, journal=None, year=None, reference_list=['10.5244/C.29.32', '10.1109/CVPR.2012.6248017', '10.1109/CVPR.2013.470', '10.1109/ICCV.2017.530', '10.1007/978-3-319-46493-0_22', '10.1109/ICCV.2015.384', '10.1109/CVPR.2016.350', '10.1109/CVPR.2014.307', '10.1109/CVPR.2016.259', '10.1109/TPAMI.2014.2300479', '10.5244/C.23.91', '10.1109/TPAMI.2011.155', '10.1109/WACV.2017.111', '10.1007/978-3-642-15567-3_18', '10.1109/CVPR.2010.5540111', '10.1109/ICCV.2007.4409092', '10.1109/TPAMI.2009.167', '10.1109/ICCV.2015.169', '10.1109/CVPR.2015.7299034', '10.1109/CVPR.2013.406', '10.1109/CVPR.2017.106', '10.1007/978-3-319-46448-0_2', '10.1109/CVPR.2014.120', '10.1109/CVPR.2017.639', '10.1109/ICCV.2013.322', '10.1109/ICCV.2013.190', '10.1109/ICPR.2016.7900151', '10.1109/ICCV.2013.257', '10.1109/CVPR.2013.411', '10.1109/CVPR.2013.414', '10.1007/978-3-319-10593-2_36', '10.1023/A:1008162616689', '10.1109/CVPR.2013.422', '10.1109/CVPR.2017.690', '10.1109/TPAMI.2016.2577031', '10.1109/CVPR.2013.465', '10.1007/s11263-013-0608-1', '10.1109/CVPR.2007.383133', '10.5244/C.26.9', '10.1109/ICCV.2015.221', '10.1109/CVPR.2015.7299143', '10.5244/C.29.175', '10.1007/s11263-013-0620-5', '10.1023/B:VISI.0000013087.49260.fb', '10.1109/ICCV.2009.5459207', '10.1109/CVPR.2018.00811', '10.1109/CVPR.2013.390', '10.1109/ICCV.2015.18', '10.1109/CVPR.2016.234', '10.5244/C.29.176', '10.1007/978-3-319-46475-6_28', '10.1109/CVPR.2014.126', '10.1109/CVPR.2016.141', '10.1109/CVPR.2015.7298784', '10.1109/CVPR.2017.474', '10.1109/CVPR.2018.00442', '10.1007/978-3-319-69923-3_1', '10.1109/BTAS.2017.8272675', '10.1109/ICCV.2017.30', '10.1007/978-3-319-54184-6_19', '10.1109/ICCV.2017.377', '10.1007/978-3-319-10602-1_26'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Face Recognition', 'Biometrics', 'Computer Vision', 'Pattern Recogniton', 'Artificial Intelligence'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Learning discriminant face descriptor': Paper(DOI='10.1007/978-3-642-37444-9_58', crossref_json=None, google_schorlar_metadata=None, title='Learning discriminant face descriptor', authors=['Zhen Lei', 'Matti Pietikäinen', 'Stan Z Li'], abstract='Local feature descriptor is an important module for face recognition and those like Gabor and local binary patterns (LBP) have proven effective face descriptors. Traditionally, the form of such local descriptors is predefined in a handcrafted way. In this paper, we propose a method to learn a discriminant face descriptor (DFD) in a data-driven way. The idea is to learn the most discriminant local features that minimize the difference of the features between images of the same person and maximize that between images from different people. In particular, we propose to enhance the discriminative ability of face representation in three aspects. First, the discriminant image filters are learned. Second, the optimal neighborhood sampling strategy is soft determined. Third, the dominant patterns are statistically constructed. Discriminative learning is incorporated to extract effective and robust features. We further apply the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/954339.954342', '10.1109/34.598228', '10.1016/0165-1684(94)90029-9', '10.1109/TIP.2002.999679', '10.1007/978-3-540-74549-5_6', '10.1109/TPAMI.2006.244', '10.1109/TIP.2006.884956', '10.1109/TIP.2010.2060207', '10.1007/978-3-540-74549-5_87', '10.1109/FG.2011.5771444', '10.1109/CVPR.2010.5539992', '10.1007/978-3-642-19318-7_15', '10.1007/978-3-642-19282-1_49', '10.1109/34.879790', '10.1109/TSMCA.2007.909557', '10.1007/978-3-642-12304-7_9', '10.1007/978-3-540-75690-3_13', '10.1016/j.sigpro.2009.02.016', '10.1007/978-3-642-15549-9_23', '10.1109/ICCV.2009.5459197'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Face Recognition', 'Biometrics', 'Computer Vision', 'Pattern Recogniton', 'Artificial Intelligence'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Face alignment in full pose range: A 3d total solution': Paper(DOI='10.1109/tpami.2017.2778152', crossref_json=None, google_schorlar_metadata=None, title='Face alignment in full pose range: A 3d total solution', authors=['Xiangyu Zhu', 'Xiaoming Liu', 'Zhen Lei', 'Stan Z Li'], abstract='Face alignment, which fits a face model to an image and extracts the semantic meanings of facial pixels, has been an important topic in the computer vision community. However, most algorithms are designed for faces in small to medium poses (yaw angle is smaller than 45 degree), which lack the ability to align faces in large poses up to 90 degree. The challenges are three-fold. First, the commonly used landmark face model assumes that all the landmarks are visible and is therefore not suitable for large poses. Second, the face appearance varies more drastically across large poses, from the frontal view to the profile view. Third, labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed. In this paper, we propose to tackle these three challenges in an new alignment framework termed 3D Dense Face Alignment (3DDFA), in which a dense 3D Morphable Model\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2015.2469286', '10.1007/978-3-319-46654-5_5', '10.1109/ICCVW.2013.126', '10.1109/CVPR.2016.372', '10.1016/S0262-8856(02)00055-0', '10.1109/CVPR.2016.454', '10.1109/ICCV.2007.4409106', '10.1109/TPAMI.2009.167', '10.1007/s11263-010-0380-4', '10.1109/CVPR.2014.218', '10.1109/CVPR.2010.5539996', '10.1109/CVPR.2015.7299058', '10.1109/TPAMI.2011.123', '10.5244/C.12.68', '10.1109/CVPR.2016.523', '10.1016/j.patcog.2008.01.024', '10.1109/CVPR.2017.166', '10.1109/ICCVW.2013.59', '10.1109/CVPR.2016.453', '10.1109/CVPR.2011.5995602', '10.1109/ICCVW.2013.58', '10.1145/2461912.2462012', '10.1007/978-3-642-33712-3_49', '10.1109/ICCVW.2011.6130513', '10.1109/CVPRW.2013.132', '10.1109/CVPR.2016.23', '10.1109/TPAMI.2003.1227983', '10.1145/311535.311556', '10.5244/C.13.48', '10.1109/TPAMI.2012.206', '10.1109/ICCV.2013.448', '10.1007/s11263-011-0426-2', '10.1109/CVPR.2016.512', '10.1561/0600000001', '10.1109/TPAMI.2008.106', '10.1109/TVCG.2013.249', '10.1109/AVSS.2009.58', '10.1109/CVPR.2016.455', '10.1109/CVPR.2013.75', '10.1109/AFGR.2002.1004174', '10.1109/CVPR.2013.446', '10.1109/ICCV.2013.244', '10.1109/ICCVW.2013.56', '10.1109/CVPR.2015.7298882', '10.1006/cviu.1995.1004', '10.1109/CVPR.2013.146', '10.5244/C.20.95', '10.1109/34.927467', '10.1109/ICCV.2017.347', '10.1109/CVPR.2010.5540094', '10.1145/2601097.2601204', '10.1109/ICCV.2015.421', '10.1007/s11263-017-1012-z', '10.1109/FG.2015.7163142', '10.1109/ICCV.2013.191', '10.1016/j.imavis.2005.08.001', '10.1109/CVPR.2005.145'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Face Recognition', 'Biometrics', 'Computer Vision', 'Pattern Recogniton', 'Artificial Intelligence'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Embedding deep metric for person re-identification: A study against large variations': Paper(DOI='10.1007/978-3-319-46448-0_44', crossref_json=None, google_schorlar_metadata=None, title='Embedding deep metric for person re-identification: A study against large variations', authors=['Hailin Shi', 'Yang Yang', 'Xiangyu Zhu', 'Shengcai Liao', 'Zhen Lei', 'Weishi Zheng', 'Stan Z Li'], abstract=' Person re-identification is challenging due to the large variations of pose, illumination, occlusion and camera view. Owing to these variations, the pedestrian data is distributed as highly-curved manifolds in the feature space, despite the current convolutional neural networks (CNN)’s capability of feature extraction. However, the distribution is unknown, so it is difficult to use the geodesic distance when comparing two samples. In practice, the current deep embedding methods use the Euclidean distance for the training and test. On the other hand, the manifold learning methods suggest to use the Euclidean distance in the local range, combining with the graphical relationship between samples, for approximating the geodesic distance. From this point of view, selecting suitable positive (i.e. intra-class) training samples within a local range is critical for training the CNN embedding, especially when the data has\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2015.7299016', '10.1109/AVSS.2011.6027316', '10.1016/j.patrec.2011.11.016', '10.1162/089976603321780317', '10.1145/1273496.1273523', '10.1016/j.patcog.2015.04.005', '10.1109/CVPR.2010.5539926', '10.1109/ICCV.2009.5459197', '10.1109/CVPR.2014.242', '10.1007/978-3-319-16199-0_10', '10.1109/CVPR.2012.6247939', '10.1109/CVPR.2013.461', '10.1007/978-3-642-37331-2_3', '10.1109/CVPR.2014.27', '10.1109/CVPR.2013.463', '10.1109/CVPR.2015.7298832', '10.1007/978-3-642-33863-2_39', '10.5244/C.26.57', '10.1201/b16974', '10.1007/978-3-319-16199-0_14', '10.1109/CVPR.2012.6247987', '10.1109/CVPR.2015.7298794', '10.5244/C.29.41', '10.1109/CVPR.2013.426', '10.1126/science.290.5500.2323', '10.1109/CVPR.2015.7298682', '10.1126/science.290.5500.2319', '10.1007/978-3-319-10590-1_35', '10.1007/978-3-319-16199-0_9', '10.1109/ICCV.2015.446', '10.1109/ICCV.2013.314', '10.1109/CVPR.2013.460', '10.1109/CVPR.2014.26', '10.1109/ICCV.2015.133', '10.1109/CVPR.2011.5995598'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Pattern Recognition', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Towards fast, accurate and stable 3d dense face alignment': Paper(DOI='10.1007/978-3-030-58529-7_10', crossref_json=None, google_schorlar_metadata=None, title='Towards fast, accurate and stable 3d dense face alignment', authors=['Jianzhu Guo', 'Xiangyu Zhu', 'Yang Yang', 'Fan Yang', 'Zhen Lei', 'Stan Z Li'], abstract=' Existing methods of 3D dthus limiting the scope of their practical applications. In this paper, we propose a novel regression framework which makes a balance among speed, accuracy and stability. Firstly, on the basis of a lightweight backbone, we propose a meta-joint optimization strategy to dynamically regress a small set of 3DMM parameters, which greatly enhances speed and accuracy simultaneously. To further improve the stability on videos, we present a virtual synthesis method to transform one still image to a short-video which incorporates in-plane and out-of-plane face moving. On the premise of high accuracy and stability, our model runs at 50\\xa0fps on a single CPU core and outperforms other state-of-the-art heavy models simultaneously. Experiments on several challenging datasets validate the efficiency of our method. The code and models will be available at                  https://github.com/cleardusk\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/2072572.2072597', '10.1109/TPAMI.2013.23', '10.1109/ICCV.2017.429', '10.1145/311535.311556', '10.1109/CVPR.2016.598', '10.1109/ICCV.2017.116', '10.1145/3272127.3275093', '10.1145/2461912.2462012', '10.1007/978-3-030-58529-7_24', '10.1007/s11263-013-0667-3', '10.1007/978-3-030-11018-5_3', '10.1109/CVPRW.2019.00038', '10.1109/CVPR.2018.00045', '10.1007/978-3-030-01264-9_33', '10.1109/CVPR.2019.00125', '10.1109/ACCESS.2018.2831927', '10.1109/FG.2017.103', '10.1007/978-3-319-97909-0_30', '10.1109/ICB45273.2019.8987415', '10.1109/CVPR42600.2020.00620', '10.1109/ICCV.2017.117', '10.1109/ICCVW.2011.6130513', '10.1561/9781933019536', '10.1007/978-3-319-46454-1_33', '10.1109/TPAMI.2017.2734779', '10.1109/ICCVW.2017.190', '10.1007/978-3-319-46484-8_29', '10.1007/978-3-319-46448-0_3', '10.1609/aaai.v34i07.6866', '10.1109/ICCVW.2013.59', '10.1109/ICCVW.2015.132', '10.1609/aaai.v33i01.33018893', '10.1109/CVPR.2014.220', '10.1109/CVPR.2019.01107', '10.1109/ICCV.2017.401', '10.1109/CVPR.2017.163', '10.1109/CVPR42600.2020.00509', '10.1109/CVPR.2015.7298882', '10.1109/CVPR.2013.146', '10.1109/ICCV.2017.506', '10.1007/978-3-030-58571-6_33', '10.1109/CVPR42600.2020.00534', '10.1109/ICCVW.2017.16', '10.1109/ICCVW.2013.58', '10.1109/CVPR.2016.23', '10.1109/TPAMI.2017.2778152', '10.1007/978-3-030-58598-3_21'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Face Recognition', 'Biometrics', 'Computer Vision', 'Pattern Recogniton', 'Artificial Intelligence'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'ORB-SLAM: A Versatile and Accurate Monocular SLAM System': Paper(DOI='10.1109/tro.2015.2463671', crossref_json=None, google_schorlar_metadata=None, title='ORB-SLAM: A Versatile and Accurate Monocular SLAM System', authors=['Raúl Mur-Artal', 'José M.M. Montiel', 'Juan D. Tardós'], abstract='This paper presents ORB-SLAM, a feature-based monocular simultaneous localization and mapping (SLAM) system that operates in real time, in small and large indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1177/0278364909103911', '10.1109/IROS.2012.6385773', '10.1109/IROS.2011.6094588', '10.1016/j.imavis.2012.02.009', '10.1109/CVPR.2006.263', '10.1109/ICRA.2011.5979949', '10.5244/C.27.13', '10.1109/ICRA.2013.6631246', '10.1177/0278364913491297', '10.1109/ICRA.2014.6906953', '10.1016/j.robot.2009.06.010', '10.1109/CVPR.2006.264', '10.1177/0278364910385483', '10.1023/B:VISI.0000029664.99615.94', '10.1023/A:1008140928553', '10.1109/ISMAR.2007.4538852', '10.1098/rspb.1986.0030', '10.1109/CVPR.2006.236', '10.15607/RSS.2010.VI.010', '10.1109/34.993559', '10.1109/TRO.2012.2197158', '10.1109/IROS.2010.5652266', '10.1109/ICCV.2011.6126517', '10.1017/CBO9780511811685', '10.1109/ICCV.2011.6126544', '10.1007/3-540-44480-7_21', '10.1007/3-540-44480-7_19', '10.1109/TPAMI.2007.1049', '10.1109/IVS.2011.5940546', '10.1109/ICRA.2014.6906584', '10.1109/TRO.2008.2003276', '10.1364/JOSAA.4.000629', '10.1007/s11263-008-0152-6', '10.1142/S0218001488000285', '10.1109/ICCV.2011.6126513', '10.1109/TPAMI.2004.17', '10.1109/TRO.2013.2279412', '10.1109/ICRA.2014.6907055'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision', 'SLAM'], conference_acronym='IEEE transactions on robotics', publisher=None, query_handler=None),\n",
       " 'ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras': Paper(DOI='10.1109/tro.2017.2705103', crossref_json=None, google_schorlar_metadata=None, title='ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras', authors=['Raul Mur-Artal', 'Juan D Tardos'], abstract='We present ORB-SLAM2, a complete simultaneous localization and mapping (SLAM) system for monocular, stereo and RGB-D cameras, including map reuse, loop closing, and relocalization capabilities. The system works in real time on standard central processing units in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our back-end, based on bundle adjustment with monocular and stereo observations, allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches with map points that allow for zero-drift localization. The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/IROS.2015.7353546', '10.1109/IROS.2015.7353631', '10.1177/0278364914551008', '10.1109/TRO.2013.2279412', '10.1109/IROS.2013.6696650', '10.1177/0278364916669237', '10.1109/TRO.2012.2197158', '10.1109/ICCV.2011.6126544', '10.1109/ICRA.2014.6906953', '10.1109/ISMAR.2011.6092378', '10.1109/IROS.2012.6385773', '10.1109/TRO.2008.2003276', '10.1109/TRO.2008.2004637', '10.1109/ICCV.2011.6126517', '10.1016/j.imavis.2012.02.009', '10.1177/0278364913491297', '10.1109/TRO.2015.2463671', '10.1007/s11263-010-0361-7', '10.15607/RSS.2010.VI.010', '10.1109/LRA.2017.2653359', '10.1177/0278364915620033'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Visual SLAM', 'Robotics', 'Computer Vision', 'SLAM'], conference_acronym='IEEE transactions on robotics', publisher=None, query_handler=None),\n",
       " 'Bags of binary words for fast place recognition in image sequences': Paper(DOI='10.1109/tro.2012.2197158', crossref_json=None, google_schorlar_metadata=None, title='Bags of binary words for fast place recognition in image sequences', authors=['Dorian Galvez-Lopez', 'Juan D Tardos'], abstract='We propose a novel method for visual place recognition using bag of words obtained from accelerated segment test (FAST)+BRIEF features. For the first time, we build a vocabulary tree that discretizes a binary descriptor space and use the tree to speed up correspondences for geometrical verification. We present competitive results with no false positives in very different datasets, using exactly the same vocabulary and settings. The whole technique, including feature extraction, requires 22 ms/frame in a sequence with 26\\xa0300 images that is one order of magnitude faster than previous approaches.', conference=None, journal=None, year=None, reference_list=['10.1016/j.cviu.2007.09.014', '10.1109/IROS.2011.6094885', '10.1177/0278364910385483', '10.1109/ROBOT.2010.5509587', '10.1023/B:VISI.0000029664.99615.94', '10.1109/TRO.2008.2004514', '10.1002/rob.20355', '10.1109/CVPR.2006.264', '10.1109/ICCV.2003.1238663', '10.1177/0278364908090961', '10.1016/j.robot.2009.06.010', '10.1109/ICCV.2011.6126542', '10.1109/ICCV.2011.6126544', '10.1177/0278364909103911', '10.1007/s10514-009-9138-7', '10.1177/0278364911400640'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision', 'SLAM'], conference_acronym='IEEE transactions on robotics', publisher=None, query_handler=None),\n",
       " 'ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM': Paper(DOI='10.1109/tro.2021.3075644', crossref_json=None, google_schorlar_metadata=None, title='ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM', authors=['C Campos', 'R Elvira', 'JJ Gómez-Rodríguez', 'JMM Montiel', 'JD Tardós'], abstract='This article presents ORB-SLAM3, the first system able to perform visual, visual-inertial and multimap SLAM with monocular, stereo and RGB-D cameras, using pin-hole and fisheye lens models. The first main novelty is a tightly integrated visual-inertial SLAM system that fully relies on maximum  a posteriori  (MAP) estimation, even during IMU initialization, resulting in real-time robust operation in small and large, indoor and outdoor environments, being two to ten times more accurate than previous approaches. The second main novelty is a multiple map system relying on a new place recognition method with improved recall that lets ORB-SLAM3 survive to long periods of poor visual information: when it gets lost, it starts a new map that will be seamlessly merged with previous maps when revisiting them. Compared with visual odometry systems that only use information from the last few seconds, ORB-SLAM3 is the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s11263-008-0152-6', '10.1371/journal.pone.0195878', '10.1002/rob.21854', '10.1109/ICRA.2017.7989445', '10.1023/B:VISI.0000029664.99615.94', '10.1364/JOSAA.4.000629', '10.1177/0278364914554813', '10.1109/ICRA.2014.6906953', '10.15607/RSS.2013.IX.037', '10.1109/ICRA.2018.8460664', '10.1177/0278364915620033', '10.1109/ROBOT.2007.364024', '10.1109/TRO.2020.2991614', '10.1109/CVPR.2018.00497', '10.1109/ICRA.2017.7989022', '10.1177/0278364913481251', '10.1109/TRO.2011.2170332', '10.1007/s11263-013-0647-7', '10.1109/TRO.2016.2597321', '10.1109/LRA.2016.2521413', '10.1109/LRA.2018.2855443', '10.1109/ICRA.2019.8793718', '10.1109/TPAMI.2017.2658577', '10.5244/C.22.6', '10.1109/ISWC.2008.4911577', '10.1109/ICCV.2017.421', '10.1109/IROS.2013.6696923', '10.1016/j.robot.2013.11.007', '10.1007/978-4-431-55879-8_9', '10.1109/TRO.2015.2463671', '10.1109/TRO.2016.2624754', '10.1007/978-3-319-10605-2_54', '10.1109/IROS.2015.7353631', '10.1109/TRO.2016.2623335', '10.1109/ICRA.2014.6906584', '10.1109/LRA.2018.2889156', '10.1109/IROS.2018.8593376', '10.1109/ICCV.2011.6126517', '10.15607/RSS.2010.VI.010', '10.1016/j.imavis.2012.02.009', '10.1002/rob.20345', '10.15607/RSS.2007.III.038', '10.1109/TRO.2008.2003276', '10.1109/IROS40897.2019.8967572', '10.1109/JRA.1987.1087109', '10.1109/TPAMI.2006.153', '10.1109/ICCV.2003.1238654', '10.1109/TPAMI.2007.1049', '10.1109/ISMAR.2007.4538852', '10.1007/978-3-540-88688-4_59', '10.1109/IROS.2012.6385773', '10.1109/ISMAR.2009.5336495', '10.1109/CVPR42600.2020.00136', '10.1109/IROS.2018.8593419', '10.1109/LRA.2017.2653359', '10.1109/TRO.2017.2705103', '10.1109/ICRA40945.2020.9197334', '10.1109/TRO.2021.3075644', '10.1109/ICRA40945.2020.9196885', '10.1109/TRO.2018.2853729', '10.1109/TRO.2012.2197158', '10.1109/ICRA.2018.8462905', '10.1109/LRA.2019.2961227', '10.1177/0278364917728574', '10.1109/IROS.2015.7353389'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Robotics', 'SLAM', 'Robot  Vision'], conference_acronym='IEEE transactions on robotics', publisher=None, query_handler=None),\n",
       " 'Data association in stochastic mapping using the joint compatibility test': Paper(DOI='10.1109/70.976019', crossref_json=None, google_schorlar_metadata=None, title='Data association in stochastic mapping using the joint compatibility test', authors=['José Neira', 'Juan D Tardós'], abstract='In this paper, we address the problem of robust data association for simultaneous vehicle localization and map building. We show that the classical gated nearest neighbor approach, which considers each matching between sensor observations and features independently, ignores the fact that measurement prediction errors are correlated. This leads to easily accepting incorrect matchings when clutter or vehicle errors increase. We propose a new measurement of the joint compatibility of a set of pairings that successfully rejects spurious matchings. We show experimentally that this restrictive criterion can be used to efficiently search for the best solution to data association. Unlike the nearest neighbor, this method provides a robust solution in complex situations, such as cluttered environments or when revisiting previously mapped regions.', conference=None, journal=None, year=None, reference_list=['10.1016/0004-3702(94)90029-9', '10.1109/ROBOT.1994.351403', '10.1109/70.833191', '10.1109/ROBOT.2000.844077', '10.1109/CIRA.1999.810068', '10.1109/ROBOT.2000.846406', '10.1007/978-1-4615-4405-0', '10.1109/70.744604', '10.1109/7.256316', '10.1177/027836499201100401', '10.1177/02783649922066484', '10.1109/70.795798', '10.1007/3-540-45118-8_53', '10.1109/ROBOT.1999.774040', '10.1007/3-540-55426-2_91', '10.1016/S0952-1976(98)00010-4', '10.1007/b98818', '10.1109/70.938382'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence', 'SLAM'], conference_acronym='IEEE transactions on robotics and automation', publisher=None, query_handler=None),\n",
       " 'Visual-inertial monocular SLAM with map reuse': Paper(DOI='10.1109/lra.2017.2653359', crossref_json=None, google_schorlar_metadata=None, title='Visual-inertial monocular SLAM with map reuse', authors=['Raúl Mur-Artal', 'Juan D Tardós'], abstract='In recent years there have been excellent results in visual-inertial odometry techniques, which aim to compute the incremental motion of the sensor with high accuracy and robustness. However, these approaches lack the capability to close loops and trajectory estimation accumulates drift even if the sensor is continually revisiting the same place. In this letter, we present a novel tightly coupled visual-inertial simultaneous localization and mapping system that is able to close loops and reuse its map to achieve zero-drift localization in already mapped areas. While our approach can be applied to any camera configuration, we address here the most general problem of a monocular camera, with its well-known scale ambiguity. We also propose a novel IMU initialization method, which computes the scale, the gravity direction, the velocity, and gyroscope and accelerometer biases, in a few seconds with high accuracy. We\\xa0…', conference=None, journal=None, year=None, reference_list=['10.15607/RSS.2015.XI.037', '10.1109/TRO.2011.2170332', '10.1109/TRO.2015.2463671', '10.1109/ISMAR.2007.4538852', '10.1177/0278364915620033', '10.1007/s11263-013-0647-7', '10.1109/TASE.2016.2550621', '10.1109/LRA.2016.2521413', '10.1017/CBO9780511811685', '10.1109/IROS.2013.6696514', '10.1016/j.robot.2013.05.001', '10.1109/IROS.2015.7353389', '10.1109/ICRA.2016.7487335', '10.1177/0278364914554813', '10.1109/ICRA.2016.7487266', '10.1109/TRO.2016.2597321', '10.15607/RSS.2015.XI.008', '10.1177/0278364910388963', '10.1364/JOSAA.4.000629', '10.1109/IROS.2012.6385773', '10.15607/RSS.2010.VI.010', '10.1109/CVPR.2012.6248074'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Visual SLAM', 'Robotics', 'Computer Vision', 'SLAM'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Robust mapping and localization in indoor environments using sonar data': Paper(DOI='10.1177/027836402320556340', crossref_json=None, google_schorlar_metadata=None, title='Robust mapping and localization in indoor environments using sonar data', authors=['Juan D Tardós', 'José Neira', 'Paul M Newman', 'John J Leonard'], abstract='In this paper we describe a new technique for the creation of feature-based stochastic maps using standard Polaroid sonar sensors. The fundamental contributions of our proposal are: (1) a perceptual grouping process that permits the robust identification and localization of environmental features, such as straight segments and corners, from the sparse and noisy sonar data; (2) a map joining technique that allows the system to build a sequence of independent limited-size stochastic maps and join them in a globally consistent way; (3) a robust mechanism to determine which features in a stochastic map correspond to the same environment feature, allowing the system to update the stochastic map accordingly, and perform tasks such as revisiting and loop closing. We demonstrate the practicality of this approach by building a geometric map of a medium size, real indoor environment, with several people moving\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-1-4612-4356-4', '10.1109/70.88101', '10.1109/34.56192', '10.1109/70.795798', '10.1109/70.976024', '10.1007/978-1-4615-4405-0', '10.1109/70.928558', '10.1016/0004-3702(94)90029-9', '10.1109/70.938381', '10.1007/978-1-4757-3437-9', '10.1109/JRA.1987.1087096', '10.7551/mitpress/3259.001.0001', '10.1177/02783649922066484', '10.1016/S0921-8890(01)00144-0', '10.1109/70.938382', '10.1016/S0734-189X(88)80033-1', '10.1177/027836499501400401', '10.1109/TPAMI.1987.4767983', '10.1016/S0004-3702(00)00017-5', '10.1007/978-1-4615-3652-9', '10.1007/978-1-4471-0765-1_21', '10.1023/A:1008854305733', '10.1109/70.976019', '10.1017/S0263574700003143', '10.1109/70.210793', '10.1177/027836498600500404', '10.1177/02783640122067435', '10.1109/70.897785'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics'], conference_acronym='The international journal of robotics research', publisher=None, query_handler=None),\n",
       " 'Hierarchical SLAM: Real-time accurate mapping of large environments': Paper(DOI='10.1109/tro.2005.844673', crossref_json=None, google_schorlar_metadata=None, title='Hierarchical SLAM: Real-time accurate mapping of large environments', authors=['Carlos Estrada', 'José Neira', 'Juan D Tardós'], abstract='In this paper, we present a hierarchical mapping method that allows us to obtain accurate metric maps of large environments in real time. The lower (or local) map level is composed of a set of local maps that are guaranteed to be statistically independent. The upper (or global) level is an adjacency graph whose arcs are labeled with the relative location between local maps. An estimation of these relative locations is maintained at this level in a relative stochastic map. We propose a close to optimal loop closing method that, while maintaining independence at the local level, imposes consistency at the global level at a computational cost that is linear with the size of the loop. Experimental results demonstrate the efficiency and precision of the proposed method by mapping the Ada Byron building at our campus. We also analyze, using simulations, the precision and convergence of our method for larger loops.', conference=None, journal=None, year=None, reference_list=['10.1007/978-1-4615-4405-0', '10.1109/ROBOT.2003.1241632', '10.1023/A:1007436523611', '10.1109/CIRA.1999.810068', '10.1023/A:1008854305733', '10.1109/ROBOT.2003.1242260', '10.1109/70.938382', '10.1109/IROS.2001.973391', '10.1109/ROBOT.2003.1241760', '10.1117/12.444158', '10.1177/027836402320556340', '10.1109/TRA.2003.814500', '10.1109/ROBOT.2003.1241872', '10.1109/ROBOT.2003.1241875', '10.1109/70.976019', '10.1002/0471221279', '10.1007/3-540-44480-7_21', '10.1109/70.795798', '10.1007/b98874', '10.1177/027836498600500404'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence', 'SLAM'], conference_acronym='IEEE transactions on robotics', publisher=None, query_handler=None),\n",
       " 'The SPmap: A probabilistic framework for simultaneous localization and map building': Paper(DOI='10.1109/70.795798', crossref_json=None, google_schorlar_metadata=None, title='The SPmap: A probabilistic framework for simultaneous localization and map building', authors=['Jose A Castellanos', 'José MM Montiel', 'José Neira', 'Juan D Tardós'], abstract='This article describes a rigorous and complete framework for the simultaneous localization and map building problem for mobile robots: the symmetries and perturbation map (SPmap), which is based on a general probabilistic representation of uncertain geometric information. We present a complete experiment with a LabMate/sup TM/ mobile robot navigating in a human-made indoor environment and equipped with a rotating 2D laser rangefinder. Experiments validate the appropriateness of our approach and provide a real measurement of the precision of the algorithms.', conference=None, journal=None, year=None, reference_list=['10.1109/ROBOT.1997.614274', '10.1109/ROBOT.1992.220119', '10.1109/70.744604', '10.1109/ROBOT.1998.677271', '10.1109/IROS.1991.174711', '10.1177/027836499201100402', '10.1007/978-1-4615-3652-9', '10.1016/B978-0-444-70396-5.50042-X', '10.1109/ROBOT.1998.677271'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence', 'SLAM'], conference_acronym='IEEE transactions on robotics and automation', publisher=None, query_handler=None),\n",
       " 'Mobile robot localization and map building: A multisensor fusion approach': Paper(DOI='10.1016/s0005-1098(03)00066-9', crossref_json=None, google_schorlar_metadata=None, title='Mobile robot localization and map building: A multisensor fusion approach', authors=['Jose A Castellanos', 'Juan D Tardos'], abstract=None, conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.1986.4767808', '10.1109/ROBOT.1998.677271', '10.1109/70.795798', '10.1109/70.976019'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision', 'SLAM'], conference_acronym='Automatica (Oxford)', publisher=None, query_handler=None),\n",
       " 'A comparison of loop closing techniques in monocular SLAM': Paper(DOI='10.1016/j.robot.2009.06.010', crossref_json=None, google_schorlar_metadata=None, title='A comparison of loop closing techniques in monocular SLAM', authors=['Brian Williams', 'Mark Cummins', 'José Neira', 'Paul Newman', 'Ian Reid', 'Juan Tardós'], abstract='Loop closure detection systems for monocular SLAM come in three broad categories: (i) map-to-map, (ii) image-to-image and (iii) image-to-map. In this paper, we have chosen an implementation of each and performed experiments allowing the three approaches to be compared. The sequences used include both indoor and outdoor environments and single and multiple loop trajectories.', conference=None, journal=None, year=None, reference_list=['10.15607/RSS.2007.III.038', '10.1177/0278364908090961', '10.1109/ICCV.2007.4409115', '10.1109/ROBOT.2008.4543473', '10.1109/IROS.2008.4650996', '10.1109/ICCV.2003.1238654', '10.1109/TPAMI.2007.1049', '10.15607/RSS.2006.II.011', '10.1109/70.976019', '10.1177/0278364904049393', '10.1109/ICCV.2003.1238663', '10.1007/11744023_32', '10.1145/358669.358692', '10.1109/TPAMI.2006.188', '10.15607/RSS.2009.V.039', '10.5244/C.22.6'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics'], conference_acronym='Robotics and autonomous systems (Print)', publisher=None, query_handler=None),\n",
       " 'Large-scale 6-dof slam with stereo-in-hand': Paper(DOI='10.1109/tro.2008.2004637', crossref_json=None, google_schorlar_metadata=None, title='Large-scale 6-dof slam with stereo-in-hand', authors=['Lina M Paz', 'Pedro Piniés', 'Juan D Tardós', 'José Neira'], abstract=' In this paper, we describe a system that can carry out simultaneous localization and mapping (SLAM) in large indoor and outdoor environments using a stereo pair moving with 6 DOF as the only sensor. Unlike current visual SLAM systems that use either bearing-only monocular information or 3-D stereo information, our system accommodates both monocular and stereo. Textured point features are extracted from the images and stored as 3-D points if seen in both images with sufficient disparity, or stored as inverse depth points otherwise. This allows the system to map both near and far features: the first provide distance and orientation, and the second provide orientation information. Unlike other vision-only SLAM systems, stereo does not suffer from “scale drift” because of unobservability problems, and thus, no other information such as gyroscopes or accelerometers is required in our system. Our SLAM algorithm\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1002/rob.20184', '10.1109/ROBOT.2007.363762', '10.1007/s11263-007-0042-3', '10.1007/s11263-006-0021-0', '10.1109/CRV.2006.25', '10.1109/CVPR.2005.461', '10.1002/rob.20103', '10.1109/IROS.2004.1389939', '10.1007/BF00126394', '10.1109/ROBOT.2007.364218', '10.1109/ICCV.2003.1238450', '10.1177/027836402128964611', '10.1177/0278364904042200', '10.1002/rob.20174', '10.1002/rob.20175', '10.1007/s11263-006-0023-y', '10.1007/s11263-006-0025-9', '10.1109/TPAMI.2002.1017615', '10.1007/3-540-45118-8_52', '10.1109/TRO.2008.2004639', '10.1109/TRO.2008.2004636', '10.1109/ROBOT.2007.363893', '10.1109/70.976019', '10.1109/ROBOT.2007.363892', '10.1109/ICCV.2003.1238654', '10.1109/TRO.2008.2003276', '10.1109/IROS.2004.1389440', '10.1109/ROBOT.2005.1570264', '10.1109/ICPR.2006.962', '10.1109/IROS.2005.1545392', '10.1109/IROS.2005.1545393', '10.1109/ROBOT.2006.1641990', '10.1109/IROS.2006.282483', '10.1109/TPAMI.2007.1049', '10.1109/ROBOT.2005.1570092', '10.5244/C.20.3', '10.15607/RSS.2007.III.038', '10.1002/rob.20173', '10.1002/rob.20209', '10.1002/rob.20178', '10.1177/027836402320556340', '10.1109/ROBOT.2003.1241882', '10.1109/ROBOT.2007.364221', '10.1109/ROBOT.2007.363563', '10.1177/0278364906072768', '10.1016/j.robot.2006.06.005', '10.1109/IROS.2005.1545053', '10.1177/0278364904045479'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision', 'SLAM'], conference_acronym='IEEE transactions on robotics', publisher=None, query_handler=None),\n",
       " 'Limits to the consistency of EKF-based SLAM': Paper(DOI='10.1016/s1474-6670(17)32063-3', crossref_json=None, google_schorlar_metadata=None, title='Limits to the consistency of EKF-based SLAM', authors=['José A Castellanos', 'José Neira', 'Juan D Tardós'], abstract='This paper analyzes the consistency of the classical extended Kalman filter (EKF) solution to the simultaneous localization and map building (SLAM) problem. Our results show that in large environments the map quickly becomes inconsistent due to linearization errors. We propose a new EKF-based SLAM algorithm,robocentric-mapping, that greatly reduces linearization errors, improving map consistency. We also present results showing that large-scale mapping methods based on building local maps with a local uncertainty representation (Tardós et al.,2002) have better consistency than methods that work with global uncertainties.', conference=None, journal=None, year=None, reference_list=['10.1109/70.795798', '10.1109/70.938381', '10.1109/70.938382', '10.1017/S0263574701003411', '10.1177/027836402320556340'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence', 'SLAM'], conference_acronym='IFAC proceedings volumes', publisher=None, query_handler=None),\n",
       " 'Divide and Conquer: EKF SLAM in O(n)': Paper(DOI='10.1109/robot.2007.363561', crossref_json=None, google_schorlar_metadata=None, title='Divide and Conquer: EKF SLAM in O(n)', authors=['Lina M Paz', 'Juan D Tardós', 'José Neira'], abstract=' In this paper, we show that all processes associated with the move-sense-update cycle of extended Kalman filter (EKF) Simultaneous Localization and Mapping (SLAM) can be carried out in time linear with the number of map features. We describe Divide and Conquer SLAM, which is an EKF SLAM algorithm in which the computational complexity per step is reduced from  to  , and the total cost of SLAM is reduced from  to . Unlike many current large-scale EKF SLAM techniques, this algorithm computes a solution without relying on approximations or simplifications (other than linearizations) to reduce computational complexity. Also, estimates and covariances are available when needed by data association without any further computation. Furthermore, as the method works most of the time in local maps, where angular errors remain small, the effect of linearization errors is limited. The resulting\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1177/0278364904045479', '10.1109/IROS.2006.282529', '10.1177/027836402320556340', '10.1109/IROS.2005.1545002', '10.1109/IROS.2001.973391', '10.1007/978-1-4615-4405-0', '10.1002/0471221279', '10.1109/IROS.2006.281644', '10.1109/ROBOT.2004.1307180', '10.1109/IROS.2005.1545053', '10.1016/j.robot.2006.06.005', '10.1109/70.938382', '10.1007/s10514-006-9043-2'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision', 'SLAM'], conference_acronym='Proceedings - IEEE International Conference on Robotics and Automation', publisher=None, query_handler=None),\n",
       " 'Robocentric map joining: Improving the consistency of EKF-SLAM': Paper(DOI='10.1016/j.robot.2006.06.005', crossref_json=None, google_schorlar_metadata=None, title='Robocentric map joining: Improving the consistency of EKF-SLAM', authors=['José A Castellanos', 'Ruben Martinez-Cantin', 'Juan D Tardós', 'José Neira'], abstract='In this paper we study the Extended Kalman Filter approach to simultaneous localization and mapping (EKF-SLAM), describing its known properties and limitations, and concentrate on the filter consistency issue. We show that linearization of the inherent nonlinearities of both the vehicle motion and the sensor models frequently drives the solution of the EKF-SLAM out of consistency, specially in those situations where uncertainty surpasses a certain threshold. We propose a mapping algorithm, Robocentric Map Joining, which improves consistency of the EKF-SLAM algorithm by limiting the level of uncertainty in the continuous evolution of the stochastic map: (1) by building a sequence of independent local maps, and (2) by using a robot centered representation of each local map. Simulations and a large-scale indoor/outdoor experiment validate the proposed approach.', conference=None, journal=None, year=None, reference_list=['10.1109/70.938381', '10.1016/S1474-6670(17)32063-3', '10.1109/IROS.2005.1545002', '10.1177/027836402320556340', '10.1109/70.976019', '10.1109/70.938382', '10.1017/S0263574701003411'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence', 'SLAM'], conference_acronym='Robotics and autonomous systems (Print)', publisher=None, query_handler=None),\n",
       " 'Underwater SLAM in man‐made structured environments': Paper(DOI='10.1002/rob.20249', crossref_json=None, google_schorlar_metadata=None, title='Underwater SLAM in man‐made structured environments', authors=['David Ribas', 'Pere Ridao', 'Juan Domingo Tardós', 'José Neira'], abstract=\" This paper describes a navigation system for autonomous underwater vehicles (AUVs) in partially structured environments, such as dams, harbors, marinas, and marine platforms. A mechanically scanned imaging sonar is used to obtain information about the location of vertical planar structures present in such environments. A robust voting algorithm has been developed to extract line features, together with their uncertainty, from the continuous sonar data flow. The obtained information is incorporated into a feature‐based simultaneous localization and mapping (SLAM) algorithm running an extended Kalman filter. Simultaneously, the AUV's position estimate is provided to the feature extraction algorithm to correct the distortions that the vehicle motion produces in the acoustic images. Moreover, a procedure to build and maintain a sequence of local maps and to posteriorly recover the full global map has been\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1109/MRA.2006.1678144', '10.1177/0278364904049393', '10.1109/70.795798', '10.1007/978-1-4615-4405-0', '10.15607/RSS.2007.III.038', '10.1109/MRA.2006.1638022', '10.1109/TRO.2005.844673', '10.1109/70.938382', '10.1109/TRO.2007.903811', '10.1016/S0734-189X(88)80033-1', '10.1017/S0263574701003411', '10.1007/978-1-4615-3652-9', '10.1109/48.972094', '10.1109/70.976019', '10.1007/11008941_44', '10.1109/ROBOT.2007.363564', '10.1109/IROS.2007.4399302', '10.1109/IROS.2006.282532', '10.1109/ROBOT.2007.363779', '10.1109/IROS.2005.1545340', '10.1007/978-1-4613-8997-2_14', '10.1177/027836402320556340', '10.1177/0278364904045479', '10.1109/ROBOT.2004.1308080', '10.1017/S0263574701003423'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence', 'SLAM'], conference_acronym='Journal of field robotics', publisher=None, query_handler=None),\n",
       " 'Semantic 3D object maps for everyday manipulation in human living environments': Paper(DOI='10.1007/s13218-010-0059-6', crossref_json=None, google_schorlar_metadata=None, title='Semantic 3D object maps for everyday manipulation in human living environments', authors=['Radu Bogdan Rusu'], abstract=' Environment models serve as important resources for an autonomous robot by providing it with the necessary task-relevant information about its habitat. Their use enables robots to perform their tasks more reliably, flexibly, and efficiently. As autonomous robotic platforms get more sophisticated manipulation capabilities, they also need more expressive and comprehensive environment models: for manipulation purposes their models have to include the objects present in the world, together with their position, form, and other aspects, as well as an interpretation of these objects with respect to the robot tasks. The dissertation presented in this article (Rusu, PhD thesis, 2009) proposes Semantic 3D Object Models as a novel representation of the robot’s operating environment that satisfies these requirements and shows how these models can be automatically acquired from dense 3D range data.', conference=None, journal=None, year=None, reference_list=['10.1007/s13218-010-0059-6', '10.1016/j.robot.2008.08.005', '10.1109/ROBOT.2009.5152473', '10.1109/ICHR.2009.5379597', '10.1109/IROS.2009.5354759', '10.1109/IROS.2009.5354396'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['3D Perception', 'Computer Vision', 'Robotics'], conference_acronym='KI. Künstliche Intelligenz (Oldenbourg)', publisher=None, query_handler=None),\n",
       " 'Towards 3D point cloud based object maps for household environments': Paper(DOI='10.1016/j.robot.2008.08.005', crossref_json=None, google_schorlar_metadata=None, title='Towards 3D point cloud based object maps for household environments', authors=['Radu Bogdan Rusu', 'Zoltan Csaba Marton', 'Nico Blodow', 'Mihai Dolha', 'Michael Beetz'], abstract='This article investigates the problem of acquiring 3D object maps of indoor household environments, in particular kitchens. The objects modeled in these maps include cupboards, tables, drawers and shelves, which are of particular importance for a household robotic assistant. Our mapping approach is based on PCD (point cloud data) representations. Sophisticated interpretation methods operating on these representations eliminate noise and resample the data without deleting the important details, and interpret the improved point clouds in terms of rectangular planes and 3D geometric shapes. We detail the steps of our mapping approach and explain the key techniques that make it work. The novel techniques include statistical analysis, persistent histogram features estimation that allows for a consistent registration, resampling with additional robust fitting techniques, and segmentation of the environment into\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TVCG.2003.1175093', '10.1109/34.3881', '10.1109/34.121791', '10.5244/C.15.43', '10.1145/1186822.1073227', '10.1109/IROS.2005.1545511', '10.1111/1467-8659.00439', '10.1016/j.isprsjprs.2005.02.006', '10.1145/133994.134011', '10.1109/34.765655', '10.1109/3DIM.2007.15', '10.1007/11780519_30', '10.1109/IM.2003.1240274', '10.1016/j.cviu.2004.04.002', '10.1109/IROS.2008.4650967', '10.1016/j.robot.2008.06.010', '10.1111/j.1467-8659.2007.01016.x', '10.1109/34.982886', '10.1006/cviu.1999.0832', '10.1016/j.robot.2006.12.008', '10.1109/IM.2003.1240284', '10.1007/BF01427149'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['3D Perception', 'Computer Vision', 'Robotics'], conference_acronym='Robotics and autonomous systems (Print)', publisher=None, query_handler=None),\n",
       " 'Tutorial: Point cloud library: Three-dimensional object recognition and 6 dof pose estimation': Paper(DOI='10.1109/mra.2012.2206675', crossref_json=None, google_schorlar_metadata=None, title='Tutorial: Point cloud library: Three-dimensional object recognition and 6 dof pose estimation', authors=['Aitor Aldoma', 'Zoltan-Csaba Marton', 'Federico Tombari', 'Walter Wohlkinger', 'Christian Potthast', 'Bernhard Zeisl', 'Radu Bogdan Rusu', 'Suat Gedikli', 'Markus Vincze'], abstract='With the advent of new-generation depth sensors, the use of three-dimensional (3-D) data is becoming increasingly popular. As these sensors are commodity hardware and sold at low cost, a rapidly growing group of people can acquire 3- D data cheaply and in real time.', conference=None, journal=None, year=None, reference_list=['10.1109/ROBIO.2011.6181760', '10.1007/978-3-642-19315-6_11', '10.2197/ipsjtcva.4.20', '10.1016/j.patrec.2007.02.009', '10.1007/s001380050048', '10.1109/ROBOT.2009.5152473', '10.1109/IROS.2008.4650967', '10.1109/IROS.2010.5651280', '10.1145/1877808.1877821', '10.1109/34.993558', '10.1177/0278364911415897', '10.1364/JOSAA.4.000629', '10.15607/RSS.2011.VII.030', '10.1109/IROS.2011.6094554', '10.1109/34.1000236', '10.1167/7.8.2', '10.1109/CVPR.2010.5539774'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', '3D Perception'], conference_acronym='IEEE robotics & automation magazine', publisher=None, query_handler=None),\n",
       " 'Real-time plane segmentation using RGB-D cameras': Paper(DOI='10.1007/978-3-642-32060-6_26', crossref_json=None, google_schorlar_metadata=None, title='Real-time plane segmentation using RGB-D cameras', authors=['Dirk Holz', 'Stefan Holzer', 'Radu Bogdan Rusu', 'Sven Behnke'], abstract=' Real-time 3D perception of the surrounding environment is a crucial precondition for the reliable and safe application of mobile service robots in domestic environments. Using a RGB-D camera, we present a system for acquiring and processing 3D (semantic) information at frame rates of up to 30Hz that allows a mobile robot to reliably detect obstacles and segment graspable objects and supporting surfaces as well as the overall scene geometry. Using integral images, we compute local surface normals. The points are then clustered, segmented, and classified in both normal space and spherical coordinates. The system is tested in different setups in a real household environment. The results show that the system is capable of reliably detecting obstacles at high frame rates, even in case of obstacles that move fast or do not considerably stick out of the ground. The segmentation of all planes in the 3D\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ROBOT.2010.5509571', '10.15607/RSS.2009.V.015', '10.1007/978-3-642-20217-9_11', '10.15607/RSS.2009.V.022', '10.1142/S0218195907002252', '10.1016/j.robot.2008.08.001', '10.1109/IROS.2009.5354683', '10.15607/RSS.2010.VI.009', '10.1109/ROBOT.2004.1308934'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Robotics', 'Artificial Intelligence', 'Computer Vision', 'Humanoid Robots', 'Micro Air Vehicles'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Registration with the point cloud library: A modular framework for aligning in 3-D': Paper(DOI='10.1109/mra.2015.2432331', crossref_json=None, google_schorlar_metadata=None, title='Registration with the point cloud library: A modular framework for aligning in 3-D', authors=['Dirk Holz', 'Alexandru E Ichim', 'Federico Tombari', 'Radu B Rusu', 'Sven Behnke'], abstract='Registration is an important step when processing three-dimensional (3-D) point clouds. Applications for registration range from object modeling and tracking, to simultaneous localization and mapping (SLAM). This article presents the open-source point cloud library (PCL) and the tools available for point cloud registration. The PCL incorporates methods for the initial alignment of point clouds using a variety of local shape feature descriptors, as well as methods for refining initial alignments using different variants of the well-known iterative closest point (ICP) algorithm. This article provides an overview on registration algorithms, usage examples of their PCL implementations, and tips for their application. Since the choice and parameterization of the right algorithm for a particular type of data is one of the biggest problems in 3-D point cloud registration, we present three complete examples of data (and applications\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1364/JOSAA.4.000629', '10.1016/j.imavis.2003.09.004', '10.1109/3DIMPVT.2012.84', '10.1177/0278364911434148', '10.15607/RSS.2009.V.021', '10.1109/IM.2003.1240258', '10.1007/s11263-012-0545-4', '10.1109/ICCVW.2009.5457637', '10.1007/978-3-319-08338-4_114', '10.1016/0262-8856(92)90066-C', '10.1145/358669.358692', '10.1109/ISMAR.2011.6092378', '10.1109/TPAMI.1987.4767965', '10.1109/IROS.2004.1389948', '10.1109/34.121791', '10.1007/s001380050048', '10.1109/IM.2001.924423', '10.1109/MRA.2012.2206675', '10.1109/ROBOT.2009.5152473', '10.1109/34.765655', '10.1145/1877808.1877821', '10.1007/978-3-540-24672-5_18'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', '3D Perception'], conference_acronym='IEEE robotics & automation magazine', publisher=None, query_handler=None),\n",
       " 'Evolution of genes and genomes on the Drosophila phylogeny': Paper(DOI='10.1038/nature06341', crossref_json=None, google_schorlar_metadata=None, title='Evolution of genes and genomes on the Drosophila phylogeny', authors=['Andrew G Clark', 'Lior Pachter'], abstract='Comparative analysis of multiple genomes in a phylogenetic framework dramatically improves the precision and sensitivity of evolutionary inference, producing more robust results than single-genome analyses can provide. The genomes of 12 Drosophila species, ten of which are presented here for the first time (sechellia, simulans, yakuba, erecta, ananassae, persimilis, willistoni, mojavensis, virilis and grimshawi), illustrate how rates and patterns of sequence divergence across taxa can illuminate evolutionary processes on a genomic scale. These genome sequences augment the formidable genetic tools that have made Drosophila melanogaster a pre-eminent model for animal genetics, and will further catalyse fundamental research on mechanisms of development, cell biology, genetics, disease, neurobiology, behaviour, physiology and evolution. Despite remarkable similarities among these Drosophila species, we identified many putatively non-neutral changes in protein-coding genes, non-coding RNA genes, and cis-regulatory regions. These may prove to underlie differences in the ecology and behaviour of these diverse species.', conference=None, journal=None, year=None, reference_list=['10.1534/genetics.107.074112', '10.1126/science.287.5461.2185', '10.1186/gb-2002-3-12-research0079', '10.1101/gr.3059305', '10.1126/science.287.5461.2196', '10.1126/science.7542800', '10.1038/nature06340', '10.1371/journal.pbio.0050310', '10.1007/BF02099755', '10.1093/oxfordjournals.molbev.a026394', '10.1186/gb-2005-6-3-r23', '10.1093/bioinformatics/bti1003', '10.1016/j.gene.2006.09.011', '10.1371/journal.pcbi.0010043', '10.1186/gb-2006-7-11-r112', '10.1016/0022-2836(92)90130-C', '10.1186/1471-2105-5-59', '10.1089/cmb.2006.13.379', '10.1101/gr.1865504', '10.1186/1471-2105-6-31', '10.1186/gb-2006-7-4-r29', '10.1038/nature05260', '10.1186/gb-2007-8-1-r13', '10.1038/nature06323', '10.1038/ng1875', '10.1126/science.278.5338.631', '10.1006/jmbi.2000.4042', '10.1038/nature02426', '10.1038/nature01262', '10.1093/nar/gkg169', '10.1371/journal.pbio.0050152', '10.1534/genetics.107.070672', '10.1038/276565a0', '10.1093/molbev/msg238', '10.1038/380116a0', '10.1016/j.tig.2006.12.001', '10.1073/pnas.79.15.4570', '10.1073/pnas.0732024100', '10.1534/genetics.104.032250', '10.1534/genetics.105.051714', '10.1093/oxfordjournals.molbev.a003814', '10.1093/molbev/msm116', '10.1093/molbev/msh174', '10.1093/molbev/msh180', '10.1146/annurev.bi.61.070192.000553', '10.1038/nature03154', '10.1038/35057062', '10.1126/science.282.5396.2012', '10.1261/rna.259207', '10.1073/pnas.0403400101', '10.1016/j.tibs.2006.07.007', '10.1016/S0092-8674(00)81886-3', '10.1101/gr.3567505', '10.1371/journal.pcbi.0030197', '10.1073/pnas.0509809103', '10.1016/j.gene.2006.02.011', '10.1371/journal.pgen.0020077', '10.1534/genetics.105.050336', '10.1101/gr.6049', '10.1093/bioinformatics/bti042', '10.1038/75556', '10.1111/1467-9868.00346', '10.1093/genetics/155.1.431', '10.1186/gb-2002-3-12-research0086', '10.1093/molbev/msh134', '10.1007/s00239-003-0022-3', '10.1073/pnas.0701572104', '10.1038/4151022a', '10.1534/genetics.106.056911', '10.1073/pnas.0504070102', '10.1093/molbev/msj038', '10.1093/genetics/158.2.927', '10.1038/nrg1838', '10.1073/pnas.0501761102', '10.1016/j.tig.2006.06.004', '10.1086/284701', '10.1007/s10577-006-1064-3', '10.1007/s10577-006-1061-6', '10.1016/S0960-9822(01)00408-0', '10.1016/S0959-437X(98)80038-5', '10.1093/genetics/129.3.897', '10.1017/S0016672399003912', '10.1007/BF02099948', '10.1093/genetics/146.1.295', '10.1007/s00239-003-0030-3', '10.1093/genetics/153.1.339', '10.1093/oxfordjournals.molbev.a026269', '10.1007/s002399910001', '10.1186/1741-7007-4-37', '10.1093/genetics/139.2.1067', '10.1093/genetics/144.3.1297', '10.1534/genetics.105.049676', '10.1534/genetics.167.1.171', '10.1007/PL00006535', '10.1093/molbev/msl146', '10.1093/oxfordjournals.molbev.a003918', '10.1186/gb-2007-8-11-r235', '10.1073/pnas.0608424104', '10.1126/science.1076781', '10.1016/S0378-1119(00)00533-3', '10.1111/j.1365-2583.2006.00672.x', '10.1093/embo-reports/kve151', '10.1074/jbc.M100422200', '10.1146/annurev.immunol.25.022106.141615', '10.1038/335167a0', '10.1016/0092-8674(93)90571-7', '10.1093/genetics/164.4.1471', '10.1007/BF00173190', '10.1139/g03-109', '10.1530/rep.1.00357', '10.1186/gb-2003-4-7-r42', '10.1093/genetics/154.2.909', '10.1093/genetics/144.1.419', '10.1093/bioinformatics/bti794', '10.1093/bioinformatics/bti173', '10.1101/gr.5022906', '10.1038/nature04107', '10.1016/j.gde.2006.10.003', '10.1038/sj.hdy.6800869', '10.1242/dev.125.5.949', '10.1371/journal.pcbi.0030007', '10.1093/molbev/msg236', '10.1093/bib/5.2.150', '10.1371/journal.pgen.0020173'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Genomics', 'Bioinformatics', 'Oncology', 'Data Science'], conference_acronym='Nature (London)', publisher=None, query_handler=None),\n",
       " 'Low-molecular-weight heparin in the treatment of patients with venous thromboembolism': Paper(DOI='10.1097/00005392-199804000-00138', crossref_json=None, google_schorlar_metadata=None, title='Low-molecular-weight heparin in the treatment of patients with venous thromboembolism', authors=['Columbus Investigators'], abstract=' Background Low-molecular-weight heparin is known to be safe and effective for the initial treatment of patients with proximal deep-vein thrombosis. However, its application to patients with pulmonary embolism or previous episodes of thromboembolism has not been studied. Methods We randomly assigned 1021 patients with symptomatic venous thromboembolism to fixed-dose, subcutaneous low-molecular-weight heparin (reviparin sodium) or adjusted-dose, intravenous unfractionated heparin. Oral anticoagulant therapy with a coumarin derivative was started concomitantly and continued for 12 weeks. Approximately one third of the patients had associated pulmonary embolism. The outcome events studied over the 12 weeks were symptomatic recurrent venous thromboembolism, major bleeding, and death. We sought to determine whether low-molecular-weight heparin is at least equivalent to unfractionated\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Genomics', 'Bioinformatics', 'Oncology', 'Data Science'], conference_acronym='The Journal of urology', publisher=None, query_handler=None),\n",
       " 'Aggressive assembly of pyrosequencing reads with mates': Paper(DOI='10.1093/bioinformatics/btn548', crossref_json=None, google_schorlar_metadata=None, title='Aggressive assembly of pyrosequencing reads with mates', authors=['Jason R Miller', 'Arthur L Delcher', 'Sergey Koren', 'Eli Venter', 'Brian P Walenz', 'Anushka Brownley', 'Justin Johnson', 'Kelvin Li', 'Clark Mobarry', 'Granger Sutton'], abstract='  Motivation: DNA sequence reads from Sanger and pyrosequencing platforms differ in cost, accuracy, typical coverage, average read length and the variety of available paired-end protocols. Both read types can complement one another in a ‘hybrid’ approach to whole-genome shotgun sequencing projects, but assembly software must be modified to accommodate their different characteristics. This is true even of pyrosequencing mated and unmated read combinations. Without special modifications, assemblers tuned for homogeneous sequence data may perform poorly on hybrid data.  Results: Celera Assembler was modified for combinations of ABI 3730 and 454 FLX reads. The revised pipeline called CABOG (Celera Assembler with the Best Overlap Graph) is robust to homopolymer run length uncertainty, high read coverage and heterogeneous read lengths. In tests on four genomes, it\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.gde.2006.10.009', '10.1126/science.277.5331.1453', '10.1101/gr.7088808', '10.1093/bioinformatics/17.12.1093', '10.1093/bioinformatics/btn074', '10.1101/gr.8.9.967', '10.1073/pnas.0604351103', '10.1017/CBO9780511574931', '10.1242/jeb.001370', '10.1002/0471250953.bi1103s11', '10.1073/pnas.0307971100', '10.1101/gr.828403', '10.2144/000112894', '10.1126/science.1149504', '10.1093/nar/29.22.4633', '10.1186/gb-2004-5-2-r12', '10.1371/journal.pbio.0050254', '10.1038/nature03959', '10.1126/science.287.5461.2196', '10.1128/JB.185.18.5591-5601.2003', '10.1073/pnas.171285098', '10.1089/cmb.2004.11.734', '10.1038/nature03062', '10.1089/gst.1995.1.9', '10.1038/nature06884', '10.1093/bioinformatics/btm632', '10.1186/1471-2164-7-275', '10.1101/gr.074492.107'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Genomics', 'Bioinformatics', 'Oncology', 'Data Science'], conference_acronym='Bioinformatics (Oxford. Online)', publisher=None, query_handler=None),\n",
       " 'Patterns and Implications of Gene Gain and Loss in the Evolution of Prochlorococcus': Paper(DOI='10.1371/journal.pgen.0030231.eor', crossref_json=None, google_schorlar_metadata=None, title='Patterns and Implications of Gene Gain and Loss in the Evolution of Prochlorococcus', authors=['Gregory C Kettler', 'Adam C Martiny', 'Katherine Huang', 'Jeremy Zucker', 'Maureen L Coleman', 'Sebastien Rodrigue', 'Feng Chen', 'Alla Lapidus', 'Steven Ferriera', 'Justin Johnson', 'Claudia Steglich', 'George M Church', 'Paul Richardson', 'Sallie W Chisholm'], abstract=\"Prochlorococcus is a marine cyanobacterium that numerically dominates the mid-latitude oceans and is the smallest known oxygenic phototroph. Numerous isolates from diverse areas of the world's oceans have been studied and shown to be physiologically and genetically distinct. All isolates described thus far can be assigned to either a tightly clustered high-light (HL)-adapted clade, or a more divergent low-light (LL)-adapted group. The 16S rRNA sequences of the entire Prochlorococcus group differ by at most 3%, and the four initially published genomes revealed patterns of genetic differentiation that help explain physiological differences among the isolates. Here we describe the genomes of eight newly sequenced isolates and combine them with the first four genomes for a comprehensive analysis of the core (shared by all isolates) and flexible genes of the Prochlorococcus group, and the patterns of loss and gain of the flexible genes over the course of evolution. There are 1,273 genes that represent the core shared by all 12 genomes. They are apparently sufficient, according to metabolic reconstruction, to encode a functional cell. We describe a phylogeny for all 12 isolates by subjecting their complete proteomes to three different phylogenetic analyses. For each non-core gene, we used a maximum parsimony method to estimate which ancestor likely first acquired or lost each gene. Many of the genetic differences among isolates, especially for genes involved in outer membrane synthesis and nutrient transport, are found within the same clade. Nevertheless, we identified some genes defining HL and LL ecotypes, and clades within these\\xa0…\", conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Genomics', 'Bioinformatics', 'Oncology', 'Data Science'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Photodynamic therapy of subfoveal choroidal neovascularization in pathologic myopia with verteporfin-1-year results of a randomized clinical trial-VIP report no. 1': Paper(DOI='10.1016/s0161-6420(01)00544-9', crossref_json=None, google_schorlar_metadata=None, title='Photodynamic therapy of subfoveal choroidal neovascularization in pathologic myopia with verteporfin-1-year results of a randomized clinical trial-VIP report no. 1', authors=['J Arnold', 'D Kilmartin', 'J Olson', 'S Neville', 'K Robinson', 'A Laird', 'C Richmond', 'A Farrow', 'S McKay', 'DA Saperstein', 'TM Aaberg', 'JB Johnson', 'R Waldron', 'D Loupe', 'J Gillman', 'B Myles', 'AP Schachat', 'NM Bressler', 'SB Bressler', 'P Nesbitt', 'T Porter', 'P Hawse', 'M Hartnett', 'A Eager', 'J Belt', 'D Cain', 'D Emmert', 'T George', 'M Herring', 'J McDonald', 'J Mones', 'B Corcostegui', 'M Gilbert', 'N Duran', 'M Sisquella', 'A Nolla', 'A Margalef', 'JW Miller', 'ES Gragoudas', 'AM Lane', 'N Emmanuel', 'A Holbrook', 'C Evans', 'US Lord', 'DK Walsh', 'CD Callahan', 'JL DuBois', 'H Lewis', 'PK Kaiser', 'LJ Holody', 'E Lesak', 'S Lichterman', 'H Siegel', 'A Fattori', 'G Ambrose', 'T Fecko', 'D Ross', 'S Burke', 'L Singerman', 'H Zegarra', 'M Novak', 'M Bartel', 'K Tilocco-DuBois', 'M Iic', 'S Schura', 'SJ Mayes', 'V Tanner', 'P Rowe', 'S Smith-Brewer', 'D Kukula', 'G Greanoff', 'G Daley', 'J DuBois', 'D Lehnhardt', 'GE Fish', 'BF Jost', 'R Anand', 'D Callanan', 'S Arceneaux', 'J Arnwine', 'P Ellenich', 'J King', 'H Aguado', 'R Rollins', 'B Jurklies', 'D Pauleikhoff', 'A Hintzmann', 'M Fischer', 'C Sowa', 'E Behne', 'CJ Pournaras', 'G Donati', 'AD Kapetanios', 'K Cavaliere', 'S Guney-Wagner', 'N Gerber', 'M Sickenberg', 'V Sickenberg', 'A Gans', 'B Hosner', 'A Sbressa', 'C Kozma', 'M Curchod', 'SA Cancelli', 'S Harding', 'YC Yang', 'M Briggs', 'S Briggs', 'V Tompkin', 'R Jackson', 'S Pearson', 'S Natha', 'J Sharp', 'JI Lim', 'C Flaxel', 'M Padilla', 'L Levin', 'F Walonker', 'L Cisneros', 'T Nichols', 'U Schmidt-Erfurth', 'I Barbazetto', 'H Laqua', 'R Kupfer', 'R Bulow', 'B Glisovic', 'T Bredfeldt', 'H Elsner', 'V Wintzer', 'D Bahlmann', 'S Michels', 'MS Blumenkranz', 'HL Little', 'R Jack', 'LM Espiritu', 'L Unyi', 'J Regan', 'L Lamborn', 'C Silvestri', 'RH Rosa', 'PJ Rosenfeld', 'ML Lewis', 'B Rodriguez', 'A Torres', 'N Munoz', 'T Contreras', 'M Galvez', 'D Hess', 'T Cubillas', 'I Rams'], abstract='Photodynamic therapy of subfoveal choroidal neovascularization in pathologic myopia with \\nverteporfin - 1-year results of a randomized clinical trial - VIP report no. 1 English Français login \\nMenu Search Browse Collections Help English Français login Infoscience Photodynamic \\ntherapy of subfoveal choroidal neovascularization in pathologic myopia with verteporfin - 1-year \\nresults of a randomized clinical trial - VIP report no. 1 Arnold, J.; Kilmartin, D.; Olson, J.; Neville, \\nS.; Robinson, K.; Laird, A.; Richmond, C.; Farrow, A.; McKay, S.; Saperstein, DA; Aaberg, TM; \\nJohnson, JB; Waldron, R.; Loupe, D.; Gillman, J.; Myles, B.; Schachat, AP; Bressler, NM; \\nBressler, SB; Nesbitt, P.; Porter, T.; Hawse, P.; Hartnett, M.; Eager, A.; Belt, J.; Cain, D.; Emmert, \\nD.; George, T.; Herring, M.; McDonald, J.; Mones, J.; Corcostegui, B.; Gilbert, M.; Duran, N.; \\nSisquella, M.; Nolla, A.; Margalef, A.; Miller, JW; Gragoudas, ES; Lane, AM; Emmanuel, N.; …', conference=None, journal=None, year=None, reference_list=['10.1016/0002-9394(81)90170-7', '10.1001/archopht.118.3.327', '10.1001/archopht.117.10.1329', '10.1001/archopht.1991.01080090066027'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Genomics', 'Bioinformatics', 'Oncology', 'Data Science'], conference_acronym='Ophthalmology (Rochester, Minn.)', publisher=None, query_handler=None),\n",
       " 'Genome sequence of the necrotrophic plant pathogen Pythium ultimum reveals original pathogenicity mechanisms and effector repertoire': Paper(DOI='10.1186/gb-2010-11-7-r73', crossref_json=None, google_schorlar_metadata=None, title='Genome sequence of the necrotrophic plant pathogen Pythium ultimum reveals original pathogenicity mechanisms and effector repertoire', authors=['C Andre Levesque', 'Henk Brouwer', 'Liliana Cano', 'John P Hamilton', 'Carson Holt', 'Edgar Huitema', 'Sylvain Raffaele', 'Gregg P Robideau', 'Marco Thines', 'Joe Win', 'Marcelo M Zerillo', 'Gordon W Beakes', 'Jeffrey L Boore', 'Dana Busam', 'Bernard Dumas', 'Steve Ferriera', 'Susan I Fuerstenberg', 'Claire MM Gachon', 'Elodie Gaulin', 'Francine Govers', 'Laura Grenville-Briggs', 'Neil Horner', 'Jessica Hostetler', 'Rays HY Jiang', 'Justin Johnson', 'Theerapong Krajaejun', 'Haining Lin', 'Harold JG Meijer', 'Barry Moore', 'Paul Morris', 'Vipaporn Phuntmart', 'Daniela Puiu', 'Jyoti Shetty', 'Jason E Stajich', 'Sucheta Tripathy', 'Stephan Wawra', 'Pieter van West', 'Brett R Whitty', 'Pedro M Coutinho', 'Bernard Henrissat', 'Frank Martin', 'Paul D Thomas', 'Brett M Tyler', 'Ronald P De Vries', 'Sophien Kamoun', 'Mark Yandell', 'Ned Tisserat', 'C Robin Buell'], abstract=' Background Pythium ultimum is a ubiquitous oomycete plant pathogen responsible for a variety of diseases on a broad range of crop and ornamental species. Results The P. ultimum genome (42.8 Mb) encodes 15,290 genes and has extensive sequence similarity and synteny with related Phytophthora species, including the potato blight pathogen Phytophthora infestans. Whole transcriptome sequencing revealed expression of 86% of genes, with detectable differential expression of suites of genes under abiotic stress and in the presence of a host. The predicted proteome includes a large repertoire of proteins involved in plant pathogen interactions, although, surprisingly, the P. ultimum genome does not encode any classical RXLR effectors and relatively few Crinkler genes in comparison to related phytopathogenic oomycetes. A lower number of\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Genomics', 'Bioinformatics', 'Oncology', 'Data Science'], conference_acronym='GenomeBiology.com (London. Print)', publisher=None, query_handler=None),\n",
       " 'Survey Sequencing and Comparative Analysis of the Elephant Shark (Callorhinchus milii) Genome': Paper(DOI='10.1371/journal.pbio.0050101', crossref_json=None, google_schorlar_metadata=None, title='Survey Sequencing and Comparative Analysis of the Elephant Shark (Callorhinchus milii) Genome', authors=['Byrappa Venkatesh', 'Ewen F Kirkness', 'Yong-Hwee Loh', 'Aaron L Halpern', 'Alison P Lee', 'Justin Johnson', 'Nidhi Dandona', 'Lakshmi D Viswanathan', 'Alice Tay', 'J Craig Venter', 'Robert L Strausberg', 'Sydney Brenner'], abstract='Owing to their phylogenetic position, cartilaginous fishes (sharks, rays, skates, and chimaeras) provide a critical reference for our understanding of vertebrate genome evolution. The relatively small genome of the elephant shark, Callorhinchus milii, a chimaera, makes it an attractive model cartilaginous fish genome for whole-genome sequencing and comparative analysis. Here, the authors describe survey sequencing (1.4× coverage) and comparative analysis of the elephant shark genome, one of the first cartilaginous fish genomes to be sequenced to this depth. Repetitive sequences, represented mainly by a novel family of short interspersed element–like and long interspersed element–like sequences, account for about 28% of the elephant shark genome. Fragments of approximately 15,000 elephant shark genes reveal specific examples of genes that have been lost differentially during the evolution of tetrapod and teleost fish lineages. Interestingly, the degree of conserved synteny and conserved sequences between the human and elephant shark genomes are higher than that between human and teleost fish genomes. Elephant shark contains putative four Hox clusters indicating that, unlike teleost fish genomes, the elephant shark genome has not experienced an additional whole-genome duplication. These findings underscore the importance of the elephant shark as a critical reference vertebrate genome for comparative analysis of the human and other vertebrate genomes. This study also demonstrates that a survey-sequencing approach can be applied productively for comparative analysis of distantly related vertebrate genomes.', conference=None, journal=None, year=None, reference_list=['10.1007/978-1-4020-2997-4', '10.1007/978-3-642-59674-2_11', '10.1007/978-4-431-65930-3_3'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Genomics', 'Bioinformatics', 'Oncology', 'Data Science'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Unraveling the genomic mosaic of a ubiquitous genus of marine cyanobacteria': Paper(DOI='10.1186/gb-2008-9-5-r90', crossref_json=None, google_schorlar_metadata=None, title='Unraveling the genomic mosaic of a ubiquitous genus of marine cyanobacteria', authors=['Alexis Dufresne', 'Martin Ostrowski', 'David J Scanlan', 'Laurence Garczarek', 'Sophie Mazard', 'Brian P Palenik', 'Ian T Paulsen', 'Nicole Tandeau de Marsac', 'Patrick Wincker', 'Carole Dossat', 'Steve Ferriera', 'Justin Johnson', 'Anton F Post', 'Wolfgang R Hess', 'Frédéric Partensky'], abstract=\"The picocyanobacterial genus Synechococcus occurs over wide oceanic expanses, having colonized most available niches in the photic zone. Large scale distribution patterns of the different Synechococcus clades (based on 16S rRNA gene markers) suggest the occurrence of two major lifestyles ('opportunists'/'specialists'), corresponding to two distinct broad habitats ('coastal'/'open ocean'). Yet, the genetic basis of niche partitioning is still poorly understood in this ecologically important group. Here, we compare the genomes of 11 marine Synechococcus isolates, representing 10 distinct lineages. Phylogenies inferred from the core genome allowed us to refine the taxonomic relationships between clades by revealing a clear dichotomy within the main subcluster, reminiscent of the two aforementioned lifestyles. Genome size is strongly correlated with the cumulative lengths of hypervariable regions (or 'islands'). One of these, encompassing most genes encoding the light-harvesting phycobilisome rod complexes, is involved in adaptation to changes in light quality and has clearly been transferred between members of different Synechococcus lineages. Furthermore, we observed that two strains (RS9917 and WH5701) that have similar pigmentation and physiology have an unusually high number of genes in common, given their phylogenetic distance. We propose that while members of a given marine Synechococcus lineage may have the same broad geographical distribution, local niche occupancy is facilitated by lateral gene transfers, a process in which genomic islands play a key role as a repository for transferred genes. Our work also\\xa0…\", conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Genomics', 'Bioinformatics', 'Oncology', 'Data Science'], conference_acronym='GenomeBiology.com (London. Print)', publisher=None, query_handler=None),\n",
       " 'Comparative genomics of two ecotypes of the marine planktonic copiotroph Alteromonas macleodii suggests alternative lifestyles associated with different kinds of particulate\\xa0…': Paper(DOI='10.1038/ismej.2008.74', crossref_json=None, google_schorlar_metadata=None, title='Comparative genomics of two ecotypes of the marine planktonic copiotroph Alteromonas macleodii suggests alternative lifestyles associated with different kinds of particulate\\xa0…', authors=['Elena Ivars-Martinez', 'Ana-Belen Martin-Cuadrado', \"Giuseppe D'auria\", 'Alex Mira', 'Steve Ferriera', 'Justin Johnson', 'Robert Friedman', 'Francisco Rodriguez-Valera'], abstract='Alteromonas macleodii is a common marine heterotrophic γ-proteobacterium. Isolates from this microbe cluster by molecular analysis into two major genotypic groups or ecotypes, one found in temperate latitudes in the upper water column and another that is for the most part found in the deep water column of the Mediterranean. Here, we describe the genome of one strain of the ‘deep ecotype’(AltDE) isolated from 1000 m in the Eastern Mediterranean and compare this genome with that of the type strain ATCC 27126, a representative of the global ‘surface’ecotype. The genomes are substantially different with DNA sequence similarity values that are borderline for microbes belonging to the same species, and a large differential gene content, mainly found in islands larger than 20 kbp, that also recruit poorly to the Global Ocean Sampling project (GOS). These genomic differences indicate that AltDE is probably better\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1128/AEM.65.2.514-522.1999', '10.4319/lo.1976.21.1.0014', '10.1016/S0022-2836(05)80360-2', '10.1128/AEM.56.6.1695-1701.1990', '10.1128/JB.110.1.402-429.1972', '10.1007/BF00395999', '10.1186/1471-2164-6-122', '10.1099/mic.0.2006/000687-0', '10.1093/bioinformatics/bti553', '10.1111/j.1574-6968.2006.00555.x', '10.1128/JB.52.4.461-466.1946', '10.1016/j.tim.2007.07.001', '10.1126/science.1122050', '10.1128/AEM.70.2.729-735.2004', '10.1128/AEM.00184-06', '10.1038/ismej.2007.35', '10.1016/S0966-842X(00)01703-0', '10.1007/s002849900130', '10.1093/nar/27.23.4636', '10.1126/science.1120250', '10.1128/JB.00437-06', '10.1038/nbt886', '10.1186/1471-2105-5-113', '10.1128/AEM.66.7.3044-3051.2000', '10.1128/JB.00858-07', '10.1086/519476', '10.1126/science.1127573', '10.1111/j.0014-3820.2004.tb01642.x', '10.1046/j.1462-2920.2002.00255.x', '10.1074/jbc.M110470200', '10.1186/1471-2105-5-198', '10.1126/science.1114057', '10.1128/AEM.65.8.3721-3726.1999', '10.1007/s00239-005-0223-z', '10.1073/pnas.0604351103', '10.1128/AEM.69.6.3500-3509.2003', '10.1111/j.1574-6968.2006.00520.x', '10.1098/rstb.2006.1926', '10.1126/science.1088157', '10.1128/JB.186.11.3663-3669.2004', '10.1093/plankt/8.4.819', '10.1111/j.1365-2958.2006.05255.x', '10.1016/S0378-1119(99)00388-1', '10.1111/j.1365-294X.2008.03883.x', '10.1099/00221287-143-8-2673', '10.1093/nar/gkh063', '10.1038/332438a0', '10.1371/journal.pgen.0030231', '10.1016/S0378-1119(96)00741-X', '10.1128/JB.188.9.3420-3423.2006', '10.1126/science.1147248', '10.1073/pnas.0409727102', '10.1101/gr.6835308', '10.1186/gb-2004-5-2-r12', '10.1007/s00792-006-0059-5', '10.1128/AEM.01726-06', '10.1128/JB.01176-07', '10.1186/1471-2164-7-171', '10.1073/pnas.95.21.12474', '10.1111/j.1462-2920.2005.00733.x', '10.1093/nar/25.5.955', '10.1371/journal.pone.0000914', '10.1038/ismej.2008.40', '10.1126/science.287.5461.2196', '10.1093/nar/gki866', '10.1128/AEM.67.9.4077-4083.2001', '10.1128/JB.00506-06', '10.4319/lo.1980.25.4.0643', '10.1099/00207713-49-2-513', '10.1111/j.1462-2920.2007.01500.x', '10.1016/S0168-9525(00)02024-2', '10.1073/pnas.67.4.1710', '10.1007/s00248-008-9369-8', '10.1016/S0008-6215(97)10061-1', '10.1371/journal.pbio.0050077', '10.1093/bioinformatics/16.10.944', '10.1016/j.carres.2003.07.009', '10.1007/s002039900121', '10.3354/meps096043', '10.1007/s00203-007-0322-x', '10.1146/annurev.micro.50.1.753', '10.3354/ame028175', '10.1016/j.jbiotec.2006.03.038', '10.1038/nrmicro1793', '10.1371/journal.pgen.0030015', '10.1128/jb.178.6.1742-1749.1996', '10.1093/molbev/msm092', '10.1093/nar/29.1.22', '10.1073/pnas.0506758102', '10.1093/nar/22.22.4673', '10.1016/S0966-842X(02)02319-3', '10.1098/rspb.2005.3068', '10.1126/science.1093857', '10.1126/science.1103341', '10.1021/cr050196r', '10.1128/JB.01043-06', '10.1016/j.gene.2007.04.023', '10.1111/j.1574-6941.2006.00060.x', '10.1016/j.tig.2004.03.009', '10.1186/1743-422X-3-50'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Genomics', 'Bioinformatics', 'Oncology', 'Data Science'], conference_acronym='The ISME journal (Print)', publisher=None, query_handler=None),\n",
       " 'Survival differences by sex for patients with advanced non-small cell lung cancer on Eastern Cooperative Oncology Group trial 1594': Paper(DOI='10.1097/01243894-200606000-00011', crossref_json=None, google_schorlar_metadata=None, title='Survival differences by sex for patients with advanced non-small cell lung cancer on Eastern Cooperative Oncology Group trial 1594', authors=['Heather A Wakelee', 'Wei Wang', 'Joan H Schiller', 'Corey J Langer', 'Alan B Sandler', 'Chandra P Belani', 'David H Johnson', 'Eastern Cooperative Oncology Group'], abstract='IntroductionPrevious data suggest that women may live longer with advanced non-small cell lung cancer (NSCLC) than men. We evaluated whether sex affected survival in the Eastern Cooperative Oncology Group (ECOG) E1594 trial. E1594 randomized patients with advanced NSCLC to one of four platinum doublets and found that all four regimens had comparable efficacy.Patients and MethodsPatients in the E1594 database were divided into male and female cohorts; response and survival were calculated separately for each cohort. Known prognostic factors and differences in toxicity profiles were compared between the two cohorts.ResultsAll 1157 eligible patients (431 women, 726 men) from E1594 were included in this analysis. There was no statistically significant difference in performance status, weight loss of >10%, stage, or incidence of brain metastases between women and men. Response rates were\\xa0…', conference=None, journal=None, year=None, reference_list=['10.3322/canjclin.55.1.10', '10.1093/annonc/mdf187', '10.1016/j.athoracsur.2003.11.021', '10.1016/j.lungcan.2004.08.014', '10.1016/S0003-4975(99)01078-4', '10.1016/0003-4975(89)90779-0', '10.1016/S0169-5002(02)00103-4', '10.1016/S0360-3016(00)00801-4', '10.1200/JCO.1986.4.5.702', '10.1200/JCO.1991.9.9.1618', '10.1200/JCO.1995.13.5.1221', '10.1200/JCO.1986.4.11.1604', '10.1056/NEJMoa011954', '10.1038/sj.bjc.6601781', '10.1136/thx.51.12.1266', '10.1016/0169-5002(95)00497-1', '10.1093/jnci/89.21.1580', '10.1200/JCO.1990.8.8.1402', '10.1200/jco.2004.22.14_suppl.7008', '10.1093/jnci/92.6.440', '10.1146/annurev.pa.36.040196.001223', '10.1200/JCO.2003.12.046', '10.1016/S0169-5002(05)80945-6'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Genomics', 'Bioinformatics', 'Oncology', 'Data Science'], conference_acronym='Journal of thoracic oncology', publisher=None, query_handler=None),\n",
       " 'Characterization of a marine gammaproteobacterium capable of aerobic anoxygenic photosynthesis': Paper(DOI='10.1073/pnas.0608046104', crossref_json=None, google_schorlar_metadata=None, title='Characterization of a marine gammaproteobacterium capable of aerobic anoxygenic photosynthesis', authors=['Bernhard M Fuchs', 'Stefan Spring', 'Hanno Teeling', 'Christian Quast', 'Jörg Wulf', 'Martha Schattenhofer', 'Shi Yan', 'Steve Ferriera', 'Justin Johnson', 'Frank Oliver Glöckner', 'Rudolf Amann'], abstract='Members of the gammaproteobacterial clade NOR5/OM60 regularly form an abundant part, up to 11%, of the bacterioplankton community in coastal systems during the summer months. Here, we report the nearly complete genome sequence of one cultured representative, Congregibacter litoralis strain KT71, isolated from North Sea surface water. Unexpectedly, a complete photosynthesis superoperon, including genes for accessory pigments, was discovered. It has a high sequence similarity to BAC clones from Monterey Bay [Beja O, Suzuki MT, Heidelberg JF, Nelson WC, Preston CM, et al. (2002) Nature 415:630–633], which also share a nearly identical gene arrangement. Although cultures of KT71 show no obvious pigmentation, bacteriochlorophyll a and spirilloxanthin-like carotenoids could be detected by HPLC analysis in cell extracts. The presence of two potential BLUF (blue light using flavin adenine\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1128/AEM.67.11.5134-5142.2001', '10.4319/lo.1997.42.5.0811', '10.1128/AEM.66.7.3044-3051.2000', '10.1111/j.1574-6941.2001.tb00791.x', '10.1128/AEM.65.7.3192-3204.1999', '10.1046/j.1462-2920.2000.00133.x', '10.1128/AEM.65.2.514-522.1999', '10.1111/j.1574-6941.2001.tb00775.x', '10.1128/AEM.68.8.3878-3885.2002', '10.1128/AEM.70.1.432-440.2004', '10.1128/AEM.69.11.6610-6619.2003', '10.1016/j.femsec.2005.04.002', '10.1264/jsme2.20.253', '10.1128/AEM.71.8.4638-4644.2005', '10.1007/BF00260637', '10.1186/1471-2105-5-163', '10.1038/415630a', '10.1111/j.1462-2920.2005.00843.x', '10.1099/00207713-49-2-449', '10.1128/MMBR.62.3.695-724.1998', '10.1128/jb.134.2.381-388.1978', '10.1023/B:PRES.0000011924.89742.f9', '10.1073/pnas.90.8.3309', '10.1074/jbc.M200198200', '10.1128/AEM.71.4.1709-1716.2005', '10.1007/s00203-001-0396-9', '10.1128/AEM.71.2.858-866.2005', '10.1146/annurev.micro.56.012302.160938', '10.1016/j.mib.2005.04.005', '10.1038/nature03170', '10.1099/mic.0.28523-0', '10.1016/S0006-291X(02)00936-1', '10.1111/j.1574-6968.2001.tb10806.x', '10.1128/AEM.66.10.4237-4246.2000', '10.1046/j.1462-2920.2001.00196.x', '10.4319/lo.2001.46.7.1624', '10.4319/lo.2005.50.1.0113', '10.1038/35107174', '10.1073/pnas.1431443100', '10.1111/j.1462-2920.2006.01152.x', '10.4319/lo.2004.49.6.2212', '10.1016/j.watres.2006.05.023', '10.1007/s00216-005-3360-8', '10.1126/science.289.5486.1902', '10.1038/35025044', '10.1126/science.1059707', '10.1128/AEM.72.1.557-564.2006', '10.4319/lo.2002.47.1.0290', '10.4319/lo.2005.50.2.0620', '10.4319/lo.2006.51.1.0038', '10.1016/S0723-2020(11)80292-4', '10.1038/nature02272', '10.1073/pnas.0604351103', '10.1126/science.287.5461.2196', '10.1016/S0168-9525(00)02024-2', '10.1099/mic.0.27582-0', '10.1093/bioinformatics/btg152', '10.1016/0097-8485(93)85004-V', '10.1093/nar/26.4.1107', '10.1093/nar/27.23.4636', '10.1093/nar/25.5.955', '10.1093/nar/gkg312', '10.1093/protein/12.1.3', '10.1006/jmbi.2000.4315', '10.1186/1471-2105-4-41', '10.1093/nar/gkh293', '10.4319/lo.1989.34.2.0474', '10.1099/00207713-50-3-1113', '10.1002/elps.1150190416'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Genomics', 'Bioinformatics', 'Oncology', 'Data Science'], conference_acronym='Proceedings of the National Academy of Sciences of the United States of America', publisher=None, query_handler=None),\n",
       " 'Guidelines for management of incidental pulmonary nodules detected on CT images: from the Fleischner Society 2017': Paper(DOI='10.1016/s8756-3452(08)70360-8', crossref_json=None, google_schorlar_metadata=None, title='Guidelines for management of incidental pulmonary nodules detected on CT images: from the Fleischner Society 2017', authors=['Heber MacMahon', 'David P Naidich', 'Jin Mo Goo', 'Kyung Soo Lee', 'Ann NC Leung', 'John R Mayo', 'Atul C Mehta', 'Yoshiharu Ohno', 'Charles A Powell', 'Mathias Prokop', 'Geoffrey D Rubin', 'Cornelia M Schaefer-Prokop', 'William D Travis', 'Paul E Van Schil', 'Alexander A Bankier'], abstract='The Fleischner Society Guidelines for management of solid nodules were published                    in 2005, and separate guidelines for subsolid nodules were issued in 2013. Since                    then, new information has become available; therefore, the guidelines have been                    revised to reflect current thinking on nodule management. The revised guidelines                    incorporate several substantive changes that reflect current thinking on the                    management of small nodules. The minimum threshold size for routine follow-up                    has been increased, and recommended follow-up intervals are now given as a range                    rather than as a precise time period to give radiologists, clinicians, and                    patients greater discretion to accommodate individual risk factors and                    preferences. The guidelines for solid and subsolid nodules have been combined in                    one simplified table\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1148/radiology.200.2.8685321', '10.1378/chest.123.1_suppl.89S', '10.1016/S0169-5002(03)91785-5', '10.1148/radiol.2311030634', '10.1148/radiol.2372041887'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Assessment of coronary artery disease by cardiac computed tomography: a scientific statement from the American Heart Association Committee on Cardiovascular Imaging and\\xa0…': Paper(DOI='10.1016/s0084-3741(08)70049-0', crossref_json=None, google_schorlar_metadata=None, title='Assessment of coronary artery disease by cardiac computed tomography: a scientific statement from the American Heart Association Committee on Cardiovascular Imaging and\\xa0…', authors=['Matthew J Budoff', 'Stephan Achenbach', 'Roger S Blumenthal', 'J Jeffrey Carr', 'Jonathan G Goldin', 'Philip Greenland', 'Alan D Guerci', 'Joao AC Lima', 'Daniel J Rader', 'Geoffrey D Rubin', 'Leslee J Shaw', 'Susan E Wiegers'], abstract='This scientific statement reviews the scientific data for cardiac computed tomography (CT) related to imaging of coronary artery disease (CAD) and atherosclerosis. Cardiac CT is a CT imaging technique that accounts for cardiac motion, typically through the use of ECG gating. The utility and limitations of generations of cardiac CT systems are reviewed in this statement with emphasis on CT measurement of CAD and coronary artery calcified plaque (CACP) and noncalcified plaque. Successive generations of CT technology have been applied to cardiac imaging beginning in the early 1980s with conventional CT, electron beam CT (EBCT) in 1987, and multidetector CT (MDCT) in 1999. Compared with other imaging modalities, cardiac CT has undergone an accelerated progression in imaging capabilities over the past decade, and this is expected to continue for the foreseeable future. As a result, the diagnostic\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1161/CIRCULATIONAHA.106.178458'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'ACCF/SCCT/ACR/AHA/ASE/ASNC/NASCI/SCAI/SCMR 2010 appropriate use criteria for cardiac computed tomography: a report of the American college of cardiology foundation appropriate\\xa0…': Paper(DOI='10.1016/j.yrad.2011.03.025', crossref_json=None, google_schorlar_metadata=None, title='ACCF/SCCT/ACR/AHA/ASE/ASNC/NASCI/SCAI/SCMR 2010 appropriate use criteria for cardiac computed tomography: a report of the American college of cardiology foundation appropriate\\xa0…', authors=['Allen J Taylor', 'Manuel Cerqueira', 'John McB Hodgson', 'Daniel Mark', 'James Min', \"Patrick O'Gara\", 'Geoffrey D Rubin'], abstract='The American College of Cardiology Foundation (ACCF), along with key specialty and subspecialty societies, conducted an appropriate use review of common clinical scenarios where cardiac computed tomography (CCT) is frequently considered. The present document is an update to the original CCT/cardiac magnetic resonance (CMR) appropriateness criteria published in 2006, written to reflect changes in test utilization, to incorporate new clinical data, and to clarify CCT use where omissions or lack of clarity existed in the original criteria (1).', conference=None, journal=None, year=None, reference_list=['10.1016/j.jacc.2010.07.005', '10.1161/CIR.0b013e3181fcae66', '10.1161/CIR.0b013e3182051b4c', '10.1016/j.jcct.2010.11.001'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym='The Year book of diagnostic radiology', publisher=None, query_handler=None),\n",
       " 'Accf/acr/scct/scmr/asnc/nasci/scai/sir 2006 appropriateness criteria for cardiac computed tomography and cardiac magnetic resonance imaging: A report of the american college of\\xa0…': Paper(DOI='10.1016/j.jacr.2006.08.008', crossref_json=None, google_schorlar_metadata=None, title='Accf/acr/scct/scmr/asnc/nasci/scai/sir 2006 appropriateness criteria for cardiac computed tomography and cardiac magnetic resonance imaging: A report of the american college of\\xa0…', authors=['Robert C Hendel', 'Manesh R Patel', 'Christopher M Kramer', 'Michael Poon', 'Robert C Hendel', 'James C Carr', 'Nancy A Gerstad', 'Linda D Gillam', 'John McB Hodgson', 'Raymond J Kim', 'Christopher M Kramer', 'John R Lesser', 'Edward T Martin', 'Joseph V Messer', 'Rita F Redberg', 'Geoffrey D Rubin', 'John S Rumsfeld', 'Allen J Taylor', 'Wm Guy Weigold', 'Pamela K Woodard', 'Ralph G Brindis', 'Robert C Hendel', 'Pamela S Douglas', 'Eric D Peterson', 'Michael J Wolk', 'Joseph M Allen', 'Manesh R Patel'], abstract='ABSTRACT...... 1476PREFACE...... 1476INTRODUCTION...... 1477METHODS...... 1478RESULTS OF RATINGS...... 1478ABBREVIATIONS...... 1478CCT APPROPRIATENESS CRITERIA (BY INDICATION)...... 1479Table 1. Detection of CAD: Symptomatic...... 1479Table 1. Detection of CAD: SymptomaticIndicationAppropriateness Criteria (Median Score)Evaluation of Chest Pain Syndrome (Use of CT Angiogram)1.•Intermediate pre-test probability of CAD•ECG interpretable AND able to exerciseU (5)2.•Intermediate pre-test probability of CAD•ECG uninterpretable OR unable to exerciseA (7)3.• High pre-test probability of CAD I (2)Evaluation of Intra-Cardiac Structures (Use of CT Angiogram)4.• Evaluation of suspected coronary anomalies A (9)Acute Chest Pain (Use of CT Angiogram)5.• Low pre-test probability of CAD U (5)• No ECG changes and serial enzymes negative6.• Intermediate pre-test probability of CAD A (7\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.jacc.2005.08.030', '10.1016/S0002-9343(97)00086-7', '10.1016/0002-9343(83)90406-0', '10.1056/NEJM197906143002402', '10.1016/S0735-1097(83)80093-X', '10.1016/S0735-1097(99)00387-3', '10.1016/j.jacc.2005.08.029', '10.1016/j.ehj.2004.06.040', '10.1016/S0735-1097(02)02164-2', '10.1161/01.CIR.0000047041.66447.29'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym='Journal of the American College of Radiology', publisher=None, query_handler=None),\n",
       " 'Perspective volume rendering of CT and MR images: applications for endoscopic imaging.': Paper(DOI='10.1148/radiology.199.2.8668772', crossref_json=None, google_schorlar_metadata=None, title='Perspective volume rendering of CT and MR images: applications for endoscopic imaging.', authors=['Geoffrey D Rubin', 'Christopher F Beaulieu', 'Vincent Argiro', 'Helmut Ringl', 'Alexander M Norbash', 'John F Feller', 'Michael D Dake', 'R Brooke Jeffrey', 'Sandy Napel'], abstract='PURPOSE To use perspective volume rendering (PVR) of computed tomographic (CT) and magnetic resonance (MR) imaging data sets to simulate endoscopic views of human organ systems. MATERIALS AND METHODS Perspective views of helical CT and MR images were reconstructed from the data, and tissues were classified by assigning color and opacity based on their CT attenuation or MR signal intensity. \"Flight paths\" were constructed through anatomic regions by defining key views along a spline path. Twelve movies of the thoracic aorta (n=3), tracheobronchial tree (n=4), colon (n=3), paranasal sinuses (n=1), and shoulder joint (n=1) were generated to display images along the flight path. All abnormal results were confirmed at surgery. RESULTS PVR fly-through enabled evaluation of the full range of tissue densities, signal intensities, and their three-dimensional spatial relationships. CONCLUSION\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym='Radiology', publisher=None, query_handler=None),\n",
       " 'CAD-RADSTM coronary artery disease–reporting and data system. An expert consensus document of the Society of Cardiovascular Computed Tomography (SCCT), the American College of\\xa0…': Paper(DOI='10.1016/j.jcct.2016.04.005', crossref_json=None, google_schorlar_metadata=None, title='CAD-RADSTM coronary artery disease–reporting and data system. An expert consensus document of the Society of Cardiovascular Computed Tomography (SCCT), the American College of\\xa0…', authors=['Ricardo C Cury', 'Suhny Abbara', 'Stephan Achenbach', 'Arthur Agatston', 'Daniel S Berman', 'Matthew J Budoff', 'Karin E Dill', 'Jill E Jacobs', 'Christopher D Maroules', 'Geoffrey D Rubin', 'Frank J Rybicki', 'U Joseph Schoepf', 'Leslee J Shaw', 'Arthur E Stillman', 'Charles S White', 'Pamela K Woodard', 'Jonathon A Leipsic'], abstract='The intent of CAD-RADS – Coronary Artery Disease Reporting and Data System is to create a standardized method to communicate findings of coronary CT angiography (coronary CTA) in order to facilitate decision-making regarding further patient management. The suggested CAD-RADS classification is applied on a per-patient basis and represents the highest-grade coronary artery lesion documented by coronary CTA. It ranges from CAD-RADS 0 (Zero) for the complete absence of stenosis and plaque to CAD-RADS 5 for the presence of at least one totally occluded coronary artery and should always be interpreted in conjunction with the impression found in the report. Specific recommendations are provided for further management of patients with stable or acute chest pain based on the CAD-RADS classification. The main goal of CAD-RADS is to standardize reporting of coronary CTA results and to facilitate\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.jcct.2014.07.006', '10.1016/j.jcct.2014.07.003', '10.1016/j.jcct.2009.03.004', '10.1016/j.jcct.2011.06.001', '10.1016/j.jcct.2012.11.002', '10.1016/j.jcct.2010.10.010', '10.1016/j.jacr.2013.05.002', '10.1016/j.jacc.2013.11.009', '10.1002/hep.27304', '10.1016/j.jacr.2014.10.002', '10.1148/radiol.13122233', '10.1016/j.jcct.2014.06.002', '10.2214/AJR.12.8808', '10.1056/NEJMoa1415516', '10.1016/j.jacc.2011.03.068', '10.1056/NEJMoa1201163', '10.1056/NEJMoa1201161', '10.1016/j.ijcard.2014.10.090', '10.2214/AJR.12.8808', '10.1016/j.jacc.2013.04.040', '10.1016/j.jacc.2009.02.068', '10.1016/j.jacc.2014.05.039', '10.1016/j.jacc.2012.07.013'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'ACCF/ACR/AHA/NASCI/SAIP/SCAI/SCCT 2010 expert consensus document on coronary computed tomographic angiography: a report of the American College of Cardiology Foundation Task\\xa0…': Paper(DOI='10.1161/cir.0b013e3181d4b618', crossref_json=None, google_schorlar_metadata=None, title='ACCF/ACR/AHA/NASCI/SAIP/SCAI/SCCT 2010 expert consensus document on coronary computed tomographic angiography: a report of the American College of Cardiology Foundation Task\\xa0…', authors=['WRITING COMMITTEE MEMBERS', 'Daniel B Mark', 'Daniel S Berman', 'Matthew J Budoff', 'J Jeffrey Carr', 'Thomas C Gerber', 'Harvey S Hecht', 'Mark A Hlatky', 'John McB Hodgson', 'Michael S Lauer', 'Julie M Miller', 'Richard L Morin', 'Debabrata Mukherjee', 'Michael Poon', 'Geoffrey D Rubin', 'Robert S Schwartz'], abstract='The writing committee consisted of acknowledged experts in the field of CTA, as well as a liaison from the ACCF Task Force on Clinical ECDs, the oversight group for this document. In addition to 2 ACCF members, the writing committee included 2 representatives from the ACR and AHA and 1 representative from ASNC, NASCI, SAIP, SCAI, and SCCT. Representation by an outside organization does not necessarily imply endorsement.', conference=None, journal=None, year=None, reference_list=['10.1016/j.jacc.2006.07.003', '10.1016/j.jacc.2006.10.001', '10.1148/radiol.2451061791', '10.2214/ajr.184.2.01840649', '10.1001/jama.2009.54', '10.1016/j.ejrad.2008.08.013', '10.1007/s00330-005-2919-2', '10.1016/j.rcl.2008.11.001', '10.1016/j.jcmg.2008.05.006', '10.1007/s00330-008-1255-8', '10.1016/j.acra.2004.11.011', '10.1007/s00330-006-0213-6', '10.1148/radiol.2453061953', '10.1148/radiol.2433060080', '10.1097/00004728-199309000-00036', '10.1016/j.jcct.2009.01.001', '10.1016/0735-1097(88)90226-4', '10.1161/circ.92.8.2333', '10.1056/NEJM198403293101304', '10.1161/circ.90.6.7994804', '10.1161/circ.89.5.8181125', '10.1056/NEJMoa0806576', '10.1001/jama.293.20.2471', '10.1016/j.jacc.2005.03.071', '10.1016/j.jacc.2005.05.056', '10.1016/j.amjcard.2004.09.004', '10.1136/hrt.2004.055798', '10.1016/S1071-3581(02)43243-6', '10.1007/s00259-006-0207-2', '10.1093/eurheartj/ehl563', '10.1016/j.jacc.2006.10.069', '10.1016/j.jacc.2006.05.080', '10.1016/j.jcmg.2008.04.015', '10.1001/jama.296.4.403', '10.1148/radiol.2261021292', '10.1016/S0140-6736(05)66422-7', '10.1056/NEJM200106213442501', '10.1016/S0140-6736(02)08020-0', '10.1016/S0140-6736(02)11522-4', '10.1001/jama.286.19.2405', '10.1161/circulationaha.106.672402', '10.1001/archinte.165.21.2454', '10.1001/jama.297.6.611', '10.1148/radiol.2453061899', '10.1016/j.ejrad.2007.05.003', '10.1016/j.amjmed.2005.06.071', '10.1016/j.amjmed.2008.02.039', '10.1016/j.ejrad.2007.07.014', '10.1148/radiol.2442061218', '10.1093/eurheartj/ehm294', '10.1016/j.jacc.2007.07.007', '10.1007/s00330-008-1203-7', '10.1016/j.jacc.2008.07.031', '10.1016/j.jacc.2008.08.058', '10.1007/s00259-006-0307-z', '10.1016/j.atherosclerosis.2007.07.002', '10.2967/jnumed.107.042481', '10.1148/radiol.2482071307', '10.1016/j.annemergmed.2006.06.043', '10.1016/j.jacc.2008.05.024', '10.1016/j.jcin.2009.04.012', '10.1016/j.jacc.2007.03.067', '10.1016/j.jacc.2008.07.027', '10.1016/j.jacc.2008.10.043', '10.1161/CIRCIMAGING.108.792572', '10.1016/j.ahj.2008.03.016', '10.1016/j.jacc.2006.08.064', '10.1161/circulationaha.106.634808', '10.1016/j.jacc.2009.01.052', '10.1016/j.annemergmed.2008.09.025', '10.1016/j.jacc.2005.11.085', '10.1016/j.crad.2006.04.016', '10.1016/j.jacc.2006.06.054', '10.1016/j.amjcard.2005.12.039', '10.1016/j.amjcard.2007.03.087', '10.1093/oxfordjournals.eurheartj.a015317', '10.1016/j.amjcard.2006.04.027', '10.1016/j.healun.2006.06.018', '10.1016/j.jacc.2006.08.012', '10.1161/circulationaha.106.631051', '10.1007/s00330-005-0062-8', '10.1007/s00330-008-1132-5', '10.1080/02841850701678804', '10.1016/j.ejrad.2008.10.025', '10.1002/ccd.21130', '10.1186/1471-2261-8-2', '10.1016/j.amjcard.2007.06.061', '10.1136/hrt.2007.116715', '10.1016/j.jacc.2006.04.103', '10.1093/eurheartj/ehn072', '10.1148/radiol.2453070094', '10.1016/j.jacc.2007.09.062', '10.1080/02841850600977760', '10.1016/j.ijcard.2007.06.017', '10.1016/j.ejrad.2007.07.022', '10.1016/j.amjcard.2006.08.021', '10.1016/j.jacc.2006.08.067', '10.1161/01.cir.0000135215.75876.41', '10.1016/S0735-1097(02)01955-1', '10.1001/jama.291.9.1071', '10.1001/jama.295.13.jpc60002', '10.1161/circ.104.4.387', '10.2174/1568006033481401', '10.1111/j.1742-1241.2007.01597.x', '10.1161/circ.104.3.249', '10.1016/j.ejrad.2008.04.035', '10.1097/00019501-200609000-00009', '10.1161/01.cir.0000111517.69230.0f', '10.1016/j.jacc.2003.09.053', '10.1007/s10554-007-9290-0', '10.1016/j.acra.2007.09.007', '10.1253/circj.71.363', '10.1097/00019501-200309000-00007', '10.1016/S0735-1097(01)01115-9', '10.2214/AJR.05.0189', '10.1016/j.jacc.2006.01.041', '10.1055/s-2007-963113', '10.1016/j.amjcard.2007.10.016', '10.1016/j.ijcard.2007.07.013', '10.2214/AJR.07.2763', '10.1016/j.ahj.2007.07.020', '10.1093/eurheartj/ehn356', '10.1007/s00330-005-2800-3', '10.1148/radiol.2443061397', '10.1007/s00330-006-0570-1', '10.2214/AJR.04.1988', '10.1053/euhj.2000.2586', '10.1016/j.amjmed.2008.10.039', '10.1016/j.jacc.2006.04.071', '10.1097/00005382-200404000-00004', '10.1097/01.rct.0000233125.83184.33', '10.1161/01.cir.0000027136.56615.de', '10.1097/RTI.0b013e31813434a9', '10.1161/circulationaha.106.637512', '10.1016/j.jcmg.2008.07.001', '10.1002/ccd.20924', '10.1097/RTI.0b013e3180317a5b', '10.1007/s10554-008-9417-y', '10.1148/radiol.2372041887', '10.1016/j.jacc.2007.11.021', '10.1001/jama.281.8.727', '10.1161/circ.102.4.374', '10.1056/NEJM199806043382302', '10.1161/circ.97.18.1837', '10.1001/jama.291.2.210', '10.1016/j.jacc.2005.02.088', '10.1016/j.jacc.2008.02.086', '10.1001/jama.293.16.2012', '10.1007/s00330-004-2441-y', '10.1148/radiol.2432060447', '10.1148/radiol.2453061481', '10.1148/radiol.2221010481', '10.1007/s11547-006-0044-1', '10.1148/radiol.2482072169', '10.2214/AJR.07.3387', '10.1001/jama.283.7.897', '10.1007/s10554-007-9226-8', '10.1148/radiol.2481071451', '10.1016/j.jcmg.2009.01.002', '10.1161/circulationaha.108.191650', '10.1136/hrt.2008.149971', '10.1148/radiol.2483072032', '10.1001/jama.2009.814', '10.1148/radiology.185.1.1523331', '10.1148/radiol.2452062111', '10.1161/circulationaha.105.602490', '10.1148/radiol.2433061165', '10.1148/radiol.2463070989', '10.1007/s11892-007-0077-4', '10.1016/S0735-1097(00)00917-7', '10.1001/jama.1996.03530430033035', '10.1016/S0002-9343(97)00150-2', '10.1161/01.cir.0000016043.87291.33', '10.1016/j.amjcard.2006.01.022', '10.1056/NEJMoa021833', '10.1097/01.rli.0000242807.01818.24', '10.1016/j.jacc.2006.06.047', '10.1038/ki.1995.32', '10.1161/circulationaha.106.671644', '10.1148/radiol.2362040468', '10.1097/01.rli.0000178434.43939.a4', '10.1007/s003300050356', '10.5414/CNP61170', '10.1001/jama.291.19.2328', '10.1056/NEJM200007203430304', '10.1016/j.amjcard.2008.04.045', '10.2214/AJR.07.3611', '10.1111/j.1553-2712.2008.00161.x', '10.1161/circulationaha.105.168237', '10.1016/j.jacr.2006.06.006', '10.1016/j.jcct.2009.03.004', '10.1016/j.jacr.2008.04.002', '10.1161/CIRCULATIONAHA.108.191365', '10.1016/j.jacr.2005.03.001'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym='Circulation (New York, N.Y.)', publisher=None, query_handler=None),\n",
       " 'Three-dimensional spiral CT angiography of the abdomen: initial clinical experience.': Paper(DOI='10.1148/radiology.186.1.8416556', crossref_json=None, google_schorlar_metadata=None, title='Three-dimensional spiral CT angiography of the abdomen: initial clinical experience.', authors=['GD Rubin', 'MD Dake', 'SA Napel', 'CH McDonnell', 'RB Jeffrey Jr'], abstract='Spiral computed tomography (CT) is a new technology that couples continuous tube rotation with continuous table feed. This allows compilation of a data set that has continuous anatomic information without the establishment of arbitrary boundaries at section interfaces as in conventional CT. The unique method of data collection of the spiral scanner has been combined with a dynamic intravenous contrast material bolus to image abdominal vasculature, specifically, the aorta, renal arteries, and splanchnic circulation. Through various techniques of image processing, including surface renderings and maximum-intensity projections, it is possible to obtain excellent anatomic detail of the aorta and its major branches. The authors applied this technique in 15 patients and reliably saw third-order aortic branches as well as third-order splenic-portal venous anatomic detail with remarkable clarity. Pathologic conditions\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym='Radiology', publisher=None, query_handler=None),\n",
       " 'CT angiography with spiral CT and maximum intensity projection.': Paper(DOI='10.1148/radiology.185.2.1410382', crossref_json=None, google_schorlar_metadata=None, title='CT angiography with spiral CT and maximum intensity projection.', authors=['Sandy Napel', 'Michael P Marks', 'Geoffrey D Rubin', 'Michael D Dake', 'Charles H McDonnell', 'Samuel M Song', 'Dieter R Enzmann', 'RB Jeffrey Jr'], abstract='The authors describe a technique for obtaining angiographic images by means of spiral computed tomography (CT), preprocessing of reconstructed three-dimensional sections to suppress bone, and maximum intensity projection. The technique has some limitations, but preliminary results in 48 patients have shown excellent anatomic correlation with conventional angiography in studies of the abdomen, the circle of Willis in the brain, and the extracranial carotid arteries. With continued development and evaluation, CT angiography may prove useful as a screening tool or replacement for conventional angiography in some patients.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym='Radiology', publisher=None, query_handler=None),\n",
       " 'Multi–detector row CT angiography of lower extremity arterial inflow and runoff: initial experience': Paper(DOI='10.1148/radiol.2211001325', crossref_json=None, google_schorlar_metadata=None, title='Multi–detector row CT angiography of lower extremity arterial inflow and runoff: initial experience', authors=['Geoffrey D Rubin', 'Andrew J Schmidt', 'Laura J Logan', 'Mark C Sofilos'], abstract='PURPOSE: To assess the patterns of lower extremity arterial inflow and runoff opacification with four-channel multi–detector row computed tomographic (CT) angiography in a cohort of patients with disease warranting imaging of the lower extremity arterial system. MATERIALS AND METHODS: Twenty-four patients with symptomatic lower extremity arterial occlusive or aneurysmal disease underwent imaging with four-channel multi–detector row CT from the supraceliac abdominal aorta through the feet. Transverse sections were acquired with a 2.5-mm nominal detector width and pitch of 6.0 (3.2-mm effective section thickness) following intravenous injection of 174–185 mL of iodinated contrast medium (300 mg iodine per milliliter). In each patient, attenuation measurements were recorded in 16 arterial and 16 venous locations. In 18 patients, two radiologists assessed the detectability and stenosis degree of 21\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1148/radiology.186.1.8416556', '10.1148/radiology.198.2.8596847', '10.1148/radiology.205.1.9314973', '10.1148/radiology.206.3.9494508', '10.1148/radiology.215.1.r00ap28138', '10.1148/radiology.215.1.r00ap4863', '10.1148/radiology.215.3.r00jn18670', '10.1118/1.598470', '10.1016/S0720-048X(00)00270-9', '10.1067/mva.1986.avs0030074', '10.1161/01.CIR.89.1.511', '10.1148/radiology.214.2.r00fe42325', '10.1148/radiology.194.3.7862999', '10.2214/ajr.166.2.8553929', '10.1148/radiology.203.2.9114108', '10.1148/radiology.197.1.316-c', '10.1016/S0887-2171(96)90018-9', '10.1016/S0033-8389(22)00562-0', '10.1148/radiology.214.2.r00fe18363', '10.1148/radiology.207.3.9609886', '10.1148/radiology.207.3.9609887', '10.1148/radiology.185.2.1410365', '10.1148/radiology.189.1.8372191', '10.2214/ajr.174.1.1740061', '10.1093/ije/25.6.1172', '10.1016/0002-9610(86)90246-1', '10.1016/S0741-5214(96)70139-8', '10.1148/radiology.206.3.9494486', '10.2214/ajr.174.4.1741127'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym='Radiology', publisher=None, query_handler=None),\n",
       " 'Spiral CT of renal artery stenosis: comparison of three-dimensional rendering techniques.': Paper(DOI='10.1016/0741-5214(94)90256-9', crossref_json=None, google_schorlar_metadata=None, title='Spiral CT of renal artery stenosis: comparison of three-dimensional rendering techniques.', authors=['Geoffrey D Rubin', 'Michael D Dake', 'Sandy Napel', 'R Brooke Jeffrey Jr', 'Charles H McDonnell', 'F Graham Sommer', 'Lewis Wexler', 'David M Williams'], abstract='PURPOSE To evaluate the accuracy of computed tomographic (CT) angiography in the detection of renal artery stenosis (RAS). MATERIALS AND METHODS CT angiography was performed in 31 patients undergoing conventional renal arteriography. CT angiographic data were reconstructed with shaded surface display (SSD) and maximum-intensity projection (MIP). Stenosis was graded with a four-point scale (grades 0-3). The presence of mural calcification, poststenotic dilatation, and nephrographic abnormalities was also noted. RESULTS CT angiography depicted all main (n = 62) and accessory (n = 11) renal arteries that were seen at conventional arteriography. MIP CT angiography was 92% sensitive and 83% specific for the detection of grade 2-3 stenoses (> or = 70% stenosis). SSD CT angiography was 59% sensitive and 82% specific for the detection of grade 2-3 stenoses. The accuracy of stenosis\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym='Journal of vascular surgery (Print)', publisher=None, query_handler=None),\n",
       " 'Aorta and iliac arteries: single versus multiple detector-row helical CT angiography': Paper(DOI='10.1148/radiology.215.3.r00jn18670', crossref_json=None, google_schorlar_metadata=None, title='Aorta and iliac arteries: single versus multiple detector-row helical CT angiography', authors=['Geoffrey D Rubin', 'Maria C Shiau', 'Ann N Leung', 'Stephen T Kee', 'Laura J Logan', 'Mark C Sofilos'], abstract='PURPOSE: To compare single- versus four-channel helical computed tomographic (CT) aortography. MATERIALS AND METHODS: Forty-eight patients with aortic aneurysm or dissection underwent four- and one-channel CT angiography. Scan pairs covered the thoracic inlet to the diaphragm (n = 10) and supraceliac abdominal aorta (n = 19) or thoracic inlet (n = 19) to the femoral arterial bifurcations. For four-channel CT, nominal section thickness and pitch were 2.5 mm and 6.0, respectively, and for one-channel CT, 3.0 mm and 2.0 to the infrarenal aorta and 5.0 mm and 2.0 to the femoral arteries. Effective section thickness, scanning duration, scanning coverage, dose of iodinated contrast material, and mean aortoiliac attenuation were compared. Data were summarized as speed (coverage/duration), scanning efficiency (speed/section thickness), and contrast efficiency (mean aortic attenuation/dose of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1148/radiology.186.1.8416556', '10.1148/radiology.206.3.9494508', '10.1148/radiology.198.2.8596847', '10.1148/radiology.205.1.9314973', '10.1148/radiology.215.1.r00ap28138', '10.1148/radiology.215.1.r00ap4863', '10.1016/S0009-9260(99)90557-3', '10.1118/1.598470', '10.1016/S0033-8389(22)00630-3', '10.1016/0741-5214(93)90075-W', '10.1148/radiology.185.1.1523331', '10.1148/radiology.197.1.316-c', '10.2214/ajr.169.2.9242750', '10.1148/radiology.208.3.9722858', '10.2214/ajr.160.6.8498233', '10.2214/ajr.166.6.8633446', '10.1148/radiology.205.1.9314997', '10.1148/radiology.201.3.8939232'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym='Radiology', publisher=None, query_handler=None),\n",
       " 'Surface normal overlap: a computer-aided detection algorithm with application to colonic polyps and lung nodules in helical CT': Paper(DOI='10.1109/tmi.2004.826362', crossref_json=None, google_schorlar_metadata=None, title='Surface normal overlap: a computer-aided detection algorithm with application to colonic polyps and lung nodules in helical CT', authors=['David S Paik', 'Christopher F Beaulieu', 'Geoffrey D Rubin', 'Burak Acar', 'R Brooke Jeffrey', 'Judy Yee', 'Joyoni Dey', 'Sandy Napel'], abstract='We developed a novel computer-aided detection (CAD) algorithm called the surface normal overlap method that we applied to colonic polyp detection and lung nodule detection in helical computed tomography (CT) images. We demonstrate some of the theoretical aspects of this algorithm using a statistical shape model. The algorithm was then optimized on simulated CT data and evaluated using a per-lesion cross-validation on 8 CT colonography datasets and on 8 chest CT datasets. It is able to achieve 100% sensitivity for colonic polyps 10 mm and larger at 7.0 false positives (FPs)/dataset and 90% sensitivity for solid lung nodules 6 mm and larger at 5.6 FP/dataset.', conference=None, journal=None, year=None, reference_list=['10.1118/1.598603', '10.1148/radiology.216.1.r00jl43284', '10.1148/radiology.219.1.r01ap0751', '10.1109/42.974921', '10.1148/radiol.2222010506', '10.1016/S1076-6332(03)80184-8', '10.1007/s003300101040', '10.1109/42.974920', '10.1109/TMI.2002.806405', '10.1118/1.596358', '10.1001/jama.284.15.1977', '10.1378/chest.107.6_Supplement.322S', '10.1002/1097-0142(20001201)89:11+<2422::AID-CNCR16>3.0.CO;2-E', '10.1109/TMI.2005.844167', '10.1097/00004424-199404000-00013', '10.1148/radiology.201.3.8939225', '10.1109/42.974919', '10.1118/1.1387272', '10.1056/NEJM199312303292701', '10.1109/42.932744', '10.1093/oxfordjournals.epirev.a036132', '10.1109/TPAMI.1986.4767851', '10.1016/S0022-5223(97)70397-0'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym='IEEE transactions on medical imaging (Print)', publisher=None, query_handler=None),\n",
       " 'Improved uniformity of aortic enhancement with customized contrast medium injection protocols at CT angiography': Paper(DOI='10.1148/radiology.214.2.r00fe18363', crossref_json=None, google_schorlar_metadata=None, title='Improved uniformity of aortic enhancement with customized contrast medium injection protocols at CT angiography', authors=['Dominik Fleischmann', 'Geoffrey D Rubin', 'Alexander A Bankier', 'Karl Hittmair'], abstract=\"PURPOSE: To compare the uniformity of aortoiliac opacification obtained from uniphasic contrast medium injections versus individualized biphasic injections at computed tomographic (CT) angiography. MATERIALS AND METHODS: Thirty-two patients with an abdominal aortic aneurysm underwent CT angiography. In 16 patients (group 1), 120 mL of contrast material was administered at a flow rate of 4 mL/sec. In the other 16 patients (group 2), biphasic injection protocols were computed by using mathematic deconvolution of each patient's time-attenuation response to a standardized test injection. Attenuation uniformity was quantified as the “plateau deviation” of enhancement values, which were calculated as the SD of the time-contiguous attenuation values observed during the 30-second scanning period. RESULTS: Group 2 patients received between 77 and 165 mL (mean, 115 mL) of contrast medium\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1148/radiology.207.3.9609886', '10.1148/radiology.206.2.9457200', '10.1097/00004424-199602000-00006', '10.1259/bjr.70.832.9166070', '10.1016/S0735-1097(84)80219-3', '10.1148/radiology.186.1.8416556', '10.1148/radiology.189.1.8372191', '10.2214/ajr.167.3.8751690', '10.1148/radiology.153.2.6484168', '10.1097/00004728-199503000-00009', '10.1016/S1076-6332(96)80553-8', '10.1016/S1076-6332(96)80552-6', '10.2214/ajr.172.1.9888738', '10.1016/S0033-8389(22)00562-0', '10.2214/ajr.165.1.7785637', '10.1148/radiology.206.3.9494508', '10.1097/00004728-199905000-00026', '10.1148/radiology.207.3.9609887', '10.1118/1.598470', '10.1118/1.598230', '10.1148/radiology.205.1.9314988', '10.1148/radiology.209.1.9769823', '10.1097/00004728-199505000-00016'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym='Radiology', publisher=None, query_handler=None),\n",
       " 'Pulmonary nodules on multi–detector row CT scans: performance comparison of radiologists and computer-aided detection': Paper(DOI='10.1148/radiol.2341040589', crossref_json=None, google_schorlar_metadata=None, title='Pulmonary nodules on multi–detector row CT scans: performance comparison of radiologists and computer-aided detection', authors=['Geoffrey D Rubin', 'John K Lyo', 'David S Paik', 'Anthony J Sherbondy', 'Lawrence C Chow', 'Ann N Leung', 'Robert Mindelzun', 'Pamela K Schraedley-Desmond', 'Steven E Zinck', 'David P Naidich', 'Sandy Napel'], abstract='PURPOSE: To compare the performance of radiologists and of a computer-aided detection (CAD) algorithm for pulmonary nodule detection on thin-section thoracic computed tomographic (CT) scans. MATERIALS AND METHODS: The study was approved by the institutional review board. The requirement of informed consent was waived. Twenty outpatients (age range, 15–91 years; mean, 64 years) were examined with chest CT (multi–detector row scanner, four detector rows, 1.25-mm section thickness, and 0.6-mm interval) for pulmonary nodules. Three radiologists independently analyzed CT scans, recorded the locus of each nodule candidate, and assigned each a confidence score. A CAD algorithm with parameters chosen by using cross validation was applied to the 20 scans. The reference standard was established by two experienced thoracic radiologists in consensus, with blind review of all nodule\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1148/radiology.191.1.8134580', '10.1080/02841859409173302', '10.1016/S1076-6332(96)80296-0', '10.1016/S0895-4356(01)00382-1', '10.1016/S0009-9260(05)81850-1', '10.1259/0007-1285-68-813-958', '10.1148/radiol.2213010308', '10.1148/radiographics.20.4.g00jl211169', '10.1118/1.596358', '10.1016/S1076-6332(03)80164-2', '10.1148/radiology.174.3.2305073', '10.1118/1.1524631', '10.1016/S0720-048X(99)00016-9', '10.1097/00004424-198903000-00012', '10.1097/00004728-199803000-00014', '10.2214/ajr.168.5.9129439', '10.1148/radiology.201.3.8939234', '10.1016/S0140-6736(99)06093-6', '10.1148/radiology.212.1.r99jn1461', '10.1016/S0003-4975(97)00568-7', '10.1002/1097-0142(197903)43:3<913::AID-CNCR2820430319>3.0.CO;2-Q', '10.1055/s-2007-1022188', '10.1016/0003-4975(93)90344-H', '10.1148/radiology.164.3.3615867', '10.1016/S0003-4975(00)01806-3', '10.2214/ajr.172.2.9930781', '10.1007/s00330-003-1915-7', '10.2214/ajr.164.3.7863879', '10.1097/00004424-199404000-00013', '10.1016/S0895-6111(98)00017-2', '10.1148/radiographics.19.5.g99se181303', '10.1109/42.974919', '10.1148/radiology.218.1.r01ja39267', '10.1118/1.1387272', '10.1109/42.932744', '10.1118/1.1515762', '10.1007/s003300101126', '10.1148/radiol.2253011376', '10.1148/radiol.2261011708', '10.1148/radiol.2253011375', '10.1148/radiology.199.1.8633131', '10.1148/radiology.199.1.8633132'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym='Radiology', publisher=None, query_handler=None),\n",
       " 'Data explosion: the challenge of multidetector-row CT': Paper(DOI='10.1016/s0720-048x(00)00270-9', crossref_json=None, google_schorlar_metadata=None, title='Data explosion: the challenge of multidetector-row CT', authors=['Geoffrey D Rubin'], abstract='The development of multi detector-row CT has brought many exciting advancements to clinical CT scanning. While multi detector-row CT offers unparalleled speed of acquisition, spatial resolution, and anatomic coverage, a challenge presented by these advantages is the substantial increase on the number of reconstructed cross-sections that are rapidly created and in need of analysis. This manuscript discusses currently available alternative visualization tecvhniques for the assessment of volumetric data acquired with multi detector-row CT. Although the current capabilities of 3-D workstations offer many possibilities for alternative analysis of MCDT data, substantial improvements both in automated processing, processing speed and user interface will be necessary to realize the vision of replacing the primary analysis of transverse reconstruction′s with alternative analyses. The direction that some of these future\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1148/radiology.196.2.7617851', '10.1148/radiology.200.2.8685316', '10.1148/radiology.173.2.2798885', '10.2214/ajr.164.6.7754876', '10.1097/00004728-199309000-00036', '10.1097/00004728-199103000-00035', '10.1016/0895-6111(91)90083-8', '10.1148/radiology.190.1.8259402', '10.1145/378456.378484', '10.1148/radiology.171.1.2928536', '10.1148/radiology.163.3.3575725', '10.1148/radiology.199.2.8668772', '10.2214/ajr.167.3.8751655', '10.1007/s002560050066', '10.1148/radiology.200.1.8657944', '10.1053/gast.1996.v110.pm8536869', '10.1097/00004728-199609000-00018', '10.1378/chest.109.2.549', '10.1097/00005382-199701000-00003', '10.1148/radiology.198.2.8596868', '10.2214/ajr.166.2.8553956', '10.2214/ajr.168.5.9129415', '10.1118/1.598244', '10.1148/radiology.206.3.9494508'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym='European journal of radiology', publisher=None, query_handler=None),\n",
       " 'Current status of three-dimensional spiral CT scanning for imaging the vasculature.': Paper(DOI='10.1016/s0033-8389(22)00562-0', crossref_json=None, google_schorlar_metadata=None, title='Current status of three-dimensional spiral CT scanning for imaging the vasculature.', authors=['Geoffrey D Rubin', 'Michael D Dake', 'Charles P Semba'], abstract='Three-dimensional CT angiography is a new modality for minimally invasive vascular imaging. Meticulous attention to technique is critical to optimizing image quality and achieving diagnostic images. In this article, the properties of spiral CT acquisitions, intravenous contrast delivery, and three-dimensional rendering techniques, as they pertain to the optimization of CT angiograms, are discussed. Next, a review of initial investigations into the clinical applicability of CT angiography in the cranium, neck, chest, abdomen, and pelvis is provided. Finally, CT angiography is compared with MR angiography and conventional angiography.', conference=None, journal=None, year=None, reference_list=['10.1007/BF02733898', '10.1146/annurev.me.39.020188.001421', '10.2214/ajr.161.2.8333383', '10.1016/0730-725X(87)90124-X', '10.1097/00004728-199103000-00035', '10.1148/radiology.183.3.1584915', '10.2214/ajr.158.5.1566679', '10.2214/ajr.157.5.1927823', '10.1148/radiology.189.1.8372196', '10.2214/ajr.160.6.8498233', '10.1148/radiology.183.3.1584940', '10.1148/radiology.189.1.8372191', '10.2214/ajr.158.5.1566658', '10.2214/ajr.153.1.5', '10.1016/S0039-6109(16)44790-0', '10.1148/radiology.176.1.2353088', '10.1016/S1051-0443(92)72194-0', '10.1148/radiology.173.2.2798885', '10.1016/0741-5214(91)90224-I', '10.1148/radiology.174.3.2305057', '10.2214/ajr.152.4.785', '10.1097/00004728-198805010-00002', '10.1016/0895-6111(91)90083-8', '10.2214/ajr.160.6.8498231', '10.1016/0899-7071(91)90154-N', '10.1148/radiology.185.2.1410382', '10.1097/00004728-199309000-00036', '10.1007/BF03167805', '10.1148/radiology.167.1.2964677', '10.2214/ajr.146.4.711', '10.1148/radiology.185.1.1523331', '10.1016/S1051-0443(91)72222-7', '10.1148/radiology.182.3.1535899', '10.1148/radiology.185.2.1410342', '10.1097/00004728-199007000-00035', '10.1148/radiology.190.1.8259402', '10.1148/radiology.186.1.8416556', '10.1016/0741-5214(93)90075-W', '10.1148/radiology.185.2.1410365', '10.3109/02841859109177600', '10.1148/radiology.176.3.2389050'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym='The Radiologic clinics of North America', publisher=None, query_handler=None),\n",
       " 'Recommendations for measuring pulmonary nodules at CT: a statement from the Fleischner Society': Paper(DOI='10.1148/radiol.2017162894', crossref_json=None, google_schorlar_metadata=None, title='Recommendations for measuring pulmonary nodules at CT: a statement from the Fleischner Society', authors=['Alexander A Bankier', 'Heber MacMahon', 'Jin Mo Goo', 'Geoffrey D Rubin', 'Cornelia M Schaefer-Prokop', 'David P Naidich'], abstract='These recommendations for measuring pulmonary nodules at computed tomography (CT)                    are a statement from the Fleischner Society and, as such, incorporate the                    opinions of a multidisciplinary international group of thoracic radiologists,                    pulmonologists, surgeons, pathologists, and other specialists. The                    recommendations address nodule size measurements at CT, which is a topic of                    importance, given that all available guidelines for nodule management are                    essentially based on nodule size or changes thereof. The recommendations are                    organized according to practical questions that commonly arise when nodules are                    measured in routine clinical practice and are, together with their answers,                    summarized in a table. The recommendations include technical requirements for                    accurate nodule measurement\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1378/chest.07-1353', '10.1148/radiol.2372041887', '10.1148/radiol.12120628', '10.1136/thoraxjnl-2015-207168', '10.1097/RTI.0b013e31824f83e1', '10.1148/radiol.09090571', '10.1148/radiol.2015150919', '10.1148/radiol.2015150892', '10.1148/radiol.13121849', '10.1378/chest.129.1.174', '10.1097/JTO.0b013e318206a221', '10.7326/0003-4819-158-4-201302190-00004', '10.1148/radiol.12120240', '10.1038/modpathol.2011.151', '10.1016/S0140-6736(99)06093-6', '10.1016/j.jacr.2014.10.002', '10.1056/NEJMoa1102873', '10.1056/NEJMoa1208962', '10.1148/radiol.14132950', '10.1016/j.jacr.2014.08.004', '10.7326/M14-2086', '10.1093/jnci/dju284', '10.1148/radiol.2017161659', '10.1016/j.acra.2005.04.009', '10.1016/j.acra.2007.09.005', '10.1148/radiol.2312030167', '10.1016/S0272-5231(21)01152-7', '10.1148/radiol.2411050860', '10.1148/radiol.2513081313', '10.3348/kjr.2013.14.4.683', '10.2214/AJR.05.1063', '10.1148/radiol.2462070712', '10.1016/j.lungcan.2013.10.017', '10.1016/j.rmed.2013.02.014', '10.1378/chest.11-3306', '10.1148/radiol.11101372', '10.1016/j.lungcan.2014.03.009', '10.1148/radiol.14132187', '10.1007/s11604-013-0264-y', '10.1177/0284185113502336', '10.1371/journal.pone.0104066', '10.1097/JTO.0000000000000117', '10.1007/s11748-012-0066-7', '10.1378/chest.12-2987', '10.1148/radiol.2353040737', '10.1148/radiol.2472070868', '10.1007/s00330-009-1634-9', '10.1148/radiol.2283020059', '10.1016/j.jtcvs.2003.08.048', '10.1259/bjr/13711326', '10.1007/s00330-015-3616-4', '10.1097/JTO.0000000000000019', '10.1007/s00330-016-4495-z', '10.1016/j.jtho.2016.03.025', '10.1016/j.acra.2007.01.008', '10.1148/radiol.2452061054', '10.3348/kjr.2006.7.4.243', '10.1148/rg.284085035', '10.1097/PAS.0000000000000134', '10.1016/j.athoracsur.2007.07.016', '10.1016/j.lungcan.2011.08.001', '10.1093/icvts/ivs055', '10.1148/radiol.2016152771', '10.1148/radiol.2015150714', '10.1148/radiol.2015142700'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computed Tomography', 'Image Processing', 'Cardiovascular Imaging', 'Computer Vision'], conference_acronym='Radiology', publisher=None, query_handler=None),\n",
       " 'An FPGA-based people detection system': Paper(DOI='10.1155/asp.2005.1047', crossref_json=None, google_schorlar_metadata=None, title='An FPGA-based people detection system', authors=['Vinod Nair', 'Pierre-Olivier Laprise', 'James J Clark'], abstract='This paper presents an FPGA-based system for detecting people from video. The system is designed to use JPEG-compressed frames from a network camera. Unlike previous approaches that use techniques such as background subtraction and motion detection, we use a machine-learning-based approach to train an accurate detector. We address the hardware design challenges involved in implementing such a detector, along with JPEG decompression, on an FPGA. We also present an algorithm that efficiently combines JPEG decompression with the detection process. This algorithm carries out the inverse DCT step of JPEG decompression only partially. Therefore, it is computationally more efficient and simpler to implement, and it takes up less space on the chip than the full inverse DCT algorithm. The system is demonstrated on an automated video surveillance application and the performance of both\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Mimicking go experts with convolutional neural networks': Paper(DOI='10.1007/978-3-540-87559-8_11', crossref_json=None, google_schorlar_metadata=None, title='Mimicking go experts with convolutional neural networks', authors=['Ilya Sutskever', 'Vinod Nair'], abstract=' Building a strong computer Go player is a longstanding open problem. In this paper we consider the related problem of predicting the moves made by Go experts in professional games. The ability to predict experts’ moves is useful, because it can, in principle, be used to narrow the search done by a computer Go player. We applied an ensemble of convolutional neural networks to this problem. Our main result is that the ensemble learns to predict 36.9% of the moves made in test expert Go games, improving upon the state of the art, and that the best single convolutional neural network of the ensemble achieves 34% accuracy. This network has less than 104 parameters.', conference=None, journal=None, year=None, reference_list=['10.1007/3-540-45579-5_27', '10.1016/S0004-3702(01)00127-8', '10.1126/science.1144079', '10.1145/1143844.1143954', '10.1109/ICDAR.2007.4377108', '10.1109/5.726791', '10.1007/978-3-540-40031-8_26', '10.1007/978-0-387-35706-5_7', '10.1145/203330.203343', '10.1109/CIG.2007.368097'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Photobook: Content-based manipulation of image databases': Paper(DOI='10.1007/bf00123143', crossref_json=None, google_schorlar_metadata=None, title='Photobook: Content-based manipulation of image databases', authors=['A Pentland', 'RW Picard', 'S Sclaroff'], abstract=' We describe the Photobook system, which is a set of interactive tools for browsing and searching images and image sequences. These query tools differ from those used in standard image databases in that they make direct use of the image content rather than relying on text annotations. Direct search on image content is made possible by use of semantics-preserving image compression, which reduces images to a small set of perceptually-significant coefficients. We discuss three types of Photobook descriptions in detail: one that allows search based on appearance, one that uses 2-D shape, and a third that allows search based on textural properties. These image content descriptions can be combined with each other and with text-based descriptions to provide a sophisticated browsing and search capability. In this paper we demonstrate Photobook on databases containing images of people, video\\xa0…', conference=None, journal=None, year=None, reference_list=['10.21236/ADA229320', '10.1016/0031-3203(91)90034-3', '10.1016/0306-4573(92)90028-X', '10.1016/0031-3203(91)90051-6', '10.1109/VL.1994.365599', '10.1016/0169-023X(92)90044-C', '10.1145/115790.115821', '10.1109/CCV.1988.589995', '10.1016/0031-3203(90)90004-5', '10.1016/0031-3203(92)90112-V', '10.1145/65445.65452', '10.1016/0031-3203(92)90099-5', '10.1109/70.88019', '10.1117/12.191877', '10.1117/12.143648', '10.1109/CVPR.1994.323892', '10.1109/34.85660', '10.1109/CVPR.1994.323814', '10.1109/ICASSP.1993.319772', '10.1163/156856894X00341', '10.1109/ICASSP.1994.389431', '10.1007/BF01236575', '10.1109/VISUAL.1993.398872', '10.1109/34.387502', '10.1364/JOSAA.4.000519', '10.1109/93.311653', '10.1109/ICPR.1994.577117', '10.1007/BF00130487', '10.1162/jocn.1991.3.1.71', '10.1109/ICPR.1990.119415'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'machine learning', 'multimedia'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Photobook: tools for content-based manipulation of image databases': Paper(DOI='10.1007/978-1-4613-1387-8_2', crossref_json=None, google_schorlar_metadata=None, title='Photobook: tools for content-based manipulation of image databases', authors=['Alexander P. Pentland', 'W Picard', 'Rosalind', 'S Sclaroff'], abstract='We describe the Photobook system, which is a set of interactive tools for browsing and searching images and image sequences. These tools differ from those used in standard image databases in that they make direct use of the image content rather than relying on annotations. Direct search on image content is made possible by use of semantics-preserving image compression, which reduces images to a small set of perceptually-significant coefficients. We describe three Photobook tools in particular: one that allows search based on grey-level appearance, one that uses 2D shape, and a third that allows search based on textural properties.', conference=None, journal=None, year=None, reference_list=['10.21236/ADA229320', '10.1016/0031-3203(91)90034-3', '10.1016/0306-4573(92)90028-X', '10.1016/0031-3203(91)90051-6', '10.1109/WVM.1991.212810', '10.1109/93.311653', '10.1016/0169-023X(92)90044-C', '10.1145/115790.115821', '10.1016/0031-3203(90)90004-5', '10.1016/0031-3203(92)90112-V', '10.1016/0031-3203(92)90099-5', '10.1109/70.88019', '10.1109/34.85660', '10.1163/156856894X00341', '10.1109/ICASSP.1993.319772', '10.1109/VISUAL.1993.398872', '10.1364/JOSAA.4.000519', '10.1007/BF00130487', '10.1162/jocn.1991.3.1.71', '10.1109/ICPR.1990.119415'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'machine learning', 'multimedia'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'MEEM: robust tracking via multiple experts using entropy minimization': Paper(DOI='10.1007/978-3-319-10599-4_13', crossref_json=None, google_schorlar_metadata=None, title='MEEM: robust tracking via multiple experts using entropy minimization', authors=['Jianming Zhang', 'Shugao Ma', 'Stan Sclaroff'], abstract=' We propose a multi-expert restoration scheme to address the model drift problem in online tracking. In the proposed scheme, a tracker and its historical snapshots constitute an expert ensemble, where the best expert is selected to restore the current tracker when needed based on a minimum entropy criterion, so as to correct undesirable model updates. The base tracker in our formulation exploits an online SVM on a budget algorithm and an explicit feature mapping method for efficient model update and inference. In experiments, our tracking method achieves substantially better overall performance than 32 trackers on a benchmark dataset of 50 video sequences under various evaluation settings. In addition, in experiments with a newly collected dataset of challenging sequences, we show that the proposed multi-expert restoration scheme significantly improves the robustness of our base tracker, especially\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2004.53', '10.1109/TPAMI.2007.35', '10.1109/TPAMI.2010.226', '10.1109/ICCV.2013.255', '10.1109/AVSS.2010.85', '10.1109/CVPR.2011.5995733', '10.5244/C.20.6', '10.1007/978-3-540-88682-2_19', '10.1109/CVPR.2007.382995', '10.1109/ICCV.2011.6126251', '10.1109/CVPR.2013.314', '10.1109/CVPR.2010.5540231', '10.1109/CVPR.2010.5539821', '10.1145/2508037.2508039', '10.1109/ICCV.2009.5459203', '10.1109/TPAMI.2004.16', '10.1109/TPAMI.2011.66', '10.1109/CVPR.2011.5995421', '10.1109/CVPR.2013.308', '10.1109/ICCV.2007.4408954', '10.1109/CVPR.2013.307', '10.1002/sam.10075', '10.1109/CVPR.2013.312', '10.1145/1177352.1177355', '10.1007/978-3-540-88688-4_50', '10.1007/978-3-642-33783-3_34'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'machine learning', 'multimedia'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Fast, reliable head tracking under varying illumination: an approach based on registration of texture-mapped 3D models': Paper(DOI='10.1109/34.845375', crossref_json=None, google_schorlar_metadata=None, title='Fast, reliable head tracking under varying illumination: an approach based on registration of texture-mapped 3D models', authors=['Marco La Cascia', 'Stan Sclaroff'], abstract=\"A technique for 3D head tracking under varying illumination is proposed. The head is modeled as a texture mapped cylinder. Tracking is formulated as an image registration problem in the cylinder's texture map image. The resulting dynamic texture map provides a stabilized view of the face that can be used as input to many existing 2D techniques for face recognition, facial expressions analysis, lip reading, and eye tracking. To solve the registration problem with lighting variation and head motion, the residual registration error is modeled as a linear combination of texture warping templates and orthogonal illumination templates. Fast stable online tracking is achieved via regularized weighted least-squares error minimization. The regularization tends to limit potential ambiguities that arise in the warping and illumination templates. It enables stable tracking over extended sequences. Tracking does not require a\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1109/34.655647', '10.1109/CVPR.1997.609312', '10.1023/A:1008159011682', '10.1109/CVPR.1999.787001', '10.1109/34.216724', '10.1109/34.598227', '10.1109/CVPR.1997.609309', '10.1109/ICCV.1998.710707', '10.1364/JOSAA.4.000629', '10.1109/AFGR.1996.557271', '10.1109/CVPR.1997.609393', '10.1023/A:1007977618277', '10.1109/34.598232', '10.1109/CVPR.1996.517079', '10.1023/A:1007939232436', '10.1109/IROS.1998.724619', '10.1109/ICCV.1995.466915', '10.1109/34.216726', '10.1109/34.722606', '10.1109/ICPR.1996.547019', '10.1109/ACSSC.1997.679194', '10.1109/CVPR.1994.323941', '10.1109/34.506414', '10.1109/CVPR.1997.609292', '10.21236/ADA259443', '10.1109/CVPR.1997.609345', '10.1109/34.216730', '10.1109/TPAMI.1986.4767767', '10.1109/ICCV.1998.710860'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'machine learning', 'multimedia'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Top-down neural attention by excitation backprop': Paper(DOI='10.1007/s11263-017-1059-x', crossref_json=None, google_schorlar_metadata=None, title='Top-down neural attention by excitation backprop', authors=['Jianming Zhang', 'Sarah Adel Bargal', 'Zhe Lin', 'Jonathan Brandt', 'Xiaohui Shen', 'Stan Sclaroff'], abstract=' We aim to model the top-down attention of a convolutional neural network (CNN) classifier for generating task-specific attention maps. Inspired by a top-down human visual attention model, we propose a new backpropagation scheme, called Excitation Backprop, to pass along top-down signals downwards in the network hierarchy via a probabilistic Winner-Take-All process. Furthermore, we introduce the concept of contrastive attention to make the top-down attention maps more discriminative. We show a theoretic connection between the proposed contrastive attention formulation and the Class Activation Map computation. Efficient implementation of Excitation Backprop for common neural network layers is also presented. In experiments, we visualize the evidence of a model’s classification decision by computing the proposed top-down attention maps. For quantitative evaluation, we report the accuracy of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1073/pnas.84.17.6297', '10.1109/CVPR.2014.49', '10.1371/journal.pone.0130140', '10.1016/j.tins.2011.02.003', '10.1109/WACV.2016.7477688', '10.1016/j.visres.2008.07.012', '10.1109/ICCV.2015.338', '10.5244/C.28.6', '10.1098/rstb.1998.0280', '10.1146/annurev.ne.18.030195.001205', '10.1109/MMUL.2012.26', '10.1007/s11263-009-0275-4', '10.1109/CVPR.2015.7298754', '10.1007/s11263-014-0713-9', '10.1109/CVPR.2016.90', '10.1016/j.media.2017.07.002', '10.1145/2647868.2654889', '10.1145/2818346.2830587', '10.1007/978-3-319-10602-1_48', '10.1109/CVPR.2015.7298965', '10.1109/CVPR.2015.7298668', '10.1109/ICCV.2015.209', '10.1109/CVPR.2015.7298780', '10.1109/ICCV.2015.303', '10.1016/j.neuron.2009.01.002', '10.1007/s11263-015-0816-y', '10.1109/CVPR.2015.7298594', '10.1016/0010-0285(80)90005-5', '10.1016/0004-3702(95)00025-9', '10.1162/jocn.1996.8.4.311', '10.3758/BF03200774', '10.1007/978-3-319-10590-1_53', '10.1109/CVPR.2016.319', '10.1007/978-3-319-10602-1_26'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'machine learning', 'multimedia'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Modal matching for correspondence and recognition': Paper(DOI='10.1016/j.patcog.2016.10.006', crossref_json=None, google_schorlar_metadata=None, title='Modal matching for correspondence and recognition', authors=['S Sclaroff', 'A Pentland'], abstract=\"Modal matching is a new method for establishing correspondences and computing canonical descriptions. The method is based on the idea of describing objects in terms of generalized symmetries, as defined by each object's eigenmodes. The resulting modal description is used for object recognition and categorization, where shape similarities are expressed as the amounts of modal deformation energy needed to align the two objects. In general, modes provide a global-to-local ordering of shape deformation and thus allow for selecting which types of deformations are used in object alignment and comparison. In contrast to previous techniques, which required correspondence to be computed with an initial or prototype shape, modal matching utilizes a new type of finite element formulation that allows for an object's eigenmodes to be computed directly from available image information. This improved formulation\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2010.226', '10.1023/A:1014573219977', '10.1109/ICCV.2009.5459437', '10.1109/CVPR.2011.5995637', '10.1109/ICCV.2005.166', '10.1109/TPAMI.2011.46', '10.1109/TPAMI.2010.93', '10.1109/TPAMI.2002.1017623', '10.1007/978-3-642-15561-1_56', '10.1007/BFb0028345', '10.1109/CVPR.2007.383248', '10.1023/B:VISI.0000029664.99615.94', '10.1109/TPAMI.2010.136', '10.1109/TPAMI.2013.138', '10.1023/A:1007958904918', '10.1109/34.969113', '10.1109/CVPR.2005.188', '10.1109/CVPR.2007.383198', '10.1109/ICCV.2011.6126542', '10.1109/TIP.2008.925372', '10.1109/TPAMI.2006.70', '10.1109/TPAMI.2010.147', '10.1007/978-3-319-10593-2_21', '10.1109/TPAMI.2009.77', '10.1023/A:1020830525823'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'machine learning', 'multimedia'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'A unified framework for gesture recognition and spatiotemporal gesture segmentation': Paper(DOI='10.1109/tpami.2008.203', crossref_json=None, google_schorlar_metadata=None, title='A unified framework for gesture recognition and spatiotemporal gesture segmentation', authors=['Jonathan Alon', 'Vassilis Athitsos', 'Quan Yuan', 'Stan Sclaroff'], abstract='Within the context of hand gesture recognition, spatiotemporal gesture segmentation is the task of determining, in a video sequence, where the gesturing hand is located and when the gesture starts and ends. Existing gesture recognition methods typically assume either known spatial segmentation or known temporal segmentation, or both. This paper introduces a unified framework for simultaneously performing spatial segmentation, temporal segmentation, and recognition. In the proposed framework, information flows both bottom-up and top-down. A gesture can be recognized even when the hand location is highly ambiguous and when information about when the gesture begins and ends is unavailable. Thus, the method can be applied to continuous image streams where gestures are performed in front of moving, cluttered backgrounds. The proposed method consists of three novel contributions: a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2007.70711', '10.1109/34.910878', '10.1109/CVPR.2001.990517', '10.1109/ICCV.2005.85', '10.1109/CVPR.2005.547', '10.1093/comjnl/41.8.559', '10.1109/ICIP.1998.999009', '10.1109/34.799904', '10.1006/cviu.2002.0967', '10.1016/S0031-3203(00)00096-0', '10.1109/AFGR.2004.1301645', '10.1016/j.patrec.2004.06.016', '10.1109/34.546259', '10.1109/ICCV.1999.791206', '10.1109/CVPR.1997.609450', '10.1109/34.790429', '10.1109/ACVMOT.2005.27', '10.1023/A:1013200319198', '10.1109/34.655647', '10.1109/AFGR.2002.1004188', '10.1109/ICPR.2000.906112', '10.1109/FGR.2006.99', '10.1109/TPAMI.2007.1124', '10.1006/cviu.2000.0837', '10.1109/34.598226', '10.1109/TPAMI.2002.1023803', '10.1023/A:1008078328650', '10.1109/AFGR.2004.1301646', '10.1109/AFGR.1998.671009', '10.1109/MCG.2002.1046630', '10.1109/89.536930', '10.1016/S0262-8856(03)00070-2', '10.1016/B978-155860869-6/50043-3', '10.1109/34.735811', '10.1109/ACVMOT.2005.110', '10.1109/TPAMI.2008.203', '10.1109/CVPR.1993.341109', '10.1109/ICPR.2002.1048351', '10.1109/RATFG.2001.938914', '10.1109/CVPR.2005.518', '10.1109/AFGR.1998.670984', '10.1109/CVPR.2004.345', '10.1109/CVPR.2004.474', '10.1109/TPAMI.2004.1262308', '10.1214/aoms/1177697196', '10.1109/ICCV.2003.1238467'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'machine learning', 'multimedia'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Object, scene and actions: Combining multiple features for human action recognition': Paper(DOI='10.1007/978-3-642-15549-9_36', crossref_json=None, google_schorlar_metadata=None, title='Object, scene and actions: Combining multiple features for human action recognition', authors=['Nazli Ikizler-Cinbis', 'Stan Sclaroff'], abstract=' In many cases, human actions can be identified not only by the singular observation of the human body in motion, but also properties of the surrounding scene and the related objects. In this paper, we look into this problem and propose an approach for human action recognition that integrates multiple feature channels from several entities such as objects, scenes and people. We formulate the problem in a multiple instance learning (MIL) framework, based on multiple feature channels. By using a discriminative approach, we join multiple feature channels embedded to the MIL space. Our experiments over the large YouTube dataset show that scene and object information can be used to complement person features for human action recognition.', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2008.284', '10.1109/CVPR.2009.5206737', '10.1109/ICCV.2005.28', '10.1109/TPAMI.2006.248', '10.1109/TPAMI.2003.1195991', '10.1109/ICCV.2003.1238420', '10.1109/CVPR.2008.4587597', '10.1561/0600000005', '10.1109/ICCV.2009.5459169', '10.1109/CVPR.2007.383331', '10.1109/CVPR.2009.5206795', '10.1007/s11263-008-0142-8', '10.1109/ICCV.2009.5459368', '10.1109/ICCV.2007.4408988', '10.1109/ICCV.2007.4409011', '10.1109/CVPR.2008.4587756', '10.1109/CVPR.2009.5206744', '10.1109/CVPRW.2009.5206557', '10.1109/CVPR.2008.4587628', '10.1109/ICCV.1999.791201', '10.1007/978-3-540-88693-8_39', '10.1023/A:1011139631724', '10.1109/ICPR.2004.1334462', '10.1007/978-3-540-88682-2_42', '10.1109/CVPR.2008.4587632'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer vision', 'pattern recognition', 'machine learning', 'multimedia'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Multilevel language and vision integration for text-to-clip retrieval': Paper(DOI='10.1609/aaai.v33i01.33019062', crossref_json=None, google_schorlar_metadata=None, title='Multilevel language and vision integration for text-to-clip retrieval', authors=['Huijuan Xu', 'Kun He', 'Bryan A Plummer', 'Leonid Sigal', 'Stan Sclaroff', 'Kate Saenko'], abstract='We address the problem of text-based activity retrieval in video. Given a sentence describing an activity, our task is to retrieve matching clips from an untrimmed video. To capture the inherent structures present in both text and video, we introduce a multilevel model that integrates vision and language features earlier and more tightly than prior work. First, we inject text features early on when generating clip proposals, to help eliminate unlikely clips and thus speed up processing and boost performance. Second, to learn a fine-grained similarity metric for retrieval, we use visual features to modulate the processing of query sentences at the word level in a recurrent neural network. A multi-task loss is also employed by adding query re-generation as an auxiliary task. Our approach significantly outperforms prior work on two challenging benchmarks: Charades-STA and ActivityNet Captions.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'A first-order primal-dual algorithm for convex problems with applications to imaging': Paper(DOI='10.1007/s10851-010-0251-1', crossref_json=None, google_schorlar_metadata=None, title='A first-order primal-dual algorithm for convex problems with applications to imaging', authors=['Antonin Chambolle', 'Thomas Pock'], abstract=' In this paper we study a first-order primal-dual algorithm for non-smooth convex optimization problems with known saddle-point structure. We prove convergence to a saddle-point with rate O(1/N) in finite dimensions for the complete class of problems. We further show accelerations of the proposed algorithm to yield improved rates on problems with some degree of smoothness. In particular we show that we can achieve O(1/N 2) convergence on problems, where the primal or the dual objective is uniformly convex, and we can show linear convergence, i.e. O(ω  N ) for some ω∈(0,1), on smooth problems. The wide applicability of the proposed algorithm is demonstrated on several imaging problems such as image denoising, image deconvolution, image inpainting, motion estimation and multi-label image segmentation.', conference=None, journal=None, year=None, reference_list=['10.1137/080716542', '10.1007/978-1-4613-9940-7_3', '10.1007/BF02921771', '10.1137/05064182X', '10.1007/11585978_10', '10.1007/s10851-010-0251-1', '10.1007/BF01581204', '10.1109/ICIP.2009.5413571', '10.1109/MCSE.2010.14', '10.2140/pjm.1994.166.55', '10.1137/0716071', '10.1007/BF00938486', '10.1002/cpa.3160420503', '10.1007/s10957-009-9539-y', '10.1137/S1052623403425629', '10.1007/978-1-4419-8853-9', '10.1007/s10107-004-0552-5', '10.1137/0314056', '10.1016/0167-2789(92)90242-F', '10.1109/WVM.1989.47097', '10.1007/978-3-540-74936-3_22'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Convex optimization', 'Image processing', 'Variational methods', 'Machine learning', 'Computer vision'], conference_acronym='Journal of mathematical imaging and vision', publisher=None, query_handler=None),\n",
       " 'Learning a variational network for reconstruction of accelerated MRI data': Paper(DOI='10.1002/mrm.26977', crossref_json=None, google_schorlar_metadata=None, title='Learning a variational network for reconstruction of accelerated MRI data', authors=['Kerstin Hammernik', 'Teresa Klatzer', 'Erich Kobler', 'Michael P Recht', 'Daniel K Sodickson', 'Thomas Pock', 'Florian Knoll'], abstract=' Purpose To allow fast and high‐quality reconstruction of clinical accelerated multi‐coil MR data by learning a variational network that combines the mathematical structure of variational models with deep learning. Theory and Methods Generalized compressed sensing reconstruction formulated as a variational model is embedded in an unrolled gradient descent scheme. All parameters of this formulation, including the prior model defined by filter kernels and activation functions as well as the data term weights, are learned during an offline training procedure. The learned model can then be applied online to previously unseen data. Results The variational network approach is evaluated on a clinical knee imaging protocol for different acceleration factors and sampling patterns using retrospectively and prospectively undersampled data. The variational network reconstructions outperform standard reconstruction\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1038/nature14539', '10.1109/ICCV.2015.316', '10.1109/CVPR.2015.7299163', '10.1016/j.neuroimage.2014.12.061', '10.1109/TMI.2016.2548501', '10.1109/TMI.2016.2551324', '10.1016/j.neuroimage.2016.01.024', '10.1002/mrm.1910380414', '10.1002/(SICI)1522-2594(199911)42:5<952::AID-MRM16>3.0.CO;2-S', '10.1002/mrm.10171', '10.1109/TIT.2005.862083', '10.1109/TIT.2006.871582', '10.1002/mrm.21391', '10.1109/T-AIEE.1928.5055024', '10.1109/JRPROC.1949.232969', '10.1002/mrm.21236', '10.1016/0167-2789(92)90242-F', '10.1002/mrm.22595', '10.1002/mrm.22964', '10.1088/0031-9155/60/21/R297', '10.2307/2372313', '10.1007/s002110050158', '10.1017/S096249291600009X', '10.1007/s11263-008-0197-6', '10.1109/ICCPHOT.2016.7492871', '10.1007/978-3-319-24947-6_29', '10.1137/16M1064064', '10.1007/978-3-319-66709-6_23', '10.1007/978-3-642-35289-8_3', '10.1002/mrm.24751', '10.1137/090769521', '10.1109/TMI.2010.2090538', '10.1109/TIP.2003.819861', '10.1109/ACCESS.2016.2624938', '10.1109/TMI.2014.2301271', '10.1109/TCI.2016.2567299', '10.1109/ISBI.2016.7493320', '10.1109/ISBI.2017.7950457', '10.1007/978-3-319-10590-1_53', '10.1364/JOSAA.2.001160', '10.1016/0031-3203(91)90143-S', '10.1038/381607a0', '10.1109/34.632983'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Convex optimization', 'Image processing', 'Variational methods', 'Machine learning', 'Computer vision'], conference_acronym='Magnetic resonance in medicine (Print)', publisher=None, query_handler=None),\n",
       " 'Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration': Paper(DOI='10.1109/tpami.2016.2596743', crossref_json=None, google_schorlar_metadata=None, title='Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration', authors=['Yunjin Chen', 'Thomas Pock'], abstract='Image restoration is a long-standing problem in low-level computer vision with many interesting applications. We describe a flexible learning framework based on the concept of nonlinear reaction diffusion models for various image restoration problems. By embodying recent improvements in nonlinear diffusion models, we propose a dynamic nonlinear reaction diffusion model with time-dependent parameters (i.e., linear filters and influence functions). In contrast to previous nonlinear diffusion models, all the parameters, including the filters and the influence functions, are simultaneously learned from training data through a loss based approach. We call this approach TNRD-Trainable Nonlinear Reaction Diffusion. The TNRD approach is applicable for a variety of image restoration tasks by incorporating appropriate reaction force. We demonstrate its capabilities with three representative applications, Gaussian image\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF01589116', '10.5244/C.23.115', '10.1137/0716071', '10.1109/CVPR.2011.5995309', '10.1109/5.726791', '10.1109/CVPR.2010.5539959', '10.1109/ICCV.2011.6126278', '10.1007/s11263-010-0330-1', '10.1007/s10851-010-0256-9', '10.1109/ICCV.2015.123', '10.1109/83.923285', '10.1109/TIP.2012.2235847', '10.1109/TIP.2007.891788', '10.1137/S0036139994277580', '10.1109/TIP.2014.2362057', '10.1109/TIP.2002.800883', '10.1109/CVPR.2014.366', '10.1007/978-1-4471-4072-6_12', '10.1109/ICCV.2003.1238435', '10.1023/A:1008344608808', '10.1109/34.632983', '10.1145/2366145.2366158', '10.1007/978-3-642-02256-2_44', '10.1109/ICCV.2013.241', '10.1109/CVPR.2015.7299003', '10.1109/CVPR.2014.349', '10.1109/TIP.2014.2299065', '10.1109/ICCV.2009.5459452', '10.1109/CVPR.2015.7299163', '10.2307/2153246', '10.1109/LGRS.2013.2271650', '10.1109/TIP.2007.901238', '10.1007/11550518_56', '10.1007/s10851-009-0166-x', '10.1109/TIP.2003.814242', '10.1109/TIP.2009.2028254', '10.1109/72.279181', '10.1007/978-3-642-38241-3_16', '10.1109/ICIP.1997.647755', '10.1016/0167-2789(92)90242-F', '10.1109/TSP.2013.2290508', '10.1109/TPAMI.2012.140', '10.1109/34.56205', '10.1007/s11263-008-0197-6', '10.1109/TIP.2014.2325774', '10.1109/MSP.2011.2179329', '10.1137/130942954', '10.1016/0262-8856(90)80008-H'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Convex optimization', 'Image processing', 'Variational methods', 'Machine learning', 'Computer vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Second order total generalized variation (TGV) for MRI': Paper(DOI='10.1002/mrm.22595', crossref_json=None, google_schorlar_metadata=None, title='Second order total generalized variation (TGV) for MRI', authors=['Florian Knoll', 'Kristian Bredies', 'Thomas Pock', 'Rudolf Stollberger'], abstract=' Total variation was recently introduced in many different magnetic resonance imaging applications. The assumption of total variation is that images consist of areas, which are piecewise constant. However, in many practical magnetic resonance imaging situations, this assumption is not valid due to the inhomogeneities of the exciting B1 field and the receive coils. This work introduces the new concept of total generalized variation for magnetic resonance imaging, a new mathematical framework, which is a generalization of the total variation theory and which eliminates these restrictions. Two important applications are considered in this article, image denoising and image reconstruction from undersampled radial data sets with multiple coils. Apart from simulations, experimental results from in vivo measurements are presented where total generalized variation yielded improved image quality over conventional total\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/0167-2789(92)90242-F', '10.1002/mrm.21799', '10.1002/mrm.20346', '10.1002/mrm.21391', '10.1002/mrm.21236', '10.1137/090769521', '10.1109/TIT.2005.862083', '10.1109/TIT.2006.871582', '10.1109/MSP.2007.914728', '10.1007/s10107-004-0552-5', '10.1137/S1052623403425629', '10.1007/s10851-010-0251-1', '10.1002/mrm.1241', '10.1109/TSP.2002.807005', '10.1007/s10334-010-0207-x', '10.1002/mrm.10171'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Convex optimization', 'Image processing', 'Variational methods', 'Machine learning', 'Computer vision'], conference_acronym='Magnetic resonance in medicine (Print)', publisher=None, query_handler=None),\n",
       " 'iPiano: Inertial proximal algorithm for nonconvex optimization': Paper(DOI='10.1137/130942954', crossref_json=None, google_schorlar_metadata=None, title='iPiano: Inertial proximal algorithm for nonconvex optimization', authors=['Peter Ochs', 'Yunjin Chen', 'Thomas Brox', 'Thomas Pock'], abstract='In this paper we study an algorithm for solving a minimization problem composed of a differentiable (possibly nonconvex) and a convex (possibly nondifferentiable) function. The algorithm iPiano combines forward-backward splitting with an inertial force. It can be seen as a nonsmooth split version of the Heavy-ball method from Polyak. A rigorous analysis of the algorithm for the proposed class of problems yields global convergence of the function values and the arguments. This makes the algorithm robust for usage on nonconvex problems. The convergence result is obtained based on the Kurdyka--Łojasiewicz inequality. This is a very weak restriction, which was used to prove convergence for several other gradient methods. First, an abstract convergence theorem for a generic algorithm is proved, and then iPiano is shown to satisfy the requirements of this theorem. Furthermore, a convergence rate is established\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1137/S1052623403427859', '10.1023/A:1011253113155', '10.1007/s10107-007-0133-5', '10.1287/moor.1100.0449', '10.1007/s10107-011-0484-9', '10.1137/080716542', '10.1007/BF00131148', '10.1090/S0002-9947-09-05048-X', '10.1007/s10851-010-0251-1', '10.1137/050626090', '10.1007/s10107-013-0653-0', '10.1007/BF01581204', '10.1080/00207728108963798', '10.1007/s10851-008-0087-0', '10.1109/TPAMI.1984.4767596', '10.1090/S0002-9904-1964-11178-2', '10.1137/100814494', '10.5802/aif.1638', '10.1016/0041-5553(66)90114-5', '10.1137/0716071', '10.1007/BF01589116', '10.5802/aif.1384', '10.1016/j.patcog.2010.08.004', '10.1016/S0377-0427(02)00906-8', '10.1007/s10107-012-0629-5', '10.1016/0041-5553(64)90137-5', '10.1137/0314056', '10.1007/s11263-008-0197-6', '10.1023/A:1022602123316', '10.1007/BF01128757'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Convex optimization', 'Image processing', 'Variational methods', 'Machine learning', 'Computer vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'On the ergodic convergence rates of a first-order primal–dual algorithm': Paper(DOI='10.1007/s10107-015-0957-3', crossref_json=None, google_schorlar_metadata=None, title='On the ergodic convergence rates of a first-order primal–dual algorithm', authors=['Antonin Chambolle', 'Thomas Pock'], abstract=' We revisit the proofs of convergence for a first order primal–dual algorithm for convex optimization which we have studied a few years ago. In particular, we prove rates of convergence for a more general version, with simpler proofs and more complete results. The new results can deal with explicit terms and nonlinear proximity operators in spaces with quite general norms.', conference=None, journal=None, year=None, reference_list=['10.1023/A:1011253113155', '10.1016/S0167-6377(02)00231-6', '10.1137/080716542', '10.1007/s10107-014-0766-0', '10.1007/s10851-010-0251-1', '10.1137/0803026', '10.1137/130919362', '10.1109/ICIP.2014.7025841', '10.1007/s10957-012-0245-9', '10.1016/j.orl.2015.02.001', '10.1145/1390156.1390191', '10.1287/moor.18.1.202', '10.1007/BF01581204', '10.1137/09076934X', '10.1137/100814494', '10.1137/110836936', '10.1007/s10851-014-0523-2', '10.1137/S1052623403425629', '10.1007/978-1-4419-8853-9', '10.1007/s10107-004-0552-5', '10.1090/S0002-9904-1967-11761-0', '10.1109/ICCV.2009.5459348', '10.1137/0314056', '10.1137/130910774', '10.1287/moor.17.3.670', '10.1007/s10444-011-9254-8'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Convex optimization', 'Image processing', 'Variational methods', 'Machine learning', 'Computer vision'], conference_acronym='Mathematical programming (Print)', publisher=None, query_handler=None),\n",
       " 'An inertial forward-backward algorithm for monotone inclusions': Paper(DOI='10.1080/02331934.2015.1127371', crossref_json=None, google_schorlar_metadata=None, title='An inertial forward-backward algorithm for monotone inclusions', authors=['Dirk A Lorenz', 'Thomas Pock'], abstract=' In this paper, we propose an inertial forward-backward splitting algorithm to compute a zero of the sum of two monotone operators, with one of the two operators being co-coercive. The algorithm is inspired by the accelerated gradient method of Nesterov, but can be applied to a much larger class of problems including convex-concave saddle point problems and general monotone inclusions. We prove convergence of the algorithm in a Hilbert space setting and show that several recently proposed first-order methods can be obtained as special cases of the general algorithm. Numerical results show that the proposed algorithm converges faster than existing methods, while keeping the computational cost of each iteration basically unchanged.', conference=None, journal=None, year=None, reference_list=['10.1007/BF02575559', '10.1137/090754297', '10.1137/1.9781611970838', '10.1007/BFb0089606', '10.1137/050626090', '10.1002/cpa.20042', '10.1016/j.jco.2011.01.003', '10.1137/110844805', '10.1007/s10589-013-9628-6', '10.1007/978-1-4614-7621-4_9', '10.1007/BF01582258', '10.1137/0329006', '10.1137/S1052623494250415', '10.1287/moor.1070.0253', '10.1137/070704277', '10.1080/02331930412331327157', '10.1137/S1052623495290179', '10.1080/02331934.2012.733883', '10.1007/s10957-012-0245-9', '10.1109/ICIP.2014.7025841', '10.1007/s10957-014-0526-6', '10.1007/s10851-014-0523-2', '10.1023/A:1011253113155', '10.1016/S0377-0427(02)00906-8', '10.1016/0041-5553(64)90137-5', '10.1137/140971233', '10.1007/978-1-4419-9467-7', '10.1007/978-3-642-20212-4', '10.1137/120872802', '10.1137/130904160', '10.1088/0266-5611/29/2/025011', '10.1088/0266-5611/27/12/125007'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Convex optimization', 'Image processing', 'Variational methods', 'Machine learning', 'Computer vision'], conference_acronym='Optimization (Print)', publisher=None, query_handler=None),\n",
       " 'Visualrank: Applying pagerank to large-scale image search': Paper(DOI='10.1109/tpami.2008.121', crossref_json=None, google_schorlar_metadata=None, title='Visualrank: Applying pagerank to large-scale image search', authors=['Yushi Jing', 'Shumeet Baluja'], abstract='Because of the relative ease in understanding and processing text, commercial image-search systems often rely on techniques that are largely indistinguishable from text search. Recently, academic studies have demonstrated the effectiveness of employing image-based features to provide either alternative or additional signals to use in this process. However, it remains uncertain whether such techniques will generalize to a large number of popular Web queries and whether the potential improvement to search quality warrants the additional computational cost. In this work, we cast the image-ranking problem into the task of identifying \"authority\" nodes on an inferred visual similarity graph and propose VisualRank to analyze the visual link structures among images. The images found to be \"authorities\" are chosen as those that answer the image-queries well. To understand the performance of such an approach in a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2007.382971', '10.1145/1348246.1348248', '10.1023/A:1007465528199', '10.1109/CVPR.2003.1211479', '10.1007/978-3-540-24593-3_34', '10.1145/1126004.1126008', '10.1109/TPAMI.2002.1023800', '10.1007/s005300050121', '10.1109/34.895972', '10.1109/TPAMI.2005.188', '10.1109/CVPR.2003.1211486', '10.1007/BF00123143', '10.1109/34.993558', '10.5244/C.2.23', '10.1109/TKDE.2003.1208999', '10.1145/1282280.1282324', '10.1126/science.1136800', '10.1145/324133.324140', '10.1109/SFCS.2000.892082', '10.1016/S0169-7552(98)00110-X', '10.1145/1291233.1291446', '10.1145/276698.276876', '10.1023/B:VISI.0000029664.99615.94', '10.1109/CVPR.2007.382969', '10.1109/CVPR.2007.383150', '10.1145/997817.997857', '10.1109/CVPR.2007.383172', '10.1109/ICCV.2007.4408863', '10.1109/CVPR.2006.264', '10.1109/ICCV.2007.4408839', '10.1145/1367497.1367618', '10.1007/11526346_68'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Deep Learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Human face detection in visual scenes': Paper(DOI='10.1080/13506285.2023.2277475', crossref_json=None, google_schorlar_metadata=None, title='Human face detection in visual scenes', authors=['Henry Rowley', 'Shumeet Baluja', 'Takeo Kanade'], abstract='We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image, and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with another state-of-the-art face detection system are presented; our system has better performance in terms of detection and false-positive rates.', conference=None, journal=None, year=None, reference_list=['10.1016/j.visres.2008.09.001', '10.1111/j.1551-6709.2009.01035.x', '10.1016/j.cognition.2004.11.004', '10.1111/j.1469-8986.2008.00663.x', '10.3758/s13423-013-0445-9', '10.1037/a0019057', '10.1016/j.visres.2009.05.012', '10.1037/0096-1523.35.1.108', '10.1167/9.12.10', '10.1167/10.4.16', '10.3389/fpsyg.2011.00342', '10.1080/13506285.2019.1676855', '10.1016/j.jecp.2012.04.012', '10.1037/0033-295X.96.3.433', '10.1068/p080431', '10.1016/0042-6989(94)00273-O', '10.1111/j.1469-8986.2009.00853.x', '10.1167/10.10.21', '10.1016/j.visres.2004.12.021', '10.3758/APP.71.7.1478', '10.1068/p3252', '10.3758/BF03194801', '10.1111/desc.12829', '10.1068/p250037', '10.1016/j.cognition.2007.07.012', '10.1068/p5007', '10.1093/scan/nss078', '10.1111/cdev.13543', '10.1177/0165025417738058', '10.1080/026432900380571', '10.1016/j.neuropsychologia.2011.09.046', '10.1016/S0001-6918(01)00058-0', '10.3758/s13428-018-01193-y', '10.1016/j.visres.2015.02.008', '10.1111/bjop.v114.S1', '10.1016/j.cognition.2022.105227', '10.1080/13506280600590434', '10.1167/5.10.1', '10.1016/j.visres.2003.11.011', '10.1016/j.jecp.2019.05.001', '10.1371/journal.pone.0034144', '10.1037/0096-1523.17.3.652', '10.1111/j.2044-8295.1988.tb02747.x', '10.1371/journal.pone.0177239', '10.1037/0096-1523.15.3.419', '10.1080/13506280701822991', '10.1037/h0027474', '10.1068/p3376', '10.1068/p140737'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Visual cognition (Print)', publisher=None, query_handler=None),\n",
       " 'Boosting sex identification performance': Paper(DOI='10.1007/s11263-006-8910-9', crossref_json=None, google_schorlar_metadata=None, title='Boosting sex identification performance', authors=['Shumeet Baluja', 'Henry A Rowley'], abstract=' This paper presents a method based on AdaBoost to identify the sex of a person from a low resolution grayscale picture of their face. The method described here is implemented in a system that will process well over 109 images. The goal of this work is to create an efficient system that is both simple to implement and maintain; the methods described here are extremely fast and have straightforward implementations. We achieve 80% accuracy in sex identification with less than 10 pixel comparisons and 90% accuracy with less than 50 pixel comparisons. The best classifiers published to date use Support Vector Machines; we match their accuracies with as few as 500 comparison operations on a 20× 20 pixel image. The AdaBoost based classifiers presented here achieve over 93% accuracy; these match or surpass the accuracies of the SVM-based classifiers, and yield performance that is 50 times faster.', conference=None, journal=None, year=None, reference_list=['10.1109/ICIP.2004.1418823', '10.1007/BF00994018', '10.1109/34.1000244', '10.1109/34.879790', '10.1109/34.655647', '10.1109/AFGR.2002.1004124'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Hiding images in plain sight: Deep steganography': Paper(DOI='10.23880/ijfsc-16000223', crossref_json=None, google_schorlar_metadata=None, title='Hiding images in plain sight: Deep steganography', authors=['Shumeet Baluja'], abstract=\"Steganography is the practice of concealing a secret message within another, ordinary, message. Commonly, steganography is used to unobtrusively hide a small message within the noisy regions of a larger image. In this study, we attempt to place a full size color image within another image of the same size. Deep neural networks are simultaneously trained to create the hiding and revealing processes and are designed to specifically work as a pair. The system is trained on images drawn randomly from the ImageNet database, and works well on natural images from a wide variety of sources. Beyond demonstrating the successful application of deep learning to hiding images, we carefully examine how the result is achieved and explore extensions. Unlike many popular steganographic methods that encode the secret message within the least significant bits of the carrier image, our approach compresses and distributes the secret image's representation across all of the available bits.\", conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Deep Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer': Paper(DOI='10.1109/tpami.2020.3019967', crossref_json=None, google_schorlar_metadata=None, title='Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer', authors=['René Ranftl', 'Katrin Lasinger', 'David Hafner', 'Konrad Schindler', 'Vladlen Koltun'], abstract='The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks. Armed with these tools, we experiment with five diverse training datasets, including a new, massive data source: 3D films. To demonstrate the generalization power of our approach we use  zero-shot cross-dataset transfer , i.e. we evaluate on datasets that were not\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2019.00575', '10.1109/CVPR.2019.00465', '10.1109/3DV.2019.00046', '10.1109/CVPR.2018.00040', '10.1109/TIP.2018.2836318', '10.1109/IROS.2012.6385773', '10.1007/978-3-642-33783-3_44', '10.1109/CVPR.2017.272', '10.1109/ICCV.2015.304', '10.1109/CVPR.2017.243', '10.1109/CVPR.2017.634', '10.1007/978-3-030-01216-8_12', '10.1109/CVPR.2012.6248074', '10.1109/CVPR.2016.85', '10.1609/aaai.v33i01.33018001', '10.1177/0301006620908207', '10.1109/CVPR.2015.7298925', '10.1145/1073204.1073232', '10.1126/scirobotics.aaw6661', '10.1109/CVPR.2018.00024', '10.1007/978-3-030-01252-6_30', '10.1109/CVPR.2018.00043', '10.1109/CVPR.2018.00594', '10.1109/CVPR.2017.700', '10.1007/978-3-319-46493-0_51', '10.1007/978-3-642-12392-4_2', '10.1109/3DV.2017.00012', '10.1109/CVPR.2009.5206848', '10.1109/CVPR.2016.90', '10.1109/CVPR.2018.00931', '10.1117/12.823927', '10.1109/CVPR.2017.699', '10.1109/CVPR.2018.00218', '10.1109/CVPR.2011.5995347', '10.1109/TPAMI.2014.2316835', '10.1109/3DV.2016.32', '10.1109/CVPR.2016.594', '10.1109/CVPR.2015.7299152', '10.1109/CVPR.2018.00214', '10.1109/TPAMI.2008.132', '10.3390/s120201437', '10.1109/ICAR.2015.7251485', '10.1007/978-1-4471-4658-2', '10.1007/s11263-016-0917-2', '10.1007/978-3-319-46484-8_45', '10.1109/CVPR.2017.261', '10.1109/CVPR.2016.350', '10.1145/3072959.3073599', '10.1109/CVPR.2017.596', '10.1109/ICCV.2019.00907', '10.1109/CVPR.2015.7298655', '10.1109/CVPR.2019.01210'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Photogrammetry', 'Remote Sensing', 'Image Analysis', 'Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'SEMANTIC3D.NET: A new large-scale point cloud classification benchmark': Paper(DOI='10.3390/s23083869', crossref_json=None, google_schorlar_metadata=None, title='SEMANTIC3D.NET: A new large-scale point cloud classification benchmark', authors=['M. Pollefeys T. Hackel', 'N. Savinov', 'L. Ladicky', 'J. D. Wegner', 'K. Schindler'], abstract=None, conference=None, journal=None, year=None, reference_list=['10.1109/TITS.2022.3155925', '10.1109/CSCWD54268.2022.9776241', '10.1109/TPAMI.2020.3005434', '10.1007/s11263-022-01601-z', '10.1109/DSAA54385.2022.10032415', '10.2139/ssrn.4396774', '10.1109/ICCV48922.2021.01595', '10.1016/j.isprsjprs.2021.01.007', '10.1016/j.eswa.2022.118815', '10.1109/ICASSP43922.2022.9746657', '10.1109/CVPR.2009.5206590', '10.3390/math9121328', '10.3390/rs10081192', '10.1145/3205326.3205355', '10.1109/ACCESS.2019.2909742', '10.1016/j.patrec.2020.06.005', '10.1080/01431161.2020.1734252'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Photogrammetry', 'Remote Sensing', 'Image Analysis', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Classification with an edge: improving semantic image segmentation with boundary detection': Paper(DOI='10.1016/j.isprsjprs.2017.11.009', crossref_json=None, google_schorlar_metadata=None, title='Classification with an edge: improving semantic image segmentation with boundary detection', authors=['Dimitrios Marmanis', 'Konrad Schindler', 'Jan Dirk Wegner', 'Silvano Galliani', 'Mihai Datcu', 'Uwe Stilla'], abstract='We present an end-to-end trainable deep convolutional neural network (DCNN) for semantic segmentation with built-in awareness of semantically meaningful boundaries. Semantic segmentation is a fundamental remote sensing task, and most state-of-the-art methods rely on DCNNs as their workhorse. A major reason for their success is that deep networks learn to accumulate contextual information over very large receptive fields. However, this success comes at a cost, since the associated loss of effective spatial resolution washes out high-frequency details and leads to blurry object boundaries. Here, we propose to counter this effect by combining semantic segmentation with semantically informed edge detection, thus making class boundaries explicit in the model. First, we construct a comparatively simple, memory-efficient model by adding boundary detection to the segnet encoder-decoder architecture. Second\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-319-54181-5_12', '10.1109/TPAMI.2016.2644615', '10.1109/ICCV.2015.65', '10.1109/36.142926', '10.1109/CVPR.2016.492', '10.1109/CVPR.2016.343', '10.1109/TGRS.2010.2048116', '10.1109/ICCV.2015.316', '10.1109/TPAMI.2012.231', '10.1080/01431169308954040', '10.1109/PROC.1969.7019', '10.1109/CVPRW.2016.90', '10.3390/rs8040329', '10.1109/CVPR.2015.7298965', '10.1109/IGARSS.2017.8128163', '10.1109/ISSPIT.2015.7394333', '10.5194/isprsannals-III-3-473-2016', '10.1007/978-3-642-15567-3_16', '10.1109/IGARSS.2016.7729468', '10.1109/ICCV.2015.178', '10.1109/CVPRW.2015.7301381', '10.1007/978-3-319-46448-0_5', '10.2352/ISSN.2470-1173.2016.10.ROBVIS-392', '10.1007/978-1-84882-935-0', '10.1109/TGRS.2014.2321423', '10.1109/TGRS.2016.2616585', '10.1109/ICCV.2015.164', '10.1109/CVPR.2016.28', '10.1007/978-3-319-10590-1_53'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Photogrammetry', 'Remote Sensing', 'Image Analysis', 'Computer Vision'], conference_acronym='ISPRS journal of photogrammetry and remote sensing', publisher=None, query_handler=None),\n",
       " 'Image retrieval: Ideas, influences, and trends of the new age': Paper(DOI='10.1145/1348246.1348248', crossref_json=None, google_schorlar_metadata=None, title='Image retrieval: Ideas, influences, and trends of the new age', authors=['Ritendra Datta', 'Dhiraj Joshi', 'Jia Li', 'James Z Wang'], abstract='We have witnessed great interest and a wealth of promise in content-based image retrieval as an emerging technology. While the last decade laid foundation to such promise, it also paved the way for a large number of new techniques and systems, got many new people involved, and triggered stronger association of weakly related fields. In this article, we survey almost 300 key theoretical and empirical contributions in the current decade related to image retrieval and automatic image annotation, and in the process discuss the spawning of related subfields. We also discuss significant challenges involved in the adaptation of existing image retrieval techniques to build systems that can be useful in the real world. In retrospect of what has been achieved so far, we also conjecture what the future may hold for image retrieval research.', conference=None, journal=None, year=None, reference_list=['10.1007/BF00393937', '10.1109/CVPR.2005.156', '10.1016/j.patrec.2005.08.019', '10.1145/1026711.1026717', '10.1177/016555159702300403', '10.1109/TVCG.2002.1044517', '10.5555/1046920.1088704', '10.5555/944919.944965', '10.1109/MSP.2005.1511835', '10.1109/TPAMI.2005.21', '10.1109/34.993558', '10.1109/34.954600', '10.1109/ICPR.2006.805', '10.1109/6046.890058', '10.1109/TMM.2002.802833', '10.1007/11527886_36', '10.1145/860435.860460', '10.1145/502807.502809', '10.1109/CVPR.2005.174', '10.1145/1027527.1027747', '10.1109/TMI.2003.819929', '10.1007/11744078_3', '10.1109/CRV.2005.53', '10.1109/TPAMI.2002.1023800', '10.1109/TSMCA.2004.838464', '10.1109/TCSVT.2002.808079', '10.1109/TPAMI.1987.4767923', '10.1109/32.6147', '10.1145/265563.265573', '10.1007/s00799-004-0097-5', '10.1007/s00530-003-0105-4', '10.1109/TPAMI.2002.1033216', '10.5555/1005332.1016789', '10.1109/TIP.2005.849770', '10.1016/S0306-4573(01)00059-0', '10.1145/1065385.1065402', '10.1109/34.1000236', '10.1109/83.817596', '10.1023/A:1026568809834', '10.1142/S0219467803000956', '10.1145/996350.996362', '10.5555/1018428.1020885', '10.1109/MMUL.2007.67', '10.1007/11744078_23', '10.1145/1291233.1291328', '10.1145/1101149.1101218', '10.1145/1291233.1291364', '10.1109/34.574790', '10.1109/34.946985', '10.1109/83.892450', '10.1109/83.982822', '10.1145/237661.237715', '10.1007/11527923_66', '10.1145/1101826.1101858', '10.1145/1027527.1027748', '10.1109/CVPR.2005.47', '10.1109/34.541413', '10.1109/2.410146', '10.1145/1101149.1101167', '10.1109/83.817602', '10.1145/502585.502652', '10.1145/1027527.1027664', '10.1109/CVPR.2005.138', '10.1145/253769.253798', '10.5555/944919.944968', '10.1109/TPAMI.2004.32', '10.1109/TIP.2004.841205', '10.1109/PROC.1979.11328', '10.1007/978-0-387-21606-5', '10.1145/1027527.1027681', '10.1145/1027527.1027531', '10.1145/1026711.1026715', '10.1145/1027527.1027530', '10.1145/1027527.1027532', '10.5555/1018429.1021109', '10.1145/1027527.1027533', '10.1023/A:1008108327226', '10.1109/TPAMI.2005.30', '10.1145/1056808.1057061', '10.1109/TIP.2004.827236', '10.1016/S0031-3203(01)00139-X', '10.1145/1026653.1026665', '10.1145/1180639.1180829', '10.1145/973264.973294', '10.1145/860435.860459', '10.1016/j.cviu.2003.10.015', '10.1109/MSP.2008.923513', '10.1145/1027527.1027732', '10.1145/1101149.1101305', '10.1109/TIP.2004.826125', '10.1109/TCSVT.2004.826775', '10.1109/TIP.2005.847289', '10.1145/1180639.1180720', '10.1145/1126004.1126008', '10.1145/958432.958469', '10.1145/1027527.1027729', '10.1145/1013208.1013210', '10.1145/872757.872829', '10.1145/347090.347169', '10.1049/ip-cds:20030481', '10.1007/PL00014575', '10.1109/TNN.2002.1021885', '10.1109/34.879802', '10.1145/1126004.1126005', '10.1145/957013.957051', '10.1145/1101149.1101261', '10.1109/18.857794', '10.1109/78.823977', '10.1109/TMM.2003.813284', '10.1109/TPAMI.2003.1227984', '10.1109/TIP.2003.821349', '10.1145/1180639.1180841', '10.1109/TPAMI.2007.70847', '10.1145/354384.354452', '10.1145/1027527.1027697', '10.1145/1101149.1101193', '10.1145/1101149.1101249', '10.1145/354384.354403', '10.1073/pnas.0406398101', '10.1002/(SICI)1097-4571(1998)49:7%3C633::AID-ASI5%3E3.3.CO;2-R', '10.1023/A:1011174803800', '10.1364/JOSAA.7.000923', '10.1214/aoms/1177692631', '10.1109/34.531803', '10.1109/76.927424', '10.1016/S0262-8856(96)01114-6', '10.1109/5.982403', '10.1002/0471721182', '10.1109/2.410154', '10.1023/B:VISI.0000027790.02288.f2', '10.1145/219717.219748', '10.1109/34.990133', '10.1109/34.391387', '10.1145/957013.957070', '10.1016/j.ijmedinf.2003.11.024', '10.1016/S0167-8655(00)00118-5', '10.1023/B:VISI.0000004832.02269.45', '10.1145/1027527.1027699', '10.1145/1119772.1119845', '10.1145/1101149.1101288', '10.1109/TKDE.2003.1262183', '10.1145/1101149.1101192', '10.1016/S0034-4257(02)00187-6', '10.1145/1180639.1180712', '10.1109/69.971189', '10.1109/69.599932', '10.1109/TPAMI.2002.1046166', '10.1109/TKDE.2002.1033768', '10.1109/TMM.2005.846796', '10.1023/A:1026553619983', '10.1145/1027527.1027650', '10.1145/365024.365097', '10.1145/642611.642682', '10.1109/MIS.2002.1024745', '10.1023/A:1026543900054', '10.1006/jvci.1999.0413', '10.1109/76.718510', '10.1109/34.589215', '10.1109/36.868886', '10.1109/34.879793', '10.1109/6046.845014', '10.1109/34.868688', '10.1109/CVPR.2005.147', '10.1145/1027527.1027678', '10.1109/34.895972', '10.1145/244130.244151', '10.1023/B:MTAP.0000046380.27575.a5', '10.1109/TIP.2003.815254', '10.1007/BF00130487', '10.1109/34.531802', '10.1109/TKDE.2005.85', '10.1117/1.1406945', '10.1023/B:VISI.0000004830.93820.78', '10.1145/500141.500159', '10.1145/956863.956874', '10.1109/34.1000239', '10.1109/83.469936', '10.1109/83.892448', '10.1109/TIT.2004.830760', '10.1109/TMM.2004.840596', '10.1109/MSP.2003.1184336', '10.1007/978-3-540-30213-1_19', '10.1007/978-3-540-31865-1_14', '10.1145/985692.985733', '10.1109/34.955109', '10.1007/s007990050026', '10.1145/1178677.1178681', '10.1109/34.899949', '10.1145/1027527.1027632', '10.1145/1027527.1027746', '10.1016/S0020-0255(03)00005-7', '10.1109/34.601251', '10.1109/99.641604', '10.1145/1101149.1101307', '10.5555/1018428.1020882', '10.1145/500141.500157', '10.1145/1027527.1027665', '10.1145/1054972.1055065', '10.1145/1101149.1101245', '10.1145/1101149.1101240', '10.1145/642611.642681', '10.1109/CVPR.2006.310', '10.1109/TPAMI.2004.1262179', '10.1145/1027527.1027730', '10.1145/1180639.1180709', '10.1145/354384.376392', '10.1145/1180639.1180719', '10.1145/957013.957090', '10.1109/42.906424', '10.1145/954339.954342', '10.1186/1471-2105-7-58', '10.1145/1027527.1027731', '10.1145/500141.500163', '10.1109/93.998050', '10.1007/s00530-002-0070-3', '10.1145/354384.354455', '10.1109/34.537343'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['data science', 'biomedical informatics', 'affective computing', 'computer vision', 'art'], conference_acronym='ACM computing surveys', publisher=None, query_handler=None),\n",
       " 'SIMPLIcity: Semantics-sensitive integrated matching for picture libraries': Paper(DOI='10.1007/3-540-40053-2_32', crossref_json=None, google_schorlar_metadata=None, title='SIMPLIcity: Semantics-sensitive integrated matching for picture libraries', authors=['James Ze Wang', 'Jia Li', 'Gio Wiederhold'], abstract='We present here SIMPLIcity (semantics-sensitive integrated matching for picture libraries), an image retrieval system, which uses semantics classification methods, a wavelet-based approach for feature extraction, and integrated region matching based upon image segmentation. An image is represented by a set of regions, roughly corresponding to objects, which are characterized by color, texture, shape, and location. The system classifies images into semantic categories. Potentially, the categorization enhances retrieval by permitting semantically-adaptive searching methods and narrowing down the searching range in a database. A measure for the overall similarity between images is developed using a region-matching scheme that integrates properties of all the regions in the images. The application of SIMPLIcity to several databases has demonstrated that our system performs significantly better and faster than\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.273714', '10.1007/3-540-48762-X_63', '10.1137/1.9781611970104', '10.1145/253769.253798', '10.1145/354384.354452', '10.1109/ICIP.1997.647976', '10.1117/12.171786', '10.1145/244130.244151', '10.1006/cviu.1999.0771', '10.1109/CAIVD.1998.646032', '10.1109/83.469936', '10.1109/IVL.1998.694464', '10.1007/s007990050026', '10.1016/S0140-3664(98)00203-5'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['data science', 'biomedical informatics', 'affective computing', 'computer vision', 'art'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Studying aesthetics in photographic images using a computational approach': Paper(DOI='10.1007/11744078_23', crossref_json=None, google_schorlar_metadata=None, title='Studying aesthetics in photographic images using a computational approach', authors=['Ritendra Datta', 'Dhiraj Joshi', 'Jia Li', 'James Z Wang'], abstract=' Aesthetics, in the world of art and photography, refers to the principles of the nature and appreciation of beauty. Judging beauty and other aesthetic qualities of photographs is a highly subjective task. Hence, there is no unanimously agreed standard for measuring aesthetic value. In spite of the lack of firm rules, certain features in photographic images are believed, by many, to please humans more than certain others. In this paper, we treat the challenge of automatically inferring aesthetic quality of pictures using their visual content as a machine learning problem, with a peer-rated online photo sharing Website as data source. We extract certain visual features based on the intuition that they can discriminate between aesthetically pleasing and displeasing images. Automated classifiers are built using support vector machines and classification trees. Linear regression on polynomial terms of the features is also\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1525/9780520351271', '10.1162/153244303322533214', '10.1016/S0004-3702(97)00063-5', '10.1109/TPAMI.2002.1023800', '10.1007/s00799-004-0097-5', '10.1137/1.9781611970104', '10.1109/TPAMI.2003.1227984', '10.1007/s005300050121', '10.1109/34.531803', '10.1023/A:1026543900054', '10.1109/34.895972', '10.1007/978-1-4757-2440-0', '10.1109/34.955109'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['data science', 'biomedical informatics', 'affective computing', 'computer vision', 'art'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'MILES: Multiple-instance learning via embedded instance selection': Paper(DOI='10.1109/tpami.2006.248', crossref_json=None, google_schorlar_metadata=None, title='MILES: Multiple-instance learning via embedded instance selection', authors=['Yixin Chen', 'Jinbo Bi', 'James Z Wang'], abstract='Multiple-instance problems arise from the situations where training class labels are attached to sets of samples (named bags), instead of individual samples within each bag (called instances). Most previous multiple-instance learning (MIL) algorithms are developed based on the assumption that a bag is positive if and only if at least one of its instances is positive. Although the assumption works well in a drug activity prediction problem, it is rather restrictive for other applications, especially those in the computer vision area. We propose a learning method, MILES (Multiple-Instance Learning via Embedded instance Selection), which converts the multiple-instance learning problem to a standard supervised learning problem that does not impose the assumption relating instance labels to bag labels. MILES maps each bag into a feature space defined by the instances in the training bags via an instance similarity\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1162/153244303322533214', '10.1109/CVPR.2005.250', '10.1162/153244303322753643', '10.1109/TPAMI.2004.55', '10.1016/S0004-3702(97)00043-X', '10.1023/A:1012460413855', '10.1109/ICCV.2003.1238407', '10.1109/CVPR.2003.1211479', '10.1109/34.574797', '10.1109/TPAMI.2003.1201827', '10.1023/B:VISI.0000027790.02288.f2', '10.1109/34.990133', '10.1109/TPAMI.2002.1114861', '10.1109/TPAMI.2004.71', '10.1109/TPAMI.2003.1227984', '10.1023/A:1007450326753', '10.1109/34.531803', '10.1109/CCPR.2010.5659221', '10.1109/ICCV.2005.77', '10.1049/cp:19991171', '10.1142/S1469026805001453', '10.1145/1102351.1102439', '10.1109/CVPR.2003.1211480', '10.1155/ASP/2006/37349', '10.1109/34.917571', '10.1109/34.481557', '10.1109/TPAMI.2004.28', '10.1109/TPAMI.2003.1233904', '10.1007/BF00058655', '10.1137/S1064827596304010', '10.1016/B978-0-12-737550-2.50019-1', '10.1023/B:NEPL.0000016836.03614.9f', '10.1023/A:1007402410823', '10.1109/ICDE.2000.839416', '10.1016/S0004-3702(96)00034-3', '10.1007/978-3-540-24775-3_35', '10.1109/TPAMI.2002.1033216', '10.1023/B:VISI.0000020671.28016.e8', '10.1109/34.955109'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['data science', 'biomedical informatics', 'affective computing', 'computer vision', 'art'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'A region-based fuzzy feature matching approach to content-based image retrieval': Paper(DOI='10.1109/tpami.2002.1033216', crossref_json=None, google_schorlar_metadata=None, title='A region-based fuzzy feature matching approach to content-based image retrieval', authors=['Yixin Chen', 'James Ze  Wang'], abstract='This paper proposes a fuzzy logic approach, UFM (unified feature matching), for region-based image retrieval. In our retrieval system, an image is represented by a set of segmented regions, each of which is characterized by a fuzzy feature (fuzzy set) reflecting color, texture, and shape properties. As a result, an image is associated with a family of fuzzy features corresponding to regions. Fuzzy features naturally characterize the gradual transition between regions (blurry boundaries) within an image and incorporate the segmentation-related uncertainties into the retrieval algorithm. The resemblance of two images is then defined as the overall similarity between two families of fuzzy features and quantified by a similarity measure, UFM measure, which integrates properties of all the regions in the images. Compared with similarity measures based on individual regions and on all regions with crisp-valued feature\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1006/cviu.1999.0771', '10.1109/34.531802', '10.1109/34.868688', '10.1145/244130.244151', '10.1109/34.589215', '10.1007/BF00123143', '10.1109/83.817597', '10.1016/S0031-3203(96)00113-6', '10.1109/MMCS.1997.609791', '10.1145/354384.354452', '10.1109/ICIP.1997.647976', '10.1109/83.855433', '10.1109/83.469936', '10.1109/NAFIPS.2000.877393', '10.1109/TIT.1979.1056067', '10.1007/s007990050026', '10.1109/83.817602', '10.1109/34.899949', '10.1137/1.9781611970104', '10.1109/34.955109', '10.1007/BF00962238', '10.1109/34.537343', '10.1007/3-540-48762-X_63', '10.1109/83.817596', '10.1109/6046.890058', '10.2307/2346830', '10.1109/34.574790', '10.1109/83.817600', '10.1007/978-94-011-2506-2', '10.1145/253769.253798'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['data science', 'biomedical informatics', 'affective computing', 'computer vision', 'art'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " \"Content-based image indexing and searching using Daubechies' wavelets\": Paper(DOI='10.1007/s007990050026', crossref_json=None, google_schorlar_metadata=None, title=\"Content-based image indexing and searching using Daubechies' wavelets\", authors=['James Ze Wang', 'Gio Wiederhold', 'Oscar Firschein', 'Sha Xin Wei'], abstract=\"  This paper describes WBIIS (Wavelet-Based Image Indexing and Searching), a new image indexing and retrieval algorithm with partial sketch image searching capability for large image databases. The algorithm characterizes the color variations over the spatial extent of the image in a manner that provides semantically meaningful image comparisons. The indexing algorithm applies a Daubechies' wavelet transform for each of the three opponent color components. The wavelet coefficients in the lowest few frequency bands, and their variances, are stored as feature vectors. To speed up retrieval, a two-step procedure is used that first does a crude selection based on the variances, and then refines the search by performing a feature vector match between the selected images and the query. For better accuracy in searching, two-level multiresolution matching may also be used. Masks are used for partial\\xa0…\", conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['data science', 'biomedical informatics', 'affective computing', 'computer vision', 'art'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Clue: Cluster-based retrieval of images by unsupervised learning': Paper(DOI='10.1109/tip.2005.849770', crossref_json=None, google_schorlar_metadata=None, title='Clue: Cluster-based retrieval of images by unsupervised learning', authors=['Yixin Chen', 'James Z Wang', 'Robert Krovetz'], abstract='In a typical content-based image retrieval (CBIR) system, target images (images in the database) are sorted by feature similarities with respect to the query. Similarities among target images are usually ignored. This paper introduces a new technique, cluster-based retrieval of images by unsupervised learning (CLUE), for improving user interaction with image retrieval systems by fully exploiting the similarity information. CLUE retrieves image clusters by applying a graph-theoretic clustering algorithm to a collection of images in the vicinity of the query. Clustering in CLUE is dynamic. In particular, clusters formed depend on which images are retrieved in response to the query. CLUE can be combined with any real-valued symmetric similarity measure (metric or nonmetric). Thus, it may be embedded in many current CBIR systems, including relevance feedback systems. The performance of an experimental image\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/83.892448', '10.1145/500156.500159', '10.1109/34.895972', '10.1109/34.868688', '10.1109/TKDE.2002.1033769', '10.1109/34.589215', '10.1126/science.290.5500.2319', '10.1109/34.531802', '10.1007/BF00130487', '10.1145/244130.244151', '10.1109/34.954598', '10.1109/34.955109', '10.1145/253769.253798', '10.1109/34.391417', '10.1145/243199.243216', '10.1109/34.232073', '10.1109/34.862197', '10.1109/83.817600', '10.1145/253260.253347', '10.1109/34.55109', '10.1109/TPAMI.2003.1227984', '10.1109/34.790428', '10.1109/TPAMI.2002.1033216', '10.1109/76.718510', '10.1109/TPAMI.2002.1023800', '10.1109/83.817596', '10.1109/34.857006', '10.1109/ICCV.1995.466815', '10.1007/BF00962238', '10.1137/1.9781611970104', '10.1109/34.574790', '10.1162/153244303322533214', '10.1016/B978-0-12-714250-0.50009-7', '10.1109/ICIP.1997.647976', '10.1007/BF01236575', '10.1007/s00530-002-0070-3', '10.1109/83.817597', '10.1109/ICCV.1999.790354', '10.1126/science.290.5500.2323'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['data science', 'biomedical informatics', 'affective computing', 'computer vision', 'art'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers': Paper(DOI='10.1088/1742-6596/1302/2/022073', crossref_json=None, google_schorlar_metadata=None, title='Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers', authors=['Jianbo Ye', 'Xin Lu', 'Zhe Lin', 'James Z Wang'], abstract='Model pruning has become a useful technique that improves the computational efficiency of deep learning, making it possible to deploy solutions in resource-limited scenarios. A widely-used practice in relevant work assumes that a smaller-norm parameter or feature plays a less informative role at the inference time. In this paper, we propose a channel pruning technique for accelerating the computations of deep convolutional neural networks (CNNs) that does not critically rely on this assumption. Instead, it focuses on direct simplification of the channel-to-channel computation graph of a CNN without the need of performing a computationally difficult and not-always-useful task of making high-dimensional tensors of CNN structured sparse. Our approach takes two stages: first to adopt an end-to- end stochastic training method that eventually forces the outputs of some channels to be constant, and then to prune those constant channels from the original neural network by adjusting the biases of their impacting layers such that the resulting compact model can be quickly fine-tuned. Our approach is mathematically appealing from an optimization perspective and easy to reproduce. We experimented our approach through several image learning benchmarks and demonstrate its interesting aspects and competitive performance.', conference=None, journal=None, year=None, reference_list=['10.1016/j.neucom.2017.04.023'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['data science', 'biomedical informatics', 'affective computing', 'computer vision', 'art'], conference_acronym='Journal of physics. Conference series (Print)', publisher=None, query_handler=None),\n",
       " 'Image processing for artist identification': Paper(DOI='10.1109/msp.2008.923513', crossref_json=None, google_schorlar_metadata=None, title='Image processing for artist identification', authors=['C Richard Johnson', 'Ella Hendriks', 'Igor J Berezhnoy', 'Eugene Brevdo', 'Shannon M Hughes', 'Ingrid Daubechies', 'Jia Li', 'Eric Postma', 'James Z Wang'], abstract=\"A survey of the literature reveals that image processing tools aimed at supplementing the art historian's toolbox are currently in the earliest stages of development. To jump-start the development of such methods, the Van Gogh and Kroller-Muller museums in The Netherlands agreed to make a data set of 101 high-resolution gray-scale scans of paintings within their collections available to groups of image processing researchers from several different universities. This article describes the approaches to brushwork analysis and artist identification developed by three research groups, within the framework of this data set.\", conference=None, journal=None, year=None, reference_list=['10.1137/1.9781611970104', '10.1109/29.1644', '10.1006/acha.2000.0343', '10.1109/78.823977', '10.1109/TIP.2003.821349', '10.1073/pnas.0406398101', '10.1016/j.patrec.2006.08.002', '10.1109/ICIP.2002.1039035', '10.1007/s00138-007-0098-7', '10.1109/MSP.2005.1511835', '10.1145/1348246.1348248', '10.1109/TIT.1965.1053827', '10.1109/ICIP.2001.959077', '10.1109/ICPR.1998.711184', '10.1214/aoms/1177692631', '10.1109/ICPR.1998.711107', '10.1109/83.931100'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['data science', 'biomedical informatics', 'affective computing', 'computer vision', 'art'], conference_acronym='IEEE signal processing magazine (Print)', publisher=None, query_handler=None),\n",
       " 'Support vector learning for fuzzy rule-based classification systems': Paper(DOI='10.1109/tfuzz.2003.819843', crossref_json=None, google_schorlar_metadata=None, title='Support vector learning for fuzzy rule-based classification systems', authors=['Yixin Chen', 'James Z Wang'], abstract='To design a fuzzy rule-based classification system (fuzzy classifier) with good generalization ability in a high dimensional feature space has been an active research topic for a long time. As a powerful machine learning approach for pattern recognition problems, the support vector machine (SVM) is known to have good generalization ability. More importantly, an SVM can work very well on a high- (or even infinite) dimensional feature space. This paper investigates the connection between fuzzy classifiers and kernel machines, establishes a link between fuzzy rules and kernels, and proposes a learning algorithm for fuzzy classifiers. We first show that a fuzzy classifier implicitly defines a translation invariant kernel under the assumption that all membership functions associated with the same input variable are generated from location transformation of a reference function. Fuzzy inference on the IF-part of a fuzzy rule\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/97.789601', '10.1109/91.669022', '10.1109/91.940974', '10.1098/rsta.1909.0016', '10.1109/TPAMI.2006.17', '10.1109/21.52551', '10.1109/26.387452', '10.1109/72.914517', '10.1109/21.24541', '10.1109/91.940970', '10.1109/91.705503', '10.1109/69.755624', '10.1109/12.324566', '10.1109/91.728456', '10.1016/S0019-9958(65)90241-X', '10.1007/978-94-015-7949-0', '10.1109/3477.865167', '10.1109/18.661502', '10.1109/91.618273', '10.1016/0165-0114(94)90022-1', '10.1109/5.364486', '10.1109/72.182710', '10.1016/0165-0114(95)00300-2', '10.1109/3477.775268', '10.1109/3477.891153', '10.1109/91.797984', '10.1109/72.159070', '10.1137/1116025', '10.1007/978-1-4757-2440-0', '10.1017/CBO9780511801389', '10.1162/089976698300017467', '10.1109/3477.517030', '10.1080/00207727808941724', '10.1109/91.705501', '10.1162/neco.1992.4.1.1', '10.1162/15324430260185646', '10.1109/3477.956035', '10.1017/CBO9780511810817', '10.1109/91.940964', '10.1023/A:1009715923555', '10.1109/3477.907569', '10.1109/41.661317', '10.1109/91.618280', '10.1016/0165-0114(88)90113-3', '10.1016/S0893-6080(98)00032-X', '10.1109/TSMC.1985.6313399', '10.1109/5.949486', '10.1109/91.868948', '10.1109/78.650102', '10.1109/3477.891144', '10.1109/5326.941843'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['data science', 'biomedical informatics', 'affective computing', 'computer vision', 'art'], conference_acronym='IEEE transactions on fuzzy systems', publisher=None, query_handler=None),\n",
       " 'Using Fourier descriptors and spatial models for traffic sign recognition': Paper(DOI='10.1007/978-3-642-21227-7_23', crossref_json=None, google_schorlar_metadata=None, title='Using Fourier descriptors and spatial models for traffic sign recognition', authors=['Fredrik Larsson', 'Michael Felsberg'], abstract=' Traffic sign recognition is important for the development of driver assistance systems and fully autonomous vehicles. Even though GPS navigator systems works well for most of the time, there will always be situations when they fail. In these cases, robust vision based systems are required. Traffic signs are designed to have distinct colored fields separated by sharp boundaries. We propose to use locally segmented contours combined with an implicit star-shaped object model as prototypes for the different sign classes. The contours are described by Fourier descriptors. Matching of a query image to the sign prototype database is done by exhaustive search. This is done efficiently by using the correlation based matching scheme for Fourier descriptors and a fast cascaded matching scheme for enforcing the spatial requirements. We demonstrated on a publicly available database state of the art performance.', conference=None, journal=None, year=None, reference_list=['10.1109/IVS.2007.4290087', '10.1007/11556985_71', '10.1109/ICCV.1999.791202', '10.1109/TC.1972.5008926', '10.1016/0146-664X(82)90034-X', '10.1049/iet-cvi.2010.0040', '10.1007/s11263-007-0095-3', '10.1109/TITS.2007.895311', '10.5244/C.16.36', '10.1007/s00521-007-0120-z', '10.1109/PROC.1981.12022', '10.1109/TSMC.1977.4309681', '10.1109/ICPR.2008.4761420'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Robot Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'The monogenic scale-space: A unifying approach to phase-based image processing in scale-space': Paper(DOI='10.1023/b:jmiv.0000026554.79537.35', crossref_json=None, google_schorlar_metadata=None, title='The monogenic scale-space: A unifying approach to phase-based image processing in scale-space', authors=['Michael Felsberg', 'Gerald Sommer'], abstract=' In this paper we address the topics of scale-space and phase-based image processing in a unifying framework. In contrast to the common opinion, the Gaussian kernel is not the unique choice for a linear scale-space. Instead, we chose the Poisson kernel since it is closely related to the monogenic signal, a 2D generalization of the analytic signal, where the Riesz transform replaces the Hilbert transform. The Riesz transform itself yields the flux of the Poisson scale-space and the combination of flux and scale-space, the monogenic scale-space, provides the local features phase-vector and attenuation in scale-space. Under certain assumptions, the latter two again form a monogenic scale-space which gives deeper insight to low-level image processing. In particular, we discuss edge detection by a new approach to phase congruency and its relation to amplitude based methods, reconstruction from local\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Robot Vision'], conference_acronym='Journal of mathematical imaging and vision', publisher=None, query_handler=None),\n",
       " 'Inverse optimal control for humanoid locomotion': Paper(DOI='10.1007/s10514-009-9170-7', crossref_json=None, google_schorlar_metadata=None, title='Inverse optimal control for humanoid locomotion', authors=['Taesung Park', 'Sergey Levine'], abstract='In this paper, we present a method for learning the reward function for humanoid locomotion from motioncaptured demonstrations of human running. We show how an approximate, local inverse optimal control algorithm can be used to learn the reward function for this high dimensional domain, and demonstrate how trajectory optimization can then be used to recreate dynamic, naturalistic running behaviors in new environments. Results are presented in simulation on a 29-DoF humanoid model, and include running on flat ground, rough terrain, and under strong lateral perturbation.', conference=None, journal=None, year=None, reference_list=['10.1177/027836498400300205', '10.1007/s10514-007-9075-2', '10.1109/TRO.2008.915449', '10.1016/S0921-8890(01)00155-5', '10.1016/S1474-6670(17)61205-9', '10.1007/978-3-540-89220-5_18', '10.1109/CASA.2003.1199309', '10.1109/ROBOT.2005.1570188', '10.1145/636886.636889', '10.1126/science.1107799', '10.1523/JNEUROSCI.05-07-01688.1985', '10.1023/B:JOCO.0000038914.26975.9b', '10.1111/j.1460-9568.2007.05836.x', '10.1007/s004220000211', '10.1109/ROBOT.2003.1241826', '10.1177/0278364904041324', '10.1109/ROBOT.2004.1307969', '10.1007/978-1-4615-4022-9', '10.1017/CBO9780511546877', '10.1016/S0098-1354(02)00158-8', '10.1016/S0098-1354(02)00195-3', '10.1145/1073204.1073314', '10.1017/CBO9780511983658', '10.1017/S0263574708004724', '10.1109/ICHR.2008.4756020', '10.1109/IRDS.2002.1041652', '10.1093/comjnl/7.4.308', '10.1093/imanum/drm047', '10.1109/IROS.2006.281645', '10.1109/ROBOT.2008.4543619', '10.1016/j.jmaa.2004.10.032', '10.1109/TRO.2008.2002312'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Autonomous robots', publisher=None, query_handler=None),\n",
       " 'Blobgan: Spatially disentangled scene representations': Paper(DOI='10.1007/978-3-031-19784-0_36', crossref_json=None, google_schorlar_metadata=None, title='Blobgan: Spatially disentangled scene representations', authors=['Dave Epstein', 'Taesung Park', 'Richard Zhang', 'Eli Shechtman', 'Alexei A Efros'], abstract='We propose an unsupervised, mid-level representation for a generative model of scenes. The representation is mid-level in that it is neither per-pixel nor per-image; rather, scenes are modeled as a collection of spatial, depth-ordered “blobs” of features. Blobs are differentiably placed onto a feature grid that is decoded into an image by a generative adversarial network. Due to the spatial uniformity of blobs and the locality inherent to convolution, our network learns to associate different blobs with different entities in a scene and to arrange these blobs to capture scene layout. We demonstrate this emergent behavior by showing that, despite training without any supervision, our method enables applications such as easy manipulation of objects within a scene (e.g. moving, removing, and restyling furniture), creation of feasible scenes given constraints (e.g. plausible rooms with drawers at a particular location), and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2019.00453', '10.1145/3447648', '10.1145/3478513.3480559', '10.1145/1531326.1531330', '10.1007/978-3-030-58452-8_21', '10.1109/ICCV.2019.00460', '10.1007/978-3-031-19787-1_29', '10.1007/3-540-48762-X_63', '10.1109/ICCV.2017.168', '10.1109/CVPR42600.2020.00581', '10.1109/ICCV.2019.00584', '10.1007/978-3-642-15561-1_35', '10.1007/978-3-319-10584-0_20', '10.1109/CVPR52688.2022.01553', '10.1109/ICCV.2009.5459411', '10.3758/BF03197475', '10.1007/s11263-006-0031-y', '10.1007/978-3-030-01219-9_11', '10.1109/ICCV.2013.457', '10.1109/CVPR.2017.632', '10.1109/CVPR.2018.00133', '10.1109/CVPR.2017.215', '10.1109/CVPR.2019.00453', '10.1109/CVPR42600.2020.00813', '10.1109/CVPR42600.2020.00813', '10.1145/3476576.3476682', '10.1109/ICCV.2019.00432', '10.1109/ICCV48922.2021.01415', '10.1109/ICCV.2019.00768', '10.1109/CVPR46437.2021.01129', '10.1016/j.tics.2007.09.009', '10.1109/CVPR.2019.00244', '10.1109/ICCV48922.2021.00209', '10.1109/CVPR.2016.278', '10.1007/978-3-030-58539-6_35', '10.1145/964965.808606', '10.1109/CVPR46437.2021.00232', '10.1145/3544777', '10.1109/CVPR52688.2022.01042', '10.1109/ICCV.2019.00467', '10.1145/3528233.3530757', '10.1109/CVPR46437.2021.00158', '10.1109/34.868688', '10.1109/CVPR46437.2021.01344', '10.1007/978-3-642-33715-4_54', '10.1109/CVPR.2008.4587842', '10.1109/ICCV48922.2021.00717', '10.1109/ICCV.2005.137', '10.1145/3450626.3459838', '10.1007/s11263-005-6642-x', '10.1109/CVPR52688.2022.01100', '10.1109/CVPR.2018.00917', '10.1109/CVPR46437.2021.01267', '10.1007/s11263-020-01429-5', '10.1109/ICCV.2017.629', '10.1109/CVPR.2018.00068', '10.1007/978-3-319-46454-1_36'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'A deep learning algorithm to predict hazardous drinkers and the severity of alcohol-related problems using K-NHANES': Paper(DOI='10.3389/fpsyt.2021.684406', crossref_json=None, google_schorlar_metadata=None, title='A deep learning algorithm to predict hazardous drinkers and the severity of alcohol-related problems using K-NHANES', authors=['Suk-Young Kim', 'Taesung Park', 'Kwonyoung Kim', 'Jihoon Oh', 'Yoonjae Park', 'Dai-Jin Kim'], abstract='Purpose: The number of patients with alcohol-related problems is steadily increasing. A large-scale survey of alcohol-related problems has been conducted. However, studies that predict hazardous drinkers and identify which factors contribute to the prediction are limited. Thus, the purpose of this study was to predict hazardous drinkers and the severity of alcohol-related problems of patients using a deep learning algorithm based on a large-scale survey data. Materials and Methods: Datasets of National Health and Nutrition Examination Survey of South Korea (K-NHANES), a nationally representative survey for the entire South Korean population, were used to train deep learning and conventional machine learning algorithms. Datasets from 69,187 and 45,672 participants were used to predict hazardous drinkers and the severity of alcohol-related problems, respectively. Based on the degree of contribution of each variable to deep learning, it was possible to determine which variable contributed significantly to the prediction of hazardous drinkers. Results: Deep learning showed the higher performance than conventional machine learning algorithms. It predicted hazardous drinkers with an AUC (Area under the receiver operating characteristic curve) of 0.870 (Logistic regression: 0.858, Linear SVM: 0.849, Random forest classifier: 0.810, K-nearest neighbors: 0.740). Among 325 variables for predicting hazardous drinkers, energy intake was a factor showing the greatest contribution to the prediction, followed by carbohydrate intake. Participants were classified into Zone I, Zone II, Zone III, and Zone IV based on the degree of alcohol-related\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1001/jamapsychiatry.2015.0584', '10.1016/S0140-6736(15)00122-1', '10.1001/archpsyc.64.7.830', '10.2146/ajhp060647', '10.1016/S0140-6736(12)61690-0', '10.1016/S0140-6736(12)61728-0', '10.1016/j.eclinm.2019.06.012', '10.1093/aje/kwq304', '10.2190/PM.42.3.b', '10.1093/alcalc/agr065', '10.1016/j.drugalcdep.2016.02.039', '10.1186/1747-597X-9-22', '10.1016/j.procs.2020.03.442', '10.1016/j.jad.2019.06.034', '10.1016/j.imu.2019.100228', '10.4082/kjfm.2013.34.2.79', '10.1046/j.1525-1497.1998.00118.x', '10.1001/archinte.163.7.821', '10.1093/alcalc/37.6.591', '10.1093/alcalc/agn036', '10.1111/j.1530-0277.2007.00403.x', '10.1097/01.ALC.0000164374.32229.A2', '10.1111/j.1360-0443.2009.02842.x', '10.1001/archinte.158.16.1789', '10.4082/kjfm.2009.30.9.695', '10.15441/ceem.17.228', '10.1111/j.2517-6161.1974.tb00994.x', '10.1001/jama.2016.7653', '10.1007/BF00994018', '10.1109/34.709601', '10.1080/00031305.1992.10475879', '10.3389/fpsyt.2018.00290', '10.1038/mp.2015.198', '10.1093/ajcn/63.4.479', '10.1093/ajcn/62.3.639', '10.1080/1355621031000117437', '10.1192/bjp.162.3.403', '10.1111/j.1360-0443.1986.tb00389.x', '10.3109/00952999209001613', '10.1111/j.1360-0443.1990.tb01653.x', '10.1111/j.1530-0277.1996.tb01109.x', '10.1002/eat.20594', '10.1016/j.biopsych.2012.08.026', '10.1161/01.CIR.102.19.2347', '10.12997/jla.2012.1.2.61', '10.1155/2012/862504', '10.1007/s13679-014-0129-4', '10.1016/j.jand.2017.09.030', '10.1136/emermed-2019-208721', '10.1016/j.psc.2010.04.012'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Asset: autoregressive semantic scene editing with transformers at high resolutions': Paper(DOI='10.1145/3528223.3530172', crossref_json=None, google_schorlar_metadata=None, title='Asset: autoregressive semantic scene editing with transformers at high resolutions', authors=['Difan Liu', 'Sandesh Shetty', 'Tobias Hinz', 'Matthew Fisher', 'Richard Zhang', 'Taesung Park', 'Evangelos Kalogerakis'], abstract=\"We present ASSET, a neural architecture for automatically modifying an input high-resolution image according to a user's edits on its semantic segmentation map. Our architecture is based on a transformer with a novel attention mechanism. Our key idea is to sparsify the transformer's attention matrix at high resolutions, guided by dense attention extracted at lower image resolutions. While previous attention mechanisms are computationally too expensive for handling high-resolution images or are overly constrained within specific image regions hampering long-range interactions, our novel attention mechanism is both computationally efficient and effective. Our sparsified attention mechanism is able to capture long-range interactions and context, leading to synthesizing interesting phenomena in scenes, such as reflections of landscapes onto water or fora consistent with the rest of the landscape, that were not\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.18653/v1/2020.acl-main.385', '10.1145/3306346.3323023', '10.1109/CVPR.2018.00132', '10.1145/2980179.2982423', '10.1109/ICCV.2017.168', '10.1145/3394171.3413551', '10.1109/CVPR42600.2020.00526', '10.1109/CVPR46437.2021.01268', '10.1109/ICCV.2013.127', '10.1109/CVPR.2019.00355', '10.1109/WACV48630.2021.00134', '10.1109/CVPR.2017.632', '10.1109/ICCV.2019.00183', '10.1145/1276377.1276497', '10.1109/CVPR42600.2020.00559', '10.18653/v1/2020.acl-main.703', '10.1007/978-3-030-01252-6_6', '10.1109/CVPR46437.2021.00925', '10.1109/CVPR46437.2021.01062', '10.1109/CVPR.2013.29', '10.1109/ICCV48922.2021.00986', '10.1109/ICCV.2019.00892', '10.1007/978-3-030-58542-6_24', '10.1109/ICCV.2011.6126423', '10.1109/CVPR.2019.00244', '10.1109/ICCV48922.2021.00209', '10.1109/ICCV.2019.00467', '10.1109/CVPR46437.2021.01464', '10.1109/CVPR.2018.00329', '10.1145/3474085.3475326', '10.1109/WACV51458.2022.00323', '10.1109/CVPR46437.2021.00787', '10.1109/ICCV48922.2021.01351', '10.1109/ICCV48922.2021.00465', '10.1109/ICCV48922.2021.00061', '10.1109/CVPR.2018.00813', '10.1109/tip.2003.819861', '10.1109/TIP.2014.2329776', '10.1109/CVPR.2007.383211', '10.1109/CVPR.2018.00577', '10.1109/ICCV.2019.00457', '10.1609/aaai.v34i07.6967', '10.1145/3474085.3475436', '10.1109/ICCV48922.2021.00299', '10.1109/CVPR42600.2020.00519', '10.1109/CVPR.2018.00068', '10.1109/CVPR.2019.00153', '10.1109/TPAMI.2022.3181587', '10.1109/CVPR.2017.544', '10.1109/CVPR42600.2020.00515'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='ACM transactions on graphics', publisher=None, query_handler=None),\n",
       " \"An operational approach to National Institute on Aging–Alzheimer's Association criteria for preclinical Alzheimer disease\": Paper(DOI='10.1097/00002093-200304002-00009', crossref_json=None, google_schorlar_metadata=None, title=\"An operational approach to National Institute on Aging–Alzheimer's Association criteria for preclinical Alzheimer disease\", authors=['Clifford R Jack Jr', 'David S Knopman', 'Stephen D Weigand', 'Heather J Wiste', 'Prashanthi Vemuri', 'Val Lowe', 'Kejal Kantarci', 'Jeffrey L Gunter', 'Matthew L Senjem', 'Robert J Ivnik', 'Rosebud O Roberts', 'Walter A Rocca', 'Bradley F Boeve', 'Ronald C Petersen'], abstract=\" Objective A workgroup commissioned by the Alzheimer's Association (AA) and the National Institute on Aging (NIA) recently published research criteria for preclinical Alzheimer disease (AD). We performed a preliminary assessment of these guidelines. Methods We employed Pittsburgh compound B positron emission tomography (PET) imaging as our biomarker of cerebral amyloidosis, and 18fluorodeoxyglucose PET imaging and hippocampal volume as biomarkers of neurodegeneration. A group of 42 clinically diagnosed AD subjects was used to create imaging biomarker cutpoints. A group of 450 cognitively normal (CN) subjects from a population‐based sample was used to develop cognitive cutpoints and to assess population frequencies of the different preclinical AD stages using different cutpoint criteria. Results The new criteria subdivide the preclinical phase of AD into stages 1 to 3. To classify our CN\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1016/S0197-4580(97)00056-0', '10.1016/S0197-4580(97)00047-X', '10.1523/JNEUROSCI.16-14-04491.1996', '10.1001/archneur.58.10.1705', '10.1001/archneur.58.9.1395', '10.1002/1531-8249(199903)45:3<358::AID-ANA12>3.0.CO;2-X', '10.1016/S0197-4580(97)00051-1'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neuroimaging', 'Mathematics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Alzheimer disease and associated disorders', publisher=None, query_handler=None),\n",
       " \"Defining imaging biomarker cut points for brain aging and Alzheimer's disease\": Paper(DOI='10.1016/j.jalz.2016.08.005', crossref_json=None, google_schorlar_metadata=None, title=\"Defining imaging biomarker cut points for brain aging and Alzheimer's disease\", authors=['Clifford R Jack Jr', 'Heather J Wiste', 'Stephen D Weigand', 'Terry M Therneau', 'Val J Lowe', 'David S Knopman', 'Jeffrey L Gunter', 'Matthew L Senjem', 'David T Jones', 'Kejal Kantarci', 'Mary M Machulda', 'Michelle M Mielke', 'Rosebud O Roberts', 'Prashanthi Vemuri', 'Denise A Reyes', 'Ronald C Petersen'], abstract='IntroductionOur goal was to develop cut points for amyloid positron emission tomography (PET), tau PET, flouro-deoxyglucose (FDG) PET, and MRI cortical thickness.MethodsWe examined five methods for determining cut points.ResultsThe reliable worsening method produced a cut point only for amyloid PET. The specificity, sensitivity, and accuracy of cognitively impaired versus young clinically normal (CN) methods labeled the most people abnormal and all gave similar cut points for tau PET, FDG PET, and cortical thickness. Cut points defined using the accuracy of cognitively impaired versus age-matched CN method labeled fewer people abnormal.DiscussionIn the future, we will use a single cut point for amyloid PET (standardized uptake value ratio, 1.42; centiloid, 19) based on the reliable worsening cut point method. We will base lenient cut points for tau PET, FDG PET, and cortical thickness on the accuracy\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1126/scitranslmed.3007941', '10.1586/14737175.2015.995637', '10.1016/S1474-4422(14)70090-0', '10.1016/j.jalz.2011.03.008', '10.1016/j.jalz.2011.03.004', '10.1016/j.jalz.2011.03.005', '10.1016/j.jalz.2011.03.003', '10.1016/j.neuroimage.2011.07.098', '10.1093/brain/awv112', '10.1093/brain/awm238', '10.1016/j.neurobiolaging.2010.04.007', '10.1523/JNEUROSCI.3189-09.2009', '10.1002/ana.22248', '10.1093/brain/awv050', '10.1016/j.jalz.2015.05.018', '10.1002/ana.22628', '10.1016/j.neurobiolaging.2015.08.029', '10.1097/NEN.0b013e31824b211b', '10.1056/NEJMoa1304839', '10.1001/jama.2015.4669', '10.1093/jnen/62.11.1087', '10.1001/archneur.65.11.1509', '10.1001/jama.2015.4668', '10.1212/WNL.0b013e31826e2696', '10.1016/S1474-4422(14)70252-2', '10.1007/s00259-013-2681-7', '10.1002/ana.24546', '10.1016/j.neuron.2016.01.028', '10.1093/brain/aww023', '10.1126/scitranslmed.aaf2362', '10.1093/brain/aww027', '10.1093/brain/aww139', '10.1002/ana.24711', '10.1016/j.jalz.2012.05.1208', '10.2217/bmm.12.49', '10.1159/000115751', '10.1002/ana.20009', '10.1016/j.neuroimage.2005.02.005', '10.1016/j.jalz.2014.07.003', '10.1016/j.neurobiolaging.2009.07.002', '10.1093/brain/aws125', '10.1515/CCLM.2004.118', '10.1093/brain/awv283', '10.1097/NEN.0b013e318232a379', '10.1212/01.WNL.0000115115.98960.37', '10.1007/BF00308809', '10.1001/archneurol.2010.179', '10.1016/j.neuron.2013.01.002', '10.1016/j.neuroimage.2010.06.025'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neuroimaging', 'Mathematics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " \"Alzheimer's disease diagnosis in individual subjects using structural MR images: validation studies\": Paper(DOI='10.1016/j.neuroimage.2007.09.073', crossref_json=None, google_schorlar_metadata=None, title=\"Alzheimer's disease diagnosis in individual subjects using structural MR images: validation studies\", authors=['Prashanthi Vemuri', 'Jeffrey L Gunter', 'Matthew L Senjem', 'Jennifer L Whitwell', 'Kejal Kantarci', 'David S Knopman', 'Bradley F Boeve', 'Ronald C Petersen', 'Clifford R Jack Jr'], abstract=\"OBJECTIVE To develop and validate a tool for Alzheimer's disease (AD) diagnosis in individual subjects using support vector machine (SVM)-based classification of structural MR (sMR) images. BACKGROUND Libraries of sMR scans of clinically well characterized subjects can be harnessed for the purpose of diagnosing new incoming subjects. METHODS One hundred ninety patients with probable AD were age- and gender-matched with 190 cognitively normal (CN) subjects. Three different classification models were implemented: Model I uses tissue densities obtained from sMR scans to give STructural Abnormality iNDex (STAND)-score; and Models II and III use tissue densities as well as covariates (demographics and Apolipoprotein E genotype) to give adjusted-STAND (aSTAND)-score. Data from 140 AD and 140 CN were used for training. The SVM parameter optimization and training were done by four-fold\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1002/hbm.460020108', '10.1006/nimg.2000.0582', '10.1016/j.neuroimage.2005.02.018', '10.1016/j.neuroimage.2004.06.028', '10.1212/01.wnl.0000228243.56665.c2', '10.1016/0197-4580(94)90032-9', '10.1212/WNL.51.1.149', '10.1002/ana.410340410', '10.1016/j.neuroimage.2004.12.036', '10.1016/j.neuroimage.2005.08.009', '10.1001/jama.1989.03430180093036', '10.1001/jama.1997.03550160069041', '10.1001/archneur.62.6.953', '10.1016/0022-3956(75)90026-6', '10.1097/00004728-199809000-00031', '10.1109/42.712137', '10.1523/JNEUROSCI.16-14-04491.1996', '10.1016/j.neulet.2005.03.038', '10.1192/bjp.140.6.566', '10.1097/00005072-199812000-00009', '10.1212/WNL.49.3.786', '10.1212/WNL.52.7.1397', '10.1001/archneur.63.5.674', '10.1002/ana.410230206', '10.1093/jnen/62.11.1087', '10.1016/j.neuroimage.2003.09.027', '10.1016/j.neuroimage.2005.02.042', '10.1212/WNL.34.7.939', '10.1385/JMN:17:2:101', '10.1089/10665270252935539', '10.1001/jama.1995.03520400044042', '10.1002/ana.10161', '10.1212/WNL.55.3.370', '10.1016/j.neuroimage.2006.06.010', '10.1002/jmri.20001', '10.1196/annals.1379.017', '10.1186/1471-2105-7-91'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neuroimaging', 'Mathematics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Neuroimaging signatures of frontotemporal dementia genetics: C9ORF72, tau, progranulin and sporadics': Paper(DOI='10.1093/brain/aws001', crossref_json=None, google_schorlar_metadata=None, title='Neuroimaging signatures of frontotemporal dementia genetics: C9ORF72, tau, progranulin and sporadics', authors=['Jennifer L Whitwell', 'Stephen D Weigand', 'Bradley F Boeve', 'Matthew L Senjem', 'Jeffrey L Gunter', 'Mariely DeJesus-Hernandez', 'Nicola J Rutherford', 'Matthew Baker', 'David S Knopman', 'Zbigniew K Wszolek', 'Joseph E Parisi', 'Dennis W Dickson', 'Ronald C Petersen', 'Rosa Rademakers', 'Clifford R Jack Jr', 'Keith A Josephs'], abstract=' A major recent discovery was the identification of an expansion of a non-coding GGGGCC hexanucleotide repeat in the C9ORF72 gene in patients with frontotemporal dementia and amyotrophic lateral sclerosis. Mutations in two other genes are known to account for familial frontotemporal dementia: microtubule-associated protein tau and progranulin. Although imaging features have been previously reported in subjects with mutations in tau and progranulin, no imaging features have been published in C9ORF72. Furthermore, it remains unknown whether there are differences in atrophy patterns across these mutations, and whether regional differences could help differentiate C9ORF72 from the other two mutations at the single-subject level. We aimed to determine the regional pattern of brain atrophy associated with the C9ORF72 gene mutation, and to determine which regions best differentiate C9ORF72 from\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s00401-011-0911-2', '10.1016/j.parkreldis.2006.10.007', '10.1006/nimg.2000.0582', '10.1016/j.neuroimage.2005.02.018', '10.1038/nature05016', '10.1093/brain/awm320', '10.1002/ana.10570', '10.1093/brain/awh356', '10.1136/jnnp.2009.204081', '10.1212/01.wnl.0000167602.38643.29', '10.1093/cercor/bhn202', '10.1038/nature05017', '10.1016/j.neuron.2011.09.011', '10.1093/hmg/ddl241', '10.1159/000113706', '10.1038/31508', '10.1212/WNL.58.8.1161', '10.1212/01.WNL.0000152047.58767.9D', '10.1002/ana.21426', '10.1007/s00401-011-0839-6', '10.1016/j.neuroimage.2005.09.046', '10.1016/j.neurobiolaging.2007.08.022', '10.1093/brain/awn012', '10.1007/s00401-006-0138-9', '10.1007/s00401-011-0845-8', '10.1007/s00401-008-0460-5', '10.1016/j.nbd.2005.12.001', '10.1093/brain/awl276', '10.1212/WNL.34.7.939', '10.1001/archneur.64.1.43', '10.1002/ana.410110607', '10.1212/WNL.41.4.479', '10.1016/j.parkreldis.2010.04.004', '10.1007/s00401-011-0907-y', '10.1212/WNL.51.6.1546', '10.1007/s00415-010-5815-x', '10.1093/brain/awm331', '10.1016/j.neuron.2011.09.010', '10.1093/jnen/59.11.990', '10.1212/WNL.0b013e318202038c', '10.1016/j.neuroimage.2009.12.088', '10.1212/WNL.0b013e3182270456', '10.1016/j.neuroimage.2005.02.005', '10.1109/42.668698', '10.1093/brain/awl267', '10.1093/brain/awm280', '10.1097/nen.0b013e3181567873', '10.1136/jnnp.57.4.416', '10.1006/nimg.2001.0978', '10.1002/1531-8249(199910)46:4<617::AID-ANA10>3.0.CO;2-I', '10.1371/journal.pone.0024239', '10.1001/archneur.64.3.371', '10.1212/WNL.0b013e3181b9c8b9', '10.1212/01.wnl.0000343851.46573.67', '10.1212/WNL.0b013e31820203c2', '10.1212/01.wnl.0000191395.69438.12', '10.1001/archneur.62.9.1402', '10.1093/brain/awp232', '10.1109/42.906424'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neuroimaging', 'Mathematics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Brain (Print)', publisher=None, query_handler=None),\n",
       " \"Neuroimaging correlates of pathologically defined subtypes of Alzheimer's disease: a case-control study\": Paper(DOI='10.1016/s1474-4422(12)70200-4', crossref_json=None, google_schorlar_metadata=None, title=\"Neuroimaging correlates of pathologically defined subtypes of Alzheimer's disease: a case-control study\", authors=['Jennifer L Whitwell', 'Dennis W Dickson', 'Melissa E Murray', 'Stephen D Weigand', 'Nirubol Tosakulwong', 'Matthew L Senjem', 'David S Knopman', 'Bradley F Boeve', 'Joseph E Parisi', 'Ronald C Petersen', 'Clifford R Jack', 'Keith A Josephs'], abstract=\"BackgroundThree subtypes of Alzheimer's disease (AD) have been pathologically defined on the basis of the distribution of neurofibrillary tangles: typical AD, hippocampal-sparing AD, and limbic-predominant AD. Compared with typical AD, hippocampal-sparing AD has more neurofibrillary tangles in the cortex and fewer in the hippocampus, whereas the opposite pattern is seen in limbic-predominant AD. We aimed to determine whether MRI patterns of atrophy differ between these subtypes and whether structural neuroimaging could be a useful predictor of pathological subtype at autopsy.MethodsWe identified patients who had been followed up in the Mayo Clinic Alzheimer's Disease Research Center (Rochester, MN, USA) or in the Alzheimer's Disease Patient Registry (Rochester, MN, USA) between 1992 and 2005. To be eligible for inclusion, participants had to have had dementia, AD pathology at autopsy\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1007/BF00308809', '10.1212/01.wnl.0000324924.91351.7d', '10.1212/WNL.58.5.750', '10.1212/WNL.58.10.1476', '10.1016/S1474-4422(11)70156-9', '10.1097/00005072-199710000-00002', '10.1212/WNL.41.4.479', '10.1093/jnen/62.11.1087', '10.1109/42.668698', '10.1006/nimg.2000.0582', '10.1016/j.neuroimage.2007.09.073', '10.1016/j.neuroimage.2005.02.018', '10.1006/nimg.2001.0978', '10.1016/j.neurobiolaging.2009.10.012', '10.1093/biostatistics/kxm050', '10.1136/bmj.316.7139.1236', '10.1097/00001648-199001000-00010', '10.1093/brain/awm213', '10.1093/brain/123.3.484', '10.1212/01.wnl.0000287073.12737.35', '10.1002/ana.21388', '10.3233/JAD-2010-1401', '10.1212/WNL.0b013e3181c0d427', '10.1177/1533317506299156', '10.1136/jnnp.2003.029876'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neuroimaging', 'Mathematics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Lancet neurology (Print)', publisher=None, query_handler=None),\n",
       " 'Non-stationarity in the “resting brain’s” modular architecture': Paper(DOI='10.1371/journal.pone.0039731', crossref_json=None, google_schorlar_metadata=None, title='Non-stationarity in the “resting brain’s” modular architecture', authors=['David T Jones', 'Prashanthi Vemuri', 'Matthew C Murphy', 'Jeffrey L Gunter', 'Matthew L Senjem', 'Mary M Machulda', 'Scott A Przybelski', 'Brian E Gregg', 'Kejal Kantarci', 'David S Knopman', 'Bradley F Boeve', 'Ronald C Petersen', 'Clifford R Jack Jr'], abstract='Task-free functional magnetic resonance imaging (TF-fMRI) has great potential for advancing the understanding and treatment of neurologic illness. However, as with all measures of neural activity, variability is a hallmark of intrinsic connectivity networks (ICNs) identified by TF-fMRI. This variability has hampered efforts to define a robust metric of connectivity suitable as a biomarker for neurologic illness. We hypothesized that some of this variability rather than representing noise in the measurement process, is related to a fundamental feature of connectivity within ICNs, which is their non-stationary nature. To test this hypothesis, we used a large (n\\u200a=\\u200a892) population-based sample of older subjects to construct a well characterized atlas of 68 functional regions, which were categorized based on independent component analysis network of origin, anatomical locations, and a functional meta-analysis. These regions were then used to construct dynamic graphical representations of brain connectivity within a sliding time window for each subject. This allowed us to demonstrate the non-stationary nature of the brain’s modular organization and assign each region to a “meta-modular” group. Using this grouping, we then compared dwell time in strong sub-network configurations of the default mode network (DMN) between 28 subjects with Alzheimer’s dementia and 56 cognitively normal elderly subjects matched 1∶2 on age, gender, and education. We found that differences in connectivity we and others have previously observed in Alzheimer’s disease can be explained by differences in dwell time in DMN sub-network configurations, rather than steady\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1038/nrn2201', '10.1073/pnas.0905267106', '10.1523/JNEUROSCI.5587-06.2007', '10.1016/j.neuroimage.2009.12.011', '10.1371/journal.pone.0021976', '10.3174/ajnr.A2330', '10.1016/j.neuroimage.2004.10.044', '10.1073/pnas.0913863107', '10.1016/j.neuroimage.2010.02.052', '10.1016/j.neuroimage.2009.10.003', '10.1016/j.neuroimage.2008.08.010', '10.1093/cercor/bhi016', '10.1016/j.neuroimage.2011.03.069', '10.3389/neuro.11.037.2009', '10.1073/pnas.0504136102', '10.1371/journal.pone.0005226', '10.1073/pnas.1018985108', '10.1212/WNL.0b013e318233b33d', '10.1016/j.neuroimage.2008.09.062', '10.1159/000115751', '10.1371/journal.pone.0025031', '10.1002/hbm.1048', '10.1152/jn.90777.2008', '10.1016/j.neuroimage.2009.05.005', '10.1016/j.neuroimage.2004.03.027', '10.1523/JNEUROSCI.4004-09.2009', '10.1186/alzrt100', '10.1371/journal.pone.0004645', '10.1016/0013-4694(94)90143-0', '10.1016/j.neulet.2003.10.063', '10.1073/pnas.1007841107', '10.1073/pnas.0606005103', '10.1016/j.neuroimage.2011.08.105', '10.1073/pnas.0911855107', '10.1006/nimg.2000.0562', '10.1001/archneurol.2011.108', '10.1523/JNEUROSCI.3987-10.2010', '10.1016/j.tics.2005.04.010', '10.1162/jocn_a_00077', '10.1148/radiol.2251011301', '10.1016/j.brainres.2009.09.028', '10.1073/pnas.0708803104', '10.1002/hbm.21140', '10.1523/JNEUROSCI.3189-09.2009', '10.1016/j.biopsych.2009.08.024', '10.1001/archneurol.2011.202'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neuroimaging', 'Mathematics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'MRI correlates of neurofibrillary tangle pathology at autopsy: a voxel-based morphometry study': Paper(DOI='10.1212/01.wnl.0000324924.91351.7d', crossref_json=None, google_schorlar_metadata=None, title='MRI correlates of neurofibrillary tangle pathology at autopsy: a voxel-based morphometry study', authors=['JL Whitwell', 'KA Josephs', 'ME Murray', 'K Kantarci', 'SA Przybelski', 'SD Weigand', 'P Vemuri', 'ML Senjem', 'JE Parisi', 'DS Knopman', 'BF Boeve', 'RC Petersen', 'DW Dickson', 'CR Jack'], abstract='Background:  Neurofibrillary tangles (NFTs), composed of hyperphosphorylated tau proteins, are one of the pathologic hallmarks of Alzheimer disease (AD). We aimed to determine whether patterns of gray matter atrophy from antemortem MRI correlate with Braak staging of NFT pathology. Methods:  Eighty-three subjects with Braak stage III through VI, a pathologic diagnosis of low- to high-probability AD, and MRI within 4 years of death were identified. Voxel-based morphometry assessed gray matter atrophy in each Braak stage compared with 20 pathologic control subjects (Braak stages 0 through II). Results:  In pairwise comparisons with Braak stages 0 through II, a graded response was observed across Braak stages V and VI, with more severe and widespread loss identified at Braak stage VI. No regions of loss were identified in Braak stage III or IV compared with Braak stages 0 through II. The lack of\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neuroimaging', 'Mathematics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Neurology', publisher=None, query_handler=None),\n",
       " 'Characterizing a neurodegenerative syndrome: primary progressive apraxia of speech': Paper(DOI='10.1093/brain/aws032', crossref_json=None, google_schorlar_metadata=None, title='Characterizing a neurodegenerative syndrome: primary progressive apraxia of speech', authors=['Keith A Josephs', 'Joseph R Duffy', 'Edythe A Strand', 'Mary M Machulda', 'Matthew L Senjem', 'Ankit V Master', 'Val J Lowe', 'Clifford R Jack Jr', 'Jennifer L Whitwell'], abstract=' Apraxia of speech is a disorder of speech motor planning and/or programming that is distinguishable from aphasia and dysarthria. It most commonly results from vascular insults but can occur in degenerative diseases where it has typically been subsumed under aphasia, or it occurs in the context of more widespread neurodegeneration. The aim of this study was to determine whether apraxia of speech can present as an isolated sign of neurodegenerative disease. Between July 2010 and July 2011, 37 subjects with a neurodegenerative speech and language disorder were prospectively recruited and underwent detailed speech and language, neurological, neuropsychological and neuroimaging testing. The neuroimaging battery included 3.0 tesla volumetric head magnetic resonance imaging, [18F]-fluorodeoxyglucose and [11C] Pittsburg compound B positron emission tomography scanning. Twelve subjects\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/s002340050466', '10.1006/nimg.2000.0582', '10.1016/j.neuroimage.2005.02.018', '10.1093/brain/85.3.411', '10.1002/mrm.10609', '10.1002/ana.410400611', '10.1002/ana.10570', '10.1080/146608200300079536', '10.1016/S0022-510X(96)00096-2', '10.1212/WNL.0b013e3181c7da8e', '10.1136/jnnp.56.8.923', '10.1212/WNL.0b013e3181c7198e', '10.1093/brain/85.4.665', '10.1038/384159a0', '10.1212/WNL.55.11.1621', '10.1080/02687030600597358', '10.1016/0022-3956(75)90026-6', '10.1016/j.jalz.2007.10.004', '10.1097/WNN.0b013e31802b6c45', '10.1212/01.wnl.0000324625.00404.15', '10.1002/mds.22340', '10.1002/ana.10825', '10.1212/WNL.0b013e31821103e6', '10.1093/brain/awh075', '10.1212/WNL.44.11.2015', '10.1192/bjp.140.6.566', '10.1080/13854049208401877', '10.1080/13554790590963004', '10.1097/WCO.0b013e3283168ddd', '10.1093/brain/awl078', '10.1212/01.wnl.0000191307.69661.c3', '10.1212/01.wnl.0000334756.18558.92', '10.1016/j.neuroimage.2005.09.046', '10.1176/jnp.12.2.233', '10.1017/S0317167100021053', '10.1093/brain/awp207', '10.1093/brain/awn234', '10.1093/arclin/14.6.481', '10.1002/ana.22424', '10.1093/brain/awq123', '10.1212/WNL.47.1.1', '10.1207/S15324826AN0803_5', '10.1007/s00401-007-0223-8', '10.1212/01.wnl.0000187889.17253.b1', '10.1212/WNL.34.7.939', '10.1001/archneur.64.1.43', '10.1002/ana.410110607', '10.1002/ana.410220414', '10.1002/ana.91', '10.1056/NEJMra022435', '10.1111/j.1532-5415.2005.53221.x', '10.1212/WNL.51.6.1546', '10.1212/WNL.0b013e3181b23564', '10.1080/13554790590962933', '10.1002/ana.21451', '10.1212/WNL.58.11.1615', '10.1080/13554790802060839', '10.1212/WNL.0b013e3181ed9c6b', '10.1109/42.668698', '10.1016/j.neuroimage.2006.02.024', '10.1016/j.neuroimage.2004.07.051', '10.1093/brain/awh099', '10.1046/j.1440-1819.2002.00938.x', '10.1006/nimg.2001.0978', '10.1080/14640747508400525', '10.1177/1533317509343104', '10.1016/j.schres.2004.01.010', '10.1016/j.parkreldis.2011.05.013', '10.1212/WNL.0b013e3181d9edde', '10.1001/archneurol.2011.107', '10.1093/brain/awp232', '10.1017/S1355617705050460'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neuroimaging', 'Mathematics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Brain (Online)', publisher=None, query_handler=None),\n",
       " 'Brain β-amyloid load approaches a plateau': Paper(DOI='10.1212/wnl.0b013e3182840bbe', crossref_json=None, google_schorlar_metadata=None, title='Brain β-amyloid load approaches a plateau', authors=['Clifford R Jack', 'Heather J Wiste', 'Timothy G Lesnick', 'Stephen D Weigand', 'David S Knopman', 'Prashanthi Vemuri', 'Vernon S Pankratz', 'Matthew L Senjem', 'Jeffrey L Gunter', 'Michelle M Mielke', 'Val J Lowe', 'Bradley F Boeve', 'Ronald C Petersen'], abstract=\"ObjectiveTo model the temporal trajectory of β-amyloid accumulation using serial amyloid PET imaging.MethodsParticipants, aged 70–92 years, were enrolled in either the Mayo Clinic Study of Aging (n = 246) or the Mayo Alzheimer's Disease Research Center (n = 14). All underwent 2 or more serial amyloid PET examinations. There were 205 participants classified as cognitively normal and 55 as cognitively impaired (47 mild cognitive impairment and 8 Alzheimer dementia). We measured baseline amyloid PET-relative standardized uptake values (SUVR) and, for each participant, estimated a slope representing their annual amyloid accumulation rate. We then fit regression models to predict the rate of amyloid accumulation given baseline amyloid SUVR, and evaluated age, sex, clinical group, and APOE as covariates. Finally, we integrated the amyloid accumulation rate vs baseline amyloid PET SUVR\\xa0…\", conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neuroimaging', 'Mathematics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Neurology', publisher=None, query_handler=None),\n",
       " 'TDP-43 is a key player in the clinical features associated with Alzheimer’s disease': Paper(DOI='10.1007/s00401-014-1269-z', crossref_json=None, google_schorlar_metadata=None, title='TDP-43 is a key player in the clinical features associated with Alzheimer’s disease', authors=['Keith A Josephs', 'Jennifer L Whitwell', 'Stephen D Weigand', 'Melissa E Murray', 'Nirubol Tosakulwong', 'Amanda M Liesinger', 'Leonard Petrucelli', 'Matthew L Senjem', 'David S Knopman', 'Bradley F Boeve', 'Robert J Ivnik', 'Glenn E Smith', 'Clifford R Jack', 'Joseph E Parisi', 'Ronald C Petersen', 'Dennis W Dickson'], abstract=' The aim of this study was to determine whether the TAR DNA-binding protein of 43\\xa0kDa (TDP-43) has any independent effect on the clinical and neuroimaging features typically ascribed to Alzheimer’s disease (AD) pathology, and whether TDP-43 pathology could help shed light on the phenomenon of resilient cognition in AD. Three-hundred and forty-two subjects pathologically diagnosed with AD were screened for the presence, burden and distribution of TDP-43. All had been classified as cognitively impaired or normal, prior to death. Atlas-based parcellation and voxel-based morphometry were used to assess regional atrophy on MRI. Regression models controlling for age at death, apolipoprotein ε4 and other AD-related pathologies were utilized to explore associations between TDP-43 and cognition or brain atrophy, stratified by Braak stage. In addition, we determined whether the effects of TDP-43\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1002/ana.21154', '10.1007/s00401-008-0480-1', '10.1016/j.neurobiolaging.2012.03.004', '10.1006/nimg.2000.0582', '10.1016/j.neuroimage.2005.02.018', '10.1007/s00401-010-0681-2', '10.1007/BF00308809', '10.3233/JAD-131880', '10.1086/420978', '10.1126/science.8346443', '10.1016/0165-0270(94)90168-6', '10.1007/s00401-011-0879-y', '10.1097/00005072-199904000-00008', '10.1007/BF00293396', '10.1016/0022-3956(75)90026-6', '10.1212/WNL.58.10.1476', '10.1007/s00401-008-0400-4', '10.1212/WNL.58.5.750', '10.1093/brain/awm336', '10.1007/s00401-012-1044-y', '10.1001/archneur.61.10.1579', '10.1212/01.wnl.0000304041.09418.b1', '10.1016/j.neurobiolaging.2006.10.032', '10.1007/s00401-013-1211-9', '10.1176/appi.neuropsych.12.2.233', '10.1212/WNL.41.4.479', '10.1212/WNL.43.11.2412-a', '10.1016/S1474-4422(11)70156-9', '10.1093/brain/awr053', '10.1126/science.1134108', '10.1097/WAD.0b013e31820f8f50', '10.1126/scitranslmed.3006373', '10.1093/brain/awt171', '10.1177/1066896906292274', '10.1001/jama.1997.03540340047031', '10.1093/ije/dys213', '10.1111/j.1365-2990.2005.00635.x', '10.1006/nimg.2001.0978', '10.1097/NEN.0b013e31817713b5', '10.1212/01.wnl.0000324924.91351.7d', '10.1016/j.neurobiolaging.2009.10.012', '10.1016/S1474-4422(12)70200-4', '10.1016/S0197-4580(97)00057-2', '10.1073/pnas.0900688106'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neuroimaging', 'Mathematics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Acta neuropathologica', publisher=None, query_handler=None),\n",
       " 'Age-related changes in the default mode network are more advanced in Alzheimer disease': Paper(DOI='10.1212/wnl.0b013e318233b33d', crossref_json=None, google_schorlar_metadata=None, title='Age-related changes in the default mode network are more advanced in Alzheimer disease', authors=['DT Jones', 'MM Machulda', 'P Vemuri', 'EM McDade', 'G Zeng', 'ML Senjem', 'JL Gunter', 'SA Przybelski', 'RT Avula', 'DS Knopman', 'BF Boeve', 'RC Petersen', 'CR Jack'], abstract=' Objective To investigate age-related default mode network (DMN) connectivity in a large cognitively normal elderly cohort and in patients with Alzheimer disease (AD) compared with age-, gender-, and education-matched controls. Methods We analyzed task-free–fMRI data with both independent component analysis and seed-based analysis to identify anterior and posterior DMNs. We investigated age-related changes in connectivity in a sample of 341 cognitively normal subjects. We then compared 28 patients with AD with 56 cognitively normal noncarriers of the APOE ϵ4 allele matched for age, education, and gender. Results The anterior DMN shows age-associated increases and decreases in fontal lobe connectivity, whereas the posterior DMN shows mainly age-associated declines in connectivity throughout. Relative to matched cognitively normal controls, subjects with AD display an accelerated pattern of\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neuroimaging', 'Mathematics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Neurology', publisher=None, query_handler=None),\n",
       " 'Age, sex, and APOE ε4 effects on memory, brain structure, and β-amyloid across the adult life span': Paper(DOI='10.1001/jamaneurol.2014.4821', crossref_json=None, google_schorlar_metadata=None, title='Age, sex, and APOE ε4 effects on memory, brain structure, and β-amyloid across the adult life span', authors=['Clifford R Jack', 'Heather J Wiste', 'Stephen D Weigand', 'David S Knopman', 'Prashanthi Vemuri', 'Michelle M Mielke', 'Val Lowe', 'Matthew L Senjem', 'Jeffrey L Gunter', 'Mary M Machulda', 'Brian E Gregg', 'V Shane Pankratz', 'Walter A Rocca', 'Ronald C Petersen'], abstract='ImportanceTypical cognitive aging may be defined as age-associated changes in cognitive performance in individuals who remain free of dementia. Ideally, the full adult age spectrum should be included to assess brain imaging findings associated with typical aging.ObjectiveTo compare age, sex, andAPOE ε4 effects on memory, brain structure (adjusted hippocampal volume [HVa]), and amyloid positron emission tomography (PET) in cognitively normal individuals aged 30 to 95 years old.Design, Setting, and ParticipantsCross-sectional observational study (March 2006 to October 2014) at an academic medical center. We studied 1246 cognitively normal individuals, including 1209 participants aged 50 to 95 years old enrolled in a population-based study of cognitive aging and 37 self-selected volunteers aged 30 to 49 years old.Main Outcomes and MeasuresMemory, HVa, and amyloid PET.ResultsOverall, memory\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neuroimaging', 'Mathematics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='JAMA neurology (Print)', publisher=None, query_handler=None),\n",
       " 'Age-specific population frequencies of cerebral β-amyloidosis and neurodegeneration among people with normal cognitive function aged 50–89 years: a cross-sectional study': Paper(DOI='10.1016/s1474-4422(14)70194-2', crossref_json=None, google_schorlar_metadata=None, title='Age-specific population frequencies of cerebral β-amyloidosis and neurodegeneration among people with normal cognitive function aged 50–89 years: a cross-sectional study', authors=['Clifford R Jack', 'Heather J Wiste', 'Stephen D Weigand', 'Walter A Rocca', 'David S Knopman', 'Michelle M Mielke', 'Val J Lowe', 'Matthew L Senjem', 'Jeffrey L Gunter', 'Gregory M Preboske', 'Vernon S Pankratz', 'Prashanthi Vemuri', 'Ronald C Petersen'], abstract=\"BackgroundAs preclinical Alzheimer's disease becomes a target for therapeutic intervention, the overlap between imaging abnormalities associated with typical ageing and those associated with Alzheimer's disease needs to be recognised. We aimed to characterise how typical ageing and preclinical Alzheimer's disease overlap in terms of β-amyloidosis and neurodegeneration.MethodsWe measured age-specific frequencies of amyloidosis and neurodegeneration in individuals with normal cognitive function aged 50–89 years. Potential participants were randomly selected from the Olmsted County (MN, USA) population-based study of cognitive ageing and invited to participate in cognitive and imaging assessments. To be eligible for inclusion, individuals must have been judged clinically to have no cognitive impairment and have undergone amyloid PET, 18F-fluorodeoxyglucose (18F-FDG) PET, and MRI. Imaging\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1016/S1474-4422(13)70044-9', '10.1016/j.jalz.2011.03.003', '10.1016/S1474-4422(14)70090-0', '10.1126/scitranslmed.3007901', '10.1073/pnas.1317918110', '10.1001/archneurol.2011.183', '10.1007/s00401-011-0826-y', '10.1002/ana.21296', '10.1523/JNEUROSCI.5506-12.2013', '10.1002/ana.22628', '10.1212/WNL.0000000000000386', '10.1212/01.wnl.0000435556.21319.e4', '10.1159/000115751', '10.1016/j.mayocp.2011.11.009', '10.1016/S0022-2275(20)43176-1', '10.1016/j.neurobiolaging.2009.07.002', '10.1212/WNL.0b013e3181f39adc', '10.1016/j.neuron.2013.01.002', '10.1038/nrneurol.2013.21', '10.1002/ana.23816', '10.1016/j.neurobiolaging.2010.04.007', '10.1212/WNL.0b013e3182563bbe', '10.1016/S1474-4422(13)70194-7', '10.1097/NEN.0b013e318232a379', '10.1016/S0197-4580(97)00047-X', '10.1016/j.neuron.2013.12.003', '10.1016/S1474-4422(12)70291-0', '10.1016/S1474-4422(09)70299-6', '10.1016/j.neurobiolaging.2012.03.004', '10.1093/brain/awt171', '10.1002/ana.21843', '10.1002/ana.22248', '10.1016/j.neurobiolaging.2004.05.004', '10.1001/jamaneurol.2013.3579', '10.1001/jama.1997.03550160069041', '10.1002/ana.24135', '10.1093/brain/aws125', '10.1212/WNL.0b013e3182840bbe'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neuroimaging', 'Mathematics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Lancet neurology (Print)', publisher=None, query_handler=None),\n",
       " 'Distinct anatomical subtypes of the behavioural variant of frontotemporal dementia: a cluster analysis study': Paper(DOI='10.1093/brain/awp232', crossref_json=None, google_schorlar_metadata=None, title='Distinct anatomical subtypes of the behavioural variant of frontotemporal dementia: a cluster analysis study', authors=['Jennifer L Whitwell', 'Scott A Przybelski', 'Stephen D Weigand', 'Robert J Ivnik', 'Prashanthi Vemuri', 'Jeffrey L Gunter', 'Matthew L Senjem', 'Maria M Shiung', 'Bradley F Boeve', 'David S Knopman', 'Joseph E Parisi', 'Dennis W Dickson', 'Ronald C Petersen', 'Clifford R Jack Jr', 'Keith A Josephs'], abstract=' The behavioural variant of frontotemporal dementia is a progressive neurodegenerative syndrome characterized by changes in personality and behaviour. It is typically associated with frontal lobe atrophy, although patterns of atrophy are heterogeneous. The objective of this study was to examine case-by-case variability in patterns of grey matter atrophy in subjects with the behavioural variant of frontotemporal dementia and to investigate whether behavioural variant of frontotemporal dementia can be divided into distinct anatomical subtypes. Sixty-six subjects that fulfilled clinical criteria for a diagnosis of the behavioural variant of frontotemporal dementia with a volumetric magnetic resonance imaging scan were identified. Grey matter volumes were obtained for 26 regions of interest, covering frontal, temporal and parietal lobes, striatum, insula and supplemental motor area, using the automated anatomical\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1006/nimg.2000.0582', '10.1016/j.neuroimage.2005.02.018', '10.1038/nature05016', '10.1001/archneur.63.10.1434', '10.1017/S1355617707070269', '10.1016/j.neurobiolaging.2004.02.019', '10.1001/archneur.63.1.81', '10.1007/BF00308809', '10.1192/bjp.bp.107.046789', '10.1093/brain/awp037', '10.1002/ana.92', '10.1001/archneur.1993.00540080076020', '10.1212/WNL.56.suppl_4.S16', '10.1093/jnen/61.11.935', '10.1093/brain/120.6.1027', '10.1016/0022-3956(75)90026-6', '10.1002/ana.20873', '10.1016/S0022-510X(00)00261-6', '10.1136/jnnp.70.2.165', '10.1212/WNL.57.2.216', '10.1093/hmg/ddl241', '10.1093/brain/awh075', '10.1212/WNL.44.11.2015', '10.1159/000049956', '10.1002/ana.20203', '10.1001/archneur.64.11.1611', '10.1097/WAD.0b013e31815bf5e5', '10.1192/bjp.140.6.566', '10.1038/31508', '10.1097/00005072-199710000-00002', '10.1002/jmri.21049', '10.1093/brain/awm336', '10.1001/archneur.56.10.1233', '10.1002/ana.21426', '10.1007/s00401-008-0397-8', '10.1212/01.wnl.0000191307.69661.c3', '10.1016/j.neurobiolaging.2006.09.019', '10.1212/01.wnl.0000287073.12737.35', '10.1001/archneur.63.11.1632', '10.1212/01.wnl.0000334756.18558.92', '10.1016/j.neuroimage.2005.09.046', '10.1093/brain/awh598', '10.1093/jnen/62.11.1087', '10.1093/brain/awl288', '10.1001/archneur.1996.00550070129021', '10.1212/01.WNL.0000113729.77161.C9', '10.1076/jcen.20.2.194.1173', '10.1136/jnnp.57.4.416', '10.1007/s00401-006-0138-9', '10.1093/brain/awn061', '10.1007/s00401-008-0460-5', '10.1159/000194658', '10.1212/01.wnl.0000197983.39436.e7', '10.1212/WNL.41.4.479', '10.1002/1531-8249(200001)47:1<36::AID-ANA8>3.0.CO;2-L', '10.1212/WNL.51.6.1546', '10.1007/s00415-005-0849-1', '10.1159/000091898', '10.1212/WNL.54.3.581', '10.1093/brain/awm331', '10.1093/brain/awh628', '10.1212/WNL.58.2.198', '10.1001/archneurol.2007.38', '10.1109/42.668698', '10.1136/jnnp.70.3.323', '10.1192/bjp.180.2.140', '10.1212/01.WNL.0000091868.28557.B8', '10.1212/WNL.43.9.1800', '10.1006/nimg.2001.0978', '10.1080/14640747508400525', '10.1001/archneur.64.3.371', '10.1212/01.wnl.0000343851.46573.67', '10.1212/01.wnl.0000191395.69438.12', '10.1001/archneur.62.9.1402', '10.1093/brain/awm112', '10.1016/j.neuroimage.2006.12.006', '10.1159/000087343', '10.1097/WAD.0b013e31815bf8a5', '10.1016/j.neuroimage.2004.10.023', '10.1212/01.wnl.0000277461.06713.23', '10.1212/01.wnl.0000324920.96835.95', '10.1109/42.906424'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Neuroimaging', 'Mathematics', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Brain (Print)', publisher=None, query_handler=None),\n",
       " 'Know your surroundings: Exploiting scene information for object tracking': Paper(DOI='10.1007/978-3-030-58592-1_13', crossref_json=None, google_schorlar_metadata=None, title='Know your surroundings: Exploiting scene information for object tracking', authors=['Goutam Bhat', 'Martin Danelljan', 'Luc Van Gool', 'Radu Timofte'], abstract=' Current state-of-the-art trackers rely only on a target appearance model in order to localize the object in each frame. Such approaches are however prone to fail in case of e.g. fast appearance changes or presence of distractor objects, where a target appearance model alone is insufficient for robust tracking. Having the knowledge about the presence and locations of other objects in the surrounding scene can be highly beneficial in such cases. This scene information can be propagated through the sequence and used to, for instance, explicitly avoid distractor objects and eliminate target candidate regions. In this work, we propose a novel tracking architecture which can utilize scene information for tracking. Our tracker represents such information as dense localized state vectors, which can encode, for example, if a local region is target, background, or distractor. These state vectors are propagated through\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-319-48881-3_56', '10.1109/ICCV.2019.00628', '10.1007/978-3-030-01216-8_30', '10.1109/CVPR.2010.5539960', '10.3115/v1/D14-1179', '10.1109/CVPR.2019.00479', '10.1109/CVPR.2017.733', '10.1109/ICCV.2015.490', '10.1007/978-3-319-46454-1_29', '10.1109/ICCV.2015.316', '10.1109/ICCV.2017.128', '10.1109/CVPR.2019.00478', '10.1109/ICPR.2016.7899807', '10.1109/ICCV.2017.196', '10.1007/978-3-030-11009-3_7', '10.1109/CVPR.2016.90', '10.1007/978-3-319-46448-0_45', '10.1109/TPAMI.2014.2345390', '10.1162/neco.1997.9.8.1735', '10.1109/ICCV.2017.129', '10.1109/CVPR.2019.00441', '10.1109/CVPR.2018.00935', '10.1109/CVPR.2018.00515', '10.1109/CVPR.2019.00146', '10.1109/ICCV.2015.352', '10.1007/978-3-030-01246-5_19', '10.1109/CVPR.2016.465', '10.1109/ISCAS.2017.8050867', '10.1109/CVPR.2016.91', '10.1109/ICCV.2017.279', '10.1109/CVPR.2018.00937', '10.1109/CVPR.2018.00058', '10.1109/CVPR.2018.00931', '10.1109/CVPR.2017.531', '10.1109/CVPR.2018.00510', '10.1109/TPAMI.2014.2388226', '10.1007/978-3-319-46493-0_8', '10.1109/CVPR.2017.615', '10.1109/ICCVW.2017.235', '10.1007/978-3-030-01240-3_10', '10.1109/ICCV.2019.00411', '10.1007/978-3-030-01240-3_22', '10.1007/978-3-030-01240-3_7', '10.1109/CVPR.2018.00064'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'The Zebrafish Book': Paper(DOI='10.1158/aacr.edb-14-1957', crossref_json=None, google_schorlar_metadata=None, title='The Zebrafish Book', authors=['Monte Westerfield'], abstract=None, conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'The status, quality, and expansion of the NIH full-length cDNA project: the Mammalian Gene Collection (MGC).': Paper(DOI='10.1101/gr.2596504', crossref_json=None, google_schorlar_metadata=None, title='The status, quality, and expansion of the NIH full-length cDNA project: the Mammalian Gene Collection (MGC).', authors=['Daniela S Gerhard', 'Lukas Wagner', 'Elise A Feingold', 'Carolyn M Shenmen', 'Lynette H Grouse', 'Greg Schuler', 'Steven L Klein', 'Susan Old', 'Rebekah Rasooly', 'Peter Good', 'Mark Guyer', 'Allison M Peck', 'Jeffery G Derge', 'David Lipman', 'Francis S Collins', 'Wonhee Jang', 'Steven Sherry', 'Mike Feolo', 'Leonie Misquitta', 'Eduardo Lee', 'Kirill Rotmistrovsky', 'Susan F Greenhut', 'Carl F Schaefer', 'Kenneth Buetow', 'Tom I Bonner', 'David Haussler', 'Jim Kent', 'Mark Kiekhaus', 'Terry Furey', 'Michael Brent', 'Christa Prange', 'Kirsten Schreiber', 'Nicole Shapiro', 'Narayan K Bhat', 'Ralph F Hopkins', 'Florence Hsie', 'Tom Driscoll', 'M Bento Soares', 'Tom L Casavant', 'Todd E Scheetz', 'Brown-stein MJ', 'Ted B Usdin', 'Shiraki Toshiyuki', 'Piero Carninci', 'Yulan Piao', 'Dawood B Dudekula', 'MS Ko', 'Koichi Kawakami', 'Yutaka Suzuki', 'Sumio Sugano', 'CE Gruber', 'MR Smith', 'Blake Simmons', 'Troy Moore', 'Richard Waterman', 'Stephen L Johnson', 'Yijun Ruan', 'Chia Lin Wei', 'S Mathavan', 'Preethi H Gunaratne', 'Jiaqian Wu', 'Angela M Garcia', 'Stephen W Hulyk', 'Edwin Fuh', 'Ye Yuan', 'Anna Sneed', 'Carla Kowis', 'Anne Hodgson', 'Donna M Muzny', 'John McPherson', 'Richard A Gibbs', 'Jessica Fahey', 'Erin Helton', 'Mark Ketteman', 'Anuradha Madan', 'Stephanie Rodrigues', 'Amy Sanchez', 'Michelle Whiting', 'Anup Madari', 'Alice C Young', 'Keith D Wetherby', 'Steven J Granite', 'Peggy N Kwong', 'Charles P Brinkley', 'Russell L Pearson', 'Gerard G Bouffard', 'Robert W Blakesly', 'Eric D Green', 'Mark C Dickson', 'Alex C Rodriguez', 'Jane Grimwood', 'Jeremy Schmutz', 'Richard M Myers', 'YS Butterfield', 'Malachi Griffith', 'Obi L Griffith', 'Martin I Krzywinski', 'Nancy Liao', 'Ryan Morin', 'Diana Palmquist', 'Anca S Petrescu', 'Ursula Skalska', 'Duane E Smailus', 'Jeff M Stott', 'Angelique Schnerch', 'Jacqueline E Schein', 'SJ Jones', 'Robert A Holt', 'Agnes Baross', 'Marco A Marra', 'Sandra Clifton', 'Kathryn A Makowski', 'Stephanie Bosak', 'Joel Malek'], abstract=\"The National Institutes of Health's Mammalian Gene Collection (MGC) project was designed to generate and sequence a publicly accessible cDNA resource containing a complete open reading frame (ORF) for every human and mouse gene. The project initially used a random strategy to select clones from a large number of cDNA libraries from diverse tissues. Candidate clones were chosen based on 5′-EST sequences, and then fully sequenced to high accuracy and analyzed by algorithms developed for this project. Currently, more than 11,000 human and 10,000 mouse genes are represented in MGC by at least one clone with a full ORF. The random selection approach is now reaching a saturation point, and a transition to protocols targeted at the missing transcripts is now required to complete the mouse and human collections. Comparison of the sequence of the MGC clones to reference genome sequences reveals that most cDNA clones are of very high sequence quality, although it is likely that some cDNAs may carry missense variants as a consequence of experimental artifact, such as PCR, cloning, or reverse transcriptase errors. Recently, a rat cDNA component was added to the project, and ongoing frog (Xenopus) and zebrafish (Danio) cDNA projects were expanded to take advantage of the high-throughput MGC pipeline.\", conference=None, journal=None, year=None, reference_list=['10.1126/science.2047873', '10.1016/S0079-6603(03)75001-6', '10.1038/nrg999', '10.1101/gr.2473704', '10.1038/ng0893-332', '10.1073/pnas.1233632100', '10.1038/nature02426', '10.1101/gr.229102. Article published online before print in May 2002', '10.1002/dvdy.10174', '10.1038/35057062', '10.1006/geno.1996.0177', '10.1007/PL00006367', '10.1093/dnares/4.1.53', '10.1038/nature01266', '10.1038/ng1285', '10.1016/S0378-5955(00)00122-2', '10.1093/nar/29.1.137', '10.1016/S0168-9525(99)01882-X', '10.1093/nar/gkg111', '10.1002/dvdy.10366', '10.1016/S0300-9084(02)01446-3', '10.1093/nar/29.1.308', '10.1002/1096-9896(200109)195:1<31::AID-PATH920>3.0.CO;2-W', '10.1126/science.286.5439.455', '10.1081/CNV-120005922', '10.1073/pnas.242603899', '10.1126/science.1058040', '10.1038/nature01262', '10.1101/gr.GR1547R', '10.1016/S1359-6446(99)01303-3'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym='Genome research', publisher=None, query_handler=None),\n",
       " 'The syntenic relationship of the zebrafish and human genomes': Paper(DOI='10.1101/gr.144700', crossref_json=None, google_schorlar_metadata=None, title='The syntenic relationship of the zebrafish and human genomes', authors=['W Bradley Barbazuk', 'Ian Korf', 'Candy Kadavi', 'Joshua Heyen', 'Stephanie Tate', 'Edmund Wun', 'Joseph A Bedell', 'John D McPherson', 'Stephen L Johnson'], abstract='The zebrafish is an important vertebrate model for the mutational analysis of genes effecting developmental processes. Understanding the relationship between zebrafish genes and mutations with those of humans will require understanding the syntenic correspondence between the zebrafish and human genomes. High throughput gene and EST mapping projects in zebrafish are now facilitating this goal. Map positions for 523 zebrafish genes and ESTs with predicted human orthologs reveal extensive contiguous blocks of synteny between the zebrafish and human genomes. Eighty percent of genes and ESTs analyzed belong to conserved synteny groups (two or more genes linked in both zebrafish and human) and 56% of all genes analyzed fall in 118 homology segments (uninterrupted segments containing two or more contiguous genes or ESTs with conserved map order between the zebrafish and human\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1126/science.282.5394.1711', '10.1038/3049', '10.1006/geno.1996.0209', '10.1126/science.282.5389.744', '10.1242/dev.123.1.37', '10.1093/oxfordjournals.molbev.a025707', '10.1101/gr.9.4.334', '10.1038/12692', '10.1242/dev.123.1.1', '10.1002/(SICI)1520-6408(1996)18:1<11::AID-DVG2>3.0.CO;2-4', '10.1101/gr.1.2.124', '10.1086/282564', '10.1073/pnas.96.17.9745', '10.1093/genetics/142.4.1277', '10.1139/bcb-75-5-623', '10.1016/0168-9525(89)90103-0', '10.1089/dna.1997.16.1357', '10.1038/ng0498-345', '10.1002/(SICI)1521-1878(199806)20:6<511::AID-BIES10>3.0.CO;2-3', '10.1101/gr.9.2.99', '10.1126/science.287.5459.1820'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym='Genome research', publisher=None, query_handler=None),\n",
       " 'nacre encodes a zebrafish microphthalmia-related protein that regulates neural-crest-derived pigment cell fate': Paper(DOI='10.1242/dev.126.17.3757', crossref_json=None, google_schorlar_metadata=None, title='nacre encodes a zebrafish microphthalmia-related protein that regulates neural-crest-derived pigment cell fate', authors=['James A Lister', 'Christie P Robertson', 'Thierry Lepage', 'Stephen L Johnson', 'David W Raible'], abstract=' We report the isolation and identification of a new mutation affecting pigment cell fate in the zebrafish neural crest. Homozygous nacre (nacw2) mutants lack melanophores throughout development but have increased numbers of iridophores. The non-crest-derived retinal pigment epithelium is normal, suggesting that the mutation does not affect pigment synthesis per se. Expression of early melanoblast markers is absent in nacre mutants and transplant experiments suggested a cell-autonomous function in melanophores. We show that nacw2 is a mutation in a zebrafish gene encoding a basic helix-loop-helix/leucine zipper transcription factor related to microphthalmia (Mitf), a gene known to be required for development of eye and crest pigment cells in the mouse. Transient expression of the wild-type nacre gene restored melanophore development in nacre−/− embryos. Furthermore, misexpression of nacre\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1074/jbc.273.31.19560', '10.1006/bbrc.1998.8838', '10.1126/science.282.5394.1711', '10.1242/dev.125.3.371', '10.1126/science.760198', '10.1016/0092-8674(94)90018-3', '10.1093/genetics/103.1.109', '10.1038/24620', '10.1016/0092-8674(91)90071-6', '10.1038/26013', '10.1016/0168-9525(93)90015-A', '10.1093/genetics/151.4.1531', '10.1242/dev.123.1.229', '10.1038/34681', '10.1101/gad.8.22.2770', '10.1242/dev.124.21.4351', '10.1038/348728a0', '10.1016/0092-8674(93)90429-T', '10.1093/hmg/7.4.703', '10.1016/0092-8674(94)90017-5', '10.1002/jez.1402030211', '10.1016/0012-1606(76)90232-3', '10.1093/genetics/139.4.1727', '10.1006/dbio.1995.1004', '10.1093/genetics/142.4.1277', '10.1242/dev.123.1.369', '10.1002/aja.1002030302', '10.1093/oxfordjournals.jhered.a106750', '10.1016/0076-6879(91)04008-C', '10.1016/0959-437X(94)90135-P', '10.1126/science.7754368', '10.1242/dev.123.1.263', '10.1038/jid.1989.77', '10.1006/dbio.1997.8800', '10.1006/dbio.1998.8864', '10.1016/S0168-9525(00)89143-X', '10.1016/S0925-4773(98)00156-7', '10.1007/s004270050179', '10.1242/dev.124.12.2377', '10.1007/s003359900832', '10.1242/dev.126.15.3425', '10.1074/jbc.273.29.17983', '10.1242/dev.120.3.495', '10.1002/aja.1001950104', '10.1038/sj.onc.1201298', '10.1242/dev.120.3.483', '10.1242/dev.125.7.1275', '10.1093/genetics/136.4.1401', '10.1007/s003359900736', '10.1093/genetics/112.2.311', '10.1038/291293a0', '10.1093/hmg/3.4.553', '10.1038/ng0996-50', '10.1038/ng1194-251', '10.1242/dev.119.4.1203', '10.1182/blood.V88.4.1225.bloodjournal8841225', '10.1038/ng0398-283', '10.1126/science.1846704', '10.1002/(SICI)1521-1878(199806)20:6&lt;511::AID-BIES10&gt;3.0.CO;2-3', '10.1074/jbc.272.1.503'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym='Development (Cambridge)', publisher=None, query_handler=None),\n",
       " 'Endothelial signalling by the Notch ligand Delta-like 4 restricts angiogenesis': Paper(DOI='10.1242/dev.003244', crossref_json=None, google_schorlar_metadata=None, title='Endothelial signalling by the Notch ligand Delta-like 4 restricts angiogenesis', authors=['Jonathan D Leslie', 'Linda Ariza-McNaughton', 'Adam L Bermange', 'Ryan McAdow', 'Stephen L Johnson', 'Julian Lewis'], abstract=\"Notch signalling by the ligand Delta-like 4 (Dll4) is essential for normal vascular remodelling, yet the precise way in which the pathway influences the behaviour of endothelial cells remains a mystery. Using the embryonic zebrafish, we show that, when Dll4-Notch signalling is defective, endothelial cells continue to migrate and proliferate when they should normally stop these processes. Artificial overactivation of the Notch pathway has opposite consequences. When vascular endothelial growth factor (Vegf) signalling and Dll4-Notch signalling are both blocked, the endothelial cells remain quiescent. Thus, Dll4-Notch signalling acts as an angiogenic `off' switch by making endothelial cells unresponsive to Vegf.\", conference=None, journal=None, year=None, reference_list=['10.1016/S0074-7742(02)47062-6', '10.1038/nature03875', '10.1242/dev.129.4.973', '10.1016/j.modgep.2004.05.004', '10.1007/s004270050082', '10.1038/nature04479', '10.1073/pnas.0506886103', '10.1101/gad.1239004', '10.1101/gad.12.19.3096', '10.1073/pnas.0407290101', '10.1016/S0070-2153(04)62005-9', '10.1038/nature05571', '10.1101/gad.14.11.1343', '10.1101/gad.1239204', '10.1038/nrg888', '10.1006/dbio.2002.0711', '10.1242/dev.128.19.3675', '10.1016/S1534-5807(02)00198-3', '10.1128/MCB.22.8.2830-2841.2002', '10.1128/MCB.23.1.14-25.2003', '10.1096/fj.05-4880fje', '10.1242/dev.01411', '10.1046/j.1432-0436.2001.690207.x', '10.1242/dev.02279', '10.1038/nature05355', '10.1158/0008-5472.CAN-05-1208', '10.1006/excr.2000.5034', '10.1038/nature05313', '10.1096/fj.04-3172fje', '10.1016/S0925-4773(01)00453-1', '10.1242/dev.128.7.1099', '10.1002/bies.20004', '10.1111/j.1749-6632.2003.tb03219.x', '10.1101/gad.14.11.1313', '10.1016/S0925-4773(99)00231-2', '10.1073/pnas.091584598', '10.1016/S1084952102001052', '10.1007/s004270050091', '10.1182/blood-2005-03-1000', '10.1016/j.ccr.2005.06.004'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym='Development (Cambridge. Online)', publisher=None, query_handler=None),\n",
       " 'An orthologue of the kit-related gene fms is required for development of neural crest-derived xanthophores and a subpopulation of adult melanocytes in the zebrafish, Danio rerio': Paper(DOI='10.1242/dev.127.14.3031', crossref_json=None, google_schorlar_metadata=None, title='An orthologue of the kit-related gene fms is required for development of neural crest-derived xanthophores and a subpopulation of adult melanocytes in the zebrafish, Danio rerio', authors=['David M Parichy', 'David G Ransom', 'Barry Paw', 'Leonard I Zon', 'Stephen L Johnson'], abstract=' Developmental mechanisms underlying traits expressed in larval and adult vertebrates remain largely unknown. Pigment patterns of fishes provide an opportunity to identify genes and cell behaviors required for postembryonic morphogenesis and differentiation. In the zebrafish, Danio rerio, pigment patterns reflect the spatial arrangements of three classes of neural crest-derived pigment cells: black melanocytes, yellow xanthophores and silver iridophores. We show that the D. rerio pigment pattern mutant panther ablates xanthophores in embryos and adults and has defects in the development of the adult pattern of melanocyte stripes. We find that panther corresponds to an orthologue of the c-fms gene, which encodes a type III receptor tyrosine kinase and is the closest known homologue of the previously identified pigment pattern gene, kit. In mouse, fms is essential for the development of macrophage and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1126/science.282.5394.1711', '10.1111/j.1469-185X.1991.tb01138.x', '10.1016/S0012-1606(98)80004-3', '10.1016/0092-8674(91)90401-J', '10.1016/S0074-7696(08)60383-6', '10.1242/dev.122.10.3023', '10.1073/pnas.88.6.2341', '10.1016/S0955-0674(98)80037-9', '10.1006/dbio.1993.1221', '10.1097/00062752-199805000-00006', '10.1126/science.220.4594.268', '10.1002/jez.1401410103', '10.1002/jez.1401250306', '10.1002/jmor.1050520207', '10.1111/1523-1747.ep12338471', '10.1016/S0070-2153(08)60383-X', '10.1007/978-1-4757-3064-7', '10.1007/BF00222271', '10.1093/genetics/139.4.1727', '10.1006/dbio.1995.1004', '10.1016/S0091-679X(08)61910-X', '10.1242/dev.127.3.515', '10.1007/BF00848526', '10.1242/dev.125.15.2915', '10.1242/dev.126.17.3757', '10.1006/dbio.1997.8738', '10.1002/(SICI)1097-4687(199907)241:1&lt;83::AID-JMOR5&gt;3.0.CO;2-H', '10.1002/jez.1402270112', '10.1016/S0925-5710(98)00010-3', '10.1007/BF02741379', '10.1073/pnas.93.18.9645', '10.1002/dvg.1020100316', '10.1242/dev.124.12.2377', '10.1006/dbio.1996.0114', '10.1006/dbio.1996.0115', '10.1242/dev.126.15.3425', '10.1016/S0168-9525(98)01622-9', '10.1038/ng0498-345', '10.1093/genetics/129.4.1099', '10.1002/j.1460-2075.1991.tb07784.x', '10.1016/0092-8674(88)90224-3', '10.1007/BF00160313', '10.1038/990040', '10.1002/bies.950150404', '10.1242/jcs.103.4.1211', '10.1016/0012-1606(86)90094-1', '10.1242/dev.121.3.731', '10.1006/bbrc.1995.1068', '10.1146/annurev.bi.57.070188.002303'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym='Development (Cambridge)', publisher=None, query_handler=None),\n",
       " 'Differential induction of four msx homeobox genes during fin development and regeneration in zebrafish': Paper(DOI='10.1242/dev.121.2.347', crossref_json=None, google_schorlar_metadata=None, title='Differential induction of four msx homeobox genes during fin development and regeneration in zebrafish', authors=['Marie-Andrée Akimenko', 'Stephen L Johnson', 'Monte Westerfield', 'Marc Ekker'], abstract=' To study the genetic regulation of growth control and pattern formation during fin development and regeneration, we have analysed the expression of four homeobox genes, msxA, msxB, msxC and msxD in zebrafish fins. The median fin fold, which gives rise to the unpaired fins, expresses these four msx genes during development. Transcripts of the genes are also present in cells of the presumptive pectoral fin buds. The most distal cells, the apical ectodermal ridge of the paired fins and the cleft and flanking cells of the median fin fold express all these msx genes with the exception of msxC. Mesenchymal cells underlying the most distal cells express all four genes. Expression of the msx genes in the fin fold and fin buds is transient and, by 3 days after fertilization, msx expression in the median fin fold falls below levels detectable by in situ hybridization. Although the fins of adult zebrafish normally have levels of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-1-4615-3310-8_10', '10.1523/JNEUROSCI.14-06-03475.1994', '10.1016/0012-1606(92)90047-K', '10.1242/dev.111.2.489', '10.1016/0925-4773(91)90051-7', '10.1242/dev.113.4.1487', '10.1038/352429a0', '10.1038/342767a0', '10.1016/0092-8674(93)90378-4', '10.1002/j.1460-2075.1989.tb03534.x', '10.1016/0896-6273(92)90217-2', '10.1242/dev.116.4.1001', '10.1016/0092-8674(89)90912-4', '10.1101/gad.3.1.26', '10.1016/0378-1119(91)90182-B', '10.1073/pnas.89.17.8293', '10.1038/350585a0', '10.1073/pnas.86.14.5459', '10.1126/science.1974085', '10.1093/nar/15.20.8125', '10.1002/mrd.1080320402', '10.1242/dev.115.2.403', '10.1242/dev.112.4.1053', '10.1038/358236a0', '10.1016/0012-1606(92)90154-9', '10.1016/0092-8674(91)90274-3', '10.1007/978-3-642-86659-3', '10.1101/gad.3.5.641', '10.1242/dev.114.3.643', '10.1002/j.1460-2075.1989.tb03352.x', '10.1101/gad.5.12b.2363', '10.1242/dev.116.3.811', '10.1073/pnas.74.12.5463', '10.1038/ng0494-348', '10.1002/j.1460-2075.1988.tb03325.x', '10.1242/dev.117.4.1397', '10.1098/rspb.1994.0061', '10.1038/360477a0', '10.1002/jez.1401810106', '10.1016/0012-1606(91)90345-4', '10.1016/0092-8674(91)90612-3', '10.1002/j.1460-2075.1991.tb07777.x', '10.1242/dev.113.Supplement_1.113', '10.1242/dev.120.7.1861', '10.1002/ar.1092040408', '10.1242/dev.113.2.431'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym='Development (Cambridge)', publisher=None, query_handler=None),\n",
       " 'Pseudoreplication in playback experiments, revisited a decade later': Paper(DOI='10.1006/anbe.2000.1676', crossref_json=None, google_schorlar_metadata=None, title='Pseudoreplication in playback experiments, revisited a decade later', authors=['Donald E Kroodsma', 'Bruce E Byers', 'Eben Goodale', 'Steven Johnson', 'Wan-Chun Liu'], abstract='About 10 years ago, several papers in Animal Behaviour addressed the quality of experimental designs in‘playback’experiments (Kroodsma 1989a, b, 1990a, 1992; Searcy 1989; Weary & Mountjoy 1992), and this debate culminated in a consensus report by McGregor et al.(1992). The key issue was ‘pseudoreplication’, defined by Hurlbert (1984, page 187) as ‘the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated (though samples may be) or replicates are not statistically independent’. McGregor et al.(1992, page 2) offered their own simplified definition,‘the use of an n (sample size) in a statistical test that is not appropriate to the hypothesis being tested’. McGregor et al. agreed that pseudoreplication was a serious issue, and that designing and implementing good experimental designs was a worthy and attainable goal.What effect did the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1006/anbe.1999.1304', '10.1006/anbe.1998.0985', '10.1006/anbe.1998.1046', '10.2307/1942661', '10.1016/S0160-9327(97)85657-8', '10.1016/S0003-3472(89)80020-X', '10.1016/0003-3472(89)90039-0', '10.1016/S0003-3472(05)80180-0', '10.1016/0003-3472(92)90070-P', '10.1111/j.1439-0310.1985.tb00124.x', '10.1006/anbe.1998.0781', '10.1163/156853998793066258', '10.1006/anbe.1999.1125', '10.1006/anbe.1999.1117', '10.1006/anbe.1998.1018', '10.1006/anbe.1998.1056', '10.1006/anbe.1998.0961', '10.1016/S0003-3472(89)80019-3', '10.2307/1370477', '10.1006/anbe.1999.1128', '10.1006/anbe.1999.1289', '10.1016/0003-3472(92)90069-L'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym='Animal behaviour (Print)', publisher=None, query_handler=None),\n",
       " 'Zebrafish sparse corresponds to an orthologue of c-kit and is required for the morphogenesis of a subpopulation of melanocytes, but is not essential for\\xa0…': Paper(DOI='10.1242/dev.126.15.3425', crossref_json=None, google_schorlar_metadata=None, title='Zebrafish sparse corresponds to an orthologue of c-kit and is required for the morphogenesis of a subpopulation of melanocytes, but is not essential for\\xa0…', authors=['David M Parichy', 'John F Rawls', 'Stephen J Pratt', 'Tanya T Whitfield', 'Stephen L Johnson'], abstract=' The relative roles of the Kit receptor in promoting the migration and survival of amniote melanocytes are unresolved. We show that, in the zebrafish, Danio rerio, the pigment pattern mutation sparse corresponds to an orthologue of c-kit. This finding allows us to further elucidate morphogenetic roles for this c-kit-related gene in melanocyte morphogenesis. Our analyses of zebrafish melanocyte development demonstrate that the c-kit orthologue identified in this study is required both for normal migration and for survival of embryonic melanocytes. We also find that, in contrast to mouse, the zebrafish c-kit gene that we have identified is not essential for hematopoiesis or primordial germ cell development. These unexpected differences may reflect evolutionary divergence in c-kit functions following gene duplication events in teleosts.', conference=None, journal=None, year=None, reference_list=['10.1182/blood.V79.3.650.650', '10.1126/science.282.5394.1711', '10.1016/0092-8674(90)90304-W', '10.1016/0925-4773(94)00338-N', '10.1016/0168-9525(96)10031-7', '10.1242/dev.122.10.3023', '10.1242/dev.119.Supplement.125', '10.1182/blood.V90.4.1345', '10.1016/0925-4773(94)00331-G', '10.1002/aja.1000400203', '10.1073/pnas.92.23.10713', '10.1007/BF00221733', '10.1111/j.1600-0749.1993.tb00611.x', '10.1006/dbio.1993.1221', '10.1093/genetics/151.4.1531', '10.1093/genetics/97.2.337', '10.1073/pnas.88.19.8696', '10.1111/1523-1747.ep12338471', '10.1016/S0070-2153(08)60383-X', '10.1006/dbio.1997.8520', '10.1242/dev.119.1.49', '10.1038/348728a0', '10.1016/0092-8674(90)90303-V', '10.1002/jez.1402260311', '10.1006/dbio.1995.1004', '10.1093/genetics/142.4.1277', '10.1016/0925-4773(94)00325-H', '10.1242/dev.123.1.369', '10.1182/blood.V91.1.100', '10.1002/aja.1002030302', '10.1182/blood.V83.4.1033.bloodjournal8341033', '10.1242/dev.125.15.2915', '10.1046/j.1432-0436.1995.5820133.x', '10.1006/dbio.1996.0079', '10.1016/0959-437X(94)90135-P', '10.1073/pnas.89.10.4519', '10.1073/pnas.23.10.535', '10.1182/blood.V91.4.1101', '10.1006/dbio.1997.8738', '10.1242/dev.116.2.369', '10.1016/0012-1606(91)90233-S', '10.1101/gr.8.8.826', '10.1016/0012-1606(68)90023-7', '10.1182/blood.V79.4.958.bloodjournal794958', '10.1002/jez.1401340202', '10.1006/dbio.1993.1246', '10.1242/dev.113.4.1207', '10.1016/0012-1606(92)90124-Y', '10.1002/j.1460-2075.1990.tb08305.x', '10.1242/dev.123.1.391', '10.1111/1523-1747.ep12319939', '10.1242/dev.124.12.2377', '10.1242/dev.109.4.911', '10.1006/dbio.1996.0114', '10.1007/s004270050263', '10.1016/S0925-4773(97)00120-2', '10.1002/j.1460-2075.1988.tb02907.x', '10.1242/dev.120.3.495', '10.1242/dev.123.1.311', '10.1006/dbio.1995.1170', '10.2307/2409177', '10.1007/BF00160313', '10.1093/genetics/34.6.708', '10.1016/S0065-2660(08)60549-0', '10.1182/blood.V6.10.892.892', '10.1242/dev.120.3.483', '10.2144/97221bm06', '10.1111/j.1600-0749.1994.tb00017.x', '10.1242/dev.108.4.605', '10.1007/978-1-4612-6164-3', '10.2144/97225bm08', '10.1093/genetics/136.4.1401', '10.1242/dev.115.4.1111', '10.1093/genetics/112.2.311', '10.1126/science.1688471', '10.1242/jcs.103.4.1211', '10.1182/blood.V78.8.1942.1942', '10.1146/annurev.cb.10.110194.001343', '10.1242/dev.125.22.4585', '10.1242/dev.125.21.4205', '10.1242/dev.121.3.731', '10.1002/bies.950190411', '10.1242/dev.123.1.303', '10.1016/S0070-2153(08)60414-7', '10.1242/dev.124.16.3157', '10.1242/dev.122.4.1207', '10.2144/97225bm09', '10.1016/0092-8674(90)90302-U'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym='Development (Cambridge)', publisher=None, query_handler=None),\n",
       " 'Radiation hybrid mapping of the zebrafish genome': Paper(DOI='10.1111/j.1601-5223.2010.02166.x', crossref_json=None, google_schorlar_metadata=None, title='Radiation hybrid mapping of the zebrafish genome', authors=['Neil A Hukriede', 'Lucille Joly', 'Michael Tsang', 'Jennifer Miles', 'Patricia Tellis', 'Jonathan A Epstein', 'William B Barbazuk', 'Frank N Li', 'Barry Paw', 'John H Postlethwait', 'Thomas J Hudson', 'Leonard I Zon', 'John D McPherson', 'Mario Chevrette', 'Igor B Dawid', 'Stephen L Johnson', 'Marc Ekker'], abstract='The zebrafish is an excellent genetic system for the study of vertebrate development and disease. In an effort to provide a rapid and robust tool for zebrafish gene mapping, a panel of radiation hybrids (RH) was produced by fusion of irradiated zebrafish AB9 cells with mouse B78 cells. The overall retention of zebrafish sequences in the 93 RH cell lines that constitute the LN54 panel is 22%. Characterization of the LN54 panel with 849 simple sequence length polymorphism markers, 84 cloned genes and 122 expressed sequence tags allowed the production of an RH map whose total size was 11,501 centiRays. From this value, we estimated the average breakpoint frequency of the LN54 RH panel to correspond to 1 centiRay = 148 kilobase. Placement of a group of 235 unbiased markers on the RH map suggests that the map generated for the LN54 panel, at present, covers 88% of the zebrafish genome\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1101/gr.10.3.350', '10.1016/j.gde.2006.10.009', '10.1158/1055-9965.EPI-04-0686', '10.1002/elps.200600674', '10.1007/s00335-001-2089-8', '10.1126/science.2218528', '10.1073/pnas.082089499', '10.1242/jcs.25.1.17', '10.1007/BF01534584', '10.1186/1471-2164-7-283', '10.1101/gr.6.8.761', '10.1016/j.ygeno.2008.05.013', '10.1101/gr.141700', '10.1016/0165-4608(93)90213-6', '10.1101/gr.5.2.151', '10.1101/gr.7156307', '10.1111/j.1365-2052.2006.01564.x', '10.1186/1472-6750-4-15', '10.1186/1471-2164-7-216', '10.1186/1471-2164-8-310', '10.1093/nar/gkn378', '10.1186/1471-2164-8-44', '10.1016/j.ygeno.2005.11.019', '10.1371/journal.pbio.0040395', '10.1089/cmb.1997.4.487', '10.1002/biot.200600213', '10.1016/0888-7543(92)90147-K', '10.1002/elps.200410121', '10.1007/BF02789107', '10.1046/j.1365-313X.2002.01351.x', '10.1038/nature06884', '10.1007/s003359900593', '10.1073/pnas.89.13.5847'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Cdc53p acts in concert with Cdc4p and Cdc34p to control the G1-to-S-phase transition and identifies a conserved family of proteins': Paper(DOI='10.1128/mcb.16.12.6634', crossref_json=None, google_schorlar_metadata=None, title='Cdc53p acts in concert with Cdc4p and Cdc34p to control the G1-to-S-phase transition and identifies a conserved family of proteins', authors=['Neal Mathias', 'Stephen L Johnson', 'Mark Winey', 'AE Adams', 'Loretta Goetsch', 'John R Pringle', 'Breck Byers', 'Mark G Goebl'], abstract='Regulation of cell cycle progression occurs in part through the targeted degradation of both activating and inhibitory subunits of the cyclin-dependent kinases. During G1, CDC4, encoding a WD-40 repeat protein, and CDC34, encoding a ubiquitin-conjugating enzyme, are involved in the destruction of these regulators. Here we describe evidence indicating that CDC53 also is involved in this process. Mutations in CDC53 cause a phenotype indistinguishable from those of cdc4 and cdc34 mutations, numerous genetic interactions are seen between these genes, and the encoded proteins are found physically associated in vivo. Cdc53p defines a large family of proteins found in yeasts, nematodes, and humans whose molecular functions are uncharacterized. These results suggest a role for this family of proteins in regulating cell cycle proliferation through protein degradation.', conference=None, journal=None, year=None, reference_list=['10.1083/jcb.111.1.131', '10.1016/S0022-2836(05)80360-2', '10.1016/S0021-9258(18)53371-8', '10.1101/SQB.1974.038.01.016', '10.1016/0092-8674(82)90384-1', '10.1002/j.1460-2075.1995.tb07004.x', '10.1073/pnas.92.4.1182', '10.1101/gad.6.9.1695', '10.1126/science.2842867', '10.1128/jb.93.5.1662-1670.1967', '10.1016/0022-2836(71)90420-7', '10.1016/0378-1119(84)90153-7', '10.1016/0955-0674(95)80031-X', '10.1146/annurev.ge.21.120187.001355', '10.1073/pnas.92.7.2563', '10.1146/annurev.ge.26.120192.001143', '10.1083/jcb.111.1.143', '10.1016/S0092-8674(00)81267-2', '10.1126/science.8372350', '10.1016/0076-6879(91)94036-C', '10.1126/science.271.5255.1597', '10.1016/0092-8674(95)90528-6', '10.1128/MCB.15.10.5635', '10.1101/gad.7.5.833', '10.1002/yea.320050503', '10.1126/science.7624798', '10.1038/371742a0', '10.1016/0076-6879(91)94055-H', '10.1146/annurev.cb.08.110192.002525', '10.1038/373081a0', '10.1002/prot.340090304', '10.1016/0092-8674(94)90193-7', '10.1101/gad.7.7a.1160', '10.1016/0092-8674(94)90540-1', '10.1101/gad.9.10.1149', '10.1073/pnas.76.3.1035', '10.1002/j.1460-2075.1993.tb05845.x', '10.1002/j.1460-2075.1992.tb05229.x', '10.1016/S0092-8674(00)80118-X', '10.1083/jcb.114.4.745', '10.1016/0092-8674(90)90361-H', '10.1128/MCB.15.2.731', '10.1016/0022-2836(87)90646-2'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Temperature-sensitive mutations that cause stage-specific defects in Zebrafish fin regeneration.': Paper(DOI='10.1093/genetics/141.4.1583', crossref_json=None, google_schorlar_metadata=None, title='Temperature-sensitive mutations that cause stage-specific defects in Zebrafish fin regeneration.', authors=['Stephen L Johnson', 'James A Weston'], abstract=' When amputated, the fins of adult zebrafish rapidly regenerate the missing tissue. Fin regeneration proceeds through several stages, including wound healing, establishment of the wound epithelium, recruitment of the blastema from mesenchymal cells underlying the wound epithelium, and differentiation and outgrowth of the regenerate. We screened for temperature-sensitive mutations that affect the regeneration of the fin. Seven mutations were identified, including five that fail to regenerate their fins, one that causes slow growth during regeneration, and one that causes dysmorphic bumps or tumors to develop in the regenerating fin. reg5 mutants fail to regenerate their caudal fins, whereas reg6 mutants develop dysmorphic bumps in their regenerates at the restrictive temperature. Temperature-shift experiments indicate that reg5 and reg6 affect different stages of regeneration. The critical period for reg5\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Lactate metabolism is associated with mammalian mitochondria': Paper(DOI='10.1038/nchembio.2172', crossref_json=None, google_schorlar_metadata=None, title='Lactate metabolism is associated with mammalian mitochondria', authors=['Ying-Jr Chen', 'Nathaniel G Mahieu', 'Xiaojing Huang', 'Manmilan Singh', 'Peter A Crawford', 'Stephen L Johnson', 'Richard W Gross', 'Jacob Schaefer', 'Gary J Patti'], abstract='It is well established that lactate secreted by fermenting cells can be oxidized or used as a gluconeogenic substrate by other cells and tissues. It is generally assumed, however, that within the fermenting cell itself, lactate is produced to replenish NAD+ and then is secreted. Here we explore the possibility that cytosolic lactate is metabolized by the mitochondria of fermenting mammalian cells. We found that fermenting HeLa and H460 cells utilize exogenous lactate carbon to synthesize a large percentage of their lipids. Using high-resolution mass spectrometry, we found that both 13C and 2-2H labels from enriched lactate enter the mitochondria. The lactate dehydrogenase (LDH) inhibitor oxamate decreased respiration of isolated mitochondria incubated in lactate, but not of isolated mitochondria incubated in pyruvate. Additionally, transmission electron microscopy (TEM) showed that LDHB localizes to the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1146/annurev.bi.15.070146.001205', '10.1152/japplphysiol.00028.2001', '10.1038/sj.jcbfm.9600127', '10.4161/cc.8.23.10238', '10.3389/fnins.2014.00366', '10.1152/ajpendo.00594.2005', '10.1073/pnas.96.3.1129', '10.1021/bi500763u', '10.1016/0003-9861(87)90507-8', '10.1126/science.1138367', '10.1136/jnnp.32.3.175', '10.1073/pnas.50.2.211', '10.1126/science.143.3609.929', '10.1172/JCI69741', '10.1074/jbc.M113.476648', '10.1097/00005768-200004000-00011', '10.1038/jcbfm.2009.35', '10.1113/jphysiol.2003.058701', '10.1021/ja300222e', '10.1002/ange.19580701707', '10.1152/ajpheart.01113.2003', '10.1371/journal.pone.0002915', '10.1021/ac401140h', '10.1038/nprot.2006.478', '10.1371/journal.pone.0074806', '10.1016/S0076-6879(09)05020-4', '10.1038/nprot.2008.61', '10.1371/journal.pone.0021746', '10.1002/polb.20931', '10.1038/nbt.2377', '10.1093/bioinformatics/btv564', '10.1021/ac403384n'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym='Nature chemical biology', publisher=None, query_handler=None),\n",
       " 'Mapping of Mhc class I and class II regions  to different linkage groups in the zebrafish, Danio rerio': Paper(DOI='10.1007/s002510050251', crossref_json=None, google_schorlar_metadata=None, title='Mapping of Mhc class I and class II regions  to different linkage groups in the zebrafish, Danio rerio', authors=['Jasna Bingulac-Popovic', 'Felipe Figueroa', 'Akie Sato', 'William S Talbot', 'Stephen L Johnson', 'Michael Gates', 'John H Postlethwait', 'Jan Klein'], abstract=' \\u2003The mammalian major histocompatibility complex (Mhc) consists of three closely linked regions, I, II, and III, occupying a single chromosomal segment. The class I loci in region I and the class II loci in region II are related in their structure, function, and evolution. Region III, which is intercalated between regions I and II, contains loci unrelated to the class I and II loci, and to one another. There are indications that a similar Mhc organization exists in birds and amphibians. Here, we demonstrate that in the zebrafish (Danio rerio), a representative of the teleost fishes, the class II loci are divided between two linkage groups which are distinct from the linkage group containing the class I loci. The β2-microglobulin-encoding gene is loosely linked to one of the class II loci. The gene coding for complement factor B, which is one of the region III genes in mammals, is linked neither to the class I nor to the class II loci in\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym='Immunogenetics (New York)', publisher=None, query_handler=None),\n",
       " 'Mutational analysis of endothelin receptor b1 (rose) during neural crest and pigment pattern development in the zebrafish Danio rerio': Paper(DOI='10.1006/dbio.2000.9899', crossref_json=None, google_schorlar_metadata=None, title='Mutational analysis of endothelin receptor b1 (rose) during neural crest and pigment pattern development in the zebrafish Danio rerio', authors=['David M Parichy', 'Eve M Mellgren', 'John F Rawls', 'Susana S Lopes', 'Robert N Kelsh', 'Stephen L Johnson'], abstract='Pigment patterns of fishes are a tractable system for studying the genetic and cellular bases for postembryonic phenotypes. In the zebrafish Danio rerio, neural crest-derived pigment cells generate different pigment patterns during different phases of the life cycle. Whereas early larvae exhibit simple stripes of melanocytes and silver iridophores in a background of yellow xanthophores, this pigment pattern is transformed at metamorphosis into that of the adult, comprising a series of dark melanocyte and iridophore stripes, alternating with light stripes of iridophores and xanthophores. Although several genes have been identified in D. rerio that contribute to the development of both early larval and adult pigment patterns, comparatively little is known about genes that are essential for pattern formation during just one or the other life cycle phase. In this study, we identify the gene responsible for the rose mutant phenotype\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1126/science.282.5394.1711', '10.1038/348730a0', '10.1016/0092-8674(94)90018-3', '10.1002/(SICI)1097-4695(19971120)33:6<749::AID-NEU4>3.0.CO;2-9', '10.1172/JCI524', '10.1086/284991', '10.1002/(SICI)1097-0177(200003)217:3<279::AID-DVDY6>3.0.CO;2-S', '10.1016/S0959-437X(00)00074-5', '10.1086/285370', '10.1093/genetics/151.4.1531', '10.1073/pnas.93.2.867', '10.1210/edrv.21.1.0390', '10.1007/s004270050051', '10.1086/280406', '10.1083/jcb.145.5.927', '10.1111/1523-1747.ep12325522', '10.1016/0092-8674(94)90017-5', '10.1042/bj3140305', '10.1093/hmg/6.10.1613', '10.1093/genetics/139.4.1727', '10.1006/dbio.1995.1004', '10.1093/genetics/142.4.1277', '10.1016/S0021-9258(17)46743-3', '10.1016/S0960-9822(00)00444-9', '10.1242/dev.123.1.369', '10.1242/dev.127.3.515', '10.1006/dbio.2000.9840', '10.1007/BF00848526', '10.1073/pnas.95.24.14214', '10.1046/j.1432-0436.1995.5820133.x', '10.1073/pnas.93.9.3892', '10.1073/pnas.95.6.3024', '10.1016/0092-8674(95)90276-7', '10.1242/dev.126.17.3757', '10.1086/279516', '10.1006/dbio.1997.8738', '10.1016/0012-1606(65)90042-4', '10.1002/jez.1401660312', '10.1016/0012-1606(64)90025-9', '10.1242/dev.127.17.3815', '10.1002/jez.1402270112', '10.1146/annurev.es.25.110194.003041', '10.1016/S0925-4773(98)00048-3', '10.1016/S0925-4773(98)00079-3', '10.1073/pnas.93.18.9645', '10.1242/dev.127.6.1209', '10.1139/o99-006', '10.1242/dev.124.12.2377', '10.1002/(SICI)1097-4687(199807)237:1<53::AID-JMOR5>3.0.CO;2-P', '10.1242/dev.127.14.3031', '10.1242/dev.126.15.3425', '10.1073/pnas.91.15.7159', '10.1038/ng0498-345', '10.1016/0092-8674(94)90016-7', '10.1242/dev.120.3.495', '10.1093/genetics/129.4.1099', '10.1242/dev.122.12.3911', '10.1007/BF00160313', '10.1038/348732a0', '10.1242/dev.120.3.483', '10.1038/990040', '10.1073/pnas.94.24.13105', '10.1038/ng0198-60', '10.1242/dev.115.4.1111', '10.1242/jcs.110.14.1673', '10.1002/(SICI)1097-0177(199812)213:4<452::AID-AJA10>3.0.CO;2-6', '10.1006/excr.1995.1109', '10.1016/S0021-9258(17)37315-5', '10.1002/bies.950150404', '10.1242/dev.121.3.731', '10.1038/332411a0', '10.1242/dev.122.4.1207', '10.1074/jbc.M910307199'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym='Developmental biology (Print)', publisher=None, query_handler=None),\n",
       " 'Fate restriction in the growing and regenerating zebrafish fin': Paper(DOI='10.1016/j.devcel.2011.04.013', crossref_json=None, google_schorlar_metadata=None, title='Fate restriction in the growing and regenerating zebrafish fin', authors=['Shu Tu', 'Stephen L Johnson'], abstract='We use transposon-based clonal analysis to identify the lineage classes that make the adult zebrafish caudal fin. We identify nine distinct lineage classes, including epidermis, melanocyte/xanthophore, iridophore, intraray glia, lateral line, osteoblast, dermal fibroblast, vascular endothelium, and resident blood. These lineage classes argue for distinct progenitors, or organ founding stem cells (FSCs), for each lineage, which retain fate restriction throughout growth of the fin. Thus, distinct FSCs exist for the four neuroectoderm lineages, and dermal fibroblasts are not progenitors for fin ray osteoblasts; however, artery and vein cells derive from a shared lineage in the fin. Transdifferentiation of cells or lineages in the regeneration blastema is often postulated. However, our studies of single progenitors or FSCs reveal no transfating or transdifferentiation between these lineages in the regenerating fin. This result shows that\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1002/dvdy.10248', '10.1038/nchembio778', '10.1007/BF00216033', '10.1007/s11248-007-9152-5', '10.1038/nmeth.1423', '10.1002/dvdy.21100', '10.1182/blood-2010-10-314120', '10.1242/dev.01155', '10.1101/gad.1568407', '10.1016/S0012-1606(03)00186-6', '10.1016/j.ydbio.2006.06.010', '10.1016/0012-1606(62)90002-7', '10.1186/1471-213X-7-42', '10.1083/jcb.4.5.583', '10.1016/0012-1606(61)90009-4', '10.1002/dvdy.20513', '10.1016/j.ydbio.2009.06.003', '10.1016/S0091-679X(04)76018-5', '10.1093/genetics/141.4.1583', '10.1016/S0091-679X(08)61831-2', '10.1006/dbio.1995.1004', '10.1038/nature08152', '10.1046/j.1432-0436.1996.6030139.x', '10.1007/BF00496739', '10.1016/0012-1606(86)90062-X', '10.1242/dev.129.11.2607', '10.1002/dvdy.22113', '10.1002/dvdy.1152', '10.1002/dvdy.10220', '10.1002/dvdy.21417', '10.1159/000259327', '10.1111/j.1440-169X.2009.01144.x', '10.1100/tsw.2006.328', '10.1242/dev.057075', '10.1002/dvdy.21710', '10.1016/S0079-6336(74)80001-X', '10.1016/j.dci.2007.08.009'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['adult stem cells', 'regeneration', 'zebrafish', 'melanocytes'], conference_acronym='Developmental cell', publisher=None, query_handler=None),\n",
       " 'Supervised discrete hashing': Paper(DOI='10.1109/access.2019.2924996', crossref_json=None, google_schorlar_metadata=None, title='Supervised discrete hashing', authors=['Fumin Shen', 'Chunhua Shen', 'Wei Liu', 'Heng Tao Shen'], abstract='Recently, learning based hashing techniques have attracted broad research interests due to the resulting efficient storage and retrieval of images, videos, documents, etc. However, a major difficulty of learning to hash lies in handling the discrete constraints imposed on the needed hash codes, which typically makes hash optimizations very challenging (NP-hard in general). In this work, we propose a new supervised hashing framework, where the learning objective for hashing is to make the optimal binary hash codes for classification. By introducing an auxiliary variable, we reformulate the objective such that it can be solved substantially efficiently by using a regularization algorithm. One of the key steps in the algorithm is to solve the regularization sub-problem associated with the NP-hard binary optimization. We show that with cyclic coordinate descent, the sub-problem admits an analytical solution. As such, a high-quality discrete solution can eventually be obtained in an efficient computing manner, which enables to tackle massive datasets. We evaluate the proposed approach, dubbed Supervised Discrete Hashing (SDH), on four large image datasets, and demonstrate that SDH outperforms the state-of-the-art hashing methods in large-scale image retrieval.', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2010.5539994', '10.1109/TPAMI.2012.48', '10.1016/j.patcog.2017.03.004', '10.1109/CVPR.2015.7298598', '10.1109/TPAMI.2017.2678475', '10.1109/CVPR.2017.247', '10.1109/TMM.2017.2699863', '10.1109/TPAMI.2012.193', '10.1109/CVPR.2014.275', '10.1109/TIP.2017.2729896', '10.1109/TMM.2017.2749160', '10.1109/TCYB.2018.2816791', '10.1561/2200000016', '10.1109/SFCS.1981.27'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Artificial Intelligence', 'Big Data'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Video captioning with attention-based LSTM and semantic consistency': Paper(DOI='10.1109/tmm.2017.2729019', crossref_json=None, google_schorlar_metadata=None, title='Video captioning with attention-based LSTM and semantic consistency', authors=['Lianli Gao', 'Zhao Guo', 'Hanwang Zhang', 'Xing Xu', 'Heng Tao Shen'], abstract='Recent progress in using long short-term memory (LSTM) for image captioning has motivated the exploration of their applications for video captioning. By taking a video as a sequence of features, an LSTM model is trained on video-sentence pairs and learns to associate a video to a sentence. However, most existing methods compress an entire video shot or frame into a static representation, without considering attention mechanism which allows for selecting salient features. Furthermore, existing approaches usually model the translating error, but ignore the correlations between sentence semantics and visual content. To tackle these issues, we propose a novel end-to-end framework named aLSTMs, an attention-based LSTM model with semantic consistency, to transfer videos to natural sentences. This framework integrates attention mechanism with LSTM to capture salient structures of video, and explores the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.neucom.2017.01.064', '10.1109/CVPR.2016.494', '10.1109/CVPR.2015.7298932', '10.1109/CVPRW.2016.61', '10.1007/978-3-319-24947-6_17', '10.21236/ADA623249', '10.1109/ICCV.2015.510', '10.1109/TMM.2012.2233723', '10.1109/TIP.2016.2601260', '10.1109/CVPR.2016.10', '10.1162/neco.1997.9.8.1735', '10.1145/2964284.2964295', '10.1109/ICCV.2015.522', '10.1109/TMM.2013.2271746', '10.1109/CVPR.2015.7299087', '10.1109/CVPR.2016.496', '10.1109/CVPR.2015.7298594', '10.1109/CVPR.2016.308', '10.1109/CVPR.2016.497', '10.1109/ICCV.2015.277', '10.3115/v1/N15-1173', '10.1109/ICCV.2015.515', '10.1109/ICCV.2015.512', '10.1145/2733373.2806314', '10.18653/v1/D15-1166', '10.1109/TBC.2015.2419824', '10.1016/j.patcog.2017.03.021', '10.1109/CVPR.2013.205', '10.1109/CVPR.2015.7298935', '10.1109/CVPR.2016.106', '10.1145/2964284.2967242', '10.1109/CVPR.2016.571', '10.3115/1073083.1073135', '10.1109/CVPR.2016.503', '10.1109/CVPR.2016.117', '10.1109/72.279181', '10.1016/j.jvcir.2016.07.018'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Artificial Intelligence', 'Big Data'], conference_acronym='IEEE transactions on multimedia', publisher=None, query_handler=None),\n",
       " 'Discovery of convoys in trajectory databases': Paper(DOI='10.14778/1453856.1453971', crossref_json=None, google_schorlar_metadata=None, title='Discovery of convoys in trajectory databases', authors=['Hoyoung Jeung', 'Man Lung Yiu', 'Xiaofang Zhou', 'Christian S Jensen', 'Heng Tao Shen'], abstract=\"As mobile devices with positioning capabilities continue to proliferate, data management for so-called trajectory databases that capture the historical movements of populations of moving objects becomes important. This paper considers the querying of such databases for convoys, a convoy being a group of objects that have traveled together for some time. More specifically, this paper formalizes the concept of a convoy query using density-based notions, in order to capture groups of arbitrary extents and shapes. Convoy discovery is relevant for real-life applications in throughput planning of trucks and carpooling of vehicles. Although there has been extensive research on trajectories in the literature, none of this can be applied to retrieve correctly exact convoy result sets. Motivated by this, we develop three efficient algorithms for convoy discovery that adopt the well-known filter-refinement framework. In the filter step, we apply line-simplification techniques on the trajectories and establish distance bounds between the simplified trajectories. This permits efficient convoy discovery over the simplified trajectories without missing any actual convoys. In the refinement step, the candidate convoys are further processed to obtain the actual convoys. Our comprehensive empirical study offers insight into the properties of the paper's proposals and demonstrates that the proposals are effective and efficient on real-world trajectory data.\", conference=None, journal=None, year=None, reference_list=['10.1109/ICDE.2008.4497588', '10.1145/1244002.1244095', '10.1109/ICDE.2006.36', '10.1145/1097064.1097091', '10.1007/s00778-005-0163-7', '10.5555/1316689.1316758', '10.1145/1066157.1066213', '10.3138/FM57-6770-U75U-7727', '10.1145/1183471.1183479', '10.1145/1032222.1032259', '10.1109/TKDE.2007.1054', '10.1111/j.1467-9671.2005.00210.x', '10.1007/11535331_21', '10.1145/1247480.1247546', '10.1145/1014052.1014129', '10.1179/caj.1988.25.2.143', '10.1007/978-3-540-24741-8_44', '10.1145/1150402.1150491', '10.1145/1097064.1097067'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Artificial Intelligence', 'Big Data'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Learning discriminative binary codes for large-scale cross-modal retrieval': Paper(DOI='10.1109/tip.2017.2676345', crossref_json=None, google_schorlar_metadata=None, title='Learning discriminative binary codes for large-scale cross-modal retrieval', authors=['Xing Xu', 'Fumin Shen', 'Yang Yang', 'Heng Tao Shen', 'Xuelong Li'], abstract='Hashing based methods have attracted considerable attention for efficient cross-modal retrieval on large-scale multimedia data. The core problem of cross-modal hashing is how to learn compact binary codes that construct the underlying correlations between heterogeneous features from different modalities. A majority of recent approaches aim at learning hash functions to preserve the pairwise similarities defined by given class labels. However, these methods fail to explicitly explore the discriminative property of class labels during hash function learning. In addition, they usually discard the discrete constraints imposed on the to-be-learned binary codes, and compromise to solve a relaxed problem with quantization to obtain the approximate binary solution. Therefore, the binary codes generated by these methods are suboptimal and less discriminative to different classes. To overcome these drawbacks, we\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/2911996.2912056', '10.1109/TIP.2016.2612883', '10.1109/TIP.2016.2564638', '10.1109/TMM.2015.2508146', '10.1007/s11263-013-0658-4', '10.1162/089976600300015349', '10.1162/0899766042321814', '10.1145/2463676.2465274', '10.1145/2600428.2609610', '10.1109/CVPR.2014.267', '10.1145/997817.997857', '10.1109/TIP.2014.2326010', '10.1109/TIP.2015.2405340', '10.1109/TMM.2015.2390499', '10.1007/s11263-011-0494-3', '10.1145/2733373.2806346', '10.1109/CVPR.2015.7299011', '10.1109/TIP.2015.2403240', '10.1109/CVPR.2012.6247923', '10.1109/TMM.2014.2323014', '10.1145/1873951.1873987', '10.1109/TIP.2016.2577885', '10.1016/j.patcog.2006.04.045', '10.1109/TPAMI.2012.193', '10.1145/1646396.1646452', '10.1109/ICDE.2011.5767837', '10.1145/2964284.2964319', '10.1109/TMM.2013.2291214', '10.1109/TPAMI.2013.225', '10.1145/2339530.2339678', '10.1109/ICCV.2015.219', '10.1145/1460096.1460104', '10.1109/CVPR.2010.5539928', '10.1109/CVPR.2015.7298598', '10.1109/TIP.2015.2419074'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Artificial Intelligence', 'Big Data'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Unsupervised deep hashing with similarity-adaptive and discrete optimization': Paper(DOI='10.1109/tpami.2018.2789887', crossref_json=None, google_schorlar_metadata=None, title='Unsupervised deep hashing with similarity-adaptive and discrete optimization', authors=['Fumin Shen', 'Yan Xu', 'Li Liu', 'Yang Yang', 'Zi Huang', 'Heng Tao Shen'], abstract='Recent vision and learning studies show that learning compact hash codes can facilitate massive data processing with significantly reduced storage and computation. Particularly, learning deep hash functions has greatly improved the retrieval performance, typically under the semantic supervision. In contrast, current unsupervised deep hashing algorithms can hardly achieve satisfactory performance due to either the relaxed optimization or absence of similarity-sensitive objective. In this work, we propose a simple yet effective unsupervised hashing framework, named Similarity-Adaptive Deep Hashing (SADH), which alternatingly proceeds over three training modules: deep hash model training, similarity graph updating and binary code optimization. The key difference from the widely-used two-step hashing method is that the output representations of the learned deep model help update the similarity graph matrix\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TMM.2017.2699863', '10.1109/TIP.2015.2405340', '10.1109/CVPR.2014.130', '10.1109/CVPR.2016.553', '10.1109/CVPR.2015.7298598', '10.1109/CVPR.2017.712', '10.1023/A:1011139631724', '10.1109/CVPR.2017.247', '10.1109/CVPR.2017.546', '10.1109/ICCV.2015.125', '10.1561/2200000016', '10.1109/CVPR.2014.253', '10.1109/CVPR.2013.313', '10.1109/TCYB.2016.2608906', '10.1109/CVPR.2016.133', '10.1109/CVPR.2016.227', '10.1109/CVPR.2016.641', '10.1145/2502081.2502107', '10.1109/CVPR.2016.165', '10.1145/2911451.2911502', '10.1109/TKDE.2017.2701825', '10.1109/TPAMI.2017.2666812', '10.1109/TIP.2017.2676345', '10.1109/CVPR.2011.5995432', '10.1109/TIP.2017.2652730', '10.1109/TIP.2016.2612883', '10.1109/TIP.2017.2749147', '10.1145/2647868.2654889', '10.1109/CVPR.2015.7298947', '10.1145/1646396.1646452', '10.1109/ICCV.2017.598', '10.1109/CVPR.2009.5206848', '10.1109/CVPR.2017.449', '10.1109/CVPR.2017.572', '10.1109/TIP.2016.2617081', '10.1109/TPAMI.2017.2699960', '10.1109/TPAMI.2012.48', '10.1109/TPAMI.2011.103', '10.1109/TPAMI.2006.134', '10.1109/TPAMI.2007.1096'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Artificial Intelligence', 'Big Data'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Beyond frame-level CNN: saliency-aware 3-D CNN with LSTM for video action recognition': Paper(DOI='10.1109/lsp.2016.2611485', crossref_json=None, google_schorlar_metadata=None, title='Beyond frame-level CNN: saliency-aware 3-D CNN with LSTM for video action recognition', authors=['Xuanhan Wang', 'Lianli Gao', 'Jingkuan Song', 'Hengtao Shen'], abstract='Human activity recognition in videos with convolutional neural network (CNN) features has received increasing attention in multimedia understanding. Taking videos as a sequence of frames, a new record was recently set on several benchmark datasets by feeding frame-level CNN sequence features to long short-term memory (LSTM) model for video activity recognition. This recurrent model-based visual recognition pipeline is a natural choice for perceptual problems with time-varying visual input or sequential outputs. However, the above-mentioned pipeline takes frame-level CNN sequence features as input for LSTM, which may fail to capture the rich motion information from adjacent frames or maybe multiple clips. Furthermore, an activity is conducted by a subject or multiple subjects. It is important to consider attention that allows for salient features, instead of mapping an entire frame into a static representation\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TBC.2015.2419824', '10.1109/TBC.2016.2580920', '10.1002/sec.1582', '10.1109/TKDE.2010.99', '10.1109/ICCV.2011.6126543', '10.1109/CVPR.2008.4587756', '10.1109/TPAMI.2016.2577031', '10.1109/CVPR.2015.7298691', '10.1145/1291233.1291311', '10.1109/TIP.2016.2601260', '10.1109/ICCV.2015.522', '10.1109/CVPR.2015.7299066', '10.1109/TCYB.2015.2403356', '10.1007/s00530-015-0494-1', '10.1109/TIP.2014.2332764', '10.1109/CVPR.2014.223', '10.1109/TPAMI.2012.59', '10.1109/CVPR.2015.7298878', '10.1109/VSPETS.2005.1570899', '10.1109/CVPR.2015.7298935', '10.1109/ICCV.2015.510', '10.1109/CVPR.2015.7298961', '10.1109/ICCV.2013.441', '10.1016/j.neucom.2015.08.115'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Artificial Intelligence', 'Big Data'], conference_acronym='IEEE signal processing letters', publisher=None, query_handler=None),\n",
       " 'Transfer independently together: A generalized framework for domain adaptation': Paper(DOI='10.1109/tcyb.2018.2820174', crossref_json=None, google_schorlar_metadata=None, title='Transfer independently together: A generalized framework for domain adaptation', authors=['Jingjing Li', 'Ke Lu', 'Zi Huang', 'Lei Zhu', 'Heng Tao Shen'], abstract='Currently, unsupervised heterogeneous domain adaptation in a generalized setting, which is the most common scenario in real-world applications, is under insufficient exploration. Existing approaches either are limited to special cases or require labeled target samples for training. This paper aims to overcome these limitations by proposing a generalized framework, named as transfer independently together (TIT). Specifically, we learn multiple transformations, one for each domain  (independently) , to map data onto a shared latent space, where the domains are well aligned. The multiple transformations are jointly optimized in a unified framework  (together)  by an effective formulation. In addition, to learn robust transformations, we further propose a novel landmark selection algorithm to reweight samples, i.e., increase the weight of pivot samples and decrease the weight of outliers. Our landmark selection is based\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICDM.2007.89', '10.1109/CVPR.2013.53', '10.1109/TIP.2017.2749147', '10.1109/TPAMI.2018.2789887', '10.1007/978-3-642-15561-1_16', '10.1109/ICCV.2013.274', '10.1109/TCYB.2016.2565898', '10.1109/TPAMI.2015.2435740', '10.1109/TPAMI.2009.57', '10.1109/TNN.2010.2091281', '10.1109/CVPR.2015.7298600', '10.1109/TPAMI.2016.2599532', '10.1109/TCYB.2015.2502483', '10.1109/TCSVT.2016.2539541', '10.1145/1646396.1646452', '10.1109/CVPR.2009.5206848', '10.1007/s11263-014-0719-3', '10.1109/ICCV.2013.368', '10.1007/978-3-319-46466-4_33', '10.1109/TCYB.2013.2281451', '10.1109/TPAMI.2005.55', '10.1109/TPAMI.2011.265', '10.1109/ICDM.2010.65', '10.1609/aaai.v25i1.8090', '10.1007/978-3-319-23525-7_32', '10.1109/TPAMI.2013.167', '10.1109/CVPR.2016.549', '10.1007/978-3-319-46454-1_25', '10.1109/CVPR.2014.183', '10.1109/TCYB.2014.2305701', '10.1109/TKDE.2009.191', '10.1109/TKDE.2014.2373376', '10.1109/ICCV.2011.6126344', '10.1109/TPAMI.2007.250598', '10.1145/1961189.1961199'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Artificial Intelligence', 'Big Data'], conference_acronym='IEEE transactions on cybernetics (Print)', publisher=None, query_handler=None),\n",
       " 'Effective multiple feature hashing for large-scale near-duplicate video retrieval': Paper(DOI='10.1109/tmm.2013.2271746', crossref_json=None, google_schorlar_metadata=None, title='Effective multiple feature hashing for large-scale near-duplicate video retrieval', authors=['Jingkuan Song', 'Yi Yang', 'Zi Huang', 'H Shen', 'Jiebo Luo'], abstract='Near-duplicate video retrieval (NDVR) has recently attracted much research attention due to the exponential growth of online videos. It has many applications, such as copyright protection, automatic video tagging and online video monitoring. Many existing approaches use only a single feature to represent a video for NDVR. However, a single feature is often insufficient to characterize the video content. Moreover, while the accuracy is the main concern in previous literatures, the scalability of NDVR algorithms for large scale video datasets has been rarely addressed. In this paper, we present a novel approach-Multiple Feature Hashing (MFH) to tackle both the accuracy and the scalability issues of NDVR. MFH preserves the local structural information of each individual feature and also globally considers the local structures for all the features to learn a group of hash functions to map the video keyframes into the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TMM.2009.2012919', '10.1145/1282280.1282289', '10.1145/1806907.1806912', '10.1145/1459359.1459506', '10.1145/2072298.2072354', '10.1109/TCSVT.2009.2017400', '10.1109/CVPR.2010.5539994', '10.1145/1571941.1572009', '10.1109/CVPR.2008.4587633', '10.1109/TMM.2010.2050737', '10.1145/1508850.1508855', '10.1145/1291233.1291280', '10.1109/CVPR.2008.4587841', '10.1109/TPAMI.2009.151', '10.1145/1282280.1282336', '10.1109/TMM.2012.2199970', '10.1145/1101149.1101193', '10.1145/2037661.2037666', '10.1145/1807167.1807216', '10.1109/ICDE.2009.17', '10.1145/1631272.1631280', '10.1145/1291233.1291271', '10.1109/TMM.2010.2046265', '10.1145/997817.997857', '10.1145/1101149.1101236', '10.1145/1989323.1989430', '10.1145/1327452.1327494', '10.1109/TMM.2008.917359', '10.1109/TMM.2009.2021794', '10.1109/TIP.2010.2044958', '10.1145/1835449.1835455', '10.1109/TPAMI.2005.188', '10.1145/1282280.1282309', '10.1109/ICPR.2010.1139', '10.1109/TMM.2008.2009673', '10.1109/CVPR.2010.5540009', '10.1145/1631272.1631298', '10.1145/1873951.1874021', '10.1145/1631272.1631353', '10.1109/ICCV.2003.1238424'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer Vision', 'Artificial Intelligence', 'Big Data'], conference_acronym='IEEE transactions on multimedia', publisher=None, query_handler=None),\n",
       " 'Simple baselines for human pose estimation and tracking': Paper(DOI='10.1007/978-3-030-01231-1_29', crossref_json=None, google_schorlar_metadata=None, title='Simple baselines for human pose estimation and tracking', authors=['Bin Xiao', 'Haiping Wu', 'Yichen Wei'], abstract='There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at https://github. com/leoxiaobin/pose. pytorch.', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2018.00542', '10.1109/CVPR.2014.471', '10.1155/2008/246309', '10.1109/CVPR.2017.143', '10.1109/CVPR.2018.00742', '10.1109/ICCV.2017.137', '10.1109/CVPR.2017.601', '10.1109/ICCV.2017.89', '10.1109/CVPR.2018.00044', '10.1109/ICCV.2017.322', '10.1109/CVPR.2016.90', '10.1109/CVPR.2017.179', '10.1007/978-3-319-10602-1_48', '10.1007/978-3-319-46484-8_29', '10.1109/CVPR.2017.395', '10.1109/CVPR.2016.533', '10.1007/s11263-015-0816-y', '10.1609/aaai.v31i1.11231', '10.1109/CVPR.2014.214', '10.1109/ICCV.2017.144', '10.1109/ICCV.2017.52', '10.1109/CVPR.2017.441'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Vision and Language', 'Machine Learning', 'Human Pose Estimation'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Geodesic saliency using background priors': Paper(DOI='10.1007/978-3-642-33712-3_3', crossref_json=None, google_schorlar_metadata=None, title='Geodesic saliency using background priors', authors=['Yichen Wei', 'Fang Wen', 'Wangjiang Zhu', 'Jian Sun'], abstract=' Generic object level saliency detection is important for many vision tasks. Previous approaches are mostly built on the prior that “appearance contrast between objects and backgrounds is high”. Although various computational models have been developed, the problem remains challenging and huge behavioral discrepancies between previous approaches can be observed. This suggest that the problem may still be highly ill-posed by using this prior only. In this work, we tackle the problem from a different viewpoint: we focus more on the background instead of the object. We exploit two common priors about backgrounds in natural images, namely boundary and connectivity priors, to provide more clues for the problem. Accordingly, we propose a novel saliency measure called geodesic saliency. It is intuitive, easy to interpret and allows fast implementation. Furthermore, it is complementary to\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.730558', '10.1016/S0042-6989(01)00250-4', '10.1109/ICCV.2009.5459462', '10.1007/978-3-642-15561-1_3', '10.1167/8.7.13', '10.1109/CVPR.2007.383047', '10.1109/CVPR.2009.5206596', '10.1109/ICCV.2009.5459240', '10.1109/CVPR.2010.5539929', '10.1109/CVPR.2011.5995344', '10.1109/ICCV.2011.6126499', '10.1145/1882262.1866186', '10.1109/ICCV.2009.5459467', '10.1109/ICCV.2009.5459262', '10.1109/CVPR.2010.5540226', '10.1109/CVPR.2007.383267', '10.1109/ICCV.2011.6126264', '10.1109/CVPR.2008.4587440', '10.1109/CVPR.2009.5206567', '10.1109/CVPR.2009.5206540', '10.1016/0167-8655(96)00010-4', '10.1007/978-3-642-15555-0_16', '10.1109/CVPRW.2010.5543739', '10.21236/ADA478319', '10.1007/978-3-540-88682-2_9', '10.1145/1180639.1180824'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['deep learning', 'computer vision', 'medical image analysis'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Integral human pose regression': Paper(DOI='10.1007/s11042-023-15057-x', crossref_json=None, google_schorlar_metadata=None, title='Integral human pose regression', authors=['Xiao Sun', 'Bin Xiao', 'Fangyin Wei', 'Shuang Liang', 'Yichen Wei'], abstract='State-of-the-art human pose estimation methods are based on heat map representation. In spite of the good performance, the representation has a few issues in nature, such as non-differentiable post-processing and quantization error. This work shows that a simple integral operation relates and unifies the heat map representation and joint regression, thus avoiding the above issues. It is differentiable, efficient, and compatible with any heat map based methods. Its effectiveness is convincingly validated via comprehensive ablation experiments under various settings, specifically on 3D pose estimation, for the first time.', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2014.471', '10.1016/j.patcog.2020.107410', '10.1109/cvpr.2017.143', '10.1007/s11760-022-02271-7', '10.1109/CVPR.2016.512', '10.48550/arXiv.1407.3399', '10.48550/arXiv.1705.00389', '10.48550/arXiv.1711.07319', '10.1109/CVPR42600.2020.00047', '10.1109/CVPR42600.2020.00543', '10.48550/arXiv.1702.07432', '10.48550/arXiv.1511.07289', '10.48550/arXiv.1612.00137', '10.1023/B:VISI.0000042934.15159.49', '10.1109/CVPR.2016.90', '10.48550/arXiv.1703.06870', '10.1109/ICCV.2017.322', '10.48550/arXiv.1709.01507', '10.48550/arXiv.1608.06993', '10.1007/978-3-319-46466-4_3', '10.1109/CVPR.2019.01225', '10.1007/s12046-022-01847-w', '10.48550/arXiv.1901.00148', '10.1109/ICSP54964.2022.9778346', '10.48550/arXiv.1312.4400', '10.1109/TPAMI.2016.2572683', '10.1016/j.cag.2019.09.002', '10.23919/FRUCT54823.2022.9770903', '10.48550/arXiv.1908.0868', '10.1007/978-3-319-46484-8_29', '10.48550/arXiv.1611.05424', '10.1007/s11042-017-5537-5', '10.48550/arXiv.1701.01779', '10.1109/CVPR.2016.533', '10.1109/TPAMI.2016.2577031', '10.48550/arXiv.1412.6550', '10.1109/CVPR.2018.00949', '10.1016/j.cviu.2016.09.002', '10.48550/arXiv.1409.1556', '10.48550/arXiv.1704.00159', '10.48550/arXiv.1711.08229', '10.1109/CVPR.2019.00584', '10.1016/j.neucom.2022.01.014', '10.1109/CVPR.2014.214', '10.1109/CVPR42600.2020.01110', '10.1109/INSAI54028.2021.00039', '10.1016/j.neucom.2021.12.083', '10.1109/CVPR.2016.511', '10.1109/TALE52509.2021.9678618', '10.1109/ICIVC55077.2022.9886287', '10.48550/arXiv.1611.05431', '10.1109/CCIS53392.2021.9754658', '10.48550/arXiv.1706.00388'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Vision and Language', 'Machine Learning', 'Human Pose Estimation'], conference_acronym='Multimedia tools and applications', publisher=None, query_handler=None),\n",
       " 'Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation': Paper(DOI='10.1007/978-3-030-01249-6_34', crossref_json=None, google_schorlar_metadata=None, title='Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation', authors=['Sachin Mehta', 'Mohammad Rastegari', 'Anat Caspi', 'Linda Shapiro', 'Hannaneh Hajishirzi'], abstract='We introduce a fast and efficient convolutional neural network, ESPNet, for semantic segmentation of high resolution images under resource constraints. ESPNet is based on a new convolutional module, efficient spatial pyramid (ESP), which is efficient in terms of computation, memory, and power. ESPNet is 22 times faster (on a standard GPU) and 180 times smaller than the state-of-the-art semantic segmentation network PSPNet, while its category-wise accuracy is only 8% less. We evaluated EPSNet on a variety of semantic segmentation datasets including Cityscapes, PASCAL VOC, and a breast biopsy whole slide image dataset. Under the same constraints on memory and computation, ESPNet outperforms all the current efficient CNN networks such as MobileNet, ShuffleNet, and ENet on both standard metrics and our newly introduced performance metrics that measure efficiency on edge devices. Our network can process high resolution images at a rate of 112 and 9 frames per second on a standard GPU and edge device, respectively.', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2017.660', '10.1007/978-3-319-10578-9_23', '10.1109/TPAMI.2017.2699184', '10.5244/C.23.84', '10.1177/0278364913491297', '10.1109/CVPR.2016.350', '10.1109/CVPR.2015.7298925', '10.1109/ICCVW.2013.36', '10.15607/RSS.2017.XIII.013', '10.1007/978-3-319-10599-4_45', '10.1109/CVPR.2015.7298594', '10.1109/CVPR.2016.308', '10.1609/aaai.v31i1.11231', '10.1109/CVPR.2017.634', '10.1109/CVPR.2017.195', '10.1109/CVPR.2018.00716', '10.1109/CVPR.2017.75', '10.1109/TITS.2017.2750080', '10.1109/CVPR.2016.521', '10.1007/978-3-030-01219-9_25', '10.5244/C.28.88', '10.1007/978-3-319-46493-0_32', '10.1109/SiPS.2014.6986082', '10.1109/CVPR.2017.98', '10.1007/978-3-642-75988-8_28', '10.1109/WACV.2018.00078', '10.1109/WACV.2018.00163', '10.1007/978-3-540-74690-4_56', '10.1109/TPAMI.2016.2644615', '10.1007/978-3-319-24574-4_28', '10.1109/CVPR.2015.7298642', '10.1109/CVPR.2015.7299025', '10.1007/978-3-319-46448-0_23', '10.1109/CVPR.2017.549', '10.1109/CVPR.2015.7298965', '10.1109/ICCV.2015.178', '10.1109/CVPR.2016.90', '10.1109/ICCV.2015.123', '10.1109/ICCV.2017.534', '10.1007/s11263-009-0275-4', '10.1109/ICCV.2011.6126343', '10.1007/978-3-319-10602-1_48', '10.1109/IISWC.2014.6983059', '10.1109/IISWC.2015.13', '10.1007/978-3-319-46487-9_32', '10.1007/978-3-030-00934-2_99'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'pattern recogntion', 'medical image analysis'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'A new connected components algorithm for virtual memory computers': Paper(DOI='10.1016/0146-664x(82)90018-1', crossref_json=None, google_schorlar_metadata=None, title='A new connected components algorithm for virtual memory computers', authors=['Ronald Lumia', 'Linda Shapiro', 'Oscar Zuniga'], abstract='A new algorithm for calculating the connected components of a binary image is presented, and a proof of correctness is given. For a large image, this algorithm required 1 hour of CPU time while the standard technique used over 36 hours. The storage requirements for this new algorithm are appropriate for small minicomputers as well as for larger machines.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'pattern recogntion', 'medical image analysis'], conference_acronym='Computer graphics and image processing (Print)', publisher=None, query_handler=None),\n",
       " 'View-based rendering: Visualizing real objects from scanned range and color data': Paper(DOI='10.1007/978-3-7091-6858-5_3', crossref_json=None, google_schorlar_metadata=None, title='View-based rendering: Visualizing real objects from scanned range and color data', authors=['Kari Pulli', 'Hugues Hoppe', 'Michael Cohen', 'Linda Shapiro', 'Tom Duchamp', 'Werner Stuetzle'], abstract=' Modeling arbitrary real objects is difficult and rendering textured models typically does not result in realistic images. We describe a new method for displaying scanned real objects, called view-based rendering. The method takes as input a collection of colored range images covering the object and creates a collection of partial object models. These partial models are rendered separately using traditional graphics hardware and blended together using various weights and soft z-buffering. We demonstrate interactive viewing of real, non-trivial objects that would be difficult to model using traditional methods.', conference=None, journal=None, year=None, reference_list=['10.1145/218380.218395', '10.1016/0262-8856(92)90066-C', '10.1145/253284.253298', '10.1145/237170.237200', '10.1145/253284.253292', '10.1007/978-3-7091-9430-0_8'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Augmented Reality', 'Computational Photography', 'Mobile Visual Computing'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'A metric for comparing relational descriptions': Paper(DOI='10.1109/tpami.1985.4767621', crossref_json=None, google_schorlar_metadata=None, title='A metric for comparing relational descriptions', authors=['Linda G Shapiro', 'Robert M Haralick'], abstract='Relational models are frequently used in high-level computer vision. Finding a correspondence between a relational model and an image description is an important operation in the analysis of scenes. In this paper the process of finding the correspondence is formalized by defining a general relational distance measure that computes a numeric distance between any two relational descriptions-a model and an image description, two models, or two image descriptions. The distance measure is proved to be a metric, and is illustrated with examples of distance between object models. A variant measure used in our past studies is shown not to be a metric.', conference=None, journal=None, year=None, reference_list=['10.1016/0020-0255(74)90008-5', '10.1016/0004-3702(77)90006-6', '10.1109/TSMC.1976.4309519', '10.1109/TSMC.1983.6313167', '10.1109/TPAMI.1980.4766989', '10.1109/TPAMI.1981.4767144', '10.1016/0167-8655(82)90025-3', '10.1109/TPAMI.1979.4766876', '10.1145/359642.359654', '10.1109/TPAMI.1979.4766903', '10.1016/0004-3702(80)90051-X', '10.1016/B978-0-12-737140-5.50006-3', '10.1016/0004-3702(77)90007-8', '10.1016/0031-3203(84)90068-2', '10.1109/TPAMI.1979.4766871', '10.1109/TPAMI.1982.4767312', '10.1145/321921.321925'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'pattern recogntion', 'medical image analysis'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Unsupervised template learning for fine-grained object recognition': Paper(DOI='10.2139/ssrn.3499468', crossref_json=None, google_schorlar_metadata=None, title='Unsupervised template learning for fine-grained object recognition', authors=['Shulin Yang', 'Liefeng Bo', 'Jue Wang', 'Linda Shapiro'], abstract='Fine-grained recognition refers to a subordinate level of recognition, such are recognizing different species of birds, animals or plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape or structure shared within a category, and the differences are in the details of the object parts. We suggest that the key to identifying the fine-grained differences lies in finding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the co-occurence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classification. Learning of the template model is efficient, and the recognition results we achieve significantly outperform the state-of-the-art algorithms.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'pattern recogntion', 'medical image analysis'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Y-Net: joint segmentation and classification for diagnosis of breast biopsy images': Paper(DOI='10.1007/978-3-030-00934-2_99', crossref_json=None, google_schorlar_metadata=None, title='Y-Net: joint segmentation and classification for diagnosis of breast biopsy images', authors=['Sachin Mehta', 'Ezgi Mercan', 'Jamen Bartlett', 'Donald Weaver', 'Joann G Elmore', 'Linda Shapiro'], abstract=' In this paper, we introduce a conceptually simple network for generating discriminative tissue-level segmentation masks for the purpose of breast cancer diagnosis. Our method efficiently segments different types of tissues in breast biopsy images while simultaneously predicting a discriminative map for identifying important areas in an image. Our network, Y-Net, extends and generalizes U-Net by adding a parallel branch for discriminative map generation and by supporting convolutional block modularity, which allows the user to adjust network efficiency without altering the network topology. Y-Net delivers state-of-the-art segmentation accuracy while learning  fewer parameters than its closest competitors. The addition of descriptive power from Y-Net’s discriminative segmentation masks improve diagnostic classification accuracy by 7% over state-of-the-art methods for diagnostic classification. Source\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2016.2644615', '10.1001/jama.2015.1405', '10.1109/TMI.2016.2613019', '10.1109/CVPR.2016.90', '10.1109/ICCV.2017.322', '10.1109/CVPR.2016.266', '10.1007/978-3-030-01249-6_34', '10.1109/WACV.2018.00078', '10.1007/978-3-319-24574-4_28', '10.1109/CVPR.2017.660'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'pattern recogntion', 'medical image analysis'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Automated insect identification through concatenated histograms of local appearance features: feature vector generation and region detection for deformable objects': Paper(DOI='10.1007/s00138-007-0086-y', crossref_json=None, google_schorlar_metadata=None, title='Automated insect identification through concatenated histograms of local appearance features: feature vector generation and region detection for deformable objects', authors=['Natalia Larios', 'Hongli Deng', 'Wei Zhang', 'Matt Sarpola', 'Jenny Yuen', 'Robert Paasch', 'Andrew Moldenke', 'David A Lytle', 'Salvador Ruiz Correa', 'Eric N Mortensen', 'Linda G Shapiro', 'Thomas G Dietterich'], abstract=' This paper describes a computer vision approach to automated rapid-throughput taxonomic identification of stonefly larvae. The long-term objective of this research is to develop a cost-effective method for environmental monitoring based on automated identification of indicator species. Recognition of stonefly larvae is challenging because they are highly articulated, they exhibit a high degree of intraspecies variation in size and color, and some species are difficult to distinguish visually, despite prominent dorsal patterning. The stoneflies are imaged via an apparatus that manipulates the specimens into the field of view of a microscope so that images are obtained under highly repeatable conditions. The images are then classified through a process that involves (a) identification of regions of interest, (b) representation of those regions as SIFT vectors (Lowe, in Int J Comput Vis 60(2):91–110, 2004) (c\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF00058655', '10.1007/BFb0054769', '10.1017/S0007485399000334', '10.1098/rstb.2003.1442', '10.5244/C.2.23', '10.2307/1467832', '10.1017/S1367943002002299', '10.1109/ICCV.2005.66', '10.1007/978-3-540-24670-1_18', '10.1007/11585978_11', '10.1109/CVPR.2005.272', '10.1023/B:VISI.0000029664.99615.94', '10.1016/j.imavis.2004.02.006', '10.1023/B:VISI.0000027790.02288.f2', '10.1007/978-3-540-24671-8_6', '10.1109/TPAMI.2006.54', '10.1023/A:1008162616689', '10.1109/ICCV.2005.63', '10.1109/34.659930', '10.1109/34.655648', '10.5244/C.14.38', '10.1023/B:VISI.0000020671.28016.e8', '10.1109/34.87344', '10.1109/ICPR.2006.1195'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'pattern recogntion', 'medical image analysis'], conference_acronym='Machine vision and applications', publisher=None, query_handler=None),\n",
       " 'Decomposition of two-dimensional shapes by graph-theoretic clustering': Paper(DOI='10.1109/tpami.1979.4766871', crossref_json=None, google_schorlar_metadata=None, title='Decomposition of two-dimensional shapes by graph-theoretic clustering', authors=['Linda G Shapiro', 'Robert M Haralick'], abstract='This paper describes a technique for transforming a twodimensional shape into a binary relation whose clusters represent the intuitively pleasing simple parts of the shape. The binary relation can be defined on the set of boundary points of the shape or on the set of line segments of a piecewise linear approximation to the boundary. The relation includes all pairs of vertices (or segments) such that the line segment joining the pair lies entirely interior to the boundary of the shape. The graph-theoretic clustering method first determines dense regions, which are local regions of high compactness, and then forms clusters by merging together those dense regions having high enough overlap. Using this procedure on handdrawn colon shapes copied from an X-ray and on handprinted characters, the parts determined by the clustering often correspond well to decompositions that a human might make.', conference=None, journal=None, year=None, reference_list=['10.1016/B978-0-12-737140-5.50020-8', '10.2172/4623800', '10.1016/0146-664X(74)90023-9', '10.1016/0031-3203(68)90006-X', '10.1016/0031-3203(72)90016-7', '10.1109/T-C.1974.224041', '10.1109/TSMC.1975.4309402', '10.1016/S0146-664X(72)80014-5', '10.1109/TIT.1962.1057695', '10.1145/356625.356627', '10.1109/T-C.1975.224276', '10.1145/321607.321608', '10.1145/321479.321480', '10.1016/S0020-7373(74)80041-6'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', 'pattern recogntion', 'medical image analysis'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " '人工神经网络与模拟进化计算（Aritificial Nerual Network and Evolutionary Computing）': Paper(DOI='10.3788/lop53.102801', crossref_json=None, google_schorlar_metadata=None, title='人工神经网络与模拟进化计算（Aritificial Nerual Network and Evolutionary Computing）', authors=['阎平凡， 张长水(Pingfan YanChangshui Zhang)'], abstract=None, conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Signal Processing', 'Computer Vision', 'Artificial Intelligence'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'A Bayesian network approach to traffic flow forecasting': Paper(DOI='10.1109/tits.2006.869623', crossref_json=None, google_schorlar_metadata=None, title='A Bayesian network approach to traffic flow forecasting', authors=['Shiliang Sun', 'Changshui Zhang', 'Guoqiang Yu'], abstract='A new approach based on Bayesian networks for traffic flow forecasting is proposed. In this paper, traffic flows among adjacent road links in a transportation network are modeled as a Bayesian network. The joint probability distribution between the cause nodes (data utilized for forecasting) and the effect node (data to be forecasted) in a constructed Bayesian network is described as a Gaussian mixture model (GMM) whose parameters are estimated via the competitive expectation maximization (CEM) algorithm. Finally, traffic flow forecasting is performed under the criterion of minimum mean square error (mmse). The approach departs from many existing traffic flow forecasting models in that it explicitly includes information from adjacent road links to analyze the trends of the current link statistically. Furthermore, it also encompasses the issue of traffic flow forecasting when incomplete data exist. Comprehensive\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/S0968-090X(97)82903-8', '10.1016/S0968-090X(01)00004-3', '10.1109/IVS.2003.1212910', '10.1109/IVS.2000.898384', '10.1007/978-1-4757-3502-4', '10.1002/0471721182', '10.1016/S0031-3203(03)00140-7', '10.1016/0191-2615(84)90002-X', '10.3141/1678-22', '10.1061/(ASCE)0733-947X(1991)117:2(178)', '10.1109/ISCC.1998.702424', '10.1080/03081068808717359', '10.1061/(ASCE)0733-947X(1997)123:4(261)', '10.1002/9780470316436', '10.1126/science.290.5500.2319', '10.1007/978-1-4757-1904-8', '10.1007/978-1-4757-2440-0'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Signal Processing', 'Computer Vision', 'Artificial Intelligence'], conference_acronym='IEEE transactions on intelligent transportation systems (Print)', publisher=None, query_handler=None),\n",
       " 'Frequency recognition based on canonical correlation analysis for SSVEP-based BCIs': Paper(DOI='10.1109/tbme.2006.889197', crossref_json=None, google_schorlar_metadata=None, title='Frequency recognition based on canonical correlation analysis for SSVEP-based BCIs', authors=['Zhonglin Lin', 'Changshui Zhang', 'Wei Wu', 'Xiaorong Gao'], abstract='Canonical correlation analysis (CCA) is applied to analyze the frequency components of steady-state visual evoked potentials (SSVEP) in electroencephalogram (EEG). The essence of this method is to extract a narrowband frequency component of SSVEP in EEG. A recognition approach is proposed based on the extracted frequency features for an SSVEP-based brain computer interface (BCI). Recognition Results of the approach were higher than those using a widely used fast Fourier transform (FFT)-based spectrum estimation method', conference=None, journal=None, year=None, reference_list=['10.1109/TBME.2002.803536', '10.1016/S0006-3495(04)74250-2', '10.1002/1522-2594(200102)45:2<323::AID-MRM1041>3.0.CO;2-#', '10.1109/ICNIC.2005.1499837', '10.1088/1741-2560/2/4/008', '10.1109/TBME.2004.827827', '10.1109/CNE.2003.1196906', '10.1109/TNSRE.2003.814449', '10.2307/2333955', '10.1017/CBO9780511612336', '10.1109/86.847819', '10.1109/HUICS.1996.549486', '10.3109/00207459008986630', '10.1109/CNE.2003.1196903', '10.1162/153244303322753706'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Signal Processing', 'Computer Vision', 'Artificial Intelligence'], conference_acronym='IEEE transactions on biomedical engineering (Print)', publisher=None, query_handler=None),\n",
       " 'Trace Ratio Criterion for Feature Selection.': Paper(DOI='10.1109/access.2018.2888924', crossref_json=None, google_schorlar_metadata=None, title='Trace Ratio Criterion for Feature Selection.', authors=['Feiping Nie', 'Shiming Xiang', 'Yangqing Jia', 'Changshui Zhang', 'Shuicheng Yan'], abstract='Fisher score and Laplacian score are two popular feature selection algorithms, both of which belong to the general graph-based feature selection framework. In this framework, a feature subset is selected based on the corresponding score (subset-level score), which is calculated in a trace ratio form. Since the number of all possible feature subsets is very huge, it is often prohibitively expensive in computational cost to search in a brute force manner for the feature subset with the maximum subset-level score. Instead of calculating the scores of all the feature subsets, traditional methods calculate the score for each feature, and then select the leading features based on the rank of these feature-level scores. However, selecting the feature subset based on the feature-level score cannot guarantee the optimum of the subset-level score. In this paper, we directly optimize the subset-level score, and propose a novel algorithm to efficiently find the global optimal feature subset such that the subset-level score is maximized. Extensive experiments demonstrate the effectiveness of our proposed algorithm in comparison with the traditional methods for feature selection.', conference=None, journal=None, year=None, reference_list=['10.21437/Interspeech.2010-739', '10.1007/3-540-57868-4_57', '10.1109/TPAMI.2007.1093', '10.1016/j.patcog.2014.08.004', '10.1016/j.isatra.2015.12.011', '10.1016/j.patcog.2013.02.012', '10.1145/1015330.1015352', '10.1109/34.9121', '10.1016/j.patcog.2008.05.018', '10.1109/ICDEW.2006.145', '10.1016/j.patcog.2009.12.013', '10.1109/TPAMI.2009.190', '10.1016/j.neucom.2016.11.047', '10.1109/TAI.1995.479783', '10.1016/B978-1-55860-247-2.50037-1', '10.1016/S0031-3203(99)00139-9', '10.1137/090776603', '10.1109/CVPR.2007.382983', '10.1063/1.1755673', '10.1109/TPAMI.2007.250598'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Signal Processing', 'Computer Vision', 'Artificial Intelligence'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Traffic sign recognition with hinge loss trained convolutional neural networks': Paper(DOI='10.1109/tits.2014.2308281', crossref_json=None, google_schorlar_metadata=None, title='Traffic sign recognition with hinge loss trained convolutional neural networks', authors=['Junqi Jin', 'Kun Fu', 'Changshui Zhang'], abstract=\"Traffic sign recognition (TSR) is an important and challenging task for intelligent transportation systems. We describe the details of our model's architecture for TSR and suggest a hinge loss stochastic gradient descent (HLSGD) method to train convolutional neural networks (CNNs). Our CNN consists of three stages (70–110–180) with 1\\u2009162\\u2009284 trainable parameters. The HLSGD is evaluated on the German Traffic Sign Recognition Benchmark, which offers a faster and more stable convergence and a state-of-the-art recognition rate of 99.65%. We write a graphics processing unit package to train several CNNs and establish the final classifier in an ensemble way.\", conference=None, journal=None, year=None, reference_list=['10.1113/jphysiol.1968.sp008455', '10.1007/BF00344251', '10.1109/5.726791', '10.1109/IJCNN.2011.6033458', '10.1109/IJCNN.2011.6033395', '10.1109/IJCNN.2011.6033589', '10.1038/381607a0', '10.1145/1390156.1390294', '10.1109/ICCV.2009.5459469', '10.1109/TITS.2012.2225618', '10.1109/TITS.2010.2051427', '10.1023/B:VISI.0000013087.49260.fb', '10.1109/TITS.2012.2208909', '10.1023/B:VISI.0000029664.99615.94', '10.1109/CVPR.2005.177', '10.1109/TITS.2011.2157497', '10.1109/TITS.2012.2225192', '10.1109/TITS.2012.2209421', '10.1109/ICASSP.2013.6639343', '10.1007/3-540-49430-8_2', '10.1109/CVPR.2012.6248110'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Signal Processing', 'Computer Vision', 'Artificial Intelligence'], conference_acronym='IEEE transactions on intelligent transportation systems (Print)', publisher=None, query_handler=None),\n",
       " 'An in-field automatic wheat disease diagnosis system': Paper(DOI='10.1016/j.compag.2017.09.012', crossref_json=None, google_schorlar_metadata=None, title='An in-field automatic wheat disease diagnosis system', authors=['Jiang Lu', 'Jie Hu', 'Guannan Zhao', 'Fenghua Mei', 'Changshui Zhang'], abstract='Crop diseases are responsible for the major production reduction and economic losses in agricultural industry worldwide. Monitoring for health status of crops is critical to control the spread of diseases and implement effective management. This paper presents an in-field automatic wheat disease diagnosis system based on a weakly supervised deep learning framework, i.e. deep multiple instance learning, which achieves an integration of identification for wheat diseases and localization for disease areas with only image-level annotation for training images in wild conditions. Furthermore, a new in-field image dataset for wheat disease, Wheat Disease Database 2017 (WDD2017), is collected to verify the effectiveness of our system. Under two different architectures, i.e. VGG-FCN-VD16 and VGG-FCN-S, our system achieves the mean recognition accuracies of 97.95% and 95.12% respectively over 5-fold cross\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/MRA.2012.2230118', '10.1094/PDIS-03-14-0290-RE', '10.1016/j.biosystemseng.2016.01.017', '10.1364/AO.47.001922', '10.1016/j.biosystemseng.2008.09.030', '10.1016/j.jplph.2006.01.011', '10.5244/C.28.6', '10.1016/S0004-3702(96)00034-3', '10.1016/j.biosystemseng.2016.08.024', '10.1109/CVPR.2014.81', '10.1016/j.compag.2016.07.003', '10.1109/CVPR.2015.7298965', '10.1007/s13593-014-0246-1', '10.3389/fpls.2016.01419', '10.1016/j.rti.2005.03.003', '10.1016/j.compag.2012.11.001', '10.1109/CVPR.2015.7298780', '10.1016/j.jfoodeng.2009.01.014', '10.3390/s16081222', '10.1155/2016/3289801', '10.1109/CVPR.2015.7298594', '10.1007/s11263-013-0620-5', '10.1016/j.compag.2013.04.014', '10.1016/j.patrec.2011.08.003'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Signal Processing', 'Computer Vision', 'Artificial Intelligence'], conference_acronym='Computers and electronics in agriculture', publisher=None, query_handler=None),\n",
       " 'Trace ratio problem revisited': Paper(DOI='10.1109/tnn.2009.2015760', crossref_json=None, google_schorlar_metadata=None, title='Trace ratio problem revisited', authors=['Yangqing Jia', 'Feiping Nie', 'Changshui Zhang'], abstract='Dimensionality reduction is an important issue in many machine learning and pattern recognition applications, and the trace ratio (TR) problem is an optimization problem involved in many dimensionality reduction algorithms. Conventionally, the solution is approximated via generalized eigenvalue decomposition due to the difficulty of the original problem. However, prior works have indicated that it is more reasonable to solve it directly than via the conventional way. In this brief, we propose a theoretical overview of the global optimum solution to the TR problem via the equivalent trace difference problem. Eigenvalue perturbation theory is introduced to derive an efficient algorithm based on the Newton-Raphson method. Theoretical issues on the convergence and efficiency of our algorithm compared with prior literature are proposed, and are further supported by extensive empirical results.', conference=None, journal=None, year=None, reference_list=['10.1109/TNN.2002.806647', '10.1109/TPAMI.2007.250598', '10.1109/TNN.2005.860852', '10.1007/978-3-642-66282-9', '10.1017/CBO9780511810817', '10.1016/S0031-3203(99)00139-9', '10.1002/anac.200410028', '10.1016/S0167-8655(02)00207-6', '10.1016/S0031-3203(00)00162-X', '10.1109/CVPR.2007.382983'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Pattern Recognition', 'Signal Processing', 'Computer Vision', 'Artificial Intelligence'], conference_acronym='IEEE transactions on neural networks', publisher=None, query_handler=None),\n",
       " 'Road extraction by deep residual u-net': Paper(DOI='10.1109/lgrs.2018.2802944', crossref_json=None, google_schorlar_metadata=None, title='Road extraction by deep residual u-net', authors=['Zhengxin Zhang', 'Qingjie Liu', 'Yunhong Wang'], abstract='Road extraction from aerial images has been a hot research topic in the field of remote sensing image analysis. In this letter, a semantic segmentation neural network, which combines the strengths of residual learning and U-Net, is proposed for road area extraction. The network is built with residual units and has similar architecture to that of U-Net. The benefits of this model are twofold: first, residual units ease training of deep networks. Second, the rich skip connections within the network could facilitate information propagation, allowing us to design networks with fewer parameters, however, better performance. We test our network on a public road data set and compare it with U-Net and other two state-of-the-art deep-learning-based road extraction methods. The proposed approach outperforms all the comparing methods, which demonstrates its superiority over recently developed state of the arts.', conference=None, journal=None, year=None, reference_list=['10.1109/LGRS.2016.2524025', '10.14358/PERS.70.12.1365', '10.1109/TGRS.2011.2136381', '10.1109/TPAMI.2016.2577031', '10.1109/IGARSS.2016.7729166', '10.1109/MGRS.2016.2540798', '10.1109/IGARSS.2016.7730419', '10.1109/CVPR.2015.7298594', '10.1109/TGRS.2012.2190078', '10.1016/j.isprsjprs.2017.02.008', '10.1186/s13640-015-0062-9', '10.1371/journal.pone.0138071', '10.1109/TGRS.2017.2669341', '10.1080/01431160802546837', '10.1109/CVPR.2016.90', '10.1109/CVPR.2015.7298965'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Pattern Recognition', 'Image Processing', 'Computer Vision'], conference_acronym='IEEE geoscience and remote sensing letters (Print)', publisher=None, query_handler=None),\n",
       " 'Receptive field block net for accurate and fast object detection': Paper(DOI='10.1007/978-3-030-01252-6_24', crossref_json=None, google_schorlar_metadata=None, title='Receptive field block net for accurate and fast object detection', authors=['Songtao Liu', 'Di Huang', 'Yunhong Wang'], abstract='Current top-performing object detectors depend on deep CNN backbones, such as ResNet-101 and Inception, benefiting from their powerful feature representations but suffering from high computational costs. Conversely, some lightweight model based detectors fulfil real time processing, while their accuracies are often criticized. In this paper, we explore an alternative to build a fast and accurate detector by strengthening lightweight features using a hand-crafted mechanism. Inspired by the structure of Receptive Fields (RFs) in human visual systems, we propose a novel RF Block (RFB) module, which takes the relationship between the size and eccentricity of RFs into account, to enhance the feature discriminability and robustness. We further assemble RFB to the top of SSD, constructing the RFB Net detector. To evaluate its effectiveness, experiments are conducted on two major benchmarks and the results show that RFB Net is able to reach the performance of advanced very deep detectors while keeping the real-time speed. Code is available at https://github. com/ruinmessi/RFBNet.', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.2010.54', '10.1109/ICCV.2017.89', '10.1007/s11263-009-0275-4', '10.1109/ICCV.2015.169', '10.1109/CVPR.2014.81', '10.1109/ICCV.2017.322', '10.1109/ICCV.2015.123', '10.1109/CVPR.2016.90', '10.1109/CVPR.2017.166', '10.1109/TIP.2014.2353814', '10.1109/CVPR.2017.351', '10.1109/CVPR.2017.472', '10.1109/CVPR.2017.106', '10.1109/ICCV.2017.324', '10.1007/978-3-319-10602-1_48', '10.1007/978-3-319-46448-0_2', '10.1109/CVPR.2016.91', '10.1109/CVPR.2017.690', '10.1007/s11263-015-0816-y', '10.1109/ICCV.2017.212', '10.1109/TPAMI.2014.2301163', '10.1609/aaai.v31i1.11231', '10.1109/CVPR.2016.308', '10.1109/CVPR.2015.7298594', '10.1109/CVPR.2008.4587673', '10.1007/s11263-013-0620-5', '10.1016/j.tics.2015.03.009', '10.1109/TIP.2015.2409739', '10.1109/CVPR.2007.382971', '10.1109/CVPR.2018.00716'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Pattern Recognition', 'Image Processing', 'Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Efficient iris recognition by characterizing key local variations': Paper(DOI='10.1109/tip.2004.827237', crossref_json=None, google_schorlar_metadata=None, title='Efficient iris recognition by characterizing key local variations', authors=['Li Ma', 'Tieniu Tan', 'Yunhong Wang', 'Dexin Zhang'], abstract='Unlike other biometrics such as fingerprints and face, the distinct aspect of iris comes from randomly distributed features. This leads to its high reliability for personal identification, and at the same time, the difficulty in effectively representing such details in an image. This paper describes an efficient algorithm for iris recognition by characterizing key local variations. The basic idea is that local sharp variation points, denoting the appearing or vanishing of an important image structure, are utilized to represent the characteristics of the iris. The whole procedure of feature extraction includes two steps: 1) a set of one-dimensional intensity signals is constructed to effectively characterize the most important information of the original two-dimensional image; 2) using a particular class of wavelets, a position sequence of local sharp variation points in such signals is recorded as features. We also present a fast matching scheme\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.608294', '10.1109/34.142909', '10.1109/18.86995', '10.1142/S0219691303000025', '10.1007/BF01246633', '10.1109/5.628669', '10.1109/78.668573', '10.1109/ICPR.2000.906196', '10.4218/etrij.01.0101.0203', '10.1007/3-540-45344-X_47', '10.1109/MAES.2002.1044509', '10.1016/0031-3203(81)90009-1', '10.1109/TPAMI.1986.4767851', '10.1109/18.119727', '10.1109/34.244676', '10.1007/978-1-4615-4519-4', '10.1023/A:1012365806338', '10.1007/3-540-44887-X_81', '10.1007/3-540-44887-X_27', '10.1142/S0218001400000581', '10.1007/3-540-44887-X_97', '10.1109/83.503927', '10.1109/ICIP.2000.899238'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Pattern Recognition', 'Image Processing', 'Computer Vision'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Local binary patterns and its application to facial image analysis: a survey': Paper(DOI='10.1109/tsmcc.2011.2118750', crossref_json=None, google_schorlar_metadata=None, title='Local binary patterns and its application to facial image analysis: a survey', authors=['Di Huang', 'Caifeng Shan', 'Mohsen Ardabilian', 'Yunhong Wang', 'Liming Chen'], abstract='Local binary pattern (LBP) is a nonparametric descriptor, which efficiently summarizes the local structures of images. In recent years, it has aroused increasing interest in many areas of image processing and computer vision and has shown its effectiveness in a number of applications, in particular for facial image analysis, including tasks as diverse as face detection, face recognition, facial expression analysis, and demographic classification. This paper presents a comprehensive survey of LBP methodology, including several more recent variations. As a typical application of the LBP approach, LBP-based facial image analysis is extensively reviewed, while its successful extensions, which deal with various tasks of facial image analysis, are also highlighted.', conference=None, journal=None, year=None, reference_list=['10.1109/IPTA.2008.4743795', '10.1142/9781848161160_0012', '10.1109/AFGR.2008.4813379', '10.1109/ICME.2007.4284844', '10.1007/978-3-540-30126-4_81', '10.1016/j.imavis.2008.08.005', '10.1109/ICIP.2006.312418', '10.1109/TPAMI.2006.244', '10.1007/978-3-540-75690-3_13', '10.1007/11527923_98', '10.1007/978-3-540-76390-1_66', '10.1109/CVPRW.2006.149', '10.1109/34.598228', '10.1007/11564386_5', '10.1109/ICIG.2007.144', '10.1007/978-3-540-76856-2_43', '10.1016/j.patcog.2006.03.013', '10.1016/S1077-3142(03)00073-0', '10.1155/2009/184617', '10.1109/ICASSP.2007.366305', '10.1109/TNN.2002.804287', '10.1109/TSMCB.2009.2029076', '10.1109/TSMCB.2007.911536', '10.1109/ICDSC.2008.4635726', '10.1145/1240624.1240684', '10.1109/TIP.2007.904421', '10.1145/954339.954342', '10.1007/978-3-540-69812-8_66', '10.1007/978-3-540-74549-5_87', '10.1109/TIP.2010.2044957', '10.5244/C.20.90', '10.1016/S0031-3203(99)00032-1', '10.1109/CVPR.2004.1315246', '10.1109/TPAMI.2007.70791', '10.1109/TSMCB.2008.927276', '10.1109/TITS.2008.922882', '10.1109/CISP.2008.520', '10.1109/IVS.2008.4621315', '10.1109/AFGR.1998.670949', '10.1109/TCSVT.2008.924108', '10.1007/s10044-002-0179-1', '10.1016/j.patrec.2008.06.015', '10.1007/BFb0028345', '10.5244/C.22.27', '10.1117/12.776193', '10.1109/AFGR.2004.1301514', '10.1109/CISP.2008.119', '10.2991/jcis.2006.259', '10.1109/AVSS.2005.1577290', '10.1016/j.patcog.2008.08.014', '10.1007/11949619_6', '10.1142/S0129065707001317', '10.1109/AFGR.2008.4813354', '10.1109/CVPR.2005.177', '10.1109/BTAS.2010.5634497', '10.1142/S0129065707001317', '10.1023/B:VISI.0000029664.99615.94', '10.1109/ICNC.2008.94', '10.1007/3-540-44887-X_2', '10.1109/AFGR.2004.1301636', '10.1007/978-3-540-30548-4_21', '10.1109/FGR.2006.72', '10.1109/ICIP.2005.1530205', '10.1007/978-3-540-72523-7_35', '10.1109/ISCAS.2006.1693438', '10.1109/ICPR.2010.305', '10.1109/TPAMI.2007.1110', '10.1155/2007/72316', '10.1109/29.1644', '10.1364/JOSA.70.001297', '10.1109/34.598235', '10.1109/ICASSP.2007.365986', '10.1109/CNNA.2005.1543152', '10.1007/978-3-540-75690-3_18', '10.1109/CNNA.2005.1543196', '10.1016/j.patcog.2007.06.022', '10.1109/ICCV.2005.147', '10.1109/ICPR.2006.163', '10.1109/ICME.2007.4284823', '10.1016/0031-3203(95)00067-4', '10.1109/ICIP.2007.4378908', '10.1109/ICDSC.2007.4357512', '10.1109/TPAMI.2002.1017623', '10.1109/ICPR.2006.80', '10.1007/978-3-540-37258-5_18', '10.1109/ICIG.2004.127', '10.1109/ICPR.2008.4761175', '10.1109/CVPR.2007.383338', '10.1007/BF00994018', '10.1109/ICCV.2007.4409096', '10.1109/TPAMI.2008.79', '10.1109/ICCV.2007.4408854', '10.1109/TIT.1967.1053964', '10.1109/BTAS.2007.4401951', '10.1109/TIP.2008.2011167', '10.1109/TIFS.2009.2026455', '10.1109/ICPR.2004.1334491', '10.1007/s10044-008-0123-0', '10.1109/ICPR.2002.1047431', '10.1016/j.patrec.2007.06.003', '10.1162/jocn.1991.3.1.71', '10.1093/ietisy/e89-d.7.2076', '10.1109/TPAMI.2006.68', '10.1007/978-3-540-75757-3_35', '10.1109/ICCV.2007.4408834', '10.1080/01431160500057723', '10.1016/S0031-3203(02)00052-3', '10.1109/TPAMI.2008.52', '10.1109/FGR.2006.13', '10.1007/978-3-540-74549-5_54', '10.1016/j.imavis.2005.12.021', '10.1007/978-3-540-30541-5_47', '10.1049/el:20071809', '10.1109/CVPR.2001.990517', '10.1109/TPAMI.2007.1014', '10.1007/11579427_33', '10.1007/978-3-540-75690-3_1', '10.1109/CVPR.2007.383459', '10.1007/978-3-540-69905-7_46'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Pattern Recognition', 'Image Processing', 'Computer Vision'], conference_acronym='IEEE transactions on systems, man and cybernetics. Part C, Applications and reviews', publisher=None, query_handler=None),\n",
       " 'Combining face and iris biometrics for identity verification': Paper(DOI='10.1007/3-540-44887-x_93', crossref_json=None, google_schorlar_metadata=None, title='Combining face and iris biometrics for identity verification', authors=['Yunhong Wang', 'Tieniu Tan', 'Anil K Jain'], abstract=' Face and iris identification have been employed in various biometric applications. Besides improving verification performance, the fusion of these two biometrics has several other advantages. We use two different strategies for fusing iris and face classifiers. The first strategy is to compute either an unweighted or weighted sum and to compare the result to a threshold. The second strategy is to treat the matching distances of face and iris classifiers as a two-dimensional feature vector and to use a classifier such as Fisher’s discriminant analysis and a neural network with radial basis function (RBFNN) to classify the vector as being genuine or an impostor. We compare the results of the combined classifier with the results of the individual face and iris classifiers.', conference=None, journal=None, year=None, reference_list=['10.1109/34.244676', '10.1109/34.735803', '10.1007/b117227', '10.1109/34.667881', '10.1109/TIP.2004.827237', '10.1007/s100440200011', '10.1162/jocn.1991.3.1.71'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Pattern Recognition', 'Image Processing', 'Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Iris recognition based on multichannel Gabor filtering': Paper(DOI='10.18535/ijecs/v5i8.32', crossref_json=None, google_schorlar_metadata=None, title='Iris recognition based on multichannel Gabor filtering', authors=['Lia Ma', 'Yunhong Wang', 'Tieniu Tan'], abstract='A new approach for personal identification based on iris recognition is presented in this paper. The body of this paper details the steps of iris recognition, including image preprocessing, feature extraction and classifier design. The proposed algorithm uses a bank of Gabor filters to capture both local and global iris characteristics to form a fixed length feature vector. Iris matching is based on the weighted Euclidean distance between the two corresponding iris vectors and is therefore very fast. Experimental results are reported to demonstrate the performance of the algorithm.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Pattern Recognition', 'Image Processing', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Learning spatial fusion for single-shot object detection': Paper(DOI='10.3390/electronics9091536', crossref_json=None, google_schorlar_metadata=None, title='Learning spatial fusion for single-shot object detection', authors=['Songtao Liu', 'Di Huang', 'Yunhong Wang'], abstract='Pyramidal feature representation is the common practice to address the challenge of scale variation in object detection. However, the inconsistency across different feature scales is a primary limitation for the single-shot detectors based on feature pyramid. In this work, we propose a novel and data driven strategy for pyramidal feature fusion, referred to as adaptively spatial feature fusion (ASFF). It learns the way to spatially filter conflictive information to suppress the inconsistency, thus improving the scale-invariance of features, and introduces nearly free inference overhead. With the ASFF strategy and a solid baseline of YOLOv3, we achieve the best speed-accuracy trade-off on the MS COCO dataset, reporting 38.1% AP at 60 FPS, 42.4% AP at 45 FPS and 43.9% AP at 29 FPS. The code is available at https://github.com/ruinmessi/ASFF', conference=None, journal=None, year=None, reference_list=['10.1145/3065386', '10.1023/B:VISI.0000029664.99615.94', '10.1007/978-3-319-46448-0_2', '10.1609/aaai.v33i01.33019259', '10.1007/s11432-019-2723-1', '10.1109/TPAMI.2016.2577031'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Pattern Recognition', 'Image Processing', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Local intensity variation analysis for iris recognition': Paper(DOI='10.1016/j.patcog.2004.02.001', crossref_json=None, google_schorlar_metadata=None, title='Local intensity variation analysis for iris recognition', authors=['Li Ma', 'Tieniu Tan', 'Yunhong Wang', 'Dexin Zhang'], abstract='As an emerging biometric for human identification, iris recognition has received increasing attention in recent years. This paper makes an attempt to reflect shape information of the iris by analyzing local intensity variations of an iris image. In our framework, a set of one-dimensional (1D) intensity signals is constructed to contain the most important local variations of the original 2D iris image. Gaussian–Hermite moments of such intensity signals reflect to a large extent their various spatial modes and are used as distinguishing features. A resulting high-dimensional feature vector is mapped into a low-dimensional subspace using Fisher linear discriminant, and then the nearest center classifier based on cosine similarity measure is adopted for classification. Extensive experimental results show that the proposed method is effective and encouraging.', conference=None, journal=None, year=None, reference_list=['10.1007/b117227', '10.1023/A:1012365806338', '10.1109/5.628669', '10.1109/34.244676', '10.1142/S0219691303000025', '10.1007/BF01246633', '10.1109/78.668573', '10.4218/etrij.01.0101.0203', '10.1016/1049-9652(92)90027-U', '10.1109/34.485554', '10.1016/S0031-2023(97)00122-2', '10.1117/12.290295', '10.1142/S0218001400000581', '10.1109/34.598228', '10.1109/TIP.2002.999679', '10.1109/34.531802'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Pattern Recognition', 'Image Processing', 'Computer Vision'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'Multi-scale positive sample refinement for few-shot object detection': Paper(DOI='10.1007/978-3-030-58517-4_27', crossref_json=None, google_schorlar_metadata=None, title='Multi-scale positive sample refinement for few-shot object detection', authors=['Jiaxi Wu', 'Songtao Liu', 'Di Huang', 'Yunhong Wang'], abstract=' Few-shot object detection (FSOD) helps detectors adapt to unseen classes with few training instances, and is useful when manual annotation is time-consuming or data acquisition is limited. Unlike previous attempts that exploit few-shot classification techniques to facilitate FSOD, this work highlights the necessity of handling the problem of scale variations, which is challenging due to the unique sample distribution. To this end, we propose a Multi-scale Positive Sample Refinement (MPSR) approach to enrich object scales in FSOD. It generates multi-scale positive samples as object pyramids and refines the prediction at various scales. We demonstrate its advantage by integrating it as an auxiliary branch to the popular architecture of Faster R-CNN with FPN, delivering a strong FSOD solution. Several experiments are conducted on PASCAL VOC and MS COCO, and the proposed approach achieves state of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2016.311', '10.1109/CVPR.2018.00644', '10.1609/aaai.v32i1.11716', '10.1109/TPAMI.2018.2844853', '10.1007/s11263-014-0733-5', '10.1007/s11263-009-0275-4', '10.1109/CVPR42600.2020.00407', '10.1109/ICCV.2019.00960', '10.1109/ICCV.2015.169', '10.1109/CVPR.2014.81', '10.1109/ICCV.2017.322', '10.1109/TPAMI.2015.2389824', '10.1109/CVPR.2016.90', '10.1109/ICCV.2019.00851', '10.1109/CVPR.2019.00534', '10.1007/978-3-030-01228-1_20', '10.1109/TPAMI.2006.79', '10.1109/CVPR.2018.00865', '10.1109/CVPR.2017.106', '10.1007/978-3-319-10602-1_48', '10.1007/978-3-030-01252-6_24', '10.1007/978-3-319-46448-0_2', '10.1109/CVPR.2015.7298982', '10.1109/CVPR.2018.00755', '10.1109/CVPR.2016.91', '10.1109/CVPR.2017.690', '10.1007/s11263-015-0816-y', '10.1109/CVPR.2018.00377', '10.1109/CVPR.2018.00131', '10.1109/CVPR.2017.326', '10.1007/978-3-030-01252-6_22', '10.1109/CVPR.2019.00230', '10.1109/ICCV.2019.00967'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Pattern Recognition', 'Image Processing', 'Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Remote sensing image fusion based on two-stream fusion network': Paper(DOI='10.1016/j.inffus.2019.07.010', crossref_json=None, google_schorlar_metadata=None, title='Remote sensing image fusion based on two-stream fusion network', authors=['Xiangyu Liu', 'Qingjie Liu', 'Yunhong Wang'], abstract='Remote sensing image fusion (also known as pan-sharpening) aims at generating a high resolution multi-spectral (MS) image from inputs of a high spatial resolution single band panchromatic (PAN) image and a low spatial resolution multi-spectral image. Inspired by the astounding achievements of convolutional neural networks (CNNs) in a variety of computer vision tasks, in this paper we propose a Two-stream Fusion Network (TFNet) to address the problem of pan-sharpening. Unlike many previous CNN based methods that consider pan-sharpening as a super-resolution problem and perform pan-sharpening through mapping the stacked PAN and MS to the target high resolution MS image, the proposed TFNet aims to fuse PAN and MS images in feature domain and reconstruct the pan-sharpened image from the fused features. The TFNet mainly consists of three parts. The first part is comprised of two networks\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TGRS.2007.912448', '10.1109/TGRS.2014.2361734', '10.1016/j.inffus.2016.03.003', '10.1016/j.inffus.2015.06.006', '10.1016/S1566-2535(01)00036-7', '10.1109/TGRS.2014.2311815', '10.1109/TGRS.2006.881758', '10.1109/36.763274', '10.1016/j.inffus.2006.02.001', '10.1109/TIP.2014.2316641', '10.1109/JSTSP.2015.2407855', '10.1109/TGRS.2010.2067219', '10.1109/TGRS.2012.2213604', '10.1109/TIP.2014.2333661', '10.1117/1.OE.53.9.093109', '10.1109/TPAMI.2015.2439281', '10.1109/JSTARS.2018.2794888', '10.3390/rs8070594', '10.1007/s11220-016-0135-6', '10.1016/j.inffus.2016.12.001', '10.1109/ACCESS.2017.2735019', '10.1109/LGRS.2014.2376034', '10.1109/LGRS.2017.2736020', '10.1113/jphysiol.1968.sp008455', '10.1109/LGRS.2014.2309695', '10.1109/LGRS.2017.2766840', '10.1109/5.726791', '10.1016/j.inffus.2017.10.007', '10.1109/TGRS.2008.916211', '10.1109/LGRS.2010.2046715', '10.1109/TCI.2016.2644865', '10.1080/014311698215973', '10.1109/97.995823', '10.1109/LGRS.2004.836784', '10.1109/TGRS.2007.907604'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Pattern Recognition', 'Image Processing', 'Computer Vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Distortion invariant object recognition in the dynamic link architecture': Paper(DOI='10.1109/12.210173', crossref_json=None, google_schorlar_metadata=None, title='Distortion invariant object recognition in the dynamic link architecture', authors=['Martin Lades', 'Jan C Vorbruggen', 'Joachim Buhmann', 'Jörg Lange', 'Christoph Von Der Malsburg', 'Rolf P Wurtz', 'Wolfgang Konen'], abstract='An object recognition system based on the dynamic link architecture, an extension to classical artificial neural networks (ANNs), is presented. The dynamic link architecture exploits correlations in the fine-scale temporal structure of cellular signals to group neurons dynamically into higher-order entities. These entities represent a rich structure and can code for high-level objects. To demonstrate the capabilities of the dynamic link architecture, a program was implemented that can recognize human faces and other objects from video images. Memorized objects are represented by sparse graphs, whose vertices are labeled by a multiresolution description in terms of a local power spectrum, and whose edges are labeled by geometrical distance vectors. Object recognition can be formulated as elastic graph matching, which is performed here by stochastic optimization of a matching cost function. The implementation on a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1088/0954-898X/1/1/007', '10.1007/BF00202897', '10.1007/BF00205968', '10.1364/AO.15.001795', '10.1162/neco.1990.2.1.94', '10.1162/neco.1991.3.2.268', '10.1038/338334a0', '10.1109/TPAMI.1981.4767176', '10.1016/0146-664X(81)90072-1', '10.1016/0893-6080(88)90016-0', '10.1088/0305-4470/21/16/006', '10.1098/rspb.1976.0087', '10.1098/rstb.1979.0056', '10.1109/IJCNN.1990.137747', '10.1364/JOSAA.4.002379', '10.1364/JOSAA.4.000519', '10.1109/IJCNN.1989.118574', '10.1002/bbpc.19850890625', '10.1007/BF00337113', '10.1109/34.41390', '10.1073/pnas.88.1.129', '10.1007/978-3-642-69421-9_19', '10.1007/BF00204396', '10.1152/jn.1987.58.6.1233', '10.1016/0042-6989(89)90006-0', '10.1145/800141.804670', '10.1038/326689a0'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Information Theory'], conference_acronym='I.E.E.E. transactions on computers (Print)', publisher=None, query_handler=None),\n",
       " 'Pairwise data clustering by deterministic annealing': Paper(DOI='10.1109/34.566806', crossref_json=None, google_schorlar_metadata=None, title='Pairwise data clustering by deterministic annealing', authors=['Thomas Hofmann', 'Joachim M.  Buhmann'], abstract='Partitioning a data set and extracting hidden structure from the data arises in different application areas of pattern recognition, speech and image processing. Pairwise data clustering is a combinatorial optimization method for data grouping which extracts hidden structure from proximity data. We describe a deterministic annealing approach to pairwise clustering which shares the robustness properties of maximum entropy inference. The resulting Gibbs probability distributions are estimated by mean-field approximation. A new structure-preserving algorithm to cluster dissimilarity data and to simultaneously embed these data in a Euclidian vector space is discussed which can be used for dimensionality reduction and data visualization. The suggested embedding algorithm which outperforms conventional approaches has been implemented to analyze dissimilarity data from protein analysis and from linguistics. The\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1162/neco.1991.3.2.268', '10.1162/neco.1994.6.2.334', '10.1109/ICIP.1996.560389', '10.1002/0471200611', '10.1007/BF02289694', '10.1109/34.236251', '10.1109/18.144705', '10.1109/T-C.1969.222678', '10.1080/14786437708235992', '10.1016/0167-8655(90)90010-Y', '10.1103/PhysRevLett.65.945', '10.1103/PhysRev.54.918', '10.1162/neco.1990.2.1.1', '10.1080/01621459.1989.10478797', '10.1109/34.134040', '10.1214/aos/1176349519', '10.1109/34.491619', '10.1007/BF00204594', '10.1109/34.56204', '10.1109/TIT.1962.1057698', '10.1364/JOSAA.2.001160', '10.1007/978-1-4615-3626-0', '10.1088/0954-898X/1/1/007', '10.1109/PROC.1982.12425', '10.1109/MASSP.1984.1162229', '10.1162/neco.1993.5.1.75', '10.1103/PhysRev.108.171', '10.1109/18.243432', '10.1109/29.17498', '10.1103/PhysRev.106.620', '10.1007/BF00940812', '10.1126/science.220.4598.671', '10.1103/PhysRevA.30.2638', '10.1007/978-3-662-02377-8', '10.1214/aop/1176996454', '10.1109/ICIP.1996.560389'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Information Theory'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Glaucoma detection using entropy sampling and ensemble learning for automatic optic cup and disc segmentation': Paper(DOI='10.1016/j.compmedimag.2016.07.012', crossref_json=None, google_schorlar_metadata=None, title='Glaucoma detection using entropy sampling and ensemble learning for automatic optic cup and disc segmentation', authors=['Julian Zilly', 'Joachim M Buhmann', 'Dwarikanath Mahapatra'], abstract='We present a novel method to segment retinal images using ensemble learning based convolutional neural network (CNN) architectures. An entropy sampling technique is used to select informative points thus reducing computational complexity while performing superior to uniform sampling. The sampled points are used to design a novel learning framework for convolutional filters based on boosting. Filters are learned in several layers with the output of previous layers serving as the input to the next layer. A softmax logistic classifier is subsequently trained on the output of all learned filters and applied on test images. The output of the classifier is subject to an unsupervised graph cut algorithm followed by a convex hull transformation to obtain the final segmentation. Our proposed algorithm for optic cup and disc segmentation outperforms existing methods on the public DRISHTI-GS data set on several metrics.', conference=None, journal=None, year=None, reference_list=['10.1109/TMI.2010.2053042', '10.1109/TMI.2010.2053042', '10.1016/j.media.2009.12.006', '10.1109/34.969114', '10.1109/TMI.2013.2247770', '10.1016/j.eswa.2010.06.019', '10.1117/1.JMI.1.2.024001', '10.1113/jphysiol.1963.sp007079', '10.1109/TMI.2011.2106509', '10.1109/TBME.2012.2187293', '10.1016/j.cviu.2016.01.006', '10.1109/34.982899', '10.1167/iovs.14-15592', '10.1109/TIP.2010.2066982', '10.7326/0003-4819-116-8-660', '10.1162/neco.2009.10-08-881', '10.1109/TITB.2012.2198668'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Information Theory'], conference_acronym='Computerized medical imaging and graphics (Print)', publisher=None, query_handler=None),\n",
       " 'Protein identification false discovery rates for very large proteomics data sets generated by tandem mass spectrometry': Paper(DOI='10.1074/mcp.m900317-mcp200', crossref_json=None, google_schorlar_metadata=None, title='Protein identification false discovery rates for very large proteomics data sets generated by tandem mass spectrometry', authors=['Lukas Reiter', 'Manfred Claassen', 'Sabine P Schrimpf', 'Marko Jovanovic', 'Alexander Schmidt', 'Joachim M Buhmann', 'Michael O Hengartner', 'Ruedi Aebersold'], abstract='Comprehensive characterization of a proteome is a fundamental goal in proteomics. To achieve saturation coverage of a proteome or specific subproteome via tandem mass spectrometric identification of tryptic protein sample digests, proteomics data sets are growing dramatically in size and heterogeneity. The trend toward very large integrated data sets poses so far unsolved challenges to control the uncertainty of protein identifications going beyond well established confidence measures for peptide-spectrum matches. We present MAYU, a novel strategy that reliably estimates false discovery rates for protein identifications in large scale data sets. We validated and applied MAYU using various large proteomics data sets. The data show that the size of the data set has an important and previously underestimated impact on the reliability of protein identifications. We particularly found that protein false discovery rates\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1038/nature01511', '10.1038/nbt1300', '10.1016/j.cell.2006.03.022', '10.1186/gb-2006-7-11-r106', '10.1002/pmic.200500358', '10.1021/pr025556v', '10.1038/85686', '10.1126/science.1157956', '10.1038/nature07341', '10.1371/journal.pbio.1000048', '10.1101/gr.089060.108', '10.1074/mcp.R500012-MCP200', '10.1038/nbt1275', '10.1038/nbt1315', '10.1038/nmeth1088', '10.1016/S0968-0004(01)02021-7', '10.1038/nmeth1019', '10.1021/pr700600n', '10.1021/ac025747h', '10.1016/S1044-0305(02)00352-5', '10.1002/pmic.200500186', '10.1021/ac025826t', '10.1021/ac0341261', '10.1074/mcp.T600049-MCP200', '10.1074/mcp.M400215-MCP200', '10.1038/nature05050', '10.1038/nbt819', '10.1038/msb4100024', '10.1186/gb-2004-6-1-r9', '10.1016/1044-0305(94)80016-2', '10.1101/gr.361602', '10.1038/nprot.2007.160', '10.1002/1097-0282(2000)55:3<188::AID-BIP20>3.0.CO;2-T', '10.1074/mcp.M700498-MCP200', '10.1021/pr049882h', '10.1002/pmic.200401303', '10.1021/pr0602085', '10.1002/pmic.200600625', '10.1016/1044-0305(95)00291-K', '10.1021/ac0498563', '10.1038/nrm1683'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Information Theory'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Sensory segmentation with coupled neural oscillators': Paper(DOI='10.1007/bf00204396', crossref_json=None, google_schorlar_metadata=None, title='Sensory segmentation with coupled neural oscillators', authors=['Christoph von der Malsburg', 'Joachim Buhmann'], abstract=' We present a model of sensory segmentation that is based on the generation and processing of temporal tags in the form of oscillations, as suggested by the Dynamic Link Architecture. The model forms the basis for a natural solution to the sensory segmentation problem. It can deal with multiple segments, can integrate different cues and has the potential for processing hierarchical structures. Temporally tagged segments can easily be utilized in neural systems and form a natural basis for object recognition and learning. The model consists of a “cortical” circuit, an array of units that act as local feature detectors. Units are formulated as neural oscillators. Knowledge relevant to segmentation is encoded by connections. In accord with simple Gestalt laws, our concrete model has intracolumnar connections, between all units with overlapping receptive fields, and intercolumnar connections, between units\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1162/neco.1990.2.4.458', '10.1103/PhysRevA.40.4145', '10.1007/BF00202899', '10.1126/science.252.5009.1177', '10.1073/pnas.88.14.6048', '10.1016/0013-4694(78)90126-8', '10.1109/TPAMI.1984.4767596', '10.1109/34.56204', '10.1073/pnas.86.5.1698', '10.1038/338334a0', '10.1073/pnas.88.15.6462', '10.1162/neco.1991.3.2.155', '10.1007/978-1-4613-2551-2', '10.1152/jn.1991.66.3.1059', '10.1080/01621459.1987.10478393', '10.1126/science.4023713', '10.1073/pnas.89.12.5670', '10.1038/scientificamerican1290-84', '10.1152/jn.1988.60.1.344', '10.1162/neco.1991.3.2.167', '10.7551/mitpress/7113.003.0030', '10.1103/PhysRevA.43.6990', '10.1073/pnas.88.1.129', '10.1016/0010-0285(80)90005-5', '10.1007/BF00337113', '10.1162/neco.1990.2.1.94', '10.1007/BF00410640'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Information Theory'], conference_acronym='Biological cybernetics', publisher=None, query_handler=None),\n",
       " 'Unsupervised texture segmentation in a deterministic annealing framework': Paper(DOI='10.1109/34.709593', crossref_json=None, google_schorlar_metadata=None, title='Unsupervised texture segmentation in a deterministic annealing framework', authors=['Thomas Hofmann', 'Jan Puzicha', 'Joachim M.  Buhmann'], abstract='We present a novel optimization framework for unsupervised texture segmentation that relies on statistical tests as a measure of homogeneity. Texture segmentation is formulated as a data clustering problem based on sparse proximity data. Dissimilarities of pairs of textured regions are computed from a multiscale Gabor filter image representation. We discuss and compare a class of clustering objective functions which is systematically derived from invariance principles. As a general optimization framework, we propose deterministic annealing based on a mean-field approximation. The canonical way to derive clustering algorithms within this framework as well as an efficient implementation of mean-field annealing and the closely related Gibbs sampler are presented. We apply both annealing variants to Brodatz-like microtexture mixtures and real-word images.', conference=None, journal=None, year=None, reference_list=['10.1109/34.491619', '10.1287/moor.13.2.311', '10.1109/18.144705', '10.1109/18.243432', '10.1109/34.134040', '10.1109/72.105426', '10.1126/science.220.4598.671', '10.1109/83.210863', '10.1007/BF00940812', '10.1109/72.238324', '10.1109/34.566806', '10.1103/PhysRev.106.620', '10.1109/34.709593', '10.1109/CVPR.1997.609331', '10.1109/83.544573', '10.1109/34.531803', '10.1162/neco.1990.2.1.1', '10.1109/ICIP.1997.632061', '10.1109/ICIP.1996.560389', '10.1007/978-1-4471-3087-1_26', '10.1109/ICCV.1998.710729', '10.1007/BF00204594', '10.1364/JOSAA.2.001160', '10.1109/34.41384', '10.1007/3-540-55426-2_2', '10.1109/83.502411', '10.1109/TPAMI.1984.4767596', '10.1109/34.56204', '10.1016/0031-3203(95)00067-4', '10.1016/0031-3203(91)90143-S', '10.1016/0031-3203(92)90099-5', '10.1142/S0129065789000414', '10.1109/72.80231', '10.1109/CVPR.1997.609407'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Information Theory'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Path-based clustering for grouping of smooth curves and texture segmentation': Paper(DOI='10.1109/tpami.2003.1190577', crossref_json=None, google_schorlar_metadata=None, title='Path-based clustering for grouping of smooth curves and texture segmentation', authors=['Bernd Fischer', 'Joachim M.  Buhmann'], abstract='Perceptual grouping organizes image parts in clusters based on psychophysically plausible similarity measures. We propose a novel grouping method in this paper, which stresses connectedness of image elements via mediating elements rather than favoring high mutual similarity. This grouping principle yields superior clustering results when objects are distributed on low-dimensional extended manifolds in a feature space, and not as local point clouds. In addition to extracting connected structures, objects are singled out as outliers when they are too far away from any cluster structure. The objective function for this perceptual organization principle is optimized by a fast agglomerative algorithm. We report on perceptual organization experiments where small edge elements are grouped to smooth curves. The generality of the method is emphasized by results from grouping textured images with texture gradients in\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/3-540-44745-8_16', '10.1006/cviu.1999.0786', '10.1109/ICPR.2002.1044854', '10.1016/0031-3203(91)90143-S', '10.1109/34.709593', '10.1109/TPAMI.1986.4767851', '10.1023/A:1011174803800', '10.1109/34.232076', '10.1109/34.868688', '10.1109/34.566806', '10.1016/S0167-8655(99)00056-2', '10.1109/CCV.1988.590008', '10.1109/34.659934', '10.1023/A:1008187804026', '10.1162/neco.1997.9.4.837'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Information Theory'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Bagging for path-based clustering': Paper(DOI='10.1109/tpami.2003.1240115', crossref_json=None, google_schorlar_metadata=None, title='Bagging for path-based clustering', authors=['Bernd Fischer', 'Joachim M Buhmann'], abstract='A resampling scheme for clustering with similarity to bootstrap aggregation (bagging) is presented. Bagging is used to improve the quality of path-based clustering, a data clustering method that can extract elongated structures from data in a noise robust way. The results of an agglomerative optimization method are influenced by small fluctuations of the input data. To increase the reliability of clustering solutions, a stochastic resampling method is developed to infer consensus clusters. A related reliability measure allows us to estimate the number of clusters, based on the stability of an optimized cluster solution under resampling. The quality of path-based clustering with resampling is evaluated on a large image data set of human segmentations.', conference=None, journal=None, year=None, reference_list=['10.1162/089976601753196030', '10.1007/BF00058655', '10.1109/ICPR.2002.1047450', '10.1109/34.868688', '10.1145/234533.234534', '10.1109/34.954598', '10.1109/ICCV.2001.937655', '10.1002/nav.3800020109', '10.1109/ICPR.2000.906045', '10.3115/981574.981598', '10.1109/34.566806', '10.1109/TPAMI.2003.1190577', '10.1016/S0167-8655(99)00056-2', '10.1162/neco.1997.9.8.1805'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Information Theory'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Pattern segmentation in associative memory': Paper(DOI='10.1162/neco.1990.2.1.94', crossref_json=None, google_schorlar_metadata=None, title='Pattern segmentation in associative memory', authors=['DeLiang Wang', 'Joachim Buhmann', 'Christoph von der Malsburg'], abstract=' The goal of this paper is to show how to modify associative memory such that it can discriminate several stored patterns in a composite input and represent them simultaneously. Segmention of patterns takes place in the temporal domain, components of one pattern becoming temporally correlated with each other and anticorrelated with the components of all other patterns. Correlations are created naturally by the usual associative connections. In our simulations, temporal patterns take the form of oscillatory bursts of activity. Model oscillators consist of pairs of local cell populations connected appropriately. Transition of activity from one pattern to another is induced by delayed self-inhibition or simply by noise.', conference=None, journal=None, year=None, reference_list=['10.1016/0167-2789(86)90238-1', '10.1162/neco.1989.1.1.123', '10.1007/BF00202899', '10.1016/0013-4694(78)90126-8', '10.1016/0893-6080(88)90001-9', '10.1038/338334a0', '10.1016/0166-2236(89)90025-8', '10.1073/pnas.79.8.2554', '10.1016/0031-9384(89)90041-3', '10.1016/0031-9384(84)90118-5', '10.1007/BF00200803', '10.1002/cne.902160305', '10.1007/BF00367781', '10.1017/S0140525X0005679X', '10.1007/BF00337113'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Information Theory'], conference_acronym='Neural computation', publisher=None, query_handler=None),\n",
       " 'Computational pathology: challenges and promises for tissue analysis': Paper(DOI='10.1016/j.compmedimag.2011.02.006', crossref_json=None, google_schorlar_metadata=None, title='Computational pathology: challenges and promises for tissue analysis', authors=['Thomas J Fuchs', 'Joachim M Buhmann'], abstract='The histological assessment of human tissue has emerged as the key challenge for detection and treatment of cancer. A plethora of different data sources ranging from tissue microarray data to gene expression, proteomics or metabolomics data provide a detailed overview of the health status of a patient. Medical doctors need to assess these information sources and they rely on data driven automatic analysis tools. Methods for classification, grouping and segmentation of heterogeneous data sources as well as regression of noisy dependencies and estimation of survival probabilities enter the processing workflow of a pathology diagnosis system at various stages. This paper reports on state-of-the-art of the design and effectiveness of computational pathology workflows and it discusses future research directions in this emergent field of medical informatics and diagnostic machine learning.', conference=None, journal=None, year=None, reference_list=['10.1002/(SICI)1097-0142(19960101)77:1<164::AID-CNCR27>3.0.CO;2-2', '10.1016/S0002-9440(10)65349-7', '10.1053/j.seminoncol.2006.06.008', '10.1002/1096-9896(200107)194:3<349::AID-PATH887>3.0.CO;2-D', '10.1158/1055-9965.EPI-09-0099', '10.1038/nm0798-844', '10.1073/pnas.171209998', '10.1016/S0002-9440(10)64120-X', '10.1007/s11548-009-0290-5', '10.1007/s00330-009-1709-7', '10.1016/S0010-4825(02)00060-4', '10.1109/CVPRW.2010.5543189', '10.1109/TMI.2004.828354', '10.1016/0031-3203(92)90008-7', '10.1016/0167-8655(96)00105-5', '10.1086/258244', '10.1086/258455', '10.1074/mcp.R800013-MCP200', '10.1002/path.2440', '10.1186/1471-2105-10-368', '10.1109/TIP.2002.800889', '10.1007/s00138-002-0120-z', '10.1186/1471-2121-8-S1-S2', '10.1109/TMI.2007.896925', '10.1016/j.media.2010.04.007', '10.1109/RBME.2009.2034865', '10.2478/v10042-008-0114-4', '10.1016/j.patrec.2008.11.005', '10.1371/journal.pone.0007847', '10.1007/978-3-642-10331-5_35', '10.1023/A:1010933404324', '10.2202/1544-6115.1071', '10.1007/s10994-006-6226-1', '10.1007/BFb0029575', '10.1007/978-3-540-88682-2_19', '10.1186/1471-2105-11-S8-S8', '10.1162/neco.1997.9.7.1545', '10.1023/A:1025619426553', '10.1023/A:1022699900025', '10.1007/BF00058613', '10.1002/cyto.a.20319', '10.1369/jhc.2007.950170', '10.1186/1471-2121-8-S1-S8', '10.1038/44565', '10.1109/TMI.2009.2015145', '10.1007/b102216', '10.1016/j.compmedimag.2009.10.004', '10.1007/978-3-642-04271-3_77', '10.1023/A:1007912009077', '10.1186/1471-2105-8-81', '10.1197/jamia.M2522', '10.1080/01621459.1958.10501452', '10.1158/1078-0432.CCR-09-0260', '10.1002/(SICI)1097-0258(19990515)18:9<1119::AID-SIM116>3.0.CO;2-V', '10.1016/j.jspi.2004.08.009', '10.2139/ssrn.485624', '10.1111/j.1467-9868.2005.00532.x', '10.1109/TIT.1982.1056489', '10.1016/j.ccr.2010.03.021', '10.1016/j.humpath.2009.08.026', '10.1371/journal.pone.0011218', '10.1371/journal.pone.0006320', '10.5858/133.11.1841'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Machine Learning', 'Computer Vision', 'Information Theory'], conference_acronym='Computerized medical imaging and graphics (Print)', publisher=None, query_handler=None),\n",
       " 'F³Net: fusion, feedback and focus for salient object detection': Paper(DOI='10.1609/aaai.v34i07.6916', crossref_json=None, google_schorlar_metadata=None, title='F³Net: fusion, feedback and focus for salient object detection', authors=['Jun Wei', 'Shuhui Wang', 'Qingming Huang'], abstract=\"Most of existing salient object detection models have achieved great progress by aggregating multi-level features extracted from convolutional neural networks. However, because of the different receptive fields of different convolutional layers, there exists big differences between features generated by these layers. Common feature fusion strategies (addition or concatenation) ignore these differences and may cause suboptimal solutions. In this paper, we propose the F 3 Net to solve above problem, which mainly consists of cross feature module (CFM) and cascaded feedback decoder (CFD) trained by minimizing a new pixel position aware loss (PPA). Specifically, CFM aims to selectively aggregate multi-level features. Different from addition and concatenation, CFM adaptively selects complementary components from input features before fusion, which can effectively avoid introducing too much redundant information that may destroy the original features. Besides, CFD adopts a multi-stage feedback mechanism, where features closed to supervision will be introduced to the output of previous layers to supplement them and eliminate the differences between features. These refined features will go through multiple similar iterations before generating the final saliency maps. Furthermore, different from binary cross entropy, the proposed PPA loss doesn't treat pixels equally, which can synthesize the local structure information of a pixel to guide the network to focus more on local details. Hard pixels from boundaries or error-prone parts will be given more attention to emphasize their importance. F 3 Net is able to segment salient object regions\\xa0…\", conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia Analysis and Retrieval', 'Image and Video Processing', 'Pattern Recognition', 'Computer Vision', 'Video Coding'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Fast and robust text detection in images and video frames': Paper(DOI='10.1016/j.imavis.2005.01.004', crossref_json=None, google_schorlar_metadata=None, title='Fast and robust text detection in images and video frames', authors=['Qixiang Ye', 'Qingming Huang', 'Wen Gao', 'Debin Zhao'], abstract='Text in images and video frames carries important information for visual content understanding and retrieval. In this paper, by using multiscale wavelet features, we propose a novel coarse-to-fine algorithm that is able to locate text lines even under complex background. First, in the coarse detection, after the wavelet energy feature is calculated to locate all possible text pixels, a density-based region growing method is developed to connect these pixels into regions which are further separated into candidate text lines by structural information. Secondly, in the fine detection, with four kinds of texture features extracted to represent the texture pattern of a text line, a forward search algorithm is applied to select the most effective features. Finally, an SVM classifier is used to identify true text from the candidates based on the selected features. Experimental results show that this approach can fast and robustly detect text\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICDAR.1999.791724', '10.1109/34.845381', '10.1109/CAIVD.1998.646033', '10.1109/34.809116', '10.1109/76.999203', '10.1016/S0031-3203(98)00067-3', '10.1016/0031-3203(95)00030-4', '10.21236/ADA458675', '10.1109/83.817607', '10.1109/TPAMI.2003.1251157', '10.1109/TNN.2002.1021896', '10.1109/34.192463', '10.1002/cpa.3160410705', '10.1007/978-1-4615-2277-5', '10.21236/ADA295738', '10.1109/TCSVT.2004.825538', '10.1117/12.335804', '10.1007/s005300050140'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia Analysis and Retrieval', 'Image and Video Processing', 'Pattern Recognition', 'Computer Vision', 'Video Coding'], conference_acronym='Image and vision computing', publisher=None, query_handler=None),\n",
       " 'Global context-aware progressive aggregation network for salient object detection': Paper(DOI='10.1609/aaai.v34i07.6633', crossref_json=None, google_schorlar_metadata=None, title='Global context-aware progressive aggregation network for salient object detection', authors=['Zuyao Chen', 'Qianqian Xu', 'Runmin Cong', 'Qingming Huang'], abstract='Deep convolutional neural networks have achieved competitive performance in salient object detection, in which how to learn effective and comprehensive features plays a critical role. Most of the previous works mainly adopted multiple-level feature integration yet ignored the gap between different features. Besides, there also exists a dilution process of high-level features as they passed on the top-down pathway. To remedy these issues, we propose a novel network named GCPANet to effectively integrate low-level appearance features, high-level semantic features, and global context features through some progressive context-aware Feature Interweaved Aggregation (FIA) modules and generate the saliency map in a supervised way. Moreover, a Head Attention (HA) module is used to reduce information redundancy and enhance the top layers features by leveraging the spatial and channel-wise attention, and the Self Refinement (SR) module is utilized to further refine and heighten the input features. Furthermore, we design the Global Context Flow (GCF) module to generate the global context information at different stages, which aims to learn the relationship among different salient regions and alleviate the dilution effect of high-level features. Experimental results on six benchmark datasets demonstrate that the proposed approach outperforms the state-of-the-art methods both quantitatively and qualitatively.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia Analysis and Retrieval', 'Image and Video Processing', 'Pattern Recognition', 'Computer Vision', 'Video Coding'], conference_acronym='Proceedings of the ... AAAI Conference on Artificial Intelligence (Online)', publisher=None, query_handler=None),\n",
       " 'Review of visual saliency detection with comprehensive information': Paper(DOI='10.1109/tcsvt.2018.2870832', crossref_json=None, google_schorlar_metadata=None, title='Review of visual saliency detection with comprehensive information', authors=['Runmin Cong', 'Jianjun Lei', 'Huazhu Fu', 'Ming-Ming Cheng', 'Weisi Lin', 'Qingming Huang'], abstract='The visual saliency detection model simulates the human visual system to perceive the scene and has been widely used in many vision tasks. With the development of acquisition technology, more comprehensive information, such as depth cue, inter-image correspondence, or temporal relationship, is available to extend image saliency detection to RGBD saliency detection, co-saliency detection, or video saliency detection. The RGBD saliency detection model focuses on extracting the salient regions from RGBD images by combining the depth information. The co-saliency detection model introduces the inter-image correspondence constraint to discover the common salient object in an image group. The goal of the video saliency detection model is to locate the motion-related salient object in video sequences, which considers the motion cue and spatiotemporal constraint jointly. In this paper, we review different\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2016.78', '10.1007/s11263-015-0822-0', '10.1109/CVPR.2014.43', '10.1109/CVPR.2013.407', '10.1109/TMM.2016.2636739', '10.1109/TMM.2016.2592325', '10.1109/TNNLS.2015.2506664', '10.1007/s11263-016-0977-3', '10.1109/CVPR.2013.271', '10.1109/TPAMI.2010.70', '10.1109/TIP.2015.2438546', '10.1109/TIP.2015.2495122', '10.1109/TPAMI.2016.2562626', '10.1109/CVPR.2011.5995344', '10.1109/ICCV.2013.370', '10.1109/TPAMI.2015.2465960', '10.1109/CVPR.2014.360', '10.1109/ICCV.2013.248', '10.1109/TPAMI.2017.2662005', '10.1109/CVPR.2015.7298961', '10.1109/CVPR.2009.5206596', '10.1007/978-3-642-33709-3_8', '10.1109/TIP.2017.2711277', '10.1007/978-3-319-10578-9_7', '10.1109/TIP.2014.2305100', '10.1016/j.neucom.2012.08.057', '10.1109/CVPR.2004.291', '10.1109/TCYB.2013.2265378', '10.1109/CVPR.2016.58', '10.1109/TIP.2017.2651369', '10.1016/j.neucom.2016.05.098', '10.1109/TMM.2015.2389616', '10.1109/JETCAS.2014.2298919', '10.1109/CVPR.2018.00941', '10.1109/TMM.2017.2660440', '10.1109/ICIP.2006.313095', '10.1109/CVPR.2015.7298731', '10.1109/ICIP.2017.8296539', '10.1109/CVPR.2017.404', '10.1109/WACV.2017.8', '10.1109/CVPR.2016.80', '10.1109/ICCV.2017.32', '10.1109/CVPR.2017.563', '10.3390/rs9060597', '10.1109/LGRS.2017.2672560', '10.1016/j.neucom.2016.05.045', '10.1109/LGRS.2016.2602885', '10.1109/CVPR.2011.5995415', '10.1109/TIP.2011.2156803', '10.1109/CVPR.2016.82', '10.1145/3158674', '10.1109/CVPR.2013.87', '10.1109/LSP.2016.2557347', '10.1109/ICME.2014.6890183', '10.1109/TIP.2015.2500820', '10.1109/LSP.2013.2292873', '10.1109/ICASSP.2013.6638027', '10.1109/TMM.2013.2271476', '10.1109/CVPR.2016.118', '10.1109/TCSVT.2015.2433171', '10.1109/LSP.2014.2364896', '10.1109/CVPR.2016.121', '10.1109/ICME.2018.8486603', '10.1109/TIP.2017.2682981', '10.1109/CVPR.2018.00322', '10.1109/TCYB.2017.2761775', '10.1145/2808492.2808551', '10.1016/j.image.2015.07.002', '10.1109/ICME.2016.7552907', '10.1109/CVPR.2016.257', '10.1109/ICASSP.2016.7471952', '10.1109/ICASSP.2016.7471896', '10.1109/TIP.2015.2487833', '10.1109/LSP.2017.2688136', '10.1109/TPAMI.2012.89', '10.1109/TIP.2017.2762594', '10.1109/TCSVT.2013.2273613', '10.5244/C.31.38', '10.1109/ICASSP.2012.6288171', '10.1109/TIP.2017.2754941', '10.1109/ICME.2012.173', '10.1109/TCYB.2017.2771488', '10.1109/TCYB.2017.2761361', '10.1109/TIP.2017.2763819', '10.1109/TCSVT.2016.2595324', '10.1109/LSP.2016.2615293', '10.1109/TIP.2017.2670143', '10.1109/TIP.2016.2631900', '10.1109/CVPR.2007.382973', '10.1109/CVPR.2013.412', '10.1109/TIP.2015.2460013', '10.1109/TIP.2015.2425544', '10.1109/TIP.2014.2336549', '10.1109/TCSVT.2014.2308642', '10.1109/TMM.2015.2400823', '10.1109/TIP.2016.2612882', '10.1109/TIP.2017.2656463', '10.1109/TCSVT.2016.2617332', '10.1109/TNNLS.2015.2488637', '10.1109/TMM.2016.2547343', '10.5244/C.24.56', '10.1016/j.neucom.2015.11.063', '10.1109/ICME.2015.7177414', '10.1109/TIP.2016.2526900', '10.1109/TVCG.2016.2600594', '10.1109/TIP.2014.2332399', '10.1109/LSP.2016.2611485', '10.1109/LSP.2017.2681687', '10.1109/ICCV.2013.273', '10.1007/s10844-016-0441-4', '10.1016/j.image.2016.03.005', '10.1109/ICCV.2005.171', '10.1109/CVPR.2017.468', '10.1007/s11263-016-0907-4', '10.1109/TIP.2013.2260166', '10.1109/CVPR.2010.5540080', '10.1109/CVPR.2016.85', '10.1109/CVPR.2015.7299072', '10.1109/LGRS.2013.2281827', '10.1117/1.JRS.9.095050', '10.1109/IGARSS.2016.7729013', '10.1109/CVPR.2015.7298918', '10.24963/ijcai.2017/424', '10.1109/ICCV.2015.75', '10.1109/TCSVT.2017.2706264'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Medical Image Analysis', 'AI for Healthcare', 'Trustworthy AI'], conference_acronym='IEEE transactions on circuits and systems for video technology (Print)', publisher=None, query_handler=None),\n",
       " 'Multimodal transformer with multi-view visual representation for image captioning': Paper(DOI='10.1109/tcsvt.2019.2947482', crossref_json=None, google_schorlar_metadata=None, title='Multimodal transformer with multi-view visual representation for image captioning', authors=['Jun Yu', 'Jing Li', 'Zhou Yu', 'Qingming Huang'], abstract='Image captioning aims to automatically generate a natural language description of a given image, and most state-of-the-art models have adopted an encoder-decoder framework. The framework consists of a convolution neural network (CNN)-based image encoder that extracts region-based visual features from the input image, and an recurrent neural network (RNN) based caption decoder that generates the output caption words based on the visual features with the attention mechanism. Despite the success of existing studies, current methods only model the co-attention that characterizes the inter-modal interactions while neglecting the self-attention that characterizes the intra-modal interactions. Inspired by the success of the Transformer model in machine translation, here we extend it to a Multimodal Transformer (MT) model for image captioning. Compared to existing image captioning approaches, the MT model\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TCYB.2014.2307862', '10.3115/v1/D14-1162', '10.1109/CVPR.2018.00637', '10.1109/CVPR.2017.232', '10.1109/TNNLS.2018.2817340', '10.1109/CVPR.2016.90', '10.1007/s11263-016-0981-7', '10.1109/CVPR.2018.00142', '10.1109/CVPR.2019.00646', '10.1109/CVPR.2017.667', '10.1109/CVPR.2019.00644', '10.18653/v1/D16-1044', '10.1145/2600428.2609563', '10.3115/v1/P15-2017', '10.1007/978-3-030-01264-9_42', '10.1109/ICCV.2017.524', '10.1162/neco.1997.9.8.1735', '10.1109/CVPR.2015.7298594', '10.1109/CVPR.2015.7298932', '10.1109/CVPR.2015.7298878', '10.1007/978-3-030-01216-8_31', '10.1007/978-3-319-46475-6_5', '10.1109/CVPR.2017.670', '10.1109/TPAMI.2017.2708709', '10.1109/TIP.2018.2889922', '10.1109/TIP.2019.2928144', '10.1109/CVPR.2015.7298935', '10.1109/CVPR.2017.131', '10.1109/TCSVT.2018.2830102', '10.1109/CVPR.2015.7299087', '10.1109/TPAMI.2012.162', '10.1007/978-3-642-15561-1_2', '10.1109/ICCV.2017.202', '10.1109/CVPR.2018.00636', '10.24963/ijcai.2018/155', '10.1109/TCSVT.2018.2867286', '10.1007/978-3-319-46478-7_28', '10.1109/CVPR.2017.634', '10.1109/CVPR.2017.345', '10.1162/tacl_a_00166', '10.1109/ICCV.2015.169', '10.1109/TCSVT.2017.2726580', '10.1109/CVPR.2016.91', '10.1109/ICCV.2017.324'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia Analysis and Retrieval', 'Image and Video Processing', 'Pattern Recognition', 'Computer Vision', 'Video Coding'], conference_acronym='IEEE transactions on circuits and systems for video technology (Print)', publisher=None, query_handler=None),\n",
       " 'Spatial pyramid-enhanced NetVLAD with weighted triplet loss for place recognition': Paper(DOI='10.1109/tnnls.2019.2908982', crossref_json=None, google_schorlar_metadata=None, title='Spatial pyramid-enhanced NetVLAD with weighted triplet loss for place recognition', authors=['Jun Yu', 'Chaoyang Zhu', 'Jian Zhang', 'Qingming Huang', 'Dacheng Tao'], abstract='We propose an end-to-end place recognition model based on a novel deep neural network. First, we propose to exploit the spatial pyramid structure of the images to enhance the vector of locally aggregated descriptors (VLAD) such that the enhanced VLAD features can reflect the structural information of the images. To encode this feature extraction into the deep learning method, we build a spatial pyramid-enhanced VLAD (SPE-VLAD) layer. Next, we impose weight constraints on the terms of the traditional triplet loss (T-loss) function such that the weighted T-loss (WT-loss) function avoids the suboptimal convergence of the learning process. The loss function can work well under weakly supervised scenarios in that it determines the semantically positive and negative samples of each query through not only the GPS tags but also the Euclidean distance between the image representations. The SPE-VLAD layer and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1162/089976603321780317', '10.1109/CVPR.2016.90', '10.1007/BF00129684', '10.1109/MCSE.2018.108164530', '10.1109/CVPR.2017.667', '10.1109/ICCV.2017.374', '10.1109/TNNLS.2017.2699674', '10.1109/TNNLS.2017.2705429', '10.1109/TNNLS.2018.2816021', '10.1109/CVPR.2007.383150', '10.1126/science.290.5500.2323', '10.1109/ICCV.2015.243', '10.1109/CVPR.2007.383172', '10.1109/CVPR.2013.119', '10.1109/TNNLS.2016.2626379', '10.1109/CVPR.2010.5540039', '10.1109/TPAMI.2011.235', '10.1109/CVPR.2016.175', '10.1109/TPAMI.2007.1049', '10.1109/TNNLS.2016.2599820', '10.1109/TNNLS.2017.2690910', '10.1109/TPAMI.2017.2711011', '10.1109/ROBOT.2010.5509226', '10.1109/TPAMI.2017.2667665', '10.1109/TNNLS.2018.2797248', '10.1109/TRO.2015.2496823', '10.1109/CVPR.2011.5995610', '10.1109/ICCV.2003.1238354', '10.1109/CVPR.2013.96', '10.1007/978-3-642-15549-9_54', '10.1126/science.1127647', '10.1109/CVPR.2013.122', '10.1109/TNNLS.2015.2504724'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia Analysis and Retrieval', 'Image and Video Processing', 'Pattern Recognition', 'Computer Vision', 'Video Coding'], conference_acronym='IEEE transactions on neural networks and learning systems (Print)', publisher=None, query_handler=None),\n",
       " 'A configurable method for multi-style license plate recognition': Paper(DOI='10.1016/j.patcog.2008.08.016', crossref_json=None, google_schorlar_metadata=None, title='A configurable method for multi-style license plate recognition', authors=['Jianbin Jiao', 'Qixiang Ye', 'Qingming Huang'], abstract='Despite the success of license plate recognition (LPR) methods in the past decades, few of them can process multi-style license plates (LPs), especially LPs from different nations, effectively. In this paper, we propose a new method for multi-style LP recognition by representing the styles with quantitative parameters, i.e., plate rotation angle, plate line number, character type and format. In the recognition procedure these four parameters are managed by relevant algorithms, i.e., plate rotation, plate line segmentation, character recognition and format matching algorithm, respectively. To recognize special style LPs, users can configure the method by defining corresponding parameter values, which will be processed by the relevant algorithms. In addition, the probabilities of the occurrence of every LP style are calculated based on the previous LPR results, which will result in a faster and more precise recognition\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TITS.2006.880641', '10.1109/ICPR.2004.1334387', '10.1016/j.patrec.2005.04.014', '10.1109/ICPR.2004.1333775', '10.1109/ICIP.2006.313121', '10.1109/25.467963', '10.1016/j.imavis.2005.01.004', '10.1109/TITS.2004.825086', '10.1016/j.patcog.2005.01.026', '10.1007/11550518_48'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia Analysis and Retrieval', 'Image and Video Processing', 'Pattern Recognition', 'Computer Vision', 'Video Coding'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'Using webcast text for semantic event detection in broadcast sports video': Paper(DOI='10.1109/tmm.2008.2004912', crossref_json=None, google_schorlar_metadata=None, title='Using webcast text for semantic event detection in broadcast sports video', authors=['Changsheng Xu', 'Yi-Fan Zhang', 'Guangyu Zhu', 'Yong Rui', 'Hanqing Lu', 'Qingming Huang'], abstract='Sports video semantic event detection is essential for sports video summarization and retrieval. Extensive research efforts have been devoted to this area in recent years. However, the existing sports video event detection approaches heavily rely on either video content itself, which face the difficulty of high-level semantic information extraction from video content using computer vision and image processing techniques, or manually generated video ontology, which is domain specific and difficult to be automatically aligned with the video content. In this paper, we present a novel approach for sports video semantic event detection based on analysis and alignment of Webcast text and broadcast video. Webcast text is a text broadcast channel for sports game which is co-produced with the broadcast video and is easily obtained from the Web. We first analyze Webcast text to cluster and detect text events in an\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ICASSP.2002.5745380', '10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9', '10.1023/A:1007617005950', '10.1109/ICPR.2004.1334691', '10.1145/641007.641081', '10.1145/500178.500181', '10.1145/1101149.1101309', '10.1109/TCSVT.2004.826768', '10.1007/3-540-45113-7_25', '10.1109/ICME.2003.1221034', '10.1109/TMM.2008.917346', '10.1145/1180639.1180699', '10.1145/1180639.1180855', '10.1109/MMCS.1995.484921', '10.1145/641007.641073', '10.1109/TIP.2003.812758', '10.1145/1027527.1027680', '10.1016/j.cviu.2003.06.004', '10.1109/TNN.2005.845141', '10.1023/B:MTAP.0000046382.62218.e1', '10.1109/6046.985555', '10.1145/1026711.1026733', '10.1145/1126004.1126007'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Multimedia', 'Computer vision'], conference_acronym='IEEE transactions on multimedia', publisher=None, query_handler=None),\n",
       " 'Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm': Paper(DOI='10.1007/978-3-030-01267-0_44', crossref_json=None, google_schorlar_metadata=None, title='Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm', authors=['Zechun Liu', 'Baoyuan Wu', 'Wenhan Luo', 'Xin Yang', 'Wei Liu', 'Kwang-Ting Cheng'], abstract='In this work, we study the 1-bit convolutional neural networks (CNNs), of which both the weights and activations are binary. While being efficient, the classification accuracy of the current 1-bit CNNs is much worse compared with their counterpart real-valued CNN models on the large-scale dataset, like ImageNet. To shrink the performance gap between the 1-bit and real-valued CNN models, we propose a novel model, dubbed Bi-Real net, which connects the real activations (after the 1-bit convolution and/or BatchNorm layer, before the sign function) to that of the consecutive block, through an identity shortcut. Consequently, compared to the standard 1-bit CNN, the representational capability of the Bi-Real net is significantly enhanced, only with a negligible additional cost on computation. Moreover, we develop a specific training algorithm including three technical novelties for 1-bit CNNs. First, we derive a tight approximation to the derivative of the non-differentiable sign function with respect to activation. Second, we propose a magnitude-aware gradient with respect to weight to update the weight parameter. Last, we pre-train the real-valued CNN model with a clip function, rather than the ReLU function, to provide a better initialization for Bi-Real net. Experiments on ImageNet show that the Bi-Real net with proposed training algorithm achieves 56.4% and 62.2% top-1 accuracy with 18 layers and 34 layers, respectively, and achieves up to 23.9 X memory saving and 17.0 X computational reduction.', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-319-46484-8_45', '10.1109/CVPR.2014.81', '10.1109/CVPR.2016.90', '10.1007/978-3-319-46493-0_38', '10.1109/TPAMI.2015.2505283', '10.1007/978-3-319-46493-0_32', '10.1007/s11263-015-0816-y', '10.1109/CVPR.2013.446', '10.1109/CVPR.2015.7298594', '10.1109/TPAMI.2018.2845842', '10.1016/j.patcog.2016.10.022', '10.1109/ICCV.2013.355', '10.1109/CVPR.2017.331', '10.1109/ICCV.2017.454', '10.1109/ICCVW.2013.58', '10.1109/CVPR.2016.23'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Design', 'Automation', 'and Test for Integrated Circuits', 'Computer Vision', 'Medical Image Analysis'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'A partial scan method for sequential circuits with feedback': Paper(DOI='10.1109/12.54847', crossref_json=None, google_schorlar_metadata=None, title='A partial scan method for sequential circuits with feedback', authors=['K-T Cheng', 'Vishwani D.  Agrawal'], abstract='A method of partial scan design is presented in which the selection of scan flip-flops is aimed at breaking up the cyclic structure of the circuit. Experimental data are given to show that the test generation complexity may grow exponentially with the length of the cycles in the circuit. This complexity grows only linearly with the sequential depth. Graph-theoretic algorithms are presented to select a minimal set of flip-flops for eliminating cycles and reducing the sequential depth. Tests for the resulting circuit are generated by a sequential logic test generator. An independent control of the scan clock allows insertion of scan sequences within the vector sequence produced by the test generator. An independent control of the scan clock allows insertion of scan sequences within the vector sequences produced by the test generator. 98% fault coverage is obtained for a 5000-gate circuit by scanning just 5% of the flip-flops.< >', conference=None, journal=None, year=None, reference_list=['10.1109/ISCAS.1989.100748', '10.1109/TEST.1989.82320', '10.1109/ICCD.1988.25661', '10.1109/JSSC.1984.1052118', '10.1109/54.2032', '10.1109/ISCAS.1989.100747'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Design', 'Automation', 'and Test for Integrated Circuits', 'Computer Vision', 'Medical Image Analysis'], conference_acronym='I.E.E.E. transactions on computers (Print)', publisher=None, query_handler=None),\n",
       " 'Biocompatible and totally disintegrable semiconducting polymer for ultrathin and ultralightweight transient electronics': Paper(DOI='10.1073/pnas.1701478114', crossref_json=None, google_schorlar_metadata=None, title='Biocompatible and totally disintegrable semiconducting polymer for ultrathin and ultralightweight transient electronics', authors=['Ting Lei', 'Ming Guan', 'Jia Liu', 'Hung-Cheng Lin', 'Raphael Pfattner', 'Leo Shaw', 'Allister F McGuire', 'Tsung-Ching Huang', 'Leilai Shao', 'Kwang-Ting Cheng', 'Jeffrey B-H Tok', 'Zhenan Bao'], abstract='Increasing performance demands and shorter use lifetimes of consumer electronics have resulted in the rapid growth of electronic waste. Currently, consumer electronics are typically made with nondecomposable, nonbiocompatible, and sometimes even toxic materials, leading to serious ecological challenges worldwide. Here, we report an example of totally disintegrable and biocompatible semiconducting polymers for thin-film transistors. The polymer consists of reversible imine bonds and building blocks that can be easily decomposed under mild acidic conditions. In addition, an ultrathin (800-nm) biodegradable cellulose substrate with high chemical and thermal stability is developed. Coupled with iron electrodes, we have successfully fabricated fully disintegrable and biocompatible polymer transistors. Furthermore, disintegrable and biocompatible pseudo-complementary metal–oxide–semiconductor (CMOS\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1063/1.4953034', '10.1002/adma.201304821', '10.1126/science.1226325', '10.1039/C3CS60235D', '10.1002/adma.200902322', '10.1038/nmat1279', '10.1038/nature12314', '10.1038/nmat2459', '10.1126/science.aaa9306', '10.1038/nchem.1422', '10.1038/nature05533', '10.1002/adfm.201001031', '10.1002/adma.201102619', '10.1038/nature20102', '10.1002/adma.201201795', '10.1021/ar400254j', '10.1038/ncomms2573', '10.1038/nphoton.2013.188', '10.1038/ncomms1772', '10.1021/ma00108a054', '10.1039/c2cs15305j', '10.1021/ja307933t', '10.1038/nchem.873', '10.1038/nmat4671', '10.1038/nnano.2012.8', '10.1038/ncomms3982', '10.1038/nmat4624', '10.1039/C3EE43024C', '10.1038/ncomms8170', '10.1039/c3ee40492g', '10.1021/ma00154a010', '10.1016/S0032-3861(03)00283-0', '10.1126/science.aaf9062', '10.1021/ja209328m', '10.1109/TED.2010.2088127', '10.1038/ncomms11425', '10.1109/ISSCC.2016.7418025', '10.1038/nmat2745', '10.1038/ncomms5097', '10.1038/nmat3711', '10.1073/pnas.1320045111', '10.1002/adma.200903559', '10.1038/nature07727', '10.1126/science.290.5499.2123', '10.1016/S1369-7021(12)70139-6', '10.1002/adfm.201301847'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Design', 'Automation', 'and Test for Integrated Circuits', 'Computer Vision', 'Medical Image Analysis'], conference_acronym='Proceedings of the National Academy of Sciences of the United States of America', publisher=None, query_handler=None),\n",
       " 'Joint segment-level and pixel-wise losses for deep learning based retinal vessel segmentation': Paper(DOI='10.1109/tbme.2018.2828137', crossref_json=None, google_schorlar_metadata=None, title='Joint segment-level and pixel-wise losses for deep learning based retinal vessel segmentation', authors=['Zengqiang Yan', 'Xin Yang', 'Kwang-Ting Cheng'], abstract='Objective Deep learning based methods for retinal vessel segmentation are usually trained based on pixel-wise losses, which treat all vessel pixels with equal importance in pixel-to-pixel matching between a predicted probability map and the corresponding manually annotated segmentation. However, due to the highly imbalanced pixel ratio between thick and thin vessels in fundus images, a pixel-wise loss would limit deep learning models to learn features for accurate segmentation of thin vessels, which is an important task for clinical diagnosis of eye-related diseases. Methods In this paper, we propose a new segment-level loss which emphasizes more on the thickness consistency of thin vessels in the training process. By jointly adopting both the segment-level and the pixel-wise losses, the importance between thick and thin vessels in the loss calculation would be more balanced. As a result, more effective\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1167/iovs.08-3018', '10.1109/42.845178', '10.1109/ISBI.2017.7950512', '10.1109/TBME.2016.2535311', '10.1109/TMI.2017.2778748', '10.1109/34.161346', '10.1109/TMI.2016.2546227', '10.1109/TMI.2015.2409024', '10.1155/2013/154860', '10.1016/j.compbiomed.2010.02.008', '10.1109/TMI.2010.2043259', '10.1016/j.cmpb.2011.08.009', '10.1109/TBME.2015.2403295', '10.1016/j.media.2014.08.002', '10.1016/j.media.2015.09.002', '10.1109/TMI.2016.2587062', '10.1109/TMI.2006.879967', '10.1109/TMI.2007.898551', '10.1109/TMI.2015.2457891', '10.1109/TMI.2006.889732', '10.1109/TBME.2007.900804', '10.1109/ISBI.2016.7493362', '10.1109/TMI.2007.909827', '10.1016/j.media.2006.11.004', '10.1109/TMI.2006.879955', '10.1007/s00138-014-0636-z', '10.1109/TMI.2009.2017941', '10.1007/s10916-009-9299-0', '10.1109/TITB.2010.2052282', '10.1109/TBME.2012.2205687', '10.1109/TMI.2010.2064333', '10.1109/TSMC.1979.4310076', '10.1016/j.patcog.2011.01.007', '10.1145/2647868.2654889', '10.1109/TMI.2004.825627', '10.1109/CVPR.2015.7298965', '10.1109/CVPR.2015.7298965', '10.1109/TMI.2011.2167982', '10.1145/3065386'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Design', 'Automation', 'and Test for Integrated Circuits', 'Computer Vision', 'Medical Image Analysis'], conference_acronym='IEEE transactions on biomedical engineering (Print)', publisher=None, query_handler=None),\n",
       " 'Reactnet: Towards precise binary neural network with generalized activation functions': Paper(DOI='10.1007/978-3-030-58568-6_9', crossref_json=None, google_schorlar_metadata=None, title='Reactnet: Towards precise binary neural network with generalized activation functions', authors=['Zechun Liu', 'Zhiqiang Shen', 'Marios Savvides', 'Kwang-Ting Cheng'], abstract=' In this paper, we propose several ideas for enhancing a binary network to close its accuracy gap from real-valued networks without incurring any additional computational cost. We first construct a baseline network by modifying and binarizing a compact real-valued network with parameter-free shortcuts, bypassing all the intermediate convolutional layers including the downsampling layers. This baseline network strikes a good trade-off between accuracy and efficiency, achieving superior performance than most of existing binary networks at approximately half of the computational cost. Through extensive experiments and analysis, we observed that the performance of binary networks is sensitive to activation distribution variations. Based on this important observation, we propose to generalize the traditional Sign and PReLU functions, denoted as RSign and RPReLU for the respective generalized functions\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2017.574', '10.1109/CVPR.2019.01167', '10.1109/CVPR.2018.00452', '10.1609/aaai.v33i01.33018344', '10.1109/ICCV.2017.155', '10.1109/ISLPED.2013.6629302', '10.1109/CVPR.2019.00280', '10.1109/ICCV.2019.00339', '10.1007/978-3-030-01267-0_44', '10.1109/ICCV.2017.298', '10.1007/978-3-030-01264-9_8', '10.1109/CVPR42600.2020.01343', '10.1109/CVPR42600.2020.00232', '10.1007/978-3-319-46493-0_32', '10.1109/CVPR.2018.00474', '10.1609/aaai.v33i01.33014886', '10.1109/JPROC.2017.2761740', '10.1609/aaai.v31i1.10862', '10.1109/CVPR.2019.00066', '10.1007/978-3-030-01237-3_23', '10.1145/3343031.3350534', '10.1109/CVPR.2018.00716', '10.1109/CVPR.2019.00506', '10.1109/CVPR.2018.00826', '10.1109/CVPR.2019.00050'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Design', 'Automation', 'and Test for Integrated Circuits', 'Computer Vision', 'Medical Image Analysis'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'A three-stage deep learning model for accurate retinal vessel segmentation': Paper(DOI='10.1109/jbhi.2018.2872813', crossref_json=None, google_schorlar_metadata=None, title='A three-stage deep learning model for accurate retinal vessel segmentation', authors=['Zengqiang Yan', 'Xin Yang', 'Kwang-Ting Cheng'], abstract='Automatic retinal vessel segmentation is a fundamental step in the diagnosis of eye-related diseases, in which both thick vessels and thin vessels are important features for symptom detection. All existing deep learning models attempt to segment both types of vessels simultaneously by using a unified pixel-wise loss that treats all vessel pixels with equal importance. Due to the highly imbalanced ratio between thick vessels and thin vessels (namely the majority of vessel pixels belong to thick vessels), the pixelwise loss would be dominantly guided by thick vessels and relatively little influence comes from thin vessels, often leading to low segmentation accuracy for thin vessels. To address the imbalance problem, in this paper, we explore to segment thick vessels and thin vessels separately by proposing a three-stage deep learning model. The vessel segmentation task is divided into three stages, namely thick vessel\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1167/iovs.08-3018', '10.1109/42.845178', '10.1007/978-3-319-46723-8_16', '10.1109/ISBI.2016.7493362', '10.1109/TBME.2016.2535311', '10.1007/978-3-319-46723-8_17', '10.1109/TMI.2017.2778748', '10.1109/TMI.2016.2546227', '10.1109/ISBI.2017.7950512', '10.1109/TMI.2015.2457891', '10.1109/TMI.2006.879955', '10.1109/34.161346', '10.1016/j.media.2006.11.004', '10.1016/j.compbiomed.2010.02.008', '10.1016/j.cmpb.2011.08.009', '10.1109/TMI.2010.2043259', '10.1016/j.media.2014.08.002', '10.1016/j.media.2015.09.002', '10.1109/TMI.2016.2587062', '10.1109/TMI.2009.2017941', '10.1109/TMI.2015.2409024', '10.1109/CVPR.2015.7298965', '10.1109/TBME.2015.2403295', '10.1145/3065386', '10.1007/s00138-014-0636-z', '10.1088/1361-6560/aa6418', '10.1109/TPAMI.2017.2699184', '10.1007/978-3-319-16811-1_1', '10.1109/TMI.2007.909827', '10.1109/TMI.2006.889732', '10.1007/s10916-009-9299-0', '10.1109/TBME.2007.900804', '10.2337/diabetes.55.02.06.db05-0546', '10.1109/TMI.2004.825627', '10.1109/TMI.2007.898551', '10.1016/j.patcog.2011.01.007', '10.1109/TMI.2011.2167982', '10.1109/TMI.2006.879967', '10.1145/2647868.2654889', '10.1109/TITB.2010.2052282', '10.1109/TBME.2012.2205687', '10.1109/TMI.2010.2064333'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Design', 'Automation', 'and Test for Integrated Circuits', 'Computer Vision', 'Medical Image Analysis'], conference_acronym='IEEE journal of biomedical and health informatics (Print)', publisher=None, query_handler=None),\n",
       " 'Pseudo-CMOS: A design style for low-cost and robust flexible electronics': Paper(DOI='10.1109/ted.2010.2088127', crossref_json=None, google_schorlar_metadata=None, title='Pseudo-CMOS: A design style for low-cost and robust flexible electronics', authors=['Tsung-Ching Huang', 'Kenjiro Fukuda', 'Chun-Ming Lo', 'Yung-Hui Yeh', 'Tsuyoshi Sekitani', 'Takao Someya', 'Kwang-Ting Cheng'], abstract='Thin-film transistors (TFTs) are a key element of flexible electronics implemented on low-cost substrates. Most TFT technologies, however, have only monotype-either n- or p-type-devices. In this paper, we propose a novel design style Pseudo-CMOS for flexible electronics that uses only monotype single-V T  TFTs but has comparable performance with the complementary-type or dual-V T  designs. The manufacturing cost and complexity can therefore be significantly reduced, whereas the circuit yield and reliability are enhanced with built-in postfabrication tunability. Digital cells are fabricated in two different TFT technologies, i.e., p-type self-assembled-monolayer-organic TFTs and n-type metal-oxide InGaZnO TFTs, to validate the proposed Pseudo-CMOS design style. To the best of our knowledge, this is the first design solution that has been experimentally proven to achieve superior performance for both types of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1038/nmat2287', '10.1038/nature09054', '10.1038/nature05533', '10.1109/ISSCC.2009.4977381', '10.1063/1.2732819', '10.1109/ISSCC.2010.5434020', '10.1109/IEDM.2009.5424345', '10.1063/1.3179166', '10.1109/JSSC.2004.837259', '10.1145/1389089.1389092', '10.1109/ISSCC.2007.373444', '10.1109/DATE.2010.5457220', '10.1063/1.2198832', '10.1063/1.2844857', '10.1126/science.1121401', '10.1063/1.2206688', '10.1063/1.3409475', '10.1109/TED.2009.2015169', '10.1109/JSSC.2006.886556', '10.1002/adfm.200601161', '10.1063/1.3299017', '10.1109/JSSC.1984.1052218', '10.1109/JSSC.1985.1052349', '10.1109/JSSC.1983.1052001', '10.1109/JDT.2008.2010273', '10.1109/JSSC.1983.1051918'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Design', 'Automation', 'and Test for Integrated Circuits', 'Computer Vision', 'Medical Image Analysis'], conference_acronym='I.E.E.E. transactions on electron devices', publisher=None, query_handler=None),\n",
       " 'Formal equivalence checking and design debugging': Paper(DOI='10.1007/978-1-4615-5693-0_8', crossref_json=None, google_schorlar_metadata=None, title='Formal equivalence checking and design debugging', authors=['Shi-Yu Huang', 'Kwang-Ting Tim Cheng'], abstract=\"Formal Equivalence Checking and Design Debugging covers two major topics in design verification: logic equivalence checking and design debugging. The first part of the book reviews the design problems that require logic equivalence checking and describes the underlying technologies that are used to solve them. Some novel approaches to the problems of verifying design revisions after intensive sequential transformations such as retiming are described in detail. The second part of the book gives a thorough survey of previous and recent literature on design error diagnosis and design error correction. This part also provides an in-depth analysis of the algorithms used in two logic debugging software programs, ErrorTracer and AutoFix, developed by the authors. From the Foreword:With the adoption of the static sign-off approach to verifying circuit implementations the application-specific integrated circuit (ASIC) industry will experience the first radical methodological revolution since the adoption of logic synthesis. Equivalence checking is one of the two critical elements of this methodological revolution. This book is timely for either the designer seeking to better understand the mechanics of equivalence checking or for the CAD researcher who wishes to investigate well-motivated research problems such as equivalence checking of retimed designs or error diagnosis in sequential circuits.'Kurt Keutzer, University of California, Berkeley\", conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Design', 'Automation', 'and Test for Integrated Circuits', 'Computer Vision', 'Medical Image Analysis'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Automatic generation of functional vectors using the extended finite state machine model': Paper(DOI='10.1145/225871.225880', crossref_json=None, google_schorlar_metadata=None, title='Automatic generation of functional vectors using the extended finite state machine model', authors=['Kwang-Ting Cheng', 'Avinash S Krishnakumar'], abstract='We present a method of automatic generation of functional vectors for sequential circuits. These vectors can be used for design verification, manufacturing testing, or power estimation. A high-level description of the circuit in VHDL or C is assumed available. Our method automatically transforms the high-level description of a circuit in VHDL or C into an extended finite state machine (EFSM) model that is used to generate functional vectors. The EFSM model is a generalization of the traditional state machine model. It is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. The theoretical background of the EFSM model is addressed in this article. Our method guarantees that the generated vectors cover every statement in the high-level description at least once. Experimental results show that a set of comprehensive functional vectors for\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TC.1986.1676819', '10.1109/43.79502', '10.1109/43.7807', '10.1109/EDAC.1991.206393'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Design', 'Automation', 'and Test for Integrated Circuits', 'Computer Vision', 'Medical Image Analysis'], conference_acronym='ACM transactions on design automation of electronic systems', publisher=None, query_handler=None),\n",
       " 'Classification and identification of nonrobust untestable path delay faults': Paper(DOI='10.1109/43.511566', crossref_json=None, google_schorlar_metadata=None, title='Classification and identification of nonrobust untestable path delay faults', authors=['K-T Cheng', 'H-C Chen'], abstract='Recently published results have shown that, for many circuits, only a small percentage of path delay faults is robust testable, Among the robust untestable faults, a significant percentage is not nonrobust testable either. In this paper, we take a closer look at the properties of these nonrobust untestable faults with the goal of determining whether and how these faults should be tested. We define a path delay fault to be functional redundant (f-redundant) if, regardless of the delays at all other signals, the circuit performance will not be determined by the path. These paths are false paths-regardless of the delays of all signals. Therefore, these paths cannot and need not be tested. We present a sufficient condition for functional redundancy. We will show that nonrobust untestable faults are not necessarily f-redundant. For those nonrobust untestable but functional irredundant (f-irredundant) faults, the corresponding path may\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/DAC.1988.14749', '10.1109/43.88928', '10.1145/157485.164970', '10.1109/DAC.1990.114859', '10.1007/978-1-4615-3960-5', '10.1109/ICCAD.1991.185233', '10.1109/DAC.1992.227744', '10.1109/TEST.1993.470604', '10.1109/TC.1980.1675555', '10.1109/TEST.1992.527878', '10.1109/TCAD.1987.1270315', '10.1109/MDT.1984.5005692', '10.1109/ICCAD.1992.279312', '10.1109/MDT.1987.295104', '10.1109/ICCD.1989.63324', '10.1109/EDTC.1994.326861', '10.1109/ICCD.1994.331861', '10.1109/ICCAD.1993.580074'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Design', 'Automation', 'and Test for Integrated Circuits', 'Computer Vision', 'Medical Image Analysis'], conference_acronym='IEEE transactions on computer-aided design of integrated circuits and systems', publisher=None, query_handler=None),\n",
       " 'Local difference binary for ultrafast and distinctive feature description': Paper(DOI='10.1109/tpami.2013.150', crossref_json=None, google_schorlar_metadata=None, title='Local difference binary for ultrafast and distinctive feature description', authors=['Xin Yang', 'Kwang-Ting Tim Cheng'], abstract='The efficiency and quality of a feature descriptor are critical to the user experience of many computer vision applications. However, the existing descriptors are either too computationally expensive to achieve real-time performance, or not sufficiently distinctive to identify correct matches from a large database with various transformations. In this paper, we propose a highly efficient and distinctive binary descriptor, called local difference binary (LDB). LDB directly computes a binary string for an image patch using simple intensity and gradient difference tests on pairwise grid cells within the patch. A multiple-gridding strategy and a salient bit-selection method are applied to capture the distinct patterns of the patch at different spatial granularities. Experimental results demonstrate that compared to the existing state-of-the-art binary descriptors, primarily designed for speed, LDB has similar construction efficiency, while\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/ISMAR.2008.4637338', '10.1109/ISMAR.2009.5336497', '10.1109/TPAMI.2005.188', '10.1109/CVPR.2005.221', '10.1109/TPAMI.2007.1049', '10.1023/B:VISI.0000013087.49260.fb', '10.1109/CVPR.2009.5206839', '10.1145/1991996.1992031', '10.1109/ICCV.2011.6126542', '10.1109/ICCV.2011.6126544', '10.1109/CVPR.2007.383198', '10.1109/CVPR.2012.6247715', '10.1023/B:VISI.0000029664.99615.94', '10.1109/TPAMI.2008.275', '10.1109/ICCV.2009.5459272', '10.1109/CVPR.2007.383172', '10.1145/358669.358692'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Design', 'Automation', 'and Test for Integrated Circuits', 'Computer Vision', 'Medical Image Analysis'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Large margin component analysis': Paper(DOI='10.1016/s0893-6080(01)00106-x', crossref_json=None, google_schorlar_metadata=None, title='Large margin component analysis', authors=['Lorenzo Torresani', 'Kuang-chih Lee'], abstract='Metric learning has been shown to significantly improve the accuracy of k-nearest neighbor (kNN) classification. In problems involving thousands of features, distance learning algorithms cannot be used due to overfitting and high computational complexity. In such cases, previous work has relied on a two-step solution: first apply dimensionality reduction methods to the data, and then learn a metric in the resulting low-dimensional subspace. In this paper we show that better classification performance can be achieved by unifying the objectives of dimensionality reduction and metric learning. We propose a method that solves for the low-dimensional projection of the inputs, which minimizes a metric objective aimed at separating points in different classes by a large margin. This projection is defined by a significantly smaller number of parameters than metrics learned in input space, and thus our optimization reduces the risks of overfitting. Theory and results are presented for both a linear as well as a kernelized version of the algorithm. Overall, we achieve classification rates similar, and in several cases superior, to those of support vector machines.', conference=None, journal=None, year=None, reference_list=['10.1109/18.661502', '10.1016/S0031-3203(99)00120-X', '10.1016/S0031-3203(99)00186-7', '10.1023/A:1011328322315', '10.1162/neco.1992.4.6.888', '10.1109/T-C.1970.222918', '10.1162/neco.1997.9.7.1483', '10.2307/2981662', '10.1109/72.363467', '10.1109/34.391396', '10.1109/83.480770', '10.1007/3-540-49097-3_1', '10.1214/aos/1024691352', '10.1162/089976698300017467', '10.1016/0031-3203(76)90014-5'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Neural networks (Print)', publisher=None, query_handler=None),\n",
       " 'Object detection in video with spatiotemporal sampling networks': Paper(DOI='10.1007/978-3-030-01258-8_21', crossref_json=None, google_schorlar_metadata=None, title='Object detection in video with spatiotemporal sampling networks', authors=['Gedas Bertasius', 'Lorenzo Torresani', 'Jianbo Shi'], abstract='We propose a Spatiotemporal Sampling Network (STSN) that uses deformable convolutions across time for object detection in videos. Our STSN performs object detection in a video frame by learning to spatially sample features from the adjacent frames. This naturally renders the approach robust to occlusion or motion blur in individual frames. Our framework does not require additional supervision, as it optimizes sampling locations directly with respect to object detection performance. Our STSN outperforms the state-of-the-art on the ImageNet VID dataset and compared to prior video object detection methods it uses a simpler design, and does not require optical flow data for training.', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2015.7298594', '10.1109/CVPR.2015.7299067', '10.1109/CVPR.2016.90', '10.1109/CVPR.2017.634', '10.1109/CVPR.2014.214', '10.1109/CVPR.2017.650', '10.1007/978-3-319-10578-9_23', '10.1109/ICCV.2017.322', '10.1109/ICCV.2017.324', '10.1109/ICCV.2015.169', '10.1109/CVPR.2014.81', '10.1007/978-3-319-10584-0_23', '10.1007/978-3-319-46448-0_2', '10.1109/CVPR.2016.91', '10.1109/CVPR.2017.690', '10.1109/CVPR.2016.95', '10.1007/978-3-319-48881-3_6', '10.1109/ICCV.2017.52', '10.1109/ICCV.2017.89', '10.1007/s11263-015-0816-y', '10.1109/ICCV.2017.330', '10.1109/CVPR.2017.441'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Fine-tuning CNN image retrieval with no human annotation': Paper(DOI='10.1109/tpami.2018.2846566', crossref_json=None, google_schorlar_metadata=None, title='Fine-tuning CNN image retrieval with no human annotation', authors=['Filip Radenović', 'Giorgos Tolias', 'Ondřej Chum'], abstract='Image descriptors based on activations of Convolutional Neural Networks (CNNs) have become dominant in image retrieval due to their discriminative power, compactness of representation, and search efficiency. Training of CNNs, either from scratch or fine-tuning, requires a large amount of annotated data, where a high quality of annotation is often crucial. In this work, we propose to fine-tune CNNs for image retrieval on a large collection of unordered images in a fully automated manner. Reconstructed 3D models obtained by the state-of-the-art retrieval and structure-from-motion methods guide the selection of the training data. We show that both hard-positive and hard-negative examples, selected by exploiting the geometry and the camera positions available from the 3D models, enhance the performance of particular-object retrieval. CNN descriptor whitening discriminatively learned from the same training data\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2005.202', '10.1016/j.patcog.2014.04.007', '10.1109/CVPR.2007.383172', '10.1007/978-3-319-10599-4_25', '10.1109/CVPR.2013.207', '10.1145/2671188.2749366', '10.1109/CVPR.2011.5995601', '10.1109/TPAMI.2013.237', '10.1007/978-3-642-33712-3_2', '10.1007/978-3-642-15561-1_27', '10.1109/CVPR.2016.445', '10.1145/2001269.2001293', '10.1007/978-3-642-41062-8_2', '10.1109/CVPR.2010.5540009', '10.1007/s11263-017-1016-8', '10.1007/978-3-642-15552-9_57', '10.1145/2733373.2807412', '10.1109/TPAMI.2011.235', '10.1109/CVPR.2008.4587635', '10.1007/978-3-540-88682-2_24', '10.1007/s11263-015-0816-y', '10.1007/s11263-010-0363-5', '10.1109/CVPR.2016.90', '10.1007/978-3-319-46466-4_15', '10.1007/978-3-319-46448-0_1', '10.1109/CVPR.2016.517', '10.1007/s11263-012-0600-1', '10.1109/CVPR.2012.6248018', '10.1007/s11263-005-3848-x', '10.5244/C.23.91', '10.1109/CVPR.2015.7298636', '10.1109/ICCV.2007.4408871', '10.1145/3078971.3078987', '10.1007/978-3-319-46604-0_48', '10.1109/CVPR.2006.100', '10.1007/978-3-319-10590-1_54', '10.1109/CVPR.2014.222', '10.1007/978-3-319-10590-1_38', '10.1109/CVPR.2015.7299148', '10.1109/TPAMI.2009.166', '10.1109/ICCV.2013.432', '10.1109/CVPRW.2015.7301270', '10.1109/CVPR.2014.81', '10.1109/CVPRW.2014.131', '10.1007/978-3-319-10584-0_26', '10.3169/mta.4.251', '10.1145/2911996.2912061', '10.1109/ICCV.2017.374', '10.1109/CVPR.2014.180', '10.1109/CVPR.2014.242', '10.1007/978-3-319-24261-3_7', '10.1109/CVPR.2015.7298682'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'CNN image retrieval learns from BoW: Unsupervised fine-tuning with hard examples': Paper(DOI='10.1007/978-3-319-46448-0_1', crossref_json=None, google_schorlar_metadata=None, title='CNN image retrieval learns from BoW: Unsupervised fine-tuning with hard examples', authors=['Filip Radenović', 'Giorgos Tolias', 'Ondřej Chum'], abstract=' Convolutional Neural Networks (CNNs) achieve state-of-the-art performance in many computer vision tasks. However, this achievement is preceded by extreme manual annotation in order to perform either training from scratch or fine-tuning for the target task. In this work, we propose to fine-tune CNN for image retrieval from a large collection of unordered images in a fully automated manner. We employ state-of-the-art retrieval and Structure-from-Motion (SfM) methods to obtain 3D models, which are used to guide the selection of the training data for CNN fine-tuning. We show that both hard positive and hard negative examples enhance the final performance in particular object retrieval with compact codes.', conference=None, journal=None, year=None, reference_list=['10.1023/B:VISI.0000029664.99615.94', '10.1109/ICCV.2003.1238663', '10.1109/CVPR.2007.383172', '10.1007/978-3-642-33712-3_2', '10.1109/TPAMI.2013.237', '10.1109/CVPR.2011.5995601', '10.1016/j.patcog.2014.04.007', '10.1109/CVPR.2010.5540009', '10.1109/TPAMI.2011.235', '10.1145/2671188.2749366', '10.1109/CVPR.2013.207', '10.1007/978-3-319-10599-4_25', '10.1007/978-3-319-10590-1_38', '10.1007/s11263-015-0816-y', '10.1109/CVPRW.2014.131', '10.1109/CVPR.2014.81', '10.1007/978-3-319-10584-0_26', '10.1007/978-3-319-46604-0_48', '10.1109/CVPRW.2015.7301270', '10.1007/978-3-319-10590-1_54', '10.1109/CVPR.2014.222', '10.1109/CVPR.2014.242', '10.1109/CVPR.2014.180', '10.1109/CVPR.2015.7298682', '10.1007/978-3-319-24261-3_7', '10.1109/CVPR.2016.572', '10.1007/978-3-319-46466-4_15', '10.1109/TPAMI.2009.166', '10.1109/ICCV.2013.432', '10.1007/s11263-010-0363-5', '10.1109/CVPR.2015.7299148', '10.1007/978-3-642-33709-3_55', '10.1109/CVPR.2015.7298636', '10.1109/ICCV.2007.4408871', '10.1109/CVPR.2016.592', '10.1007/s11263-005-3848-x', '10.1109/CVPR.2012.6248018', '10.1007/s11263-012-0600-1', '10.1007/978-3-642-15561-1_27', '10.1145/2001269.2001293', '10.1007/978-3-642-41062-8_2', '10.1007/978-3-319-16808-1_9', '10.1007/978-3-642-15552-9_57', '10.1109/CVPR.2008.4587635', '10.1007/978-3-540-88682-2_24'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Optimal randomized RANSAC': Paper(DOI='10.1109/tpami.2007.70787', crossref_json=None, google_schorlar_metadata=None, title='Optimal randomized RANSAC', authors=['Ondřej Chum', 'Jiří Matas'], abstract=\"A randomized model verification strategy for RANSAC is presented. The proposed method finds, like RANSAC, a solution that is optimal with user-specified probability. The solution is found in time that is (i) close to the shortest possible and (ii) superior to any deterministic verification strategy. A provably fastest model verification strategy is designed for the (theoretical) situation when the contamination of data by outliers is known. In this case, the algorithm is the fastest possible (on average) of all randomized \\\\\\\\RANSAC algorithms guaranteeing a confidence in the solution. The derivation of the optimality property is based on Wald's theory of sequential decision making, in particular a modified sequential probability ratio test (SPRT). Next, the R-RANSAC with SPRT algorithm is introduced. The algorithm removes the requirement for a priori knowledge of the fraction of outliers and estimates the quantity online. We\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1109/ICCV.2005.198', '10.5244/C.19.78', '10.1109/ICCV.2003.1238341', '10.1006/cviu.1997.0559', '10.1109/ICPR.2004.1334020', '10.1006/cviu.1999.0832', '10.1145/358669.358692', '10.1016/j.imavis.2004.02.009'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Generalized cross entropy loss for training deep neural networks with noisy labels': Paper(DOI='10.1007/978-3-030-88013-2_7', crossref_json=None, google_schorlar_metadata=None, title='Generalized cross entropy loss for training deep neural networks with noisy labels', authors=['Zhilu Zhang', 'Mert R Sabuncu'], abstract=\"Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and large-scale datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.\", conference=None, journal=None, year=None, reference_list=['10.1109/TIP.2019.2902115', '10.1109/TNNLS.2013.2292894', '10.1109/JSTARS.2017.2762905', '10.1016/j.media.2020.101759', '10.1109/TSMCB.2012.2223460', '10.1109/CVPR.2017.240', '10.1109/Multi-Temp.2017.8035217', '10.1109/TPAMI.2017.2750680'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['medical image computing', 'AI for radiology', 'computer vision'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'VoxelMorph: a learning framework for deformable medical image registration': Paper(DOI='10.1109/tmi.2019.2897538', crossref_json=None, google_schorlar_metadata=None, title='VoxelMorph: a learning framework for deformable medical image registration', authors=['Guha Balakrishnan', 'Amy Zhao', 'Mert R Sabuncu', 'John Guttag', 'Adrian V Dalca'], abstract='We present VoxelMorph, a fast learning-based framework for deformable, pairwise medical image registration. Traditional registration methods optimize an objective function for each pair of images, which can be time-consuming for large datasets or rich deformation models. In contrast to this approach and building on recent learning-based methods, we formulate registration as a function that maps an input image pair to a deformation field that aligns these images. We parameterize the function via a convolutional neural network and optimize the parameters of the neural network on a set of images. Given a new pair of scans, VoxelMorph rapidly computes a deformation field by directly evaluating the function. In this paper, we explore two different training strategies. In the first (unsupervised) setting, we train the model to maximize standard image matching objective functions that are based on the image intensities\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.pneurobio.2011.09.005', '10.1007/s12021-013-9184-3', '10.1038/mp.2013.78', '10.1016/j.neuroimage.2015.03.069', '10.1016/j.neuroimage.2017.07.008', '10.1007/978-3-319-66182-7_27', '10.1016/j.neuroimage.2008.10.040', '10.1016/j.neuroimage.2009.01.002', '10.1073/pnas.0503892102', '10.1109/83.855431', '10.1007/978-3-319-66182-7_31', '10.1007/978-3-319-66182-7_40', '10.1007/978-3-319-66182-7_35', '10.1117/1.JMI.1.2.024003', '10.1109/CVPR.2016.459', '10.1109/ICCV.2013.175', '10.5244/C.30.145', '10.1109/CVPR.2017.632', '10.1016/j.neuroimage.2009.04.057', '10.1109/TMI.2005.853923', '10.2307/1932409', '10.1007/s11263-009-0219-z', '10.1007/978-3-540-30135-6_80', '10.1007/978-3-319-02126-3_3', '10.1162/jocn.2007.19.9.1498', '10.1016/j.neuroimage.2008.12.037', '10.1016/j.neuroimage.2012.01.021', '10.1109/TMI.2010.2049497', '10.1006/cviu.1997.0605', '10.1007/978-3-319-59050-9_44', '10.1006/nimg.2000.0582', '10.1109/TMI.2002.803111', '10.1109/42.796284', '10.1109/CVPR.2013.316', '10.1109/ICIP.2016.7532634', '10.1109/CVPR.2017.290', '10.1007/978-3-319-46493-0_18', '10.1109/CVPR.2017.82', '10.1109/CVPRW.2016.57', '10.1109/CVPR.2017.291', '10.1109/CVPR.2017.179', '10.1109/ICCV.2015.316', '10.1023/A:1007958904918', '10.1016/j.media.2007.06.004', '10.1007/978-3-319-67558-9_24', '10.1016/j.neuroimage.2007.07.007', '10.1007/978-3-319-47118-1_8', '10.1016/j.media.2008.03.006', '10.1016/S1361-8415(98)80022-4', '10.1016/j.cmpb.2009.09.002', '10.1016/j.neuroimage.2010.09.025', '10.1109/CVPR.2018.00964', '10.1007/978-3-030-00928-1_82', '10.1016/S0734-189X(89)80014-3', '10.1109/TMI.2013.2265603', '10.1109/TPAMI.2010.147', '10.1023/B:VISI.0000043755.93987.aa', '10.1016/0004-3702(81)90024-2', '10.1007/978-3-540-24673-2_3', '10.1109/TPAMI.2010.143', '10.1109/CVPR.2010.5539939', '10.1109/ISBI.2018.8363756', '10.1016/j.media.2018.07.002'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['medical image computing', 'AI for radiology', 'computer vision'], conference_acronym='IEEE transactions on medical imaging (Print)', publisher=None, query_handler=None),\n",
       " 'Multi-Atlas Segmentation of Biomedical Images: A Survey': Paper(DOI='10.1016/j.media.2015.06.012', crossref_json=None, google_schorlar_metadata=None, title='Multi-Atlas Segmentation of Biomedical Images: A Survey', authors=['Juan Eugenio Iglesias', 'Mert Rory Sabuncu'], abstract='Multi-atlas segmentation (MAS), first introduced and popularized by the pioneering work of Rohlfing, et\\xa0al. (2004), Klein, et\\xa0al. (2005), and Heckemann, et\\xa0al. (2006), is becoming one of the most widely-used and successful image segmentation techniques in biomedical applications. By manipulating and utilizing the entire dataset of “atlases” (training images that have been previously labeled, e.g., manually by an expert), rather than some model-based average representation, MAS has the flexibility to better capture anatomical variation, thus offering superior segmentation accuracy. This benefit, however, typically comes at a high computational cost. Recent advancements in computer hardware and image processing software have been instrumental in addressing this challenge and facilitated the wide adoption of MAS. Today, MAS has come a long way and the approach includes a wide array of sophisticated\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TMI.2014.2329603', '10.1016/j.neuroimage.2009.02.018', '10.1016/j.neuroimage.2008.07.055', '10.1007/s00429-013-0503-0', '10.1097/RCT.0b013e31828004ea', '10.1109/TMI.2009.2014372', '10.1006/nimg.2000.0582', '10.1016/j.neuroimage.2005.02.018', '10.1016/j.media.2014.01.003', '10.1118/1.4794478', '10.1109/TMI.2011.2147795', '10.1109/TMI.2012.2190992', '10.1016/j.media.2012.10.002', '10.1016/j.media.2014.06.005', '10.1016/j.neuroimage.2009.09.062', '10.1109/TMI.2014.2321281', '10.1016/j.neuroimage.2009.05.029', '10.1016/j.mri.2012.02.010', '10.1109/TMI.2013.2256922', '10.1016/j.neuroimage.2013.03.029', '10.1023/B:VISI.0000043755.93987.aa', '10.1016/j.neuroimage.2012.08.075', '10.1016/j.neuroimage.2007.04.031', '10.1118/1.4864236', '10.1109/TMI.2013.2290491', '10.1016/j.media.2013.02.006', '10.1002/hbm.22092', '10.1016/j.neuroimage.2007.11.047', '10.1109/42.650882', '10.1016/j.neurobiolaging.2013.02.002', '10.1002/hbm.460030304', '10.1109/TMI.2012.2197406', '10.1016/j.neuroimage.2010.09.018', '10.1097/00004728-199607000-00031', '10.1109/42.811271', '10.1016/j.acra.2013.09.010', '10.1371/journal.pone.0070059', '10.1016/S0896-6273(02)00569-X', '10.1118/1.4810971', '10.1118/1.4871623', '10.1016/j.neuroimage.2012.01.128', '10.1016/j.media.2008.03.006', '10.1109/LSP.2013.2279269', '10.1371/journal.pone.0059990', '10.1016/j.neuroimage.2007.11.034', '10.1016/j.neuroimage.2007.02.031', '10.3389/fnins.2012.00166', '10.1002/hbm.22359', '10.1117/1.JMI.1.3.034006', '10.1016/j.neuroimage.2006.05.061', '10.1016/j.neuroimage.2011.03.014', '10.1016/j.neuroimage.2010.01.072', '10.1186/1471-2342-10-1', '10.1126/science.1127647', '10.1109/TMI.2009.2025036', '10.1016/j.neuroimage.2014.11.031', '10.1016/j.media.2013.04.005', '10.1016/j.media.2013.08.001', '10.1109/TMI.2008.2011480', '10.1038/npp.2014.185', '10.1016/j.neuroimage.2011.07.036', '10.1016/S1361-8415(96)80008-9', '10.1016/j.neuroimage.2011.01.078', '10.1038/nn.3606', '10.1016/j.neuroimage.2011.11.040', '10.1118/1.3512795', '10.1186/1471-2342-5-7', '10.1109/TMI.2009.2035616', '10.2174/1567205011666140131123653', '10.1016/j.media.2013.04.013', '10.1002/mrm.24661', '10.1002/(SICI)1097-0193(1997)5:4<238::AID-HBM6>3.0.CO;2-4', '10.1371/journal.pone.0109113', '10.1016/j.neuroimage.2011.07.085', '10.1109/TMI.2011.2172215', '10.1118/1.4816654', '10.1109/TMI.2010.2057442', '10.1016/j.media.2013.05.011', '10.1118/1.4893533', '10.1016/j.neuroimage.2010.12.067', '10.1016/j.neuroimage.2010.03.018', '10.1016/j.media.2014.06.007', '10.1109/TMI.2012.2230018', '10.1016/j.neuroimage.2008.07.058', '10.1109/TMI.2014.2303821', '10.1016/j.neuroimage.2009.10.026', '10.1371/journal.pone.0086576', '10.1109/TMI.2014.2322280', '10.1016/j.cmpb.2009.09.002', '10.1007/s12021-012-9163-0', '10.1016/j.media.2010.07.002', '10.1117/1.JMI.1.2.024002', '10.1109/TMI.2003.809139', '10.1016/j.neuroimage.2014.03.037', '10.1146/annurev.bioeng.2.1.315', '10.1016/j.neuroimage.2014.04.054', '10.1155/2014/182909', '10.3233/JAD-2012-111931', '10.1016/j.neuroimage.2005.11.044', '10.1016/j.media.2013.10.001', '10.1016/0167-8655(94)90127-9', '10.3389/fneur.2014.00071', '10.1016/j.neuroimage.2012.02.084', '10.1118/1.3147146', '10.1016/j.media.2009.10.001', '10.1109/TMI.2003.819299', '10.1016/j.neuroimage.2003.11.010', '10.1016/j.patrec.2005.03.017', '10.1109/TMI.2011.2156806', '10.1109/42.796284', '10.1109/TMI.2013.2276943', '10.1109/TMI.2010.2050897', '10.1109/42.552054', '10.1109/TMI.2014.2327516', '10.1120/jacmp.v15i4.4468', '10.1016/j.media.2009.12.004', '10.1016/j.media.2014.05.008', '10.1016/j.neuroimage.2010.02.025', '10.1016/j.cmpb.2012.12.006', '10.1186/s13014-014-0251-1', '10.1186/1748-717X-8-229', '10.1109/TMI.2013.2265603', '10.1016/j.neuroimage.2004.10.017', '10.1109/TBME.2012.2186612', '10.1371/journal.pone.0065591', '10.1371/journal.pone.0096985', '10.1186/1475-925X-9-30', '10.1016/j.neuroimage.2010.04.024', '10.1109/TMI.2011.2168420', '10.1016/j.neuroimage.2008.10.040', '10.1109/TPAMI.2012.143', '10.1118/1.4868455', '10.1016/j.neuroimage.2013.11.040', '10.1016/j.neuroimage.2013.08.008', '10.1109/TMI.2004.828354', '10.1002/hbm.22529', '10.1111/epi.12408', '10.1016/j.neuroimage.2009.09.069', '10.1109/TMI.2013.2265805', '10.1016/j.media.2013.10.013', '10.1016/j.neuroimage.2006.07.050', '10.1002/hbm.22478', '10.1118/1.4867855', '10.1016/j.media.2008.06.005', '10.1109/TMI.2010.2049497', '10.1016/j.neuroimage.2006.01.015', '10.1016/j.neuroimage.2010.06.040', '10.1016/j.media.2014.06.010'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['medical image computing', 'AI for radiology', 'computer vision'], conference_acronym='Medical image analysis (Print)', publisher=None, query_handler=None),\n",
       " 'A generative model for image segmentation based on label fusion': Paper(DOI='10.1016/j.media.2013.10.013', crossref_json=None, google_schorlar_metadata=None, title='A generative model for image segmentation based on label fusion', authors=['Mert R Sabuncu', 'BT Thomas Yeo', 'Koen Van Leemput', 'Bruce Fischl', 'Polina Golland'], abstract='We propose a nonparametric, probabilistic model for the automatic segmentation of medical images, given a training set of images and corresponding label maps. The resulting inference algorithms rely on pairwise registrations between the test image and individual training images. The training labels are then transferred to the test image and fused to compute the final segmentation of the test subject. Such label fusion methods have been shown to yield accurate segmentation, since the use of multiple registrations captures greater inter-subject anatomical variability and improves robustness against occasional registration failures. To the best of our knowledge, this manuscript presents the first comprehensive probabilistic framework that rigorously motivates label fusion as a segmentation approach. The proposed framework allows us to compare different label fusion algorithms theoretically and practically. In\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TMI.2009.2014372', '10.1016/j.media.2007.06.004', '10.1109/TPAMI.2006.64', '10.1137/040616024', '10.1007/11784012_16', '10.1016/j.neuroimage.2010.09.018', '10.1212/01.wnl.0000256697.20968.d7', '10.1016/S0197-4580(01)00271-8', '10.1214/009053604000000067', '10.1002/hbm.20744', '10.1371/journal.pone.0042325', '10.1002/jmri.10163', '10.1118/1.2335487', '10.1002/jmri.22003', '10.1126/science.283.5409.1908', '10.1109/TIP.2008.2008067', '10.1109/TMI.2011.2156806', '10.1016/j.neuroimage.2012.05.042', '10.1016/j.neuroimage.2004.07.051', '10.1109/TMI.2010.2046908', '10.1007/978-3-540-85988-8_90', '10.1016/j.neuroimage.2008.10.040', '10.1126/science.287.5456.1273', '10.1109/TPAMI.2012.143', '10.1109/CVPR.2011.5995382', '10.1109/TMI.2004.828354', '10.1093/cercor/bhq165', '10.1002/hbm.22233', '10.1007/978-3-540-73273-0_14', '10.1002/hbm.21209', '10.1016/j.neuroimage.2009.10.065', '10.1214/07-AOAS147', '10.1109/ISBI.2013.6556696', '10.1007/978-3-642-33530-3_8', '10.1016/j.media.2011.08.004', '10.1016/j.media.2012.07.007'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['medical image computing', 'AI for radiology', 'computer vision'], conference_acronym='Medical image analysis (Print)', publisher=None, query_handler=None),\n",
       " 'Unsupervised learning for fast probabilistic diffeomorphic registration': Paper(DOI='10.1016/j.media.2019.07.006', crossref_json=None, google_schorlar_metadata=None, title='Unsupervised learning for fast probabilistic diffeomorphic registration', authors=['Adrian V Dalca', 'Guha Balakrishnan', 'John Guttag', 'Mert R Sabuncu'], abstract='Traditional deformable registration techniques achieve impressive results and offer a rigorous theoretical treatment, but are computationally intensive since they solve an optimization problem for each image pair. Recently, learning-based methods have facilitated fast registration by learning spatial deformation functions. However, these approaches use restricted deformation models, require supervised labels, or do not guarantee a diffeomorphic (topology-preserving) registration. Furthermore, learning-based registration tools have not been derived from a probabilistic framework that can offer uncertainty estimates. In this paper, we present a probabilistic generative model and derive an unsupervised learning-based inference algorithm that makes use of recent developments in convolutional neural networks (CNNs). We demonstrate our method on a 3D brain registration task, and provide an empirical analysis of\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1088/1361-6560/aa8b37', '10.1016/j.neuroimage.2007.07.007', '10.1006/nimg.2000.0582', '10.1016/j.media.2007.06.004', '10.1016/S0734-189X(89)80014-3', '10.1109/CVPR.2018.00964', '10.1023/B:VISI.0000043755.93987.aa', '10.1109/TMI.2005.853923', '10.1016/j.neuroimage.2009.04.057', '10.1006/cviu.1997.0605', '10.1038/mp.2013.78', '10.2307/1932409', '10.1016/j.neuroimage.2012.01.021', '10.1016/j.media.2008.03.006', '10.1007/s12021-013-9184-3', '10.1109/TMI.2013.2246577', '10.1016/j.media.2015.09.005', '10.1007/s11263-009-0219-z', '10.1038/sdata.2015.31', '10.1016/j.media.2018.07.002', '10.1109/83.855431', '10.1016/j.neuroimage.2008.12.037', '10.1109/TMI.2019.2897112', '10.1109/ISBI.2018.8363757', '10.1162/jocn.2007.19.9.1498', '10.1016/j.pneurobio.2011.09.005', '10.1109/TMI.2003.815868', '10.1073/pnas.0503892102', '10.1117/1.JMI.1.2.024003', '10.1016/j.cmpb.2009.09.002', '10.1137/S00361445024180', '10.1109/CVPR.2019.00866', '10.1016/j.neuroimage.2009.01.002', '10.1007/10704282_64', '10.1109/TMI.2008.2004426', '10.1016/j.media.2013.03.002', '10.1109/42.796284', '10.1109/TMI.2002.803111', '10.1016/S1361-8415(98)80022-4', '10.1016/j.neuroimage.2008.10.040', '10.1016/j.media.2018.11.010', '10.1016/j.neuroimage.2017.07.008', '10.1109/TMI.2010.2049497'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['medical image computing', 'AI for radiology', 'computer vision'], conference_acronym='Medical image analysis (Print)', publisher=None, query_handler=None),\n",
       " 'Spherical demons: fast diffeomorphic landmark-free surface registration': Paper(DOI='10.1109/tmi.2009.2030797', crossref_json=None, google_schorlar_metadata=None, title='Spherical demons: fast diffeomorphic landmark-free surface registration', authors=['BT Thomas Yeo', 'Mert R Sabuncu', 'Tom Vercauteren', 'Nicholas Ayache', 'Bruce Fischl', 'Polina Golland'], abstract='We present the Spherical Demons algorithm for registering two spherical images. By exploiting spherical vector spline interpolation theory, we show that a large class of regularizors for the modified Demons objective function can be efficiently approximated on the sphere using iterative smoothing. Based on one parameter subgroups of diffeomorphisms, the resulting registration is diffeomorphic and fast. The Spherical Demons algorithm can also be modified to register a given spherical image to a probabilistic atlas. We demonstrate two variants of the algorithm corresponding to warping the atlas or warping the subject. Registration of a cortical surface mesh to an atlas mesh, both with more than 160 k nodes requires less than 5 min when warping the atlas and less than 3 min when warping the subject on a Xeon 3.2 GHz single processor machine. This is comparable to the fastest nondiffeomorphic landmark-free\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-3-540-75757-3_45', '10.1016/B978-012693019-1/50023-X', '10.1109/TMI.2009.2025654', '10.1016/j.neuroimage.2004.09.016', '10.1016/j.neuroimage.2006.07.036', '10.1023/B:JMIV.0000011326.88682.e5', '10.1007/11505730_39', '10.1109/ISBI.2008.4541188', '10.1109/TMI.2007.892506', '10.1023/A:1008001603737', '10.1007/978-3-540-73273-0_61', '10.1093/cercor/bhg087', '10.1073/pnas.95.3.788', '10.1002/(SICI)1097-0193(1999)8:4<272::AID-HBM10>3.0.CO;2-4', '10.1016/j.neuroimage.2008.10.040', '10.1016/j.media.2008.06.005', '10.1006/nimg.1999.0516', '10.1111/j.1467-9868.2007.00574.x', '10.1016/j.neuroimage.2006.01.021', '10.1016/j.media.2008.06.010', '10.1109/ICPR.1994.576361', '10.1016/j.neuroimage.2004.12.034', '10.1007/978-3-540-75757-3_84', '10.1006/nimg.1998.0396', '10.1093/cercor/bhm225', '10.1007/978-3-540-75757-3_23', '10.1006/nimg.2001.0975', '10.1002/(SICI)1097-0193(200002)9:2<81::AID-HBM3>3.0.CO;2-8', '10.1109/42.511745', '10.1016/S1361-8415(98)80022-4', '10.1109/TMI.2002.1009387', '10.1002/(SICI)1097-0193(1998)6:5/6<339::AID-HBM3>3.0.CO;2-Q', '10.1006/nimg.1998.0385', '10.1016/S1361-8415(02)00052-X', '10.1109/TPAMI.2008.51', '10.1016/S1077-3142(03)00002-X', '10.1007/978-3-540-72823-8_74', '10.1109/83.536892', '10.1016/j.neuroimage.2004.12.052', '10.1006/nimg.1998.0395', '10.1016/j.neuroimage.2007.07.007', '10.1002/ana.20426', '10.1006/nimg.1999.0437', '10.1117/12.348628', '10.1016/j.neuroimage.2004.07.010', '10.1023/B:VISI.0000043755.93987.aa', '10.1007/3-540-45787-9_70', '10.1023/A:1008282127190', '10.1007/978-1-4612-4350-2', '10.1117/12.179275', '10.1016/S1361-8415(00)00024-4', '10.1007/978-3-540-73273-0_33'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['medical image computing', 'AI for radiology', 'computer vision'], conference_acronym='IEEE transactions on medical imaging (Print)', publisher=None, query_handler=None),\n",
       " 'Statistical analysis of longitudinal neuroimage data with linear mixed effects models': Paper(DOI='10.1016/j.neuroimage.2012.10.065', crossref_json=None, google_schorlar_metadata=None, title='Statistical analysis of longitudinal neuroimage data with linear mixed effects models', authors=['Jorge L Bernal-Rusiel', 'Douglas N Greve', 'Martin Reuter', 'Bruce Fischl', 'Mert R Sabuncu'], abstract='Longitudinal neuroimaging (LNI) studies are rapidly becoming more prevalent and growing in size. Today, no standardized computational tools exist for the analysis of LNI data and widely used methods are sub-optimal for the types of data encountered in real-life studies. Linear Mixed Effects (LME) modeling, a mature approach well known in the statistics community, offers a powerful and versatile framework for analyzing real-life LNI data. This article presents the theory behind LME models, contrasts it with other popular approaches in the context of LNI, and is accompanied with an array of computational tools that will be made freely available through FreeSurfer — a popular Magnetic Resonance Image (MRI) analysis software package. Our core contribution is to provide a quantitative empirical evaluation of the performance of LME and competing alternatives popularly used in prior longitudinal structural MRI\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.neuroimage.2011.08.066', '10.1016/j.neuroimage.2011.07.007', '10.1176/appi.ajp.158.8.1248', '10.1016/j.neuroimage.2004.06.018', '10.1016/j.neuroimage.2005.05.015', '10.1080/01621459.1979.10481038', '10.1006/nimg.1998.0395', '10.1093/cercor/12.7.767', '10.1016/j.schres.2005.05.009', '10.1002/ana.22509', '10.1602/neurorx.2.2.348', '10.1016/S0197-4580(01)00271-8', '10.1002/hbm.21353', '10.1016/j.neuroimage.2012.01.021', '10.1006/nimg.1998.0396', '10.1002/(SICI)1097-0193(1999)8:4<272::AID-HBM10>3.0.CO;2-4', '10.1016/S0896-6273(02)00569-X', '10.1523/JNEUROSCI.3252-09.2009', '10.1212/01.WNL.0000154530.72969.11', '10.1093/brain/awp132', '10.1002/hbm.21304', '10.1016/S0140-6736(06)68542-5', '10.1212/WNL.54.4.813', '10.1038/13158', '10.1002/hbm.21334', '10.1002/sim.4780111411', '10.1001/archpsyc.60.6.585', '10.1073/pnas.0906053106', '10.1002/hbm.21386', '10.1002/hbm.20498', '10.1016/j.neurobiolaging.2010.04.033', '10.1212/WNL.49.3.786', '10.1212/01.wnl.0000281688.77598.35', '10.1093/brain/awp062', '10.1016/S1474-4422(09)70299-6', '10.1001/archneurol.2011.3405', '10.1002/ana.21223', '10.1111/j.1399-5618.2009.00722.x', '10.1001/archneur.59.10.1572', '10.1001/archpsyc.60.8.766', '10.2307/2533558', '10.1080/01621459.1987.10478395', '10.1016/j.neuroimage.2008.04.252', '10.1093/cercor/bhh200', '10.1001/archpsyc.58.2.148', '10.1542/peds.2008-0025', '10.1002/hbm.1058', '10.1016/S0140-6736(03)12323-9', '10.1093/brain/awl021', '10.1212/WNL.0b013e3181d3e3e9', '10.1016/j.neuroimage.2011.02.076', '10.1016/j.neuroimage.2010.07.020', '10.1016/j.neuroimage.2012.02.084', '10.1001/archneurol.2011.167', '10.2307/3002019', '10.1523/JNEUROSCI.5714-09.2010', '10.1523/JNEUROSCI.5309-07.2008', '10.1016/j.bandl.2010.03.007', '10.1148/radiol.2482070938', '10.1007/s00330-009-1512-5', '10.1016/j.neuroimage.2004.07.051', '10.1016/j.neuroimage.2011.04.003', '10.1016/j.neuroimage.2010.04.258', '10.1016/j.neuroimage.2011.07.056', '10.1016/j.neuroimage.2006.11.054', '10.1176/appi.ajp.2011.10111690', '10.1016/j.neurobiolaging.2010.04.030', '10.1093/brain/awm021', '10.1212/WNL.0b013e318227047f'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['medical image computing', 'AI for radiology', 'computer vision'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'The dynamics of cortical and hippocampal atrophy in Alzheimer disease': Paper(DOI='10.1001/archneurol.2011.167', crossref_json=None, google_schorlar_metadata=None, title='The dynamics of cortical and hippocampal atrophy in Alzheimer disease', authors=['Mert R Sabuncu', 'Rahul S Desikan', 'Jorge Sepulcre', 'Boon Thye T Yeo', 'Hesheng Liu', 'Nicholas J Schmansky', 'Martin Reuter', 'Michael W Weiner', 'Randy L Buckner', 'Reisa A Sperling', 'Bruce Fischl', \"Alzheimer's Disease Neuroimaging Initiative\"], abstract='ObjectiveTo characterize rates of regional Alzheimer disease (AD)–specific brain atrophy across the presymptomatic, mild cognitive impairment, and dementia stages.DesignMulticenter case-control study of neuroimaging, cerebrospinal fluid, and cognitive test score data from the Alzheimer’s Disease Neuroimaging Initiative.SettingResearch centers across the United States and Canada.PatientsWe examined a total of 317 participants with baseline cerebrospinal fluid biomarker measurements and 3 T1-weighted magnetic resonance images obtained within 1 year.Main Outcome MeasuresWe used automated tools to compute annual longitudinal atrophy in the hippocampus and cortical regions targeted in AD. We used Mini-Mental State Examination scores as a measure of cognitive performance. We performed a cross-subject analysis of atrophy rates and acceleration on individuals with an AD-like cerebrospinal\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['medical image computing', 'AI for radiology', 'computer vision'], conference_acronym='Archives of neurology (Chicago)', publisher=None, query_handler=None),\n",
       " 'Stepwise connectivity of the modal cortex reveals the multimodal organization of the human brain': Paper(DOI='10.1523/jneurosci.0759-12.2012', crossref_json=None, google_schorlar_metadata=None, title='Stepwise connectivity of the modal cortex reveals the multimodal organization of the human brain', authors=['Jorge Sepulcre', 'Mert R Sabuncu', 'Thomas B Yeo', 'Hesheng Liu', 'Keith A Johnson'], abstract='How human beings integrate information from external sources and internal cognition to produce a coherent experience is still not well understood. During the past decades, anatomical, neurophysiological and neuroimaging research in multimodal integration have stood out in the effort to understand the perceptual binding properties of the brain. Areas in the human lateral occipitotemporal, prefrontal, and posterior parietal cortices have been associated with sensory multimodal processing. Even though this, rather patchy, organization of brain regions gives us a glimpse of the perceptual convergence, the articulation of the flow of information from modality-related to the more parallel cognitive processing systems remains elusive. Using a method called stepwise functional connectivity analysis, the present study analyzes the functional connectome and transitions from primary sensory cortices to higher-order brain\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['medical image computing', 'AI for radiology', 'computer vision'], conference_acronym='The Journal of neuroscience', publisher=None, query_handler=None),\n",
       " 'Unsupervised learning of probabilistic diffeomorphic registration for images and surfaces': Paper(DOI='10.1016/j.media.2019.07.006', crossref_json=None, google_schorlar_metadata=None, title='Unsupervised learning of probabilistic diffeomorphic registration for images and surfaces', authors=['Adrian V Dalca', 'Guha Balakrishnan', 'John Guttag', 'Mert R Sabuncu'], abstract='Classical deformable registration techniques achieve impressive results and offer a rigorous theoretical treatment, but are computationally intensive since they solve an optimization problem for each image pair. Recently, learning-based methods have facilitated fast registration by learning spatial deformation functions. However, these approaches use restricted deformation models, require supervised labels, or do not guarantee a diffeomorphic (topology-preserving) registration. Furthermore, learning-based registration tools have not been derived from a probabilistic framework that can offer uncertainty estimates.In this paper, we build a connection between classical and learning-based methods. We present a probabilistic generative model and derive an unsupervised learning-based inference algorithm that uses insights from classical registration methods and makes use of recent developments in convolutional\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1088/1361-6560/aa8b37', '10.1016/j.neuroimage.2007.07.007', '10.1006/nimg.2000.0582', '10.1016/j.media.2007.06.004', '10.1016/S0734-189X(89)80014-3', '10.1109/CVPR.2018.00964', '10.1023/B:VISI.0000043755.93987.aa', '10.1109/TMI.2005.853923', '10.1016/j.neuroimage.2009.04.057', '10.1006/cviu.1997.0605', '10.1038/mp.2013.78', '10.2307/1932409', '10.1016/j.neuroimage.2012.01.021', '10.1016/j.media.2008.03.006', '10.1007/s12021-013-9184-3', '10.1109/TMI.2013.2246577', '10.1016/j.media.2015.09.005', '10.1007/s11263-009-0219-z', '10.1038/sdata.2015.31', '10.1016/j.media.2018.07.002', '10.1109/83.855431', '10.1016/j.neuroimage.2008.12.037', '10.1109/TMI.2019.2897112', '10.1109/ISBI.2018.8363757', '10.1162/jocn.2007.19.9.1498', '10.1016/j.pneurobio.2011.09.005', '10.1109/TMI.2003.815868', '10.1073/pnas.0503892102', '10.1117/1.JMI.1.2.024003', '10.1016/j.cmpb.2009.09.002', '10.1137/S00361445024180', '10.1109/CVPR.2019.00866', '10.1016/j.neuroimage.2009.01.002', '10.1007/10704282_64', '10.1109/TMI.2008.2004426', '10.1016/j.media.2013.03.002', '10.1109/42.796284', '10.1109/TMI.2002.803111', '10.1016/S1361-8415(98)80022-4', '10.1016/j.neuroimage.2008.10.040', '10.1016/j.media.2018.11.010', '10.1016/j.neuroimage.2017.07.008', '10.1109/TMI.2010.2049497'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['medical image computing', 'AI for radiology', 'computer vision'], conference_acronym='Medical image analysis (Print)', publisher=None, query_handler=None),\n",
       " 'Neurobiological basis of head motion in brain imaging': Paper(DOI='10.1073/pnas.1317424111', crossref_json=None, google_schorlar_metadata=None, title='Neurobiological basis of head motion in brain imaging', authors=['Ling-Li Zeng', 'Danhong Wang', 'Michael D Fox', 'Mert Sabuncu', 'Dewen Hu', 'Manling Ge', 'Randy L Buckner', 'Hesheng Liu'], abstract='Individual differences in brain metrics, especially connectivity measured with functional MRI, can correlate with differences in motion during data collection. The assumption has been that motion causes artifactual differences in brain connectivity that must and can be corrected. Here we propose that differences in brain connectivity can also represent a neurobiological trait that predisposes to differences in motion. We support this possibility with an analysis of intra- versus intersubject differences in connectivity comparing high- to low-motion subgroups. Intersubject analysis identified a correlate of head motion consisting of reduced distant functional connectivity primarily in the default network in individuals with high head motion. Similar connectivity differences were not found in analysis of intrasubject data. Instead, this correlate of head motion was a stable property in individuals across time. These findings suggest\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1002/mrm.1910350312', '10.1002/mrm.10677', '10.2967/jnumed.110.079343', '10.1109/42.563659', '10.1109/23.775596', '10.1109/TMI.2003.814790', '10.1109/TBME.2009.2038667', '10.1126/science.1194144', '10.1073/pnas.0800376105', '10.1073/pnas.0705843104', '10.1016/j.neuron.2007.10.038', '10.1093/cercor/bhm207', '10.1097/WCO.0b013e328306f2c5', '10.1016/j.neuroimage.2011.12.063', '10.1016/j.neuroimage.2011.10.018', '10.1016/j.neuroimage.2011.07.044', '10.1016/j.biopsych.2004.11.047', '10.1523/JNEUROSCI.2335-09.2009', '10.1093/cercor/bhs126', '10.1523/JNEUROSCI.4865-10.2011', '10.1162/jocn_a_00227', '10.1523/JNEUROSCI.0536-12.2012', '10.1016/j.neuroimage.2012.08.052', '10.1371/journal.pcbi.1000808', '10.1177/1073858411403316', '10.1016/j.neuroimage.2013.03.004', '10.1073/pnas.1301725110', '10.1016/j.neuroimage.2007.11.059', '10.1016/j.neuroimage.2013.04.001', '10.1016/j.neuroimage.2009.04.048', '10.1016/j.neuroimage.2003.11.025', '10.1152/jn.00598.2012', '10.1152/jn.00338.2011', '10.1016/j.neuroimage.2007.12.025', '10.1016/j.neuroimage.2009.10.003', '10.1523/JNEUROSCI.5062-08.2009', '10.1016/j.neuroimage.2005.06.058'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['medical image computing', 'AI for radiology', 'computer vision'], conference_acronym='Proceedings of the National Academy of Sciences of the United States of America', publisher=None, query_handler=None),\n",
       " 'Genetic variation and neuroimaging measures in Alzheimer disease': Paper(DOI='10.1001/archneurol.2010.108', crossref_json=None, google_schorlar_metadata=None, title='Genetic variation and neuroimaging measures in Alzheimer disease', authors=['Alessandro Biffi', 'Christopher D Anderson', 'Rahul S Desikan', 'Mert Sabuncu', 'Lynelle Cortellini', 'Nick Schmansky', 'David Salat', 'Jonathan Rosand', \"Alzheimer's Disease Neuroimaging Initiative (ADNI\"], abstract='ObjectiveTo investigate whether genome-wide association study (GWAS)–validated and GWAS-promising candidate loci influence magnetic resonance imaging measures and clinical Alzheimer’s disease (AD) status.DesignMulticenter case-control study of genetic and neuroimaging data from the Alzheimer’s Disease Neuroimaging Initiative.SettingMulticenter GWAS.PatientsA total of 168 individuals with probable AD, 357 with mild cognitive impairment, and 215 cognitively normal control individuals recruited from more than 50 Alzheimer’s Disease Neuroimaging Initiative centers in the United States and Canada. All study participants hadAPOEand genome-wide genetic data available.Main Outcome MeasuresWe investigated the influence of GWAS-validated and GWAS-promising novel AD loci on hippocampal volume, amygdala volume, white matter lesion volume, entorhinal cortex thickness, parahippocampal\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['medical image computing', 'AI for radiology', 'computer vision'], conference_acronym='Archives of neurology (Chicago)', publisher=None, query_handler=None),\n",
       " 'A surface-based analysis of language lateralization and cortical asymmetry': Paper(DOI='10.1162/jocn_a_00405', crossref_json=None, google_schorlar_metadata=None, title='A surface-based analysis of language lateralization and cortical asymmetry', authors=['Douglas N Greve', 'Lise Van der Haegen', 'Qing Cai', 'Steven Stufflebeam', 'Mert R Sabuncu', 'Bruce Fischl', 'Marc Brysbaert'], abstract=\" Among brain functions, language is one of the most lateralized. Cortical language areas are also some of the most asymmetrical in the brain. An open question is whether the asymmetry in function is linked to the asymmetry in anatomy. To address this question, we measured anatomical asymmetry in 34 participants shown with fMRI to have language dominance of the left hemisphere (LLD) and 21 participants shown to have atypical right hemisphere dominance (RLD). All participants were healthy and left-handed, and most (80%) were female. Gray matter (GM) volume asymmetry was measured using an automated surface-based technique in both ROIs and exploratory analyses. In the ROI analysis, a significant difference between LLD and RLD was found in the insula. No differences were found in planum temporale (PT), pars opercularis (POp), pars triangularis (PTr), or Heschl's gyrus (HG). The PT, POp\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1016/j.neuroimage.2003.12.031', '10.1006/nimg.2000.0582', '10.1093/cercor/bhl071', '10.1212/WNL.52.4.798', '10.1212/WNL.46.4.978', '10.1016/S1525-5050(03)00119-7', '10.1162/jocn.2008.20043', '10.1093/cercor/bhp175', '10.1073/pnas.1212956110', '10.1016/j.neuropsychologia.2012.01.016', '10.1093/brain/123.2.291', '10.1006/nimg.1998.0395', '10.1016/j.tics.2011.04.003', '10.1162/jocn.2008.20081', '10.1016/j.neuroimage.2006.01.021', '10.1016/j.neuroimage.2010.06.010', '10.1016/j.neurobiolaging.2007.07.022', '10.1016/j.neuropsychologia.2008.08.024', '10.1016/S0140-6736(05)64855-6', '10.1093/brain/awl055', '10.1016/j.neuropsychologia.2005.06.014', '10.1016/j.bandl.2006.04.002', '10.1016/j.neuroimage.2008.03.002', '10.1073/pnas.200033797', '10.1093/cercor/bhm225', '10.1016/S0896-6273(02)00569-X', '10.1006/nimg.1998.0396', '10.1002/(SICI)1097-0193(1999)8:4<272::AID-HBM10>3.0.CO;2-4', '10.1093/cercor/bhg087', '10.1080/026870399402325', '10.1016/0028-3932(94)90104-X', '10.1093/cercor/bhq010', '10.1126/science.161.3837.186', '10.1016/j.neuroimage.2006.07.036', '10.1016/j.neuroimage.2008.01.009', '10.1038/nrn2113', '10.1016/j.neuropsychologia.2007.07.007', '10.1523/JNEUROSCI.1680-09.2009', '10.1016/j.cogbrainres.2003.08.007', '10.1523/JNEUROSCI.17-11-04302.1997', '10.1162/jocn.2010.21563', '10.1093/brain/123.12.2512', '10.1037/0033-295X.94.2.148', '10.1016/0028-3932(90)90007-B', '10.1016/j.neuroimage.2004.01.032', '10.1016/0028-3932(71)90067-4', '10.1016/j.neuroimage.2011.12.058', '10.1093/cercor/bhp026', '10.1016/S1053-8119(03)00373-2', '10.1093/cercor/6.5.661', '10.1080/02643294.2011.609812', '10.1109/TMI.2008.2004426', '10.1016/j.neuroimage.2012.04.062', '10.1046/j.1469-7580.2000.19730335.x', '10.1016/j.tics.2011.04.001', '10.1212/WNL.52.5.1038', '10.1093/cercor/3.4.313', '10.1016/j.neuroimage.2010.09.055', '10.1162/jocn_a_00060', '10.1006/brcg.1997.0887', '10.1523/JNEUROSCI.2238-11.2011', '10.1016/j.neuroimage.2004.03.032', '10.1007/s00234-002-0782-2', '10.1093/brain/122.11.2033', '10.1162/jocn_a_00091', '10.1038/nrn1009', '10.1016/j.neuroimage.2012.06.019', '10.1016/j.bandl.2010.03.008', '10.1097/00001756-199803300-00012', '10.1006/nimg.2001.0978', '10.1016/j.bandl.2011.11.004', '10.1016/j.neuropsychologia.2011.06.014', '10.1016/j.neuropsychologia.2012.11.002', '10.1016/j.cortex.2011.11.003', '10.3171/jns.1960.17.2.0266', '10.1093/cercor/11.9.868', '10.1093/cercor/bhp234', '10.1016/j.neuroimage.2009.12.028', '10.1016/j.neuroimage.2012.03.026', '10.3758/BF03208845', '10.1016/j.neuroimage.2010.01.101'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['medical image computing', 'AI for radiology', 'computer vision'], conference_acronym='Journal of cognitive neuroscience', publisher=None, query_handler=None),\n",
       " 'Cost-sensitive learning of deep feature representations from imbalanced data': Paper(DOI='10.1109/tnnls.2017.2732482', crossref_json=None, google_schorlar_metadata=None, title='Cost-sensitive learning of deep feature representations from imbalanced data', authors=['Salman H Khan', 'Munawar Hayat', 'Mohammed Bennamoun', 'Ferdous A Sohel', 'Roberto Togneri'], abstract='Class imbalance is a common problem in the case of real-world object detection and classification tasks. Data of some classes are abundant, making them an overrepresented majority, and data of other classes are scarce, making them an underrepresented minority. This imbalance makes it challenging for a classifier to appropriately learn the discriminating boundaries of the majority and minority classes. In this paper, we propose a cost-sensitive (CoSen) deep neural network, which can automatically learn robust feature representations for both the majority and minority classes. During training, our learning procedure jointly optimizes the class-dependent costs and the neural network parameters. The proposed approach is applicable to both binary and multiclass problems without any modification. Moreover, as opposed to data-level approaches, we do not alter the original data distribution, which results in a lower\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2014.81', '10.1109/TKDE.2005.95', '10.1198/016214505000000907', '10.1007/978-3-319-46182-3_13', '10.1109/IJCNN.2016.7727770', '10.1007/978-3-319-10584-0_26', '10.1109/TSMCB.2008.2002909', '10.1007/s10115-009-0198-y', '10.1016/j.neucom.2013.05.051', '10.1109/TKDE.2008.239', '10.1145/1961189.1961199', '10.1145/1014052.1014056', '10.1109/TPAMI.2015.2462355', '10.1109/TIP.2016.2567076', '10.1007/11538059_91', '10.1109/CIDM.2011.5949434', '10.1007/978-3-642-01307-2_43', '10.1007/978-3-642-17534-3_19', '10.1145/1007730.1007735', '10.1007/s00521-014-1584-2', '10.1109/TFUZZ.2013.2296091', '10.1007/978-3-642-15561-1_11', '10.1109/CVPR.2013.124', '10.1109/CVPRW.2014.131', '10.1109/CVPR.2013.115', '10.1109/CVPR.2014.476', '10.5244/C.25.76', '10.1007/978-3-319-10578-9_23', '10.1613/jair.953', '10.1109/CVPR.2014.249', '10.1155/2013/196256', '10.1007/s10115-011-0465-6', '10.1109/TKDE.2006.17', '10.1016/j.asoc.2013.09.014', '10.1109/TGRS.2017.2707528', '10.1007/s11263-015-0843-8', '10.1109/CVPR.2012.6247798', '10.1109/CVPR.2010.5540018', '10.1109/ICIP.2016.7532411', '10.1109/ICCV.2009.5459183', '10.1109/ISBI.2012.6235558', '10.1007/978-94-007-5389-1_4'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Machine Learning', 'Deep Learning', 'AI'], conference_acronym='IEEE transactions on neural networks and learning systems (Print)', publisher=None, query_handler=None),\n",
       " 'A Guide to Convolutional Neural Networks for Computer Vision': Paper(DOI='10.1007/978-3-031-01821-3_4', crossref_json=None, google_schorlar_metadata=None, title='A Guide to Convolutional Neural Networks for Computer Vision', authors=['Salman Khan', 'Hossein Rahmani', 'Syed Afaq Ali Shah', 'Mohammed Bennamoun'], abstract='Computer vision has become increasingly important and effective in recent years due to its wide-ranging applications in areas as diverse as smart surveillance and monitoring, health and medicine, sports and recreation, robotics, drones, and self-driving cars. Visual recognition tasks, such as image classification, localization, and detection, are the core building blocks of many of these applications, and recent developments in Convolutional Neural Networks (CNNs) have led to outstanding performance in these state-of-the-art visual recognition tasks and systems. As a result, CNNs now form the crux of deep learning algorithms in computer vision.This self-contained guide will benefit those who seek to both understand the theory behind CNNs and to gain hands-on experience on the application of CNNs in computer vision. It provides a comprehensive introduction to CNNs starting with the essential concepts behind\\xa0…', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Object Recognition', 'Face Recognition', 'Biometrics', 'Computer Vision', 'Deep Learning'], conference_acronym='Synthesis lectures on computer vision (Print)', publisher=None, query_handler=None),\n",
       " 'Three-dimensional model-based object recognition and segmentation in cluttered scenes': Paper(DOI='10.1109/tpami.2006.213', crossref_json=None, google_schorlar_metadata=None, title='Three-dimensional model-based object recognition and segmentation in cluttered scenes', authors=['Ajmal S Mian', 'Mohammed Bennamoun', 'Robyn Owens'], abstract='Viewpoint independent recognition of free-form objects and their segmentation in the presence of clutter and occlusions is a challenging task. We present a novel 3D model-based algorithm which performs this task automatically and efficiently. A 3D model of an object is automatically constructed offline from its multiple unordered range images (views). These views are converted into multidimensional table representations (which we refer to as tensors). Correspondences are automatically established between these views by simultaneously matching the tensors of a view with those of the remaining views using a hash table-based voting scheme. This results in a graph of relative transformations used to register the views before they are integrated into a seamless 3D model. These models and their tensor representations constitute the model library. During online recognition, a tensor from the scene is simultaneously\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1006/cviu.2000.0884', '10.1109/CVPR.1994.323917', '10.1109/34.273720', '10.1109/34.765655', '10.1109/CVPR.2004.1315148', '10.1109/CVPR.2003.1211442', '10.1109/ICCV.1999.791234', '10.1109/TPCG.2004.1314466', '10.1145/37402.37422', '10.1006/dspr.2001.0412', '10.1109/34.221171', '10.1109/CCV.1988.589995', '10.1023/A:1007981719186', '10.1109/ICCIS.2004.1460404', '10.1109/3DIM.2005.22', '10.1145/237170.237269', '10.1109/ACVMOT.2005.2', '10.1007/BF00127819', '10.1109/ICSMC.2004.1401397', '10.1109/34.809117', '10.1007/s11263-005-3221-0', '10.1109/IM.1999.805366', '10.1142/S0218654305000797', '10.1006/cviu.2000.0889', '10.1109/34.121791', '10.1016/B978-0-12-266722-0.50006-3', '10.1007/978-3-540-24673-2_35', '10.1109/IM.2003.1240250', '10.1109/34.391410', '10.1109/IM.2001.924423', '10.1109/CVPR.2004.1315153', '10.1109/ROBOT.1994.350951', '10.1109/34.121785', '10.1109/34.625113', '10.1109/34.67642', '10.1145/258734.258849'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', '3D vision', 'machine learning', 'face recognition'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " '3D Object Recognition in Cluttered Scenes with Local Surface Features: A Survey': Paper(DOI='10.1109/tpami.2014.2316828', crossref_json=None, google_schorlar_metadata=None, title='3D Object Recognition in Cluttered Scenes with Local Surface Features: A Survey', authors=['Yulan Guo', 'Mohammed Bennamoun', 'Ferdous Sohel', 'Min Lu', 'Jianwei Wan'], abstract='3D object recognition in cluttered scenes is a rapidly growing research area. Based on the used types of features, 3D object recognition methods can broadly be divided into two categories-global or local feature based methods. Intensive research has been done on local surface feature based methods as they are more robust to occlusion and clutter which are frequently present in a real-world scene. This paper presents a comprehensive survey of existing local surface feature based 3D object recognition methods. These methods generally comprise three phases: 3D keypoint detection, local surface feature description, and surface matching. This paper covers an extensive literature survey of each phase of the process. It also enlists a number of popular and contemporary databases together with their relevant attributes.', conference=None, journal=None, year=None, reference_list=['10.1049/iet-cvi:20080037', '10.1109/TPAMI.2006.213', '10.1007/s11263-012-0545-4', '10.1142/S0218654305000797', '10.1016/j.imavis.2006.05.012', '10.1109/TPAMI.2007.1005', '10.1016/j.cviu.2005.05.005', '10.1109/3DIMPVT.2011.37', '10.1109/CVPR.2010.5539774', '10.1006/dspr.2001.0412', '10.1006/cviu.2000.0889', '10.1108/02602280410525995', '10.1109/IROS.2010.5651280', '10.1023/B:VISI.0000029664.99615.94', '10.1007/s11263-009-0276-3', '10.1145/4078.4081', '10.1109/ICCV.2007.4408829', '10.1145/151254.151255', '10.1109/ICCV.2001.937634', '10.1109/ICPR.1988.28178', '10.1109/CVPR.2001.990988', '10.1109/ICCV.2007.4408830', '10.1137/050639296', '10.1109/SMI.2004.1314504', '10.1145/237170.237269', '10.1007/s11263-012-0568-x', '10.1109/ICCVW.2011.6130382', '10.1109/ICIP.2011.6116679', '10.1007/s00371-011-0610-y', '10.1007/s11263-009-0296-z', '10.1109/TPAMI.2005.188', '10.1109/ICRA.2011.5980382', '10.1007/s11263-012-0605-9', '10.1109/CVPR.2010.5540108', '10.1007/s11263-012-0526-7', '10.1016/j.cviu.2010.11.021', '10.1007/s11263-005-3848-x', '10.1109/CVPRW.2008.4563030', '10.1007/s11263-013-0627-y', '10.1109/WACV.2013.6474992', '10.1109/CVPR.1996.517172', '10.1016/S0262-8856(98)00074-2', '10.1111/j.1467-8659.2009.01515.x', '10.5244/C.2.23', '10.1177/0278364911436019', '10.1007/978-3-540-93905-4_13', '10.1109/ICCV.1990.139529', '10.1109/ICCVW.2009.5457637', '10.1109/TIP.2012.2183142', '10.1109/3DIMPVT.2011.39', '10.1080/757582976', '10.1109/ICCV.2007.4408835', '10.1145/1877808.1877813', '10.1109/CCV.1988.589995', '10.1016/j.cviu.2009.06.005', '10.2197/ipsjtcva.4.20', '10.1109/3DPVT.2006.60', '10.1109/CVPR.2010.5540188', '10.1109/34.121791', '10.1007/s11263-012-0528-5', '10.1007/978-3-642-24785-9_52', '10.1109/34.765655', '10.1016/S0262-8856(00)00076-7', '10.1109/TPAMI.2002.1023806', '10.1145/1122501.1122507', '10.1016/j.patrec.2007.02.009', '10.1016/j.ijleo.2012.08.035', '10.1109/TPAMI.2006.148', '10.1016/j.patcog.2013.07.018', '10.1109/TMM.2006.886271', '10.1109/IM.1999.805366', '10.1007/s00371-009-0340-6', '10.1016/j.neucom.2012.08.064', '10.1109/3DIMPVT.2012.12', '10.1109/ICRA.2011.5980187', '10.1111/1467-8659.00675', '10.1109/CVPR.2009.5206748', '10.1016/j.cviu.2009.05.003', '10.1002/cav.244', '10.1109/TPAMI.2007.1060', '10.1109/CVPR.2001.990554', '10.1016/j.image.2013.01.004', '10.1109/CVPR.2006.197', '10.1109/34.121785', '10.1023/A:1007981719186', '10.1109/TPAMI.2002.1046148', '10.1109/ICRA.2011.5980567', '10.1016/j.neucom.2012.06.058', '10.1049/el.2010.1818', '10.1109/ICPR.2010.95', '10.1111/j.1467-8659.2008.01162.x', '10.1109/IROS.2008.4650967', '10.1145/1118890.1118893', '10.1109/TVCG.2009.159', '10.1109/ICCV.2007.4409084', '10.1016/S0923-5965(00)00020-5', '10.1007/s00371-012-0746-4', '10.1109/IM.2003.1240284', '10.1145/571647.571648', '10.1145/1877808.1877821', '10.1109/34.993558', '10.1109/CVPR.2012.6247671', '10.1007/BF00336961', '10.1109/ROBOT.2009.5152473', '10.1109/TVCG.2008.134', '10.1109/CVPR.2010.5539838', '10.1109/TPAMI.2010.210', '10.1049/iet-cvi.2009.0044', '10.1109/DICTA.2007.4426794', '10.1109/ICRA.2011.5980474'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Object Recognition', 'Face Recognition', 'Biometrics', 'Computer Vision', 'Deep Learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Rotational projection statistics for 3D local surface description and object recognition': Paper(DOI='10.1007/s11263-013-0627-y', crossref_json=None, google_schorlar_metadata=None, title='Rotational projection statistics for 3D local surface description and object recognition', authors=['Yulan Guo', 'Ferdous Sohel', 'Mohammed Bennamoun', 'Min Lu', 'Jianwei Wan'], abstract=' Recognizing 3D objects in the presence of noise, varying mesh resolution, occlusion and clutter is a very challenging task. This paper presents a novel method named Rotational Projection Statistics (RoPS). It has three major modules: local reference frame (LRF) definition, RoPS feature description and 3D object recognition. We propose a novel technique to define the LRF by calculating the scatter matrix of all points lying on the local surface. RoPS feature descriptors are obtained by rotationally projecting the neighboring points of a feature point onto 2D planes and calculating a set of statistics (including low-order central moments and entropy) of the distribution of these projected points. Using the proposed LRF and RoPS descriptor, we present a hierarchical 3D object recognition algorithm. The performance of the proposed LRF, RoPS descriptor and object recognition algorithm was rigorously tested on a\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1145/1743384.1743403', '10.1109/CVPR.2010.5539774', '10.1007/s11263-012-0526-7', '10.1109/ICPR.2010.95', '10.1109/34.993558', '10.1145/361002.361007', '10.1109/34.121791', '10.1002/cem.1122', '10.1109/ICCV.2003.1238630', '10.1016/j.patrec.2007.02.009', '10.1023/A:1007981719186', '10.1145/237170.237269', '10.1006/cviu.2000.0861', '10.1109/DICTA.2007.4426794', '10.1049/iet-cvi:20080037', '10.1007/978-3-540-24672-5_18', '10.1145/588272.588279', '10.1145/1276377.1276406', '10.1109/WACV.2013.6474992', '10.1016/j.ijleo.2012.08.035', '10.1007/978-3-642-15558-1_28', '10.1109/TIT.1962.1057692', '10.1016/S0262-8856(98)00074-2', '10.1109/34.765655', '10.1109/CVPR.2012.6247671', '10.1016/j.patcog.2012.06.023', '10.1023/B:VISI.0000029664.99615.94', '10.1109/TPAMI.2007.1060', '10.1006/dspr.2001.0412', '10.1007/s11263-005-3221-0', '10.1109/TPAMI.2006.213', '10.1007/s11263-009-0296-z', '10.1023/B:VISI.0000027790.02288.f2', '10.1109/TPAMI.2005.188', '10.1007/978-3-540-88690-7_33', '10.1109/SMI.2008.4547955', '10.1145/571647.571648', '10.1016/S0923-5965(00)00020-5', '10.1109/ICCV.2011.6126503', '10.1007/s11263-012-0568-x', '10.1007/s11263-009-0276-3', '10.1002/j.1538-7305.1948.tb01338.x', '10.1109/34.121785', '10.1109/ICCV.2007.4408830', '10.1016/j.cviu.2010.11.021', '10.1007/978-3-642-15558-1_26', '10.1007/s11263-012-0545-4', '10.1109/TPAMI.2002.1023806', '10.1109/SMI.2006.42', '10.1007/s11263-012-0528-5', '10.1109/ICCVW.2009.5457637'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Object Recognition', 'Face Recognition', 'Biometrics', 'Computer Vision', 'Deep Learning'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'An efficient multimodal 2D-3D hybrid approach to automatic face recognition': Paper(DOI='10.1109/tpami.2007.1105', crossref_json=None, google_schorlar_metadata=None, title='An efficient multimodal 2D-3D hybrid approach to automatic face recognition', authors=['Ajmal Mian', 'Mohammed Bennamoun', 'Robyn Owens'], abstract='We present a fully automatic face recognition algorithm and demonstrate its performance on the FRGC v2.0 data. Our algorithm is multimodal (2D and 3D) and performs hybrid (feature based and holistic) matching in order to achieve efficiency and robustness to facial expressions. The pose of a 3D face along with its texture is automatically corrected using a novel approach based on a single automatically detected point and the Hotelling transform. A novel 3D spherical face representation (SFR) is used in conjunction with the scale-invariant feature transform (SIFT) descriptor to form a rejection classifier, which quickly eliminates a large number of candidate faces at an early stage for efficient recognition in case of large galleries. The remaining faces are then verified using a novel region-based matching approach, which is robust to facial expressions. This approach automatically segments the eyes- forehead and\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/AMFG.2003.1240842', '10.1109/AFGR.2000.840640', '10.1023/A:1007981719186', '10.1109/CVPR.1996.517076', '10.1109/34.765655', '10.1109/TCSVT.2003.818349', '10.1023/B:VISI.0000029664.99615.94', '10.1109/34.862196', '10.1016/0262-8856(95)99726-H', '10.1109/CVPR.2005.584', '10.1145/328236.328110', '10.1007/3-540-44887-X_4', '10.1145/954339.954342', '10.1109/TPAMI.2006.213', '10.1109/3DPVT.2006.32', '10.1109/34.598227', '10.1007/s11263-005-3221-0', '10.1109/AUTOID.2007.380603', '10.5244/C.19.33', '10.1109/CVPR.2005.581', '10.1109/TPAMI.1987.4767965', '10.1109/CVPR.1996.517125', '10.1109/34.121791', '10.1109/AFGR.2004.1301549', '10.1109/34.598235', '10.1117/12.320144', '10.1007/3-540-45344-X_4', '10.1109/34.598228', '10.1023/B:VISI.0000013087.49260.fb', '10.1162/jocn.1991.3.1.71', '10.1016/0262-8856(94)90007-8', '10.1016/j.cviu.2005.05.005', '10.1109/FGR.2006.87', '10.1109/CVPR.2005.268', '10.1007/s11263-005-1085-y', '10.1109/CVPR.2005.573', '10.1109/CVPR.1994.323814'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', '3D vision', 'machine learning', 'face recognition'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'A Comprehensive Performance Evaluation of 3D Local Feature Descriptors': Paper(DOI='10.1007/s11263-015-0824-y', crossref_json=None, google_schorlar_metadata=None, title='A Comprehensive Performance Evaluation of 3D Local Feature Descriptors', authors=['Yulan Guo', 'Mohammed Bennamoun', 'Ferdous Sohel', 'Min Lu', 'Jianwei Wan', 'Ngai Ming Kwok'], abstract=' A number of 3D local feature descriptors have been proposed in the literature. It is however, unclear which descriptors are more appropriate for a particular application. A good descriptor should be descriptive, compact, and robust to a set of nuisances. This paper compares ten popular local feature descriptors in the contexts of 3D object recognition, 3D shape retrieval, and 3D modeling. We first evaluate the descriptiveness of these descriptors on eight popular datasets which were acquired using different techniques. We then analyze their compactness using the recall of feature matching per each float value in the descriptor. We also test the robustness of the selected descriptors with respect to support radius variations, Gaussian noise, shot noise, varying mesh resolution, distance to the mesh boundary, keypoint localization error, occlusion, clutter, and dataset size. Moreover, we present the performance\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/MRA.2012.2206675', '10.1007/978-3-642-33712-3_37', '10.1109/TMM.2006.886271', '10.1007/s11263-012-0526-7', '10.1109/ICPR.2010.95', '10.1002/047134608X.W8257', '10.1109/34.121791', '10.1145/1899404.1899405', '10.1016/j.cviu.2008.07.003', '10.1016/j.patrec.2007.02.009', '10.1109/TPAMI.2007.1005', '10.1007/3-540-55426-2_83', '10.1145/237170.237269', '10.1109/TIP.2012.2183142', '10.1145/1143844.1143874', '10.1109/DICTA.2007.4426794', '10.1049/iet-cvi:20080037', '10.1007/978-3-540-24672-5_18', '10.1109/MMUL.2014.20', '10.1109/WACV.2013.6474992', '10.1007/s11263-013-0627-y', '10.1109/TPAMI.2014.2316828', '10.1007/s11263-015-0824-y', '10.1109/TMM.2014.2316145', '10.1109/ICIEA.2014.6931468', '10.1016/j.ins.2014.09.015', '10.1016/S0262-8856(98)00074-2', '10.1109/34.765655', '10.1109/3DV.2013.24', '10.1016/0262-8856(92)90076-F', '10.1609/aaai.v25i1.7986', '10.1016/j.patcog.2013.07.018', '10.1016/j.cviu.2009.06.005', '10.1023/B:VISI.0000029664.99615.94', '10.1109/TPAMI.2006.148', '10.1016/S0167-8396(00)00006-6', '10.1109/TPAMI.2006.213', '10.1007/s11263-005-3221-0', '10.1007/s11263-009-0296-z', '10.1109/CVPR.2003.1211478', '10.1023/B:VISI.0000027790.02288.f2', '10.1109/TPAMI.2005.188', '10.1007/s11263-005-3848-x', '10.1109/ICCV.2005.89', '10.1007/s11263-006-9967-1', '10.5244/C.26.46', '10.1007/s11263-012-0568-x', '10.1109/CVPR.2001.990554', '10.1109/ICRA.2011.5980567', '10.1109/IROS.2008.4650967', '10.1109/ROBOT.2009.5152473', '10.1109/3DIMPVT.2011.37', '10.1109/3DIMPVT.2012.10', '10.1016/j.cviu.2014.04.011', '10.1023/A:1008199403446', '10.1007/s11263-009-0276-3', '10.1007/s00371-011-0610-y', '10.1111/j.1467-8659.2009.01515.x', '10.1016/j.cviu.2010.11.021', '10.1109/SMI.2004.1314502', '10.1145/1877808.1877821', '10.1007/978-3-642-15558-1_26', '10.1007/s11263-012-0545-4', '10.1109/CVPR.2009.5206748', '10.1007/s11263-012-0528-5', '10.1109/ICCVW.2009.5457637'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Object Recognition', 'Face Recognition', 'Biometrics', 'Computer Vision', 'Deep Learning'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Ontology learning from text: A look back and into the future': Paper(DOI='10.1145/2333112.2333115', crossref_json=None, google_schorlar_metadata=None, title='Ontology learning from text: A look back and into the future', authors=['Wilson Wong', 'Wei Liu', 'Mohammed Bennamoun'], abstract='Ontologies are often viewed as the answer to the need for interoperable semantics in modern information systems. The explosion of textual information on the Read/Write Web coupled with the increasing demand for ontologies to power the Semantic Web have made (semi-)automatic ontology learning from text a very promising research area. This together with the advanced state in related areas, such as natural language processing, have fueled research into ontology learning over the past decade. This survey looks at how far we have come since the turn of the millennium and discusses the remaining challenges that will define the research directions in this area in the near future.', conference=None, journal=None, year=None, reference_list=['10.1109/DEXA.2007.96', '10.1038/scientificamerican0501-34', '10.3115/974499.974526', '10.1109/TKDE.2007.22', '10.1016/j.cad.2006.03.002', '10.5555/89086.89095', '10.1145/290941.291008', '10.1111/j.1468-0394.1988.tb00065.x', '10.3115/1073083.1073112', '10.1002/047003033X.ch6', '10.1007/11926078_17', '10.1177/016555150202800204', '10.1145/1409360.1409378', '10.1093/comjnl/35.3.243', '10.1007/11915034_18', '10.1006/knac.1993.1008', '10.1016/S0169-023X(00)00031-8', '10.3115/1072133.1072219', '10.1109/ICSC.2009.27', '10.1017/S1351324904003535', '10.1109/TKDE.2007.36', '10.1002/asi.v61:1', '10.1109/ICIW.2010.66', '10.3115/1075096.1075150', '10.1016/j.compenvurbsys.2005.04.002', '10.1080/01638539809545028', '10.1145/318723.318728', '10.3115/991886.991970', '10.1007/978-3-540-89704-0_23', '10.1007/978-3-662-05320-1_14', '10.1109/5254.920602', '10.3115/1225753.1225767', '10.1016/j.websem.2006.11.002', '10.1093/ijl/3.4.235', '10.3115/1690219.1690287', '10.1109/MC.2002.1046976', '10.3115/974557.974588', '10.1109/WAINA.2008.117', '10.1145/290941.291008', '10.1016/j.cageo.2004.12.004', '10.5555/3013545.3013547', '10.3115/976973.976990', '10.1145/1060745.1060776', '10.1016/0306-4573(88)90021-0', '10.1016/j.datak.2007.10.001', '10.1007/978-1-4757-4305-0_2', '10.1017/S0269888903000687', '10.1016/j.ijhcs.2003.08.001', '10.3115/1220175.1220276', '10.1007/11766254_48', '10.1016/S0167-739X(97)00019-8', '10.3115/1117755.1117757', '10.5555/645328.650004', '10.1145/1041410.1041420', '10.1145/505168.505194', '10.1007/978-3-540-85569-9_7', '10.1109/DEXA.2010.53', '10.1145/1088622.1088648', '10.1007/s10579-011-9141-4', '10.1007/s10618-007-0073-y', '10.1007/978-3-540-89378-3_7', '10.4018/978-1-59904-990-8.ch030', '10.1007/978-3-642-01307-2_26', '10.5555/1609910.1609912', '10.3115/992730.992782', '10.1007/978-3-540-89533-6_10', '10.1007/s10799-007-0019-5'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Object Recognition', 'Face Recognition', 'Biometrics', 'Computer Vision', 'Deep Learning'], conference_acronym='ACM computing surveys', publisher=None, query_handler=None),\n",
       " 'On the repeatability and quality of keypoints for local feature-based 3d object retrieval from cluttered scenes': Paper(DOI='10.1007/s11263-009-0296-z', crossref_json=None, google_schorlar_metadata=None, title='On the repeatability and quality of keypoints for local feature-based 3d object retrieval from cluttered scenes', authors=['Ajmal Mian', 'Mohammed Bennamoun', 'Robyn Owens'], abstract=' 3D object recognition from local features is robust to occlusions and clutter. However, local features must be extracted from a small set of feature rich keypoints to avoid computational complexity and ambiguous features. We present an algorithm for the detection of such keypoints on 3D models and partial views of objects. The keypoints are highly repeatable between partial views of an object and its complete 3D model. We also propose a quality measure to rank the keypoints and select the best ones for extracting local features. Keypoints are identified at locations where a unique local 3D coordinate basis can be derived from the underlying surface in order to extract invariant features. We also propose an automatic scale selection technique for extracting multi-scale and scale invariant features to match objects at different unknown scales. Features are projected to a PCA subspace and matched to find\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.121791', '10.1016/j.cviu.2005.05.005', '10.1006/cviu.2000.0889', '10.1023/A:1007981719186', '10.1109/34.625113', '10.1109/IM.2003.1240258', '10.1109/34.391410', '10.1109/CVPR.2001.990988', '10.1109/34.765655', '10.1109/CVPR.1994.323917', '10.1145/1186822.1073244', '10.1007/s11263-007-0085-5', '10.1007/s11263-005-3221-0', '10.1109/TPAMI.2006.213', '10.1023/B:VISI.0000029664.99615.94', '10.1007/BF01421486', '10.1007/978-3-540-88690-7_33', '10.1109/TPAMI.2007.37', '10.1145/508352.508354', '10.1109/CVPR.2006.210', '10.1023/A:1013240031067', '10.1109/3DPVT.2006.123'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', '3D vision', 'machine learning', 'face recognition'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Hands-on Bayesian neural networks—A tutorial for deep learning users': Paper(DOI='10.1109/mci.2022.3155327', crossref_json=None, google_schorlar_metadata=None, title='Hands-on Bayesian neural networks—A tutorial for deep learning users', authors=['Laurent Valentin Jospin', 'Hamid Laga', 'Farid Boussaid', 'Wray Buntine', 'Mohammed Bennamoun'], abstract='Modern deep learning methods constitute incredibly powerful tools to tackle a myriad of challenging problems. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural network predictions. This tutorial provides deep learning practitioners with an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian neural networks,  i . e ., stochastic artificial neural networks trained using Bayesian methods.', conference=None, journal=None, year=None, reference_list=['10.1023/A:1011141724916', '10.3847/1538-3881/ab2390', '10.1109/TNN.2006.883010', '10.1162/neco.1996.8.7.1341', '10.1029/2008WR007030', '10.1016/j.advengsoft.2006.08.004', '10.1080/15732479.2014.951867', '10.1016/j.dss.2020.113246', '10.1016/j.strusafe.2008.06.020', '10.1007/BF00058655', '10.1162/neco.1992.4.3.448', '10.1109/TNNLS.2013.2292894', '10.1038/s42256-020-0209-y', '10.1016/j.eswa.2016.02.006', '10.1109/ACCESS.2017.2788044', '10.3758/s13423-017-1317-5', '10.1109/ACCESS.2018.2836917', '10.1016/S0893-6080(00)00098-8', '10.18637/jss.v076.i01', '10.1214/17-BA1082', '10.1214/088342304000000099', '10.1613/jair.62', '10.1145/3234150', '10.1145/3446776', '10.1109/TPAMI.2020.3032602', '10.1109/TPAMI.2019.2954885', '10.1109/TPAMI.2020.2992393', '10.1007/s11431-020-1647-3', '10.1093/biomet/57.1.97', '10.1080/01621459.2017.1285773', '10.1109/ICPR.2014.535', '10.1007/s12083-018-0702-9', '10.1145/3194085.3194087', '10.1109/TKDE.2009.191', '10.1145/1102351.1102457', '10.1561/2200000056', '10.4467/20838476SI.16.004.6185', '10.1145/3409383', '10.1038/s41598-021-89267-4', '10.1201/b10905-6', '10.1201/b12207', '10.1002/j.1538-7305.1948.tb01338.x', '10.1038/075450a0', '10.1214/aoms/1177729694', '10.1080/00031305.1995.10476177'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Object Recognition', 'Face Recognition', 'Biometrics', 'Computer Vision', 'Deep Learning'], conference_acronym='IEEE computational intelligence magazine', publisher=None, query_handler=None),\n",
       " 'Trends in computer-aided manufacturing in prosthodontics: a review of the available streams': Paper(DOI='10.1155/2014/783948', crossref_json=None, google_schorlar_metadata=None, title='Trends in computer-aided manufacturing in prosthodontics: a review of the available streams', authors=['Jaafar Abduo', 'Karl Lyons', 'Mohammed Bennamoun'], abstract='In prosthodontics, conventional methods of fabrication of oral and facial prostheses have been considered the gold standard for many years. The development of computer-aided manufacturing and the medical application of this industrial technology have provided an alternative way of fabricating oral and facial prostheses. This narrative review aims to evaluate the different streams of computer-aided manufacturing in prosthodontics. To date, there are two streams: the subtractive and the additive approaches. The differences reside in the processing protocols, materials used, and their respective accuracy. In general, there is a tendency for the subtractive method to provide more homogeneous objects with acceptable accuracy that may be more suitable for the production of intraoral prostheses where high occlusal forces are anticipated. Additive manufacturing methods have the ability to produce large workpieces with significant surface variation and competitive accuracy. Such advantages make them ideal for the fabrication of facial prostheses.', conference=None, journal=None, year=None, reference_list=['10.1016/j.prosdent.2005.03.013', '10.1016/j.cden.2003.12.010', '10.1016/S0022-3913(78)80019-5', '10.1016/S0022-3913(78)80075-4', '10.1016/S0022-3913(98)70111-8', '10.3109/03091902.2012.682113', '10.1016/S0011-8532(22)01149-1', '10.1097/MOO.0b013e32833bb38c', '10.1111/j.1834-7819.2010.01300.x', '10.1038/sj.bdj.2008.350', '10.1016/S0022-3913(96)90305-4', '10.1016/j.dental.2011.10.014', '10.1155/2013/768121', '10.1016/j.dental.2007.05.007', '10.1016/j.jdent.2012.03.006', '10.1016/j.prosdent.2006.05.029', '10.4012/dmj.28.44', '10.1109/10.133223', '10.11607/jomi.3025', '10.11607/jomi.3156', '10.1243/095441105X9372', '10.1016/j.prosdent.2003.10.002', '10.1108/13552541211231743', '10.4012/dmj.2010-112', '10.1016/j.cmpb.2008.10.003', '10.1080/19424396.2013.12222317', '10.1016/S0022-3913(12)60015-8', '10.1016/j.prosdent.2004.08.021', '10.1016/j.bjoms.2009.05.009', '10.1016/j.dental.2010.11.015', '10.1243/095441105X9363', '10.1016/S0022-3913(99)70231-3', '10.1177/154405910608500314', '10.1016/j.dental.2007.05.005', '10.1016/j.actbio.2008.08.016', '10.1177/0022034510391795', '10.1023/A:1008812929476', '10.1109/58.710589', '10.1177/0022034509354193', '10.1016/j.dental.2003.08.007', '10.1016/S0300-5712(98)00049-9', '10.1080/10940340008945707', '10.1016/S0043-1648(03)00195-9', '10.1081/MST-100103177', '10.1016/j.dental.2007.06.030', '10.1111/j.1600-0722.2009.00622.x', '10.1016/S0022-3913(09)60047-0', '10.1111/j.1600-0501.2010.01954.x', '10.1177/0022034510384617', '10.4012/dmj.2010-130', '10.1016/j.dental.2013.04.011', '10.1111/jopr.12016', '10.1016/j.dental.2013.09.004', '10.1097/ID.0b013e318278a576', '10.1111/j.1708-8208.2011.00428.x', '10.1016/S0022-3913(12)60110-3', '10.1111/joor.12018', '10.1111/j.1532-849X.2011.00825.x', '10.1111/j.1600-0501.2007.01468.x', '10.1111/j.1600-0501.2007.01467.x', '10.1111/j.1365-2842.2010.02113.x', '10.1111/j.1532-849X.2008.00302.x', '10.1016/S0022-3913(11)60120-0', '10.1016/j.jdent.2012.04.019', '10.1111/j.1834-7819.1995.tb05607.x', '10.1038/sj.bdj.4801473', '10.4012/dmj.23.37', '10.1016/S0924-0136(02)01042-7', '10.1016/S0924-0136(03)00126-2', '10.1007/BF01167412', '10.1016/S0109-5641(85)80019-1', '10.3109/00016359609003514', '10.1111/j.1600-0722.2008.00580.x', '10.1016/j.dental.2008.04.018', '10.1016/j.prosdent.2006.05.013', '10.1563/AAID-JOI-D-12-00117.1', '10.1111/j.1708-8208.2007.00071.x', '10.1155/2012/374315', '10.1080/03091900050163427', '10.1016/S0010-4485(03)00110-6', '10.1016/j.cad.2004.06.002', '10.1016/S0924-0136(01)00584-2', '10.1007/s00170-005-0181-z', '10.4012/dmj.2011-113', '10.1111/j.1532-849X.2010.00623.x', '10.1177/0022034509339988', '10.1016/j.dental.2008.03.029', '10.1016/S0924-0136(03)00766-0', '10.1108/13552540710776142', '10.1007/s10856-011-4515-0', '10.1007/s10856-008-3421-6', '10.1007/s10856-011-4309-4', '10.1016/S0109-5641(00)00037-3', '10.1016/j.jeurceramsoc.2008.06.020', '10.1111/j.1151-2916.2002.tb00414.x', '10.1016/S0022-3913(08)60138-9', '10.1007/s10103-008-0603-x', '10.1016/j.dental.2008.02.011', '10.1016/S0022-3913(09)60165-7', '10.1016/j.jdent.2012.09.014', '10.1016/0010-4485(95)00035-6'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Object Recognition', 'Face Recognition', 'Biometrics', 'Computer Vision', 'Deep Learning'], conference_acronym=None, publisher=None, query_handler=None),\n",
       " 'Image-based 3D object reconstruction: State-of-the-art and trends in the deep learning era': Paper(DOI='10.1109/tpami.2019.2954885', crossref_json=None, google_schorlar_metadata=None, title='Image-based 3D object reconstruction: State-of-the-art and trends in the deep learning era', authors=['Xian-Feng Han', 'Hamid Laga', 'Mohammed Bennamoun'], abstract='3D reconstruction is a longstanding ill-posed problem, which has been explored for decades by the computer vision, computer graphics, and machine learning communities. Since 2015, image-based 3D reconstruction using convolutional neural networks (CNN) has attracted increasing interest and demonstrated an impressive performance. Given this new era of rapid evolution, this article provides a comprehensive survey of the recent developments in this field. We focus on the works which use deep learning techniques to estimate the 3D shape of generic objects either from a single or multiple RGB images. We organize the literature based on the shape representations, the network architectures, and the training mechanisms they use. While this survey is intended for methods which reconstruct generic objects, we also review some of the recent works which focus on specific object classes such as human body\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/CVPR.2019.00025', '10.1109/ICCV.2017.230', '10.1145/3072959.3073608', '10.1109/CVPR.2018.00375', '10.1145/237170.237269', '10.1109/TPAMI.2019.2896296', '10.1145/3272127.3275050', '10.1109/ICCV.2017.252', '10.1109/3DV.2017.00053', '10.1109/ICCV.2017.103', '10.1109/ICCV.2017.16', '10.1002/9781119405207', '10.1109/CVPR.2016.445', '10.1109/CVPR.2017.693', '10.1109/TPAMI.2018.2871117', '10.1109/CVPR.2018.00308', '10.1007/978-3-030-28603-3_8', '10.1109/IROS.2017.8206060', '10.1109/CVPR.2019.01001', '10.1007/978-3-319-10602-1_34', '10.1109/CVPR.2017.28', '10.1109/CVPR.2016.170', '10.1109/WACV.2018.00099', '10.1007/978-3-030-01267-0_23', '10.1109/CVPR.2018.00411', '10.1109/CVPR.2018.00030', '10.1109/CVPR.2018.00492', '10.1109/CVPR.2017.91', '10.1109/CVPR.2018.00475', '10.1109/CVPR.2017.49', '10.1145/3203197', '10.1109/3DV.2017.00018', '10.1109/CVPR.2018.00202', '10.1109/ICCVW.2017.271', '10.1016/B978-0-12-811889-4.00007-5', '10.1109/CVPR.2017.30', '10.1145/37402.37422', '10.1109/CVPR.2019.00459', '10.1111/cgf.12063', '10.1109/TPAMI.2018.2868195', '10.1109/ICCV.2017.19', '10.1109/CVPR.2015.7298878', '10.1145/3072959.3073637', '10.1109/CVPR.2017.701', '10.1109/CVPR.2019.00352', '10.1109/CVPR.2019.00609', '10.1109/CVPR.2017.163', '10.1109/CVPR.2015.7298682', '10.1109/FG.2018.00021', '10.1109/CVPR.2017.264', '10.1109/CVPR.2017.589', '10.1109/3DV.2016.56', '10.1109/TPAMI.2016.2574713', '10.1109/CVPR.2018.00874', '10.1109/ICCV.2017.175', '10.1007/978-3-030-01258-8_31', '10.1109/ICCV.2017.117', '10.1109/CVPR.2017.260', '10.1109/CVPR.2018.00039', '10.1145/882262.882276', '10.1109/CVPR.2017.576', '10.1145/882262.882274', '10.1561/0600000011', '10.1109/ICCV.2013.372', '10.1109/TPAMI.2016.2647596', '10.1109/WACV.2014.6836101', '10.1145/3144456', '10.1111/cgf.13501', '10.1109/CVPR.2012.6248074', '10.1145/311535.311556', '10.1109/CVPR.2017.261', '10.1109/34.273735', '10.1109/CVPR.2014.13', '10.1109/ICCVW.2013.77', '10.1109/CVPR.2017.584', '10.1109/3DV.2016.19', '10.1109/CVPR.2019.00568', '10.1111/j.1467-8659.2011.02058.x', '10.1109/ICCVW.2017.86', '10.1109/ICCV.2019.00552', '10.1145/882262.882311', '10.1109/CVPR.2019.00127', '10.1109/ICCV.2015.336', '10.1109/ICCV.2015.308', '10.1109/CVPR.2018.00306', '10.1109/ICCVW.2017.114', '10.1007/978-3-030-01267-0_6', '10.1109/CVPR.2016.90', '10.1109/ICCV.2019.00238', '10.1609/aaai.v33i01.33018949', '10.1109/3DV.2018.00062', '10.1109/WACV.2019.00117', '10.1109/3DV.2018.00068', '10.1117/12.57955', '10.1109/CVPR.2016.533', '10.1109/CVPR.2017.269', '10.1145/2816795.2818013', '10.1145/1073204.1073207', '10.1007/978-3-319-46454-1_34', '10.1111/j.1467-8659.2009.01515.x', '10.1109/CVPR.2018.00744', '10.1145/2487228.2487237', '10.1109/34.927467', '10.1016/0262-8856(92)90066-C', '10.1109/ICCV.2019.00278'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Object Recognition', 'Face Recognition', 'Biometrics', 'Computer Vision', 'Deep Learning'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Keypoint detection and local feature matching for textured 3D face recognition': Paper(DOI='10.1007/s11263-007-0085-5', crossref_json=None, google_schorlar_metadata=None, title='Keypoint detection and local feature matching for textured 3D face recognition', authors=['Ajmal S Mian', 'Mohammed Bennamoun', 'Robyn Owens'], abstract='  Holistic face recognition algorithms are sensitive to expressions, illumination, pose, occlusions and makeup. On the other hand, feature-based algorithms are robust to such variations. In this paper, we present a feature-based algorithm for the recognition of textured 3D faces. A novel keypoint detection technique is proposed which can repeatably identify keypoints at locations where shape variation is high in 3D faces. Moreover, a unique 3D coordinate basis can be defined locally at each keypoint facilitating the extraction of highly descriptive pose invariant features. A 3D feature is extracted by fitting a surface to the neighborhood of a keypoint and sampling it on a uniform grid. Features from a probe and gallery face are projected to the PCA subspace and matched. The set of matching features are used to construct two graphs. The similarity between two faces is measured as the similarity between their\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/11919476_10', '10.1109/34.598228', '10.1109/34.121791', '10.1016/j.cviu.2005.05.005', '10.1007/11527923_106', '10.1007/3-540-44887-X_4', '10.1109/TCSVT.2003.818349', '10.1023/B:VISI.0000029664.99615.94', '10.1109/TPAMI.2006.15', '10.1007/11744078_27', '10.1007/s11263-005-3221-0', '10.1109/3DPVT.2006.32', '10.1007/11919476_86', '10.1109/TPAMI.2006.213', '10.1109/TPAMI.2007.1105', '10.1109/CVPR.2005.573', '10.1109/CVPR.2005.268', '10.1162/jocn.1991.3.1.71', '10.1145/954339.954342'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', '3D vision', 'machine learning', 'face recognition'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'A novel representation and feature matching algorithm for automatic pairwise registration of range images': Paper(DOI='10.1007/s11263-005-3221-0', crossref_json=None, google_schorlar_metadata=None, title='A novel representation and feature matching algorithm for automatic pairwise registration of range images', authors=['Ajmal S Mian', 'Mohammed Bennamoun', 'Robyn A Owens'], abstract=' Automatic registration of range images is a fundamental problem in 3D modeling of free-from objects. Various feature matching algorithms have been proposed for this purpose. However, these algorithms suffer from various limitations mainly related to their applicability, efficiency, robustness to resolution, and the discriminating capability of the used feature representation. We present a novel feature matching algorithm for automatic pairwise registration of range images which overcomes these limitations. Our algorithm uses a novel tensor representation which represents semi-local 3D surface patches of a range image by third order tensors. Multiple tensors are used to represent each range image. Tensors of two range images are matched to identify correspondences between them. Correspondences are verified and then used for pairwise registration of the range images. Experimental results show that\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/34.121791', '10.1006/cviu.2000.0889', '10.1109/34.809117', '10.1109/ROBOT.1991.132043', '10.1142/S0218001491000223', '10.1007/BF00127819', '10.1023/A:1007981719186', '10.1145/237170.237269', '10.1145/258734.258849', '10.1109/IM.1997.603857', '10.1109/34.765655', '10.1109/ICSMC.2004.1401397', '10.1109/TPCG.2004.1314466', '10.1109/IM.2003.1240250', '10.1016/S1361-8415(99)80034-6', '10.1109/IM.1999.805349', '10.1109/IM.2001.924423', '10.5244/C.4.12', '10.1006/cviu.2000.0884'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['computer vision', '3D vision', 'machine learning', 'face recognition'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Scale-space theory: A basic tool for analyzing structures at different scales': Paper(DOI='10.1080/757582976', crossref_json=None, google_schorlar_metadata=None, title='Scale-space theory: A basic tool for analyzing structures at different scales', authors=['Tony Lindeberg'], abstract='An inherent property of objects in the world is that they only exist as meaningful entities over certain ranges of scale. If one aims to describe the structure of unknown real-world signals, then a multi-scale representation of data is of crucial importance. This paper gives a tutorial review of a special type of multi-scale representation—linear scale-space representation—which has been developed by the computer vision community to handle image structures at different scales in a consistent manner. The basic idea is to embed the original signal into a one-parameter family of gradually smoothed signals in which the fine-scale details are successively suppressed. Under rather general conditions on the type of computations that are to be performed at the first stages of visual processing, in what can be termed ‘the visual front-end’, it can be shown that the Gaussian kernel and its derivatives are singled out as the only\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.1986.4767749', '10.1109/TPAMI.1987.4767980', '10.1007/978-1-4612-6333-3', '10.1007/3-540-55426-2_77', '10.1016/0146-664X(81)90092-7', '10.1109/TCOM.1983.1095851', '10.1109/TPAMI.1986.4767851', '10.1007/978-3-642-82940-6', '10.1109/34.6782', '10.1109/TPAMI.1984.4767500', '10.1109/TPAMI.1984.4767504', '10.1109/TPAMI.1987.4767876', '10.1016/0262-8856(92)90024-W', '10.1007/BF01664793', '10.1007/BF01249895', '10.1007/3-540-57956-7_40', '10.1016/0042-6989(86)90095-7', '10.1007/978-3-662-02427-0', '10.1007/BF01444162', '10.1109/29.45555', '10.1007/978-1-4615-2792-3', '10.1007/3-540-55426-2_45', '10.1007/978-3-642-61275-6', '10.1016/0167-8655(82)90020-4', '10.1007/BF00336961', '10.1109/34.141551', '10.1007/BF00337144', '10.1007/BF00318204', '10.1007/BF00318371', '10.1364/JOSAA.5.001136', '10.1109/34.6770', '10.1109/34.56189', '10.1109/34.49051', '10.1007/BF00135225', '10.1007/BF01469346', '10.1007/BF01664794', '10.1109/34.254063', '10.1007/978-1-4757-6465-9', '10.1007/978-1-4757-6465-9_13', '10.1016/0262-8856(92)90079-I', '10.1007/3-540-57956-7_42', '10.1109/34.19032', '10.1109/34.192463', '10.1109/34.142909', '10.1098/rspb.1980.0020', '10.1109/TPAMI.1987.4767939', '10.1109/34.149593', '10.1016/0262-8856(90)80008-H', '10.1109/34.56205', '10.1007/978-3-642-51590-3', '10.1109/T-C.1971.223290', '10.1109/34.87339', '10.1007/BF01420591', '10.1090/qam/15914', '10.1090/S0002-9904-1953-09695-1', '10.1016/S0146-664X(75)80003-7', '10.1016/0734-189X(83)90020-8', '10.1109/TPAMI.1986.4767769', '10.1109/34.120332', '10.1163/156856887X00222', '10.1109/TPAMI.1986.4767748'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Scale Space', 'Recognition', 'Image Analysis', 'Neuroscience'], conference_acronym='Journal of applied statistics', publisher=None, query_handler=None),\n",
       " 'Detecting salient blob-like image structures and their scales with a scale-space primal sketch: A method for focus-of-attention': Paper(DOI='10.1007/bf01469346', crossref_json=None, google_schorlar_metadata=None, title='Detecting salient blob-like image structures and their scales with a scale-space primal sketch: A method for focus-of-attention', authors=['Tony Lindeberg'], abstract=' This article presents: (i) a multiscale representation of grey-level shape called the scale-space primal sketch, which makes explicit both features in scale-space and the relations between structures at different scales, (ii) a methodology for extracting significant blob-like image structures from this representation, and (iii) applications to edge detection, histogram analysis, and junction classification demonstrating how the proposed method can be used for guiding later-stage visual processes. The representation gives a qualitative description of image structure, which allows for detection of stable scales and associated regions of interest in a solely bottom-up data-driven way. In other words, it generates coarse segmentation cues, and can hence be seen as preceding further processing, which can then be properly tuned. It is argued that once such information is available, many other processing tasks can\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1109/TPAMI.1986.4767749', '10.1109/CCV.1988.590010', '10.1109/TPAMI.1987.4767980', '10.1016/0734-189X(88)90163-6', '10.1109/34.41363', '10.1016/0262-8856(90)80005-E', '10.1007/3-540-55426-2_77', '10.1016/0146-664X(81)90092-7', '10.1109/TPAMI.1986.4767851', '10.1109/TPAMI.1987.4767877', '10.1109/TPAMI.1984.4767500', '10.1109/TPAMI.1987.4767876', '10.1007/3-540-55426-2_60', '10.1016/0262-8856(92)90024-W', '10.1007/978-1-4612-5034-0', '10.1177/027836498300200105', '10.1002/j.1538-7305.1983.tb03502.x', '10.1007/BF00133570', '10.1016/0167-8655(82)90020-4', '10.1007/BF00336961', '10.1007/BF00318204', '10.1364/JOSAA.5.001136', '10.1109/34.141551', '10.1007/3-540-55426-2_49', '10.1109/34.6770', '10.1109/34.56189', '10.1109/34.49051', '10.1109/ICCV.1990.139563', '10.1016/1047-3203(91)90035-E', '10.1007/BF00135225', '10.1016/0262-8856(92)90079-I', '10.1007/BF01664794', '10.1109/34.254063', '10.1007/978-1-4613-2551-2', '10.1007/BF00128527', '10.1098/rstb.1976.0090', '10.1080/14786447008640422', '10.1109/TPAMI.1986.4767750', '10.1007/BFb0014888', '10.1109/34.93807', '10.1016/0262-8856(92)90001-J', '10.1109/34.57672', '10.1016/0167-8655(88)90063-3', '10.1016/0004-3702(88)90080-X', '10.1017/S0140525X00079577', '10.1109/TPAMI.1986.4767748'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Scale Space', 'Recognition', 'Image Analysis', 'Neuroscience'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Shape-adapted smoothing in estimation of 3-D shape cues from affine deformations of local 2-D brightness structure': Paper(DOI='10.1016/s0262-8856(97)01144-x', crossref_json=None, google_schorlar_metadata=None, title='Shape-adapted smoothing in estimation of 3-D shape cues from affine deformations of local 2-D brightness structure', authors=['Tony Lindeberg', 'Jonas Gårding'], abstract='This article describes a method for reducing the shape distortions due to scale-space smoothing that arise in the computation of 3-D shape cues using operators (derivatives) defined from scale-space representation. More precisely, we are concerned with a general class of methods for deriving 3-D shape cues from a 2-D image data based on the estimation of locally linearized deformations of brightness patterns. This class constitutes a common framework for describing several problems in computer vision (such as shape-from-texture, shape-from disparity-gradients, and motion estimation) and for expressing different algorithms in terms of similar types of visual front-end-operations. It is explained how surface orientation estimates will be biased due to the use of rotationally symmetric smoothing in the image domain. These effects can be reduced by extending the linear scale-space concept into an affine Gaussian\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF00336961', '10.1109/TPAMI.1986.4767749', '10.1109/TPAMI.1986.4767748', '10.1109/34.49051', '10.1007/BF00203452', '10.1016/0262-8856(92)90024-W', '10.1109/34.56205', '10.1016/0262-8856(90)80008-H', '10.1016/0734-189X(83)90020-8', '10.1109/34.87339', '10.1109/34.149593', '10.1007/BF00375127', '10.1016/0021-9991(88)90002-2', '10.1007/BF01420591', '10.1016/0262-8856(95)99716-E', '10.1007/BF00326670', '10.1007/BF00121877', '10.1109/34.85667', '10.1007/BF00058750', '10.1364/JOSAA.8.000377', '10.1016/0734-189X(90)90029-U', '10.1109/34.56194', '10.1016/0004-3702(90)90011-N', '10.1109/34.85668', '10.1016/1049-9652(91)90059-S', '10.1016/0004-3702(93)90106-L', '10.1016/S0146-664X(76)80005-6', '10.1007/BF00336727', '10.1016/0004-3702(81)90019-9', '10.1016/0004-3702(84)90010-9', '10.1016/0004-3702(86)90017-2', '10.1007/BF00363944', '10.1109/34.41363', '10.1016/0004-3702(89)90066-0', '10.1016/0042-6989(70)90036-2', '10.1016/0042-6989(79)90019-1', '10.1007/978-94-015-8802-7_2', '10.1016/S0262-8856(98)00065-1'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Scale Space', 'Recognition', 'Image Analysis', 'Neuroscience'], conference_acronym='Image and vision computing', publisher=None, query_handler=None),\n",
       " 'Automatic extraction of roads from aerial images based on scale space and snakes': Paper(DOI='10.1007/s001380050121', crossref_json=None, google_schorlar_metadata=None, title='Automatic extraction of roads from aerial images based on scale space and snakes', authors=['Ivan Laptev', 'Helmut Mayer', 'Tony Lindeberg', 'Wolfgang Eckstein', 'Carsten Steger', 'Albert Baumgartner'], abstract='  Abstract. We propose a new approach for automatic road extraction from aerial imagery with a model and a strategy mainly based on the multi-scale detection of roads in combination with geometry-constrained edge extraction using snakes. A main advantage of our approach is, that it allows for the first time a bridging of shadows and partially occluded areas using the heavily disturbed evidence in the image. Additionally, it has only few parameters to be adjusted. The road network is constructed after extracting crossings with varying shape and topology. We show the feasibility of the approach not only by presenting reasonable results but also by evaluating them quantitatively based on ground truth.', conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Scale Space', 'Recognition', 'Image Analysis', 'Neuroscience'], conference_acronym='Machine vision and applications', publisher=None, query_handler=None),\n",
       " 'Local descriptors for spatio-temporal recognition': Paper(DOI='10.1016/j.ijleo.2013.10.022', crossref_json=None, google_schorlar_metadata=None, title='Local descriptors for spatio-temporal recognition', authors=['Ivan Laptev', 'Tony Lindeberg'], abstract=' This paper presents and investigates a set of local space-time descriptors for representing and recognizing motion patterns in video. Following the idea of local features in the spatial domain, we use the notion of space-time interest points and represent video data in terms of local space-time events. To describe such events, we define several types of image descriptors over local spatio-temporal neighborhoods and evaluate these descriptors in the context of recognizing human activities. In particular, we compare motion representations in terms of spatio-temporal jets, position dependent histograms, position independent histograms, and principal component analysis computed for either spatio-temporal gradients or optic flow. An experimental evaluation on a video database with human actions shows that high classification performance can be achieved, and that there is a clear advantage of using local\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.cviu.2010.10.002', '10.1023/B:VISI.0000029664.99615.94', '10.1109/34.910878', '10.1109/TIT.1962.1057692', '10.1109/TCSVT.2008.2005594', '10.1016/j.imavis.2009.11.014', '10.1016/j.cviu.2007.09.006', '10.1016/j.patrec.2008.09.006', '10.1007/s11263-005-1838-7', '10.1109/TSMCB.2005.861864', '10.1007/s11263-007-0122-4', '10.1109/34.868677', '10.1109/TPAMI.2007.70711'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Scale Space', 'Recognition', 'Image Analysis', 'Neuroscience'], conference_acronym='Optik (Stuttgart)', publisher=None, query_handler=None),\n",
       " 'Local velocity-adapted motion events for spatio-temporal recognition': Paper(DOI='10.1016/j.cviu.2006.11.023', crossref_json=None, google_schorlar_metadata=None, title='Local velocity-adapted motion events for spatio-temporal recognition', authors=['Ivan Laptev', 'Barbara Caputo', 'Christian Schüldt', 'Tony Lindeberg'], abstract='In this paper, we address the problem of motion recognition using event-based local motion representations. We assume that similar patterns of motion contain similar events with consistent motion across image sequences. Using this assumption, we formulate the problem of motion recognition as a matching of corresponding events in image sequences. To enable the matching, we present and evaluate a set of motion descriptors that exploit the spatial and the temporal coherence of motion measurements between corresponding events in image sequences. As the motion measurements may depend on the relative motion of the camera, we also present a mechanism for local velocity adaptation of events and evaluate its influence when recognizing image sequences subjected to different camera motions. When recognizing motion patterns, we compare the performance of a nearest neighbor (NN) classifier with the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1023/A:1007939232436', '10.1109/ICCV.2005.28', '10.1109/34.910878', '10.1109/ICCV.2005.70', '10.1109/72.788646', '10.1109/TPAMI.2003.1251155', '10.1006/cviu.1998.0716', '10.1109/ACVMOT.2005.42', '10.5244/C.2.23', '10.1023/A:1012460413855', '10.1109/34.141551', '10.1007/BF00318371', '10.1109/ICCV.2005.188', '10.1109/ICPR.2004.1334003', '10.1016/j.imavis.2003.07.002', '10.5244/C.17.78', '10.1109/ICPR.2004.1333965', '10.1109/ICPR.2004.1334004', '10.1016/S0262-8856(97)01144-X', '10.5244/C.16.36', '10.5244/C.20.127', '10.1023/A:1020350100748', '10.1023/A:1008120406972', '10.1109/34.589215', '10.1109/ICPR.2004.1334462', '10.5244/C.14.38', '10.1006/cviu.1998.0726', '10.1109/ICCV.2005.201'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Artificial intelligence', 'Computer Vision', 'intelligent Systems', 'Multi Modal Learning'], conference_acronym='Computer vision and image understanding (Print)', publisher=None, query_handler=None),\n",
       " 'Image matching using generalized scale-space interest points': Paper(DOI='10.1007/978-3-642-38267-3_30', crossref_json=None, google_schorlar_metadata=None, title='Image matching using generalized scale-space interest points', authors=['Tony Lindeberg'], abstract=' The performance of matching and object recognition methods based on interest points depends on both the properties of the underlying interest points and the choice of associated image descriptors. This paper demonstrates advantages of using generalized scale-space interest point detectors in this context for selecting a sparse set of points for computing image descriptors for image-based matching. For detecting interest points at any given scale, we make use of the Laplacian , the determinant of the Hessian  and four new unsigned or signed Hessian feature strength measures , ,  and , which are defined by generalizing the definitions of the Harris and Shi-and-Tomasi operators from the second moment matrix to the Hessian matrix. Then, feature selection over different scales is performed either by scale selection from local extrema over scale of scale-normalized derivates or by linking\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1023/B:VISI.0000029664.99615.94', '10.1007/BF00336961', '10.1109/34.141551', '10.1007/978-1-4757-6465-9', '10.1007/978-94-015-8845-4', '10.1007/978-1-4020-8840-7', '10.1002/9780470050118.ecse609', '10.5244/C.2.23', '10.1007/s10851-012-0378-3', '10.1007/BFb0017862', '10.1007/s11263-005-3848-x', '10.1023/B:VISI.0000027790.02288.f2'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Scale Space', 'Recognition', 'Image Analysis', 'Neuroscience'], conference_acronym='Lecture notes in computer science', publisher=None, query_handler=None),\n",
       " 'Scale-space': Paper(DOI='10.1007/s10851-010-0242-2', crossref_json=None, google_schorlar_metadata=None, title='Scale-space', authors=['Tony Lindeberg'], abstract='Scale-space theory is a framework for multiscale image representation, which has been developed by the computer vision community with complementary motivations from physics and biologic vision. The idea is to handle the multiscale nature of real-world objects, which implies that objects may be perceived in different ways depending on the scale of observation. If one aims to develop automatic algorithms for interpreting images of unknown scenes, there is no way to know a priori what scales are relevant. Hence, the only reasonable approach is to consider representations at all scales simultaneously. From axiomatic derivations is has been shown that given the requirement that coarse-scale representations should correspond to true simplifications of fine scale structures, convolution with Gaussian kernels and Gaussian derivatives is singled out as a canonical class of image operators for the earliest stages of visual processing. These image operators can be used as basis to solve a large variety of visual tasks, including feature detection, feature classification, stereo matching, motion descriptors, shape cues, and image-based recognition. By complementing scale-space representation with a module for automatic scale selection based on the maximization of normalized derivatives over scales, early visual modules can be made scale invariant. In this way, visual modules can adapt automatically to the unknown scale variations that may occur because of objects and substructures of varying physical size as well as objects with varying distances to the camera. An interesting similarity to biologic vision is that the scale-space operators resemble\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/j.amc.2003.07.038', '10.1109/83.887971', '10.1007/BF00375127', '10.1109/TPAMI.1986.4767749', '10.1023/A:1008337710072', '10.3390/e10030365', '10.1016/0146-664X(81)90092-7', '10.1016/0166-2236(95)94496-R', '10.1023/B:JMIV.0000024043.96722.aa', '10.1007/3-540-44935-3_34', '10.1016/0022-247X(72)90248-X', '10.1090/gsm/019', '10.1007/s11263-005-1837-8', '10.1007/978-3-540-72823-8_28', '10.1023/B:JMIV.0000026554.79537.35', '10.1109/34.368151', '10.1007/978-94-015-8845-4', '10.1016/0262-8856(92)90024-W', '10.1023/A:1007922215235', '10.1109/34.93808', '10.1007/BF00058750', '10.1109/83.661194', '10.1109/29.45555', '10.1007/BF00336961', '10.1007/BF00364135', '10.1007/BF00203452', '10.1109/34.141551', '10.1016/j.imavis.2003.07.002', '10.1016/j.cviu.2006.11.023', '10.1109/34.49051', '10.1007/978-1-4757-6465-9', '10.1080/757582976', '10.1007/BFb0017862', '10.1007/3-540-63167-4_44', '10.1007/3-540-47969-4_4', '10.1007/3-540-57956-7_42', '10.1016/S0262-8856(97)01144-X', '10.1109/ICPR.2004.1334004', '10.1023/B:VISI.0000027790.02288.f2', '10.1109/34.391411', '10.1016/0262-8856(92)90011-Q', '10.1090/S0002-9904-1953-09695-1', '10.1109/18.119725', '10.1007/978-1-4020-8840-7', '10.1023/B:VISI.0000020671.28016.e8', '10.1016/S0042-6989(00)00210-8', '10.1023/A:1008344623873', '10.1163/156856887X00222', '10.1163/156856801753253591', '10.1163/156856801753253582', '10.1109/TPAMI.1986.4767748', '10.1088/0305-4470/35/43/318'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Scale Space', 'Recognition', 'Image Analysis', 'Neuroscience'], conference_acronym='Journal of mathematical imaging and vision', publisher=None, query_handler=None),\n",
       " 'Fingerprint enhancement by shape adaptation of scale-space operators with automatic scale selection': Paper(DOI='10.1109/83.887971', crossref_json=None, google_schorlar_metadata=None, title='Fingerprint enhancement by shape adaptation of scale-space operators with automatic scale selection', authors=['Andrés Almansa', 'Tony Lindeberg'], abstract='This work presents two mechanisms for processing fingerprint images; shape-adapted smoothing based on second moment descriptors and automatic scale selection based on normalized derivatives. The shape adaptation procedure adapts the smoothing operation to the local ridge structures, which allows interrupted ridges to be joined without destroying essential singularities such as branching points and enforces continuity of their directional fields. The scale selection procedure estimates local ridge width and adapts the amount of smoothing to the local amount of noise. In addition, a ridgeness measure is defined, which reflects how well the local image structure agrees with a qualitative ridge model, and is used for spreading the results of shape adaptation into noisy areas. The combined approach makes it possible to resolve fine scale structures in clear areas while reducing the risk of enhancing noise in\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF00058750', '10.1109/ICCV.1993.378146', '10.1109/CVPR.1996.517113', '10.1007/BFb0017862', '10.1023/A:1008045108935', '10.1023/A:1008045108935', '10.1016/S0031-3203(99)00040-0', '10.6028/NIST.IR.5647', '10.1109/ICPR.1998.711197', '10.1109/34.85668', '10.1109/34.56194', '10.1007/3-540-57418-2', '10.1007/BF00336961', '10.1007/978-94-015-8802-7', '10.1016/S0262-8856(97)01144-X', '10.1016/0004-3702(84)90010-9', '10.1117/12.265384', '10.1049/el:19921093', '10.1109/ICCV.1995.466786', '10.1007/BF01469346', '10.1109/18.119727', '10.1109/34.464562', '10.1007/BF01211936', '10.1109/34.566808', '10.1117/12.139096', '10.1016/0031-3203(93)90006-I', '10.1016/0031-3203(89)90035-6', '10.1007/978-1-4757-6465-9', '10.1007/3-540-60268-2_301', '10.1109/34.531800', '10.1109/34.149593', '10.1109/ICASSP.1984.1172729', '10.1002/scj.4690170604', '10.1007/3-540-61577-6_30', '10.6028/NIST.IR.4892', '10.1109/ICCV.1999.790423', '10.1109/34.587996', '10.1117/12.25401', '10.1016/0031-3203(87)90078-1', '10.1109/TPAMI.1986.4767798', '10.1109/83.661195', '10.1007/978-94-017-1699-4', '10.1016/0031-3203(91)90095-M', '10.1109/34.709565', '10.1016/S0167-8655(97)00103-7'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Scale Space', 'Recognition', 'Image Analysis', 'Neuroscience'], conference_acronym='IEEE transactions on image processing', publisher=None, query_handler=None),\n",
       " 'Direct computation of shape cues using scale-adapted spatial derivative operators': Paper(DOI='10.1007/bf00058750', crossref_json=None, google_schorlar_metadata=None, title='Direct computation of shape cues using scale-adapted spatial derivative operators', authors=['Jonas Gårding', 'Tony Lindeberg'], abstract=' This paper addresses the problem of computing cues to the three-dimensional structure of surfaces in the world directly from the local structure of the brightness pattern of either a single monocular image or a binocular image pair. It is shown that starting from Gaussian derivatives of order up to two at a range of scales in scale-space, local estimates of (i) surface orientation from monocular texture foreshortening, (ii) surface orientation from monocular texture gradients, and (iii) surface orientation from the binocular disparity gradient can be computed without iteration or search, and by using essentially the same basic mechanism. The methodology is based on a multi-scale descriptor of image structure called the windowed second moment matrix, which is computed with adaptive selection of both scale levels and spatial positions. Notably, this descriptor comprises two scale parameters; a local\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/BF00363944', '10.1109/TPAMI.1986.4767749', '10.1038/333363a0', '10.1109/34.85668', '10.1016/0042-6989(93)90162-P', '10.1016/0004-3702(90)90011-N', '10.1016/0042-6989(70)90036-2', '10.1016/0734-189X(89)90068-6', '10.1109/34.41363', '10.1109/34.56194', '10.1163/156856885X00044', '10.1007/3-540-55426-2_20', '10.1109/TPAMI.1983.4767427', '10.1364/JOSAA.4.002379', '10.1016/0262-8856(92)90024-W', '10.1007/BF00121877', '10.1016/0004-3702(93)90106-L', '10.1007/3-540-57956-7_40', '10.1007/3-540-55426-2_71', '10.1152/jn.1987.58.6.1233', '10.1152/jn.1987.58.6.1187', '10.1038/290091a0', '10.1016/0004-3702(84)90010-9', '10.1016/0004-3702(89)90066-0', '10.1007/BF00336961', '10.1007/BF00326670', '10.1007/BF00203452', '10.1109/34.141551', '10.1109/34.49051', '10.1007/BF01469346', '10.1007/BF01664794', '10.1007/978-1-4757-6465-9', '10.1007/3-540-57956-7_42', '10.1364/JOSAA.7.000923', '10.1098/rstb.1976.0090', '10.1016/0004-3702(86)90017-2', '10.1068/p140449', '10.1007/BF00341922', '10.1016/0042-6989(79)90019-1', '10.1109/34.85667', '10.1016/0004-3702(81)90019-9', '10.1163/156856887X00222', '10.1109/TPAMI.1986.4767748'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Scale Space', 'Recognition', 'Image Analysis', 'Neuroscience'], conference_acronym='International journal of computer vision', publisher=None, query_handler=None),\n",
       " 'Audio-visual integration in multimodal communication': Paper(DOI='10.1109/5.664274', crossref_json=None, google_schorlar_metadata=None, title='Audio-visual integration in multimodal communication', authors=['Tsuhan Chen', 'Ram R Rao'], abstract='We review recent research that examines audio-visual integration in multimodal communication. The topics include bimodality in human speech, human and automated lip reading, facial animation, lip synchronization, joint audio-video coding, and bimodal speaker verification. We also study the enabling technologies for these research topics, including automatic facial-feature tracking and audio-to-visual mapping. Recent progress in audio-visual research shows that joint processing of audio and video provides advantages that are not available when the audio and video are processed independently.', conference=None, journal=None, year=None, reference_list=['10.1109/ICASSP.1997.595320', '10.1109/ICASSP.1997.595337', '10.1109/ICASSP.1989.266799', '10.1109/ICSLP.1996.607024', '10.1109/IJCNN.1992.226994', '10.1109/35.41402', '10.1109/ICASSP.1995.479285', '10.1109/49.363147', '10.1109/89.260341', '10.1109/ICIP.1994.413328', '10.1109/ICIP.1995.537695', '10.1109/ICIP.1994.413882', '10.1002/scj.4690220607', '10.1109/83.605417', '10.1145/57167.57170', '10.1109/ICASSP.1997.599592', '10.1016/0167-8655(88)90094-3', '10.1109/ICSLP.1996.607030', '10.1109/79.598590', '10.3758/BF03204211', '10.1038/264746a0', '10.1109/49.81956', '10.1044/jshr.2504.600', '10.1109/MMSP.1997.602606', '10.1007/978-3-662-13015-5_6', '10.1109/MASSP.1985.1163757', '10.1007/BF00127169', '10.1049/el:19941110', '10.1109/ICASSP.1994.389567', '10.1007/BF00133570', '10.1109/97.376913', '10.1044/jshr.1104.796', '10.1044/jshr.2803.381', '10.1044/jshr.2001.130', '10.1098/rstb.1992.0009', '10.1121/1.1907309', '10.1121/1.1908620', '10.1007/978-3-662-13015-5_9', '10.3758/BF03211629', '10.1007/978-3-662-13015-5_5', '10.1007/978-3-662-13015-5_7', '10.1109/MCG.1982.1674492', '10.1145/258734.258880', '10.1109/86.372898', '10.1007/3-540-60697-1_158', '10.1109/76.633499'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Processing', 'Pattern Recognition', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Proceedings of the IEEE', publisher=None, query_handler=None),\n",
       " 'Aesthetic visual quality assessment of paintings': Paper(DOI='10.1109/jstsp.2009.2015077', crossref_json=None, google_schorlar_metadata=None, title='Aesthetic visual quality assessment of paintings', authors=['Congcong Li', 'Tsuhan Chen'], abstract=\"This paper aims to evaluate the aesthetic visual quality of a special type of visual media: digital images of paintings. Assessing the aesthetic visual quality of paintings can be considered a highly subjective task. However, to some extent, certain paintings are believed, by consensus, to have higher aesthetic quality than others. In this paper, we treat this challenge as a machine learning problem, in order to evaluate the aesthetic quality of paintings based on their visual content. We design a group of methods to extract features to represent both the global characteristics and local characteristics of a painting. Inspiration for these features comes from our prior knowledge in art and a questionnaire survey we conducted to study factors that affect human's judgments. We collect painting images and ask human subjects to score them. These paintings are then used for both training and testing in our experiments\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1145/245108.245121', '10.1109/FUZZ.2002.1005020', '10.1145/1179352.1141933', '10.1109/34.969114', '10.1109/TPAMI.2004.1262177', '10.1109/TPAMI.2004.60', '10.1109/MSP.2008.923513', '10.1109/MMUL.2007.31', '10.1145/1291233.1291364', '10.1109/MMUL.2007.6', '10.1109/MMMC.2005.52', '10.1109/ICIP.2002.1038064', '10.1109/TIP.2003.821349', '10.1016/j.patrec.2006.08.002', '10.1109/MMUL.2006.78', '10.1109/MMUL.2006.50'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Processing', 'Pattern Recognition', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='IEEE journal of selected topics in signal processing', publisher=None, query_handler=None),\n",
       " 'A survey on image-based rendering—representation, sampling and compression': Paper(DOI='10.1016/j.image.2003.07.001', crossref_json=None, google_schorlar_metadata=None, title='A survey on image-based rendering—representation, sampling and compression', authors=['Cha Zhang', 'Tsuhan Chen'], abstract='Image-based rendering (IBR) has attracted a lot of research interest recently. In this paper, we survey the various techniques developed for IBR, including representation, sampling and compression. The goal is to provide an overview of research for IBR in a complete and systematic manner. We observe that essentially all the IBR representations are derived from the plenoptic function, which is seven dimensional and difficult to handle. We classify various IBR representations into two categories based on how the plenoptic function is simplified, namely restraining the viewing space and introducing source descriptions. In the former category, we summarize six common assumptions that were often made in various approaches and discuss how the dimension of the plenoptic function can be reduced based on these assumptions. In the latter category, we further categorize the methods based on what kind of source\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1007/978-1-4471-0103-1_32', '10.1145/383259.383311', '10.1109/CVPR.1997.609457', '10.1145/142920.134003', '10.1145/360349.360353', '10.1145/383259.383309', '10.1145/311535.311553', '10.1007/978-3-7091-6453-2_11', '10.1145/344779.344932', '10.1109/ICIP.2000.899215', '10.1145/311535.311571', '10.1109/ICIP.2003.1247074', '10.1145/218380.218395', '10.1145/166117.166153', '10.1145/566654.566601', '10.1145/280814.280864', '10.1145/344779.344855', '10.1145/237170.237191', '10.1145/566654.566614', '10.1007/978-3-7091-6453-2_10', '10.1111/1467-8659.00447', '10.1145/354384.376401', '10.1109/ICASSP.2003.1202754', '10.1145/237170.237200', '10.1109/MCG.1986.276738', '10.1145/192161.192173', '10.1007/978-3-7091-6858-5_1', '10.1109/34.615446', '10.1137/1012001', '10.1145/344779.344929', '10.1117/12.205854', '10.1109/CVPR.2001.990926', '10.1145/500141.500192', '10.1109/ICIP.2001.958248', '10.1109/ICPR.1994.576404', '10.1109/34.273735', '10.1002/(SICI)1099-1778(199601)7:1<3::AID-VIS131>3.0.CO;2-U', '10.1109/2.689676', '10.1145/237170.237199', '10.1109/CVPR.1999.784621', '10.1142/S0219467801000050', '10.1023/A:1020153824351', '10.1145/800250.807465', '10.1007/978-3-7091-6453-2_28', '10.1145/37401.37422', '10.1109/ICIP.2000.899866', '10.1109/ICIP.1999.817130', '10.1109/ICIP.1999.817130', '10.1109/76.836278', '10.1109/ICIP.2000.899866', '10.1145/253284.253292', '10.1145/344779.344951', '10.1145/566570.566599', '10.1145/218380.218398', '10.1109/T-C.1975.224142', '10.1002/vis.4340030305', '10.1007/978-3-7091-6453-2_26', '10.1109/CVPR.1997.609369', '10.1145/354384.376408', '10.1109/83.334985', '10.1145/235160.235164', '10.1109/CVPR.1999.786969', '10.1109/CVPR.1997.609346', '10.1109/34.879794', '10.1145/311535.311612', '10.1145/280814.280871', '10.1109/ICIP.2003.1247235', '10.1109/ICIP.2002.1039928', '10.1145/65445.65448', '10.1109/34.754589', '10.1111/1467-8659.00336', '10.1109/ICIP.2002.1039928', '10.1109/CVPR.1997.609462', '10.1109/WVRS.1995.476848', '10.1145/237170.237196', '10.1145/280814.280882', '10.1145/311535.311573', '10.1023/A:1020398016678', '10.1145/253284.253296', '10.1109/ACV.1994.341287', '10.1145/258734.258861', '10.1007/978-1-4615-0799-4', '10.1109/83.334984', '10.1109/CVPR.2001.990565', '10.1109/49.650917', '10.1109/CVPR.2001.991005', '10.1109/ICCV.1995.466831', '10.1109/ICPR.1996.545994', '10.1117/12.451074', '10.1145/344779.344925', '10.1145/258734.258859', '10.1109/TMM.2002.802835', '10.1117/12.386654', '10.1109/TMM.2002.802838', '10.1109/ICME.2003.1220897', '10.1117/12.510135', '10.1109/DCC.2000.838165', '10.1117/12.386665', '10.1145/566570.566602', '10.1007/3-540-63931-4_235', '10.1109/ICPR.1990.118082'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Processing', 'Pattern Recognition', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='Signal processing. Image communication', publisher=None, query_handler=None),\n",
       " 'An active learning framework for content-based information retrieval': Paper(DOI='10.1109/tmm.2002.1017738', crossref_json=None, google_schorlar_metadata=None, title='An active learning framework for content-based information retrieval', authors=['Cha Zhang', 'Tsuhan Chen'], abstract='We propose a general active learning framework for content-based information retrieval. We use this framework to guide hidden annotations in order to improve the retrieval performance. For each object in the database, we maintain a list of probabilities, each indicating the probability of this object having one of the attributes. During training, the learning algorithm samples objects in the database and presents them to the annotator to assign attributes. For each sampled object, each probability is set to be one or zero depending on whether or not the corresponding attribute is assigned by the annotator. For objects that have not been annotated, the learning algorithm estimates their probabilities with biased kernel regression. Knowledge gain is then defined to determine, among the objects that have not been annotated, which one the system is the most uncertain. The system then presents it as the next sample to the\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1002/0471200611', '10.1214/aos/1176348768', '10.1109/CVPR.1996.517107', '10.1109/IM.1997.603886', '10.1145/130385.130417', '10.1007/978-1-4471-2099-5_1', '10.1109/76.718510', '10.1007/978-1-4899-3324-9', '10.1109/ICME.2000.871576', '10.1109/CVPR.2000.855825', '10.1109/83.817596', '10.1145/133160.133167', '10.1145/219587.219592', '10.1613/jair.295', '10.1016/B978-044450270-4/50005-X', '10.1145/602259.602266'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Image Processing', 'Pattern Recognition', 'Computer Vision', 'Machine Learning', 'Artificial Intelligence'], conference_acronym='IEEE transactions on multimedia', publisher=None, query_handler=None),\n",
       " 'A multichannel approach to fingerprint classification': Paper(DOI='10.1109/34.761265', crossref_json=None, google_schorlar_metadata=None, title='A multichannel approach to fingerprint classification', authors=['Anil K Jain', 'Salil Prabhakar', 'Lin Hong'], abstract='Fingerprint classification provides an important indexing mechanism in a fingerprint database. An accurate and consistent classification can greatly reduce fingerprint matching time for a large database. We present a fingerprint classification algorithm which is able to achieve an accuracy better than previously reported in the literature. We classify fingerprints into five categories: whorl, right loop, left loop, arch, and tented arch. The algorithm uses a novel representation (FingerCode) and is based on a two-stage classifier to make a classification. It has been tested on 4000 images in the NIST-4 database. For the five-class problem, a classification accuracy of 90 percent is achieved (with a 1.8 percent rejection during the feature extraction phase). For the four-class problem (arch and tented arch combined into one class), we are able to achieve a classification accuracy of 94.8 percent (with 1.8 percent rejection). By\\xa0…', conference=None, journal=None, year=None, reference_list=['10.1016/0031-3203(95)00106-9', '10.1109/5.628674', '10.1109/34.588027', '10.1016/0031-3203(95)00039-3', '10.1109/34.531800', '10.1109/TPAMI.1980.4767009', '10.1016/0031-3203(96)00018-0', '10.1016/0031-3203(93)90006-I', '10.1016/0031-3203(84)90079-7', '10.1364/JOSAA.2.001160', '10.1016/S0031-3203(96)00178-1', '10.1016/S0169-7161(82)02042-2', '10.1109/34.587996', '10.1016/0042-6989(80)90065-6', '10.1109/34.244676', '10.1109/ACSSC.1997.680212', '10.1016/0031-3203(91)90143-S'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Computer Vision', 'Image Processing', 'Machine Learning', 'Medical Imaging'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " 'Decision-level fusion in fingerprint verification': Paper(DOI='10.1016/s0031-3203(01)00103-0', crossref_json=None, google_schorlar_metadata=None, title='Decision-level fusion in fingerprint verification', authors=['Salil Prabhakar', 'Anil K Jain'], abstract='A scheme is proposed for classifier combination at decision level which stresses the importance of classifier selection during combination. The proposed scheme is optimal (in the Neyman–Pearson sense) when sufficient data are available to obtain reasonable estimates of the join densities of classifier outputs. Four different fingerprint matching algorithms are combined using the proposed scheme to improve the accuracy of a fingerprint verification system. Experiments conducted on a large fingerprint database (∼2700 fingerprints) confirm the effectiveness of the proposed integration scheme. An overall matching performance increase of ∼3% is achieved. We further show that a combination of multiple impressions or multiple fingers improves the verification performance by more than 4% and 5%, respectively. Analysis of the results provide some insight into the various decision-level classifier combination strategies.', conference=None, journal=None, year=None, reference_list=['10.1007/BFb0016008', '10.1109/34.667881', '10.1109/34.273716', '10.1109/ICPR.1998.711174', '10.1109/ICPR.1994.576970', '10.1016/0167-8655(95)00050-Q', '10.1007/3-540-45014-9_34', '10.1109/34.824819', '10.1016/S0167-8655(99)00108-7', '10.1093/biomet/54.3-4.668', '10.1109/TIT.1971.1054685', '10.1109/TSMC.1974.5408535', '10.1109/TSMC.1977.4309803', '10.1016/S0169-7161(82)02042-2', '10.1109/34.75512', '10.1109/34.799913', '10.1109/5.628674', '10.1109/34.531800', '10.1109/83.841531', '10.1109/ICPR.2000.906041', '10.1049/ic:20010105', '10.1109/21.155943', '10.1109/34.761265', '10.1109/34.574797'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Machine Learning', 'Image processing', 'Pattern Recognition', 'Computer Vision'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'On the similarity of identical twin fingerprints': Paper(DOI='10.1016/s0031-3203(01)00218-7', crossref_json=None, google_schorlar_metadata=None, title='On the similarity of identical twin fingerprints', authors=['Anil K Jain', 'Salil Prabhakar', 'Sharath Pankanti'], abstract='Reliable and accurate verification of people is extremely important in a number of business transactions as well as access to privileged information. Automatic verification methods based on physical biometric characteristics such as fingerprint or iris can provide positive verification with a very high accuracy. However, the biometrics-based methods assume that the physical characteristics of an individual (as captured by a sensor) used for verification are sufficiently unique to distinguish one person from another. Identical twins have the closest genetics-based relationship and, therefore, the maximum similarity between fingerprints is expected to be found among identical twins. We show that a state-of-the-art automatic fingerprint verification system can successfully distinguish identical twins though with a slightly lower accuracy than nontwins.', conference=None, journal=None, year=None, reference_list=['10.1109/5.628674', '10.1109/5.628710', '10.1007/BF01067663', '10.1109/83.841531'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Machine Learning', 'Image processing', 'Pattern Recognition', 'Computer Vision'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'Combining multiple matchers for a high security fingerprint verification system': Paper(DOI='10.1016/s0167-8655(99)00108-7', crossref_json=None, google_schorlar_metadata=None, title='Combining multiple matchers for a high security fingerprint verification system', authors=['Anil K Jain', 'Salil Prabhakar', 'Shaoyun Chen'], abstract='Integration of various fingerprint matching algorithms is a viable method to improve the performance of a fingerprint verification system. Different fingerprint matching algorithms are often based on different representations of the input fingerprints and hence complement each other. We use the logistic transform to integrate the output scores from three different fingerprint matching algorithms. Experiments conducted on a large fingerprint database confirm the effectiveness of the proposed integration scheme.', conference=None, journal=None, year=None, reference_list=['10.1007/BFb0016008', '10.1109/34.273716', '10.1109/34.587996', '10.1109/CVPR.1999.784628', '10.1109/34.667881', '10.1109/ICPR.1994.576970', '10.1016/0167-8655(95)00050-Q', '10.1109/34.531800', '10.1109/ICPR.1998.711174', '10.1145/359863.359882', '10.1109/21.155943'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Machine Learning', 'Image processing', 'Pattern Recognition', 'Computer Vision'], conference_acronym='Pattern recognition letters', publisher=None, query_handler=None),\n",
       " 'Learning fingerprint minutiae location and type': Paper(DOI='10.1016/s0031-3203(02)00322-9', crossref_json=None, google_schorlar_metadata=None, title='Learning fingerprint minutiae location and type', authors=['Salil Prabhakar', 'Anil K Jain', 'Sharath Pankanti'], abstract=\"For simplicity of pattern recognition system design, a sequential approach consisting of sensing, feature extraction and classification/matching is conventionally adopted, where each stage transforms its input relatively independently. In practice, the interaction between these modules is limited. Some of the errors in this end-to-end sequential processing can be eliminated, especially for the feature extraction stage, by revisiting the input pattern. We propose a feedforward of the original grayscale image data to a feature (minutiae) verification stage in the context of a minutiae-based fingerprint verification system. This minutiae verification stage is based on reexamining the grayscale profile in a detected minutia's spatial neighborhood in the sensed image. We also show that a feature refinement (minutiae classification) stage that assigns one of two class labels to each detected minutia (ridge ending and ridge\\xa0…\", conference=None, journal=None, year=None, reference_list=['10.1109/5.628674', '10.1109/34.566808', '10.1016/0031-3203(87)90078-1', '10.1109/TPAMI.1986.4767798', '10.1016/0031-3203(89)90035-6', '10.1109/34.709565', '10.1016/0031-3203(95)00039-3', '10.1016/0031-3203(93)90021-N', '10.1016/0031-3203(91)90095-M', '10.1016/S0031-3203(98)00107-1', '10.1109/ICPR.2000.906207', '10.1109/CVPR.2001.991016', '10.1109/ICPR.1998.712036', '10.1109/34.476511', '10.1002/ajpa.1330770309', '10.1109/83.841531'], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Machine Learning', 'Image processing', 'Pattern Recognition', 'Computer Vision'], conference_acronym='Pattern recognition', publisher=None, query_handler=None),\n",
       " 'Introduction to the special issue on biometrics: Progress and directions': Paper(DOI='10.1109/tpami.2007.1025', crossref_json=None, google_schorlar_metadata=None, title='Introduction to the special issue on biometrics: Progress and directions', authors=['Salil Prabhakar', 'Josef Kittler', 'Davide Maltoni', \"Lawrence O'Gorman\", 'Tieniu Tan'], abstract=\"The guest editors provide an overview of the articles selected for this special issue. The issue's goal is to document the current state-of-the-art, acknowledge the latest breakthroughs achieved by scientists working in the area of biometric recognition, and identify future promising research areas. It is thought the selection of papers discussed should give readers a good idea of where researchers have been focusing, both on long- studied problems still needing more work and on newer challenges. A fundamental of the field of biometrics is an ever-increasing need for better recognition and stronger security. But, as public and commercial biometric deployments increase in number, there is also more need to understand privacy issues and to provide greater ease-of-use. The volume and quality of papers in this special issue indicate that much progress has been made in many aspects of the biometrics field and that\\xa0…\", conference=None, journal=None, year=None, reference_list=[], referenced_list=None, cite_bibtex=None, issn_type=None, url=None, is_in_favorite=False, keywords=['Biometrics', 'Machine Learning', 'Image processing', 'Pattern Recognition', 'Computer Vision'], conference_acronym='IEEE transactions on pattern analysis and machine intelligence', publisher=None, query_handler=None),\n",
       " ...}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
